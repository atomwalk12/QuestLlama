{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules.\n",
    "import uuid\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "import os\n",
    "import faiss\n",
    "import cloudpickle\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableMap    \n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Set the path for the database directory.\n",
    "DB_PATH=\"./db/\"\n",
    "\n",
    "# Function to load or create a vector database.\n",
    "# This database will be used for storing and retrieving document embeddings.\n",
    "def load_vector_db(DB_PATH=\"./db/\"):\n",
    "    # Initialize variables for the components of the database.\n",
    "    db = None\n",
    "    memoryDocStoreDict = {}\n",
    "    indexToDocStoreIdDict = {}\n",
    "    \n",
    "    # Check if the database already exists. If it does, load its components.\n",
    "    if os.path.exists(DB_PATH):\n",
    "        memoryDocStoreDict = cloudpickle.load(open(DB_PATH+\"memoryDocStoreDict.pkl\", \"rb\"))\n",
    "        indexToDocStoreIdDict = cloudpickle.load(open(DB_PATH+\"indexToDocStoreIdDict.pkl\", \"rb\"))\n",
    "        index = faiss.read_index(DB_PATH+\"faiss.index\")\n",
    "    else:\n",
    "        # If the database does not exist, create a new FAISS index.\n",
    "        index = faiss.IndexFlatL2(384)\n",
    "\n",
    "    # Create the FAISS vector database with the loaded or new components.\n",
    "    db = FAISS(\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(memoryDocStoreDict),\n",
    "        index_to_docstore_id=indexToDocStoreIdDict,\n",
    "        embedding_function=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cuda:0'})\n",
    "    )\n",
    "    return db\n",
    "\n",
    "# Function to populate the vector database with documents.\n",
    "# It processes each file in the 'wiki/' directory, splits the content into smaller chunks,\n",
    "# and stores these chunks along with their metadata in the database.\n",
    "def populate_vector_db(DB_PATH=\"./db/\"):\n",
    "    db = load_vector_db(DB_PATH=DB_PATH)\n",
    "\n",
    "    # Process each file in the 'wiki/' directory.\n",
    "    for wiki_file in os.listdir(\"wiki/\"):\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        \n",
    "        wiki_file_path  = \"wiki/\"+wiki_file\n",
    "        wiki_chunks_dir = \"wiki_chunks/\"+wiki_file\n",
    "        os.makedirs(wiki_chunks_dir, exist_ok=True)\n",
    "       \n",
    "        # Read the content of the file.\n",
    "        content = open(wiki_file_path, \"r\").read()\n",
    "        # Split the content into smaller chunks for better manageability.\n",
    "        for chunk in TokenTextSplitter(chunk_size=256).split_text(content):\n",
    "            random_uuid = str(uuid.uuid4())\n",
    "            texts.append(chunk)\n",
    "            \n",
    "            wiki_chunk_file_path = wiki_chunks_dir+\"/\"+random_uuid+\".txt\"\n",
    "            open(wiki_chunk_file_path, \"w\").write(chunk)\n",
    "            metadatas.append({\n",
    "                'wiki_file_path': wiki_file_path,\n",
    "                'wiki_chunk_file_path': wiki_chunk_file_path\n",
    "            })\n",
    "\n",
    "        # Add the text chunks and their metadata to the database.\n",
    "        db.add_texts(texts, metadatas)\n",
    "        \n",
    "    # Save the components of the database if the directory does not exist.\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        os.makedirs(DB_PATH)\n",
    "    \n",
    "    cloudpickle.dump(db.docstore._dict, open(DB_PATH+\"memoryDocStoreDict.pkl\", \"wb\"))\n",
    "    cloudpickle.dump(db.index_to_docstore_id, open(DB_PATH+\"indexToDocStoreIdDict.pkl\", \"wb\"))\n",
    "    faiss.write_index(db.index, DB_PATH+\"faiss.index\")\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Function to configure and retrieve a large language model from Hugging Face.\n",
    "def get_llm():\n",
    "    # Define the model name and retrieve the necessary token for authentication.\n",
    "    model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    token = \"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\"\n",
    "\n",
    "    # Configure the model for quantization to reduce memory usage.\n",
    "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    device_map = {\"\": 0}\n",
    "\n",
    "    # Load the model and tokenizer from Hugging Face with the specified configurations.\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        use_auth_token=token\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "\n",
    "    # Create a pipeline for text generation using the loaded model and tokenizer.\n",
    "    llama_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    llm = HuggingFacePipeline(pipeline=llama_pipeline, model_kwargs={'temperature':0.})\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# Function to format a list of documents into a single string.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Function to ask a question and receive an answer using the large language model and the document database.\n",
    "def ask(q):\n",
    "    # Define a template for the prompt to be used with the large language model.\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "    Use three sentences maximum and keep the answer as concise as possible. \n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    rag_prompt_custom = PromptTemplate.from_template(template)\n",
    "\n",
    "    llm = get_llm()\n",
    "\n",
    "    # Create a chain of operations to process the question.\n",
    "    rag_chain_from_docs = (\n",
    "        {\n",
    "            \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "        }\n",
    "        | rag_prompt_custom\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    rag_chain_with_source = RunnableMap(\n",
    "        {\"documents\": db.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    ) | {\n",
    "        \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "        \"answer\": rag_chain_from_docs,\n",
    "    }\n",
    "\n",
    "    # Invoke the chain of operations with the question.\n",
    "    response = rag_chain_with_source.invoke(q)\n",
    "    print(\"=================================\")\n",
    "    print(response[\"answer\"])\n",
    "    print(\"=================================\")\n",
    "    for doc in response[\"documents\"]:\n",
    "        print(doc['wiki_chunk_file_path'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d771bcb94447e1ac07b82c75e95336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set the path for the database directory.\n",
    "DB_PATH=\"./db/\"\n",
    "\n",
    "# Main execution block: populate and load the vector database, then use it to answer a sample question.\n",
    "db = populate_vector_db(DB_PATH=DB_PATH)\n",
    "db = load_vector_db(DB_PATH=DB_PATH)\n",
    "ask(\"Kill 1 zombie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
