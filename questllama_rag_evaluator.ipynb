{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration and Imports\n",
    "\n",
    "NOTE: Execute the javascript code to check for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import glob\n",
    "import json\n",
    "import datasets\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "import random\n",
    "from _voyager.control_primitives_context import load_control_primitives_context\n",
    "from _voyager.prompts import load_prompt\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "import re\n",
    "from langchain_community.chat_models.openai import ChatOpenAI\n",
    "from shared import file_utils as U\n",
    "import plotly.express as px\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea23ae32208c4dfe945e36d10f4dcbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_KNOWLEDGE_DATABASE: 311\n",
      "Evaluation dataset prior to removing duplicates: 286\n",
      "Evaluation dataset after to removing duplicates: 189\n"
     ]
    }
   ],
   "source": [
    "def unfold_camel(camel_str):\n",
    "    # Insert a space before each uppercase letter and convert the entire string to lowercase\n",
    "    return re.sub(r'(?<!^)(?=[A-Z])', ' ', camel_str).lower()\n",
    "\n",
    "\n",
    "def get_question(task):\n",
    "    question = (\n",
    "        f\"How to {unfold_camel(task.replace('_', ' ').replace(' ore', '').replace(' ores', '').replace('.', '').strip())}\"\n",
    "        f\" in Minecraft?\"\n",
    "    )\n",
    "    return question\n",
    "\n",
    "def remove_duplicates(dataset):\n",
    "    result = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    for item in dataset:\n",
    "        if item['question'] not in seen_ids:\n",
    "            result.append(item)\n",
    "            seen_ids.add(item['question'])\n",
    "    return result\n",
    "\n",
    "def read_json_files_recursively(root_dir):\n",
    "    all_data = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.json'):\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                with open(filepath, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    all_data.append((filepath, data))\n",
    "    return all_data\n",
    "\n",
    "json_files = read_json_files_recursively(\"questllama/skill_library\")\n",
    "eval_dataset = []\n",
    "for i, trial in enumerate(json_files):\n",
    "    for program_name, info in trial[1].items(): # info contains code and scription of a program\n",
    "        question = get_question(program_name)\n",
    "        eval_dataset.append({'question': question, 'answer': info['code'], 'description': info['description'], 'source_doc': f'{\"/\".join(trial[0].split(\"/\")[:-1])}/code/{program_name}.js'})\n",
    "\n",
    "\n",
    "## Read the dataset\n",
    "ds = U.read_skill_library(\"skill_library\", full_path=True)\n",
    "ds = [ {'text': elem[1], 'source': elem[0] } for elem in ds]\n",
    "RAW_KNOWLEDGE_BASE = [Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "print(f\"RAW_KNOWLEDGE_DATABASE: {len(RAW_KNOWLEDGE_BASE)}\")\n",
    "print(f\"Evaluation dataset prior to removing duplicates: {len(eval_dataset)}\")\n",
    "eval_dataset = remove_duplicates(eval_dataset)\n",
    "print(f\"Evaluation dataset after to removing duplicates: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[Document],\n",
    "    tokenizer_name: Optional[str],\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    JAVASCRIPT_SEPARATORS = [\n",
    "        \"\\nfunction \",\n",
    "        \"\\nconst \",\n",
    "        \"\\nlet \",\n",
    "        \"\\nvar \",\n",
    "        \"\\nclass \",\n",
    "        \"\\nif \",\n",
    "        \"\\nfor \",\n",
    "        \"\\nwhile \",\n",
    "        \"\\nswitch \",\n",
    "        \"\\ncase \",\n",
    "        \"\\ndefault \",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "    print(f'Tokenizer: {tokenizer_name}, Chunk Size: {chunk_size}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # FIXME: Is it necessary to pass the separators here? Try without them.\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=JAVASCRIPT_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[Document],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str]\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "\n",
    "        docs_processed = _split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare system and user messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_user_messages():\n",
    "    # Load the 'database' file\n",
    "    answers = json.load(open(\"community/voyager5/openai_requests.json\", \"r\"))\n",
    "\n",
    "    # Get only user messages contained in action events\n",
    "    user_messages = [msg['prompts'][1]['message'] for msg in answers if msg['prompts'][1]['message'].startswith('Code from the last round:')]\n",
    "\n",
    "    # Use regular expression to replace the task description regardless of what it is\n",
    "    # user_messages = [ re.sub(r\"(Task:).*\", \"Task: {question}\", msg) for msg in user_messages ]\n",
    "    unique = set(user_messages)\n",
    "    return list(unique)\n",
    "\n",
    "def render_system_message(skills=[]):\n",
    "    system_template = load_prompt(\"action_template_rag\")\n",
    "    # FIXME: Hardcoded control_primitives\n",
    "    base_skills = [\n",
    "        \"exploreUntil\",\n",
    "        \"mineBlock\",\n",
    "        \"craftItem\",\n",
    "        \"placeItem\",\n",
    "        \"smeltItem\",\n",
    "        \"killMob\",\n",
    "    ]\n",
    "    if True: # TODO Razvan: always import these files since questllama's model context size is enough\n",
    "        base_skills += [\n",
    "            \"useChest\",\n",
    "            \"mineflayer\",\n",
    "        ]\n",
    "    programs = \"\\n\\n\".join(load_control_primitives_context(base_skills) + skills)\n",
    "    response_format = load_prompt(\"action_response_format_rag\")\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "        system_template\n",
    "    )\n",
    "    system_message = system_message_prompt.format(\n",
    "        programs=programs, response_format=response_format, context=\"\"\n",
    "    )\n",
    "\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unexpected '{' in field name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# This is the template\u001b[39;00m\n\u001b[1;32m     16\u001b[0m RAG_PROMPT_TEMPLATE \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     17\u001b[0m     [\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mSystemMessagePromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     19\u001b[0m         HumanMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(default_config)\n\u001b[1;32m     20\u001b[0m     ]\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#RAG_PROMPT_TEMPLATE.format(context=\"test\", question=\"Mine 1 wood log\")\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# For debugging purposes.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# TEMP = RAG_PROMPT_TEMPLATE.format(context=\"here should files\", question='Mine 100 wood log')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(TEMP)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/questllama/lib/python3.9/site-packages/langchain_core/prompts/chat.py:423\u001b[0m, in \u001b[0;36m_StringImageMessagePromptTemplate.from_template\u001b[0;34m(cls, template, template_format, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a class from a string template.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    A new instance of this class.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(template, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 423\u001b[0m     prompt: Union[StringPromptTemplate, List] \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(prompt\u001b[38;5;241m=\u001b[39mprompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(template, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/questllama/lib/python3.9/site-packages/langchain_core/prompts/prompt.py:244\u001b[0m, in \u001b[0;36mPromptTemplate.from_template\u001b[0;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptTemplate:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    *Security warning*: Prefer using `template_format=\"f-string\"` instead of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     _partial_variables \u001b[38;5;241m=\u001b[39m partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _partial_variables:\n",
      "File \u001b[0;32m~/anaconda3/envs/questllama/lib/python3.9/site-packages/langchain_core/prompts/string.py:209\u001b[0m, in \u001b[0;36mget_template_variables\u001b[0;34m(template, template_format)\u001b[0m\n\u001b[1;32m    207\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m _get_jinja2_variables_from_template(template)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 209\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    210\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m _, v, _, _ \u001b[38;5;129;01min\u001b[39;00m Formatter()\u001b[38;5;241m.\u001b[39mparse(template) \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     }\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmustache\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    213\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m mustache_template_vars(template)\n",
      "File \u001b[0;32m~/anaconda3/envs/questllama/lib/python3.9/site-packages/langchain_core/prompts/string.py:209\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m _get_jinja2_variables_from_template(template)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 209\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    210\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m _, v, _, _ \u001b[38;5;129;01min\u001b[39;00m Formatter()\u001b[38;5;241m.\u001b[39mparse(template) \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     }\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmustache\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    213\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m mustache_template_vars(template)\n",
      "\u001b[0;31mValueError\u001b[0m: unexpected '{' in field name"
     ]
    }
   ],
   "source": [
    "# Read the system message, this one is unique\n",
    "system_message = render_system_message()\n",
    "\n",
    "# Here are sample user messages, containing the state of the Minecraft world.\n",
    "user_messages = read_user_messages()\n",
    "random.seed(422)\n",
    "\n",
    "# Below are some further examples (w/o context) to augment the size of the training set.\n",
    "task_type = 'action'\n",
    "default_config = U.debug_load_prompt(f\"/debugging/{task_type}/user_rag.txt\")\n",
    "\n",
    "# TODO Razvan add new test samples: user_messages = [default_config] + user_messages\n",
    "\n",
    "random.seed(42)\n",
    "# This is the template\n",
    "RAG_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_message.content),\n",
    "        HumanMessagePromptTemplate.from_template(default_config)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#RAG_PROMPT_TEMPLATE.format(context=\"test\", question=\"Mine 1 wood log\")\n",
    "# For debugging purposes.\n",
    "# TEMP = RAG_PROMPT_TEMPLATE.format(context=\"here should files\", question='Mine 100 wood log')\n",
    "# outputfile = \"logs/rag_evaluation/test.txt\"\n",
    "# with open(outputfile, \"w\") as f:\n",
    "#    f.write(TEMP)\n",
    "\n",
    "# print(TEMP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker, num_docs_final=num_docs_final, num_retrieved_docs=num_retrieved_docs)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer.content}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer.content,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            \"description\": example[\"description\"]\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "   \n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "# An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "# 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "# 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "# 3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "# 4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "# ###The instruction to evaluate:\n",
    "# {instruction}\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Is the response correct, accurate, and factual based on the reference answer?]\n",
    "# Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "# Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "# Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "# Score 4: The response is mostly correct, accurate, and factual.\n",
    "# Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "# SYNTAX_ACCURACY_PROMPT = \"\"\"###Task Description:\n",
    "# Will provide JavaScript code response to evaluate based on syntax accuracy. Include a reference answer that scores a 5. Focus solely on the correctness of the syntax, including variable declarations, use of operators, function definitions, and control structures. This format helps ensure that the syntax is assessed without requiring code execution.\n",
    "# 1. Write detailed feedback assessing the quality of the response, focusing specifically on syntax accuracy.\n",
    "# 2. Assign a score from 1 to 5 based on the rubrics after providing feedback.\n",
    "# 3. Format your output as follows: \"Feedback: [write detailed feedback on syntax issues] [RESULT] [score from 1 to 5]\"\n",
    "# 4. Exclude any additional commentary beyond the feedback and score. Ensure the inclusion of [RESULT] in your output.\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Syntax Accuracy]\n",
    "# Score 1: Contains multiple syntax errors, preventing execution.\n",
    "# Score 2: Some syntax errors present, affects overall functionality.\n",
    "# Score 3: Minor syntax errors, do not significantly impact functionality.\n",
    "# Score 4: Very minor syntax inaccuracies, nearly perfect.\n",
    "# Score 5: Perfectly accurate syntax, completely error-free.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "\n",
    "# \"\"\"###Predefined Functions and Practices:\n",
    "# The model has access to several predefined helper functions that are designed to simplify routine operations and enhance code efficiency. While these functions are available for use, their application should be contextually appropriate to the specific task being performed:\n",
    "# - `mineBlock(bot, name, count)`: Collects specified blocks, suitable for tasks involving resource collection.\n",
    "# - `craftItem(bot, name, count)`: Crafts items, applicable for tasks requiring item assembly.\n",
    "# - `smeltItem(bot, name, count)`: Manages item smelting, relevant for tasks involving material processing.\n",
    "# - `placeItem(bot, name, position)`: Places items, useful for building and construction tasks.\n",
    "# - `killMob(bot, name, timeout)`: Handles mob engagement and elimination, necessary for combat-related tasks.\n",
    "\n",
    "# It is not expected that all these functions will be used in every solution; rather, their use should be dictated by the requirements of the specific task at hand. The absence of a function in a solution where it is not relevant should not negatively impact the evaluation. The emphasis of the evaluation should be on the effective use of these functions when they are applicable to the task, ensuring they are not marked as undefined if used and not penalizing their non-use when they are not relevant to the task objectives.\"\"\"\n",
    "\n",
    "# TODO Razvan simplify the prompt\n",
    "# ###Guidelines for Code Writing:\n",
    "# 1. The main function must be an async function that takes `bot` as its sole argument.\n",
    "# 2. Internal variables and setup must be confined within the function to ensure reusability and independence from external state changes.\n",
    "# 3. Progress should be communicated through `bot.chat` to indicate milestones or status updates within the task.\n",
    "# 4. Exploration and item collection must adapt to changing conditions, employing `exploreUntil` with random directions to avoid predictable patterns.\n",
    "# 5. Maintain a strict operational boundary with a maximum distance of 32 blocks for finding blocks or entities.\n",
    "# 6. Avoid the use of infinite loops, recursive functions, and event listeners to prevent performance degradation and unpredictable behavior.\n",
    "# 7. The function's name should reflect its purpose clearly and be indicative of the task it performs.\n",
    "\n",
    "\n",
    "# EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "# An instruction including a JavaScript task, a response to evaluate, a reference answer that scores a 5, and scoring rubrics focused on syntax accuracy and adherence to the task description. This format helps ensure the generated code aligns closely with the intended task without requiring code execution for assessment.\n",
    "# 1. Write detailed feedback assessing the quality of the response, focusing on syntax accuracy and how well the response adheres to the task requirements.\n",
    "# 2. Assign a score from 1 to 5 based on the rubrics after providing feedback.\n",
    "# 3. Format your output as follows: \"Feedback: {{write feedback for each criterion}} [RESULT] {{score from 1 to 5}}\"\n",
    "# 4. Exclude any additional commentary beyond the feedback and score. Ensure the inclusion of [RESULT] in your output.\n",
    "\n",
    "# ###The instruction to evaluate:\n",
    "# {instruction}\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Syntax Accuracy]\n",
    "# Score 1: Contains multiple syntax errors, preventing execution.\n",
    "# Score 2: Some syntax errors present, affects overall functionality.\n",
    "# Score 3: Minor syntax errors, do not significantly impact functionality.\n",
    "# Score 4: Very minor syntax inaccuracies, nearly perfect.\n",
    "# Score 5: Perfectly accurate syntax, completely error-free.\n",
    "\n",
    "# [Task Adherence]\n",
    "# Score 1: Does not address the task requirements.\n",
    "# Score 2: Partially addresses the task but misses key aspects.\n",
    "# Score 3: Addresses the task adequately, though some aspects could be better aligned.\n",
    "# Score 4: Very closely adheres to the task with minor deviations.\n",
    "# Score 5: Perfectly aligns with the task requirements, fully accomplishing the specified objectives.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The Instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evalute:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "The model has access to the following predefined helper functions: `mineBlock`, `craftItem`, `smeltItem`, `placeItem` and `killMob` which may be used if needed.\n",
    "Is the response correct, accurate, and factual based on the reference answer?\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\n",
    "\"\"\"\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "       \n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        print(\"\\n===========================================\")\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests\n",
    "\n",
    "Parameters to fine-tune:\n",
    "Reader models:\n",
    "- models used: deepseek-coder:6.7b-instruct-q5_K_M, deepseek-coder:33b-instruct-q5_K_M\n",
    "- temperature used\n",
    "- model_kwargs={ \"top_k\":30, \"repetition_penalty\": 1.03 }\n",
    "\n",
    "Splitter chunk size:\n",
    "- 128 is the maximum for st-codesearch-distilroberta-base, see: SentenceTransformer('thenlper/gte-small').max_seq_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m evaluator_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgattipg/prometheus:13b-v1.0-Q5_K_M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m evaluator_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgattipg/prometheus:13b-v1.0-Q5_K_M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "\n",
    "evaluator_name = \"gattipg/prometheus:13b-v1.0-Q5_K_M\"\n",
    "eval_chat_model = ChatOpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    model=evaluator_name)\n",
    "\n",
    "for reader_model in [\"llama3:70b-instruct\", \"deepseek-coder:33b-instruct-q5_K_M\"]: # deepseek-coder:33b-instruct-q5_K_M\n",
    "    READER_LLM = ChatOpenAI(\n",
    "        base_url = 'http://localhost:11434/v1',\n",
    "        api_key='ollama', # required, but unused\n",
    "        temperature=0.0,\n",
    "        streaming=True, \n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        model=reader_model\n",
    "    )\n",
    "    # TODO Razvan Make sure that the prompt generated here and the prompt generated in the simulation are the same\n",
    "    # TODO Razvan Test codebert retriever as well\n",
    "    for num_docs_final in [3, 5]:\n",
    "        for num_retrived_docs in [30]:\n",
    "            for chunk_size in [128]:\n",
    "                for embeddings in [\"flax-sentence-embeddings/st-codesearch-distilroberta-base\", \"microsoft/codebert-base\"]: # \"thenlper/gte-small\",\n",
    "                    for rerank in [True, False]:\n",
    "                        settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_numDocsFinal:{num_docs_final}_numRetrivedDocs:{num_retrived_docs}_reader-model:{reader_model.replace('/', '~').replace('_', '~').replace(':', '~')}_evaluator:{evaluator_name.replace('/', '~').replace('_', '~').replace(':', '~')}\"\n",
    "                        output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "                        print(f\"\\nRunning evaluation for {settings_name}:\")\n",
    "\n",
    "                        print(\"Loading knowledge base embeddings...\")\n",
    "                        knowledge_index = load_embeddings(\n",
    "                            RAW_KNOWLEDGE_BASE,\n",
    "                            chunk_size=chunk_size,\n",
    "                            embedding_model_name=embeddings,\n",
    "                        )\n",
    "\n",
    "                        print(\"Running RAG...\")\n",
    "                        reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "                        run_rag_tests(\n",
    "                            eval_dataset=eval_dataset[:3],\n",
    "                            llm=READER_LLM,\n",
    "                            knowledge_index=knowledge_index,\n",
    "                            output_file=output_file_name,\n",
    "                            reranker=reranker,\n",
    "                            verbose=True,\n",
    "                            test_settings=settings_name,\n",
    "                            num_retrieved_docs=num_retrived_docs,\n",
    "                            num_docs_final=num_docs_final\n",
    "                        )\n",
    "\n",
    "                        print(\"\\nRunning evaluation...\")\n",
    "                        evaluate_answers(\n",
    "                            output_file_name,\n",
    "                            eval_chat_model,\n",
    "                            evaluator_name,\n",
    "                            evaluation_prompt_template\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO to remove\n",
    "# settings_name = \"chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:True_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M\"\n",
    "# answers = []\n",
    "# answer_path = f\"./output/rag_{settings_name}.json\"\n",
    "# if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "#     answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "# for element in answers:\n",
    "#     # Use the pop method to remove the field\n",
    "#     element.pop('eval_score_gattipg/prometheus:13b-v1.0-Q5_K_M', None) \n",
    "#     element.pop('eval_feedback_gattipg/prometheus:13b-v1.0-Q5_K_M', None) \n",
    "\n",
    "# with open(answer_path, 'w') as file:\n",
    "#     json.dump(answers, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114a40e7d4ff4634a3011d6bc556de6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "###Task Description:\n",
      "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
      "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
      "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
      "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
      "4. Please do not generate any other opening, closing, and explanations.\n",
      "\n",
      "###The Instruction to evaluate:\n",
      "How to mine wood log in Minecraft?\n",
      "\n",
      "###Response to evalute:\n",
      "Explain: The code does not complete the task because it doesn't define a function to mine wood logs. It only provides helper functions for other tasks such as mining blocks, crafting items, smelting items, and placing items. \n",
      "\n",
      "Plan:\n",
      "1) Define an async function named `mineWoodLog` that takes the bot as the argument.\n",
      "2) Use the `exploreUntil` function to explore until a wood log is found or the maximum exploration time is reached.\n",
      "3) Once a wood log is found, use the `mineBlock` function to mine it.\n",
      "4) Repeat steps 1-3 until the required number of wood logs are mined.\n",
      "5) After mining all the wood logs, return from the function.\n",
      "\n",
      "Code:\n",
      "```javascript\n",
      "async function mineWoodLog(bot) {\n",
      "    const logNames = [\"oak_log\", \"birch_log\", \"spruce_log\", \"jungle_log\", \"acacia_log\", \"dark_oak_log\", \"mangrove_log\"];\n",
      "    let count = 0; // number of logs mined\n",
      "    while (count < 1) { // change 1 to the required number of wood logs\n",
      "        const logBlock = await bot.findBlock({\n",
      "            matching: block => logNames.includes(block.name),\n",
      "            maxDistance: 32\n",
      "        });\n",
      "        if (!logBlock) {\n",
      "            await exploreUntil(bot, new Vec3(1, 0, 1)); // explore in a random direction\n",
      "            continue;\n",
      "        }\n",
      "        await bot.pathfinder.goto(new GoalNear(logBlock.position.x, logBlock.position.y, logBlock.position.z));\n",
      "        await mineBlock(bot, logNames[0]); // mine any type of wood log\n",
      "        count++;\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "###Reference Answer (Score 5):\n",
      "async function mineWoodLog(bot) {\n",
      "  const logNames = [\"oak_log\", \"birch_log\", \"spruce_log\", \"jungle_log\", \"acacia_log\", \"dark_oak_log\", \"mangrove_log\"];\n",
      "  const logBlock = bot.findBlock({\n",
      "    matching: block => logNames.includes(block.name),\n",
      "    maxDistance: 32\n",
      "  });\n",
      "  if (!logBlock) {\n",
      "    bot.chat(\"No wood log found nearby. Exploring...\");\n",
      "    await exploreUntil(bot, new Vec3(1, 0, 1), 60, () => {\n",
      "      const foundLog = bot.findBlock({\n",
      "        matching: block => logNames.includes(block.name),\n",
      "        maxDistance: 32\n",
      "      });\n",
      "      return foundLog;\n",
      "    });\n",
      "  }\n",
      "  bot.chat(\"Wood log found. Mining...\");\n",
      "  await mineBlock(bot, logBlock.name, 1);\n",
      "  bot.chat(\"Wood log mined.\");\n",
      "}\n",
      "\n",
      "###Score Rubrics:\n",
      "The model has access to the following predefined helper functions: `mineBlock`, `craftItem`, `smeltItem`, `placeItem` and `killMob` which may be used if needed.\n",
      "Is the response correct, accurate, and factual based on the reference answer?\n",
      "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
      "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
      "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
      "Score 4: The response is mostly correct, accurate, and factual.\n",
      "Score 5: The response is completely correct, accurate, and factual.\n",
      "\n",
      "###Feedback:\n",
      "\n",
      "The response provided a detailed explanation of how to mine wood logs in Minecraft using the given helper functions. It correctly identified that the code does not complete the task as it only provides helper functions for other tasks. The plan outlined in the response is mostly correct, accurate, and factual. However, there are minor issues with the code provided. For instance, the `exploreUntil` function should be called after finding a wood log to ensure that the bot doesn't keep exploring after mining all the logs. Also, the `mineBlock` function should take the name of the wood log as an argument instead of using the default value \"oak_log\". So the overall score is 4. [RESULT] 4"
     ]
    }
   ],
   "source": [
    "\"\"\"[The model has access to the following predefined helper functions: `mineBlock`, `craftItem`, `smeltItem`, `placeItem` and `killMob` which may be used if needed.\n",
    "Is the response correct, accurate, and factual based on the reference answer?]\n",
    "\n",
    "Score 1: The code is largely non-functional or irrelevant to the task.\n",
    "Score 2: The code attempts to address the task but is incomplete or incorrect in its approach.\n",
    "Score 3: The code is functional and makes a reasonable attempt at the task.\n",
    "Score 4: The code meets the main objective. Minor errors don’t significantly impact the overall functionality.\n",
    "Score 5: The code achieves the task efficiently and handles potential errors well.\n",
    "\"\"\"\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The Instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evalute:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "The model has access to the following predefined helper functions: `mineBlock`, `craftItem`, `smeltItem`, `placeItem` and `killMob` which may be used if needed.\n",
    "Is the response correct, accurate, and factual based on the reference answer?\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\n",
    "\"\"\"\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")\n",
    "settings_name = \"chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:False_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M\"\n",
    "answers = []\n",
    "answer_path = f\"./output/rag_{settings_name}.json\"\n",
    "if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "    answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "print(len(answers))\n",
    "for experiment in tqdm(answers[:1]):\n",
    "\n",
    "    print(\"\\n===========================================\")\n",
    "\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "        instruction=experiment[\"question\"],\n",
    "        response=experiment[\"generated_answer\"],\n",
    "        reference_answer=experiment[\"true_answer\"],\n",
    "    )\n",
    "    print(eval_prompt[1].content)\n",
    "    eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "    experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "    experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "    with open('logs/rag_evaluation/evaluator_debug.txt', 'w') as file:\n",
    "        # Write some text to the file\n",
    "        file.write(experiment[f\"eval_feedback_{evaluator_name}\"])\n",
    "        file.write(f\"\\n{experiment[f'eval_score_{evaluator_name}']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/rag_chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:False_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M.json    22.395833\n",
       "./output/rag_chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:True_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M.json     25.000000\n",
       "Name: eval_score_gattipg/prometheus:13b-v1.0-Q5_K_M, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)\n",
    "\n",
    "\n",
    "result[f\"eval_score_{evaluator_name}\"] = result[f\"eval_score_{evaluator_name}\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[f\"eval_score_{evaluator_name}\"] = (result[f\"eval_score_{evaluator_name}\"] - 1) / 4\n",
    "\n",
    "\n",
    "average_scores = result.groupby(\"settings\")[f\"eval_score_{evaluator_name}\"].mean()\n",
    "average_scores.sort_values()\n",
    "scaled_values = pd.Series(average_scores * 100)\n",
    "scaled_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Configuration=%{x}<br>Accuracy=%{y}<br>color=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           22.395833333333336,
           25
          ],
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "outside",
         "texttemplate": "%{y:.1f}",
         "type": "bar",
         "x": [
          "./output/rag_chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:False_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M.json",
          "./output/rag_chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:True_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M.json"
         ],
         "xaxis": "x",
         "y": [
          22.395833333333336,
          25
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(0,0,255)"
          ],
          [
           1,
           "rgb(255,0,0)"
          ]
         ],
         "showscale": false
        },
        "font": {
         "size": 15
        },
        "height": 600,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>Accuracy of different RAG configurations</b>"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "RAG settings"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          100
         ],
         "ticksuffix": "%",
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the bar chart\n",
    "fig = px.bar(\n",
    "    scaled_values,\n",
    "    color=scaled_values,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    "    font=dict(size=15),\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
