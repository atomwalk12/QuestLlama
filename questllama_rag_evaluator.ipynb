{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration and Imports\n",
    "\n",
    "NOTE: Execute the javascript code to check for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import glob\n",
    "import json\n",
    "import datasets\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "import random\n",
    "from _voyager.control_primitives_context import load_control_primitives_context\n",
    "from _voyager.prompts import load_prompt\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "import re\n",
    "from langchain_community.chat_models.openai import ChatOpenAI\n",
    "from shared import file_utils as U\n",
    "import plotly.express as px\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f035573e42d491d8683da25f4871dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_KNOWLEDGE_DATABASE: 311\n",
      "Evaluation dataset prior to removing duplicates: 286\n",
      "Evaluation dataset after to removing duplicates: 189\n"
     ]
    }
   ],
   "source": [
    "def unfold_camel(camel_str):\n",
    "    # Insert a space before each uppercase letter and convert the entire string to lowercase\n",
    "    return re.sub(r'(?<!^)(?=[A-Z])', ' ', camel_str).lower()\n",
    "\n",
    "\n",
    "def get_question(task):\n",
    "    question = (\n",
    "        f\"How to {unfold_camel(task.replace('_', ' ').replace(' ore', '').replace(' ores', '').replace('.', '').strip())}\"\n",
    "        f\" in Minecraft?\"\n",
    "    )\n",
    "    return question\n",
    "\n",
    "def remove_duplicates(dataset):\n",
    "    result = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    for item in dataset:\n",
    "        if item['question'] not in seen_ids:\n",
    "            result.append(item)\n",
    "            seen_ids.add(item['question'])\n",
    "    return result\n",
    "\n",
    "def read_json_files_recursively(root_dir):\n",
    "    all_data = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.json'):\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                with open(filepath, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    all_data.append((filepath, data))\n",
    "    return all_data\n",
    "\n",
    "json_files = read_json_files_recursively(\"questllama/skill_library\")\n",
    "eval_dataset = []\n",
    "for i, trial in enumerate(json_files):\n",
    "    for program_name, info in trial[1].items(): # info contains code and scription of a program\n",
    "        question = get_question(program_name)\n",
    "        eval_dataset.append({'question': question, 'answer': info['code'], 'description': info['description'], 'source_doc': f'{\"/\".join(trial[0].split(\"/\")[:-1])}/code/{program_name}.js'})\n",
    "\n",
    "\n",
    "## Read the dataset\n",
    "ds = U.read_skill_library(\"skill_library\", full_path=True)\n",
    "ds = [ {'text': elem[1], 'source': elem[0] } for elem in ds]\n",
    "RAW_KNOWLEDGE_BASE = [Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "print(f\"RAW_KNOWLEDGE_DATABASE: {len(RAW_KNOWLEDGE_BASE)}\")\n",
    "print(f\"Evaluation dataset prior to removing duplicates: {len(eval_dataset)}\")\n",
    "eval_dataset = remove_duplicates(eval_dataset)\n",
    "print(f\"Evaluation dataset after to removing duplicates: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[Document],\n",
    "    tokenizer_name: Optional[str],\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    JAVASCRIPT_SEPARATORS = [\n",
    "        \"\\nfunction \",\n",
    "        \"\\nconst \",\n",
    "        \"\\nlet \",\n",
    "        \"\\nvar \",\n",
    "        \"\\nclass \",\n",
    "        \"\\nif \",\n",
    "        \"\\nfor \",\n",
    "        \"\\nwhile \",\n",
    "        \"\\nswitch \",\n",
    "        \"\\ncase \",\n",
    "        \"\\ndefault \",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "    print(f'Tokenizer: {tokenizer_name}, Chunk Size: {chunk_size}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # FIXME: Is it necessary to pass the separators here? Try without them.\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=JAVASCRIPT_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[Document],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str]\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "\n",
    "        docs_processed = _split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare system and user messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_user_messages():\n",
    "    # Load the 'database' file\n",
    "    answers = json.load(open(\"community/voyager5/openai_requests.json\", \"r\"))\n",
    "\n",
    "    # Get only user messages contained in action events\n",
    "    user_messages = [msg['prompts'][1]['message'] for msg in answers if msg['prompts'][1]['message'].startswith('Code from the last round:')]\n",
    "\n",
    "    # Use regular expression to replace the task description regardless of what it is\n",
    "    # user_messages = [ re.sub(r\"(Task:).*\", \"Task: {question}\", msg) for msg in user_messages ]\n",
    "    unique = set(user_messages)\n",
    "    return list(unique)\n",
    "\n",
    "def render_system_message(skills=[]):\n",
    "    system_template = load_prompt(\"action_template_rag\")\n",
    "    # FIXME: Hardcoded control_primitives\n",
    "    base_skills = [\n",
    "        \"exploreUntil\",\n",
    "        \"mineBlock\",\n",
    "        \"craftItem\",\n",
    "        \"placeItem\",\n",
    "        \"smeltItem\",\n",
    "        \"killMob\",\n",
    "    ]\n",
    "    if True: # TODO Razvan: always import these files since questllama's model context size is enough\n",
    "        base_skills += [\n",
    "            \"useChest\",\n",
    "            \"mineflayer\",\n",
    "        ]\n",
    "    programs = \"\\n\\n\".join(load_control_primitives_context(base_skills) + skills)\n",
    "    response_format = load_prompt(\"action_response_format_rag\")\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "        system_template\n",
    "    )\n",
    "    system_message = system_message_prompt.format(\n",
    "        programs=programs, response_format=response_format, context=\"\"\n",
    "    )\n",
    "\n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: System Definition:\\nYou are a helpful assistant that writes Mineflayer javascript code to complete any Minecraft task specified by me.\\nGenerate code strictly according to the given specifications. The code should be fully functional and ready to execute as provided, requiring no further modifications.\\n\\n\\n---\\nGuidelines:\\nPlease pay close attention to the following points to avoid repeating past mistakes. The goal is to offer you information based on past mistakes, helping you avoid making them again in the future. Try to generalise whenever possible the associated example.\\n\\n1) Mistake: Not identifying specific items within a broad category to complete a task.\\n   Concept To complete a task defined by a broad category, first understand the category and then choose any specific item within it to fulfill the requirement. \\n   Example: in a task like \"Mine 1 wood log,\" the term \"wood log\" is a broad category that includes specifics such as oak, birch, or spruce logs. Simply mine any one type from these specifics to successfully complete the task.\\n\\n\\n---\\nCode Repository:\\n\\nBelow are useful programs written with Mineflayer APIs for tasks such as exploring, mining, crafting, and placement of items. Use these programs as building blocks for solving tasks.\\n\\n// Explore downward for 60 seconds: exploreUntil(bot, new Vec3(0, -1, 0), 60);\\nasync function exploreUntil(\\n    bot,\\n    direction,\\n    maxTime = 60,\\n    callback = () => {\\n        return false;\\n    }\\n) {\\n    if (typeof maxTime !== \"number\") {\\n        throw new Error(\"maxTime must be a number\");\\n    }\\n    if (typeof callback !== \"function\") {\\n        throw new Error(\"callback must be a function\");\\n    }\\n    const test = callback();\\n    if (test) {\\n        bot.chat(\"Explore success.\");\\n        return Promise.resolve(test);\\n    }\\n    if (direction.x === 0 && direction.y === 0 && direction.z === 0) {\\n        throw new Error(\"direction cannot be 0, 0, 0\");\\n    }\\n    if (\\n        !(\\n            (direction.x === 0 || direction.x === 1 || direction.x === -1) &&\\n            (direction.y === 0 || direction.y === 1 || direction.y === -1) &&\\n            (direction.z === 0 || direction.z === 1 || direction.z === -1)\\n        )\\n    ) {\\n        throw new Error(\\n            \"direction must be a Vec3 only with value of -1, 0 or 1\"\\n        );\\n    }\\n    maxTime = Math.min(maxTime, 1200);\\n    return new Promise((resolve, reject) => {\\n        const dx = direction.x;\\n        const dy = direction.y;\\n        const dz = direction.z;\\n\\n        let explorationInterval;\\n        let maxTimeTimeout;\\n\\n        const cleanUp = () => {\\n            clearInterval(explorationInterval);\\n            clearTimeout(maxTimeTimeout);\\n            bot.pathfinder.setGoal(null);\\n        };\\n\\n        const explore = () => {\\n            const x =\\n                bot.entity.position.x +\\n                Math.floor(Math.random() * 20 + 10) * dx;\\n            const y =\\n                bot.entity.position.y +\\n                Math.floor(Math.random() * 20 + 10) * dy;\\n            const z =\\n                bot.entity.position.z +\\n                Math.floor(Math.random() * 20 + 10) * dz;\\n            let goal = new GoalNear(x, y, z);\\n            if (dy === 0) {\\n                goal = new GoalNearXZ(x, z);\\n            }\\n            bot.pathfinder.setGoal(goal);\\n\\n            try {\\n                const result = callback();\\n                if (result) {\\n                    cleanUp();\\n                    bot.chat(\"Explore success.\");\\n                    resolve(result);\\n                }\\n            } catch (err) {\\n                cleanUp();\\n                reject(err);\\n            }\\n        };\\n\\n        explorationInterval = setInterval(explore, 2000);\\n\\n        maxTimeTimeout = setTimeout(() => {\\n            cleanUp();\\n            bot.chat(\"Max exploration time reached\");\\n            resolve(null);\\n        }, maxTime * 1000);\\n    });\\n}\\n\\n\\n// Mine 3 cobblestone: mineBlock(bot, \"stone\", 3);\\nasync function mineBlock(bot, name, count = 1) {\\n    const blocks = bot.findBlocks({\\n        matching: (block) => {\\n            return block.name === name;\\n        },\\n        maxDistance: 32,\\n        count: count,\\n    });\\n    const targets = [];\\n    for (let i = 0; i < Math.min(blocks.length, count); i++) {\\n        targets.push(bot.blockAt(blocks[i]));\\n    }\\n    await bot.collectBlock.collect(targets, { ignoreNoPath: true });\\n}\\n\\n\\n// Craft 8 oak_planks from 2 oak_log (do the recipe 2 times): craftItem(bot, \"oak_planks\", 2);\\n// You must place a crafting table before calling this function\\nasync function craftItem(bot, name, count = 1) {\\n    const item = mcData.itemsByName[name];\\n    const craftingTable = bot.findBlock({\\n        matching: mcData.blocksByName.crafting_table.id,\\n        maxDistance: 32,\\n    });\\n    await bot.pathfinder.goto(\\n        new GoalLookAtBlock(craftingTable.position, bot.world)\\n    );\\n    const recipe = bot.recipesFor(item.id, null, 1, craftingTable)[0];\\n    await bot.craft(recipe, count, craftingTable);\\n}\\n\\n\\n// Place a crafting_table near the player, Vec3(1, 0, 0) is just an example, you shouldn\\'t always use that: placeItem(bot, \"crafting_table\", bot.entity.position.offset(1, 0, 0));\\nasync function placeItem(bot, name, position) {\\n    const item = bot.inventory.findInventoryItem(mcData.itemsByName[name].id);\\n    // find a reference block\\n    const faceVectors = [\\n        new Vec3(0, 1, 0),\\n        new Vec3(0, -1, 0),\\n        new Vec3(1, 0, 0),\\n        new Vec3(-1, 0, 0),\\n        new Vec3(0, 0, 1),\\n        new Vec3(0, 0, -1),\\n    ];\\n    let referenceBlock = null;\\n    let faceVector = null;\\n    for (const vector of faceVectors) {\\n        const block = bot.blockAt(position.minus(vector));\\n        if (block?.name !== \"air\") {\\n            referenceBlock = block;\\n            faceVector = vector;\\n            break;\\n        }\\n    }\\n    // You must first go to the block position you want to place\\n    await bot.pathfinder.goto(new GoalPlaceBlock(position, bot.world, {}));\\n    // You must equip the item right before calling placeBlock\\n    await bot.equip(item, \"hand\");\\n    await bot.placeBlock(referenceBlock, faceVector);\\n}\\n\\n\\n// Smelt 1 raw_iron into 1 iron_ingot using 1 oak_planks as fuel: smeltItem(bot, \"raw_iron\", \"oak_planks\");\\n// You must place a furnace before calling this function\\nasync function smeltItem(bot, itemName, fuelName, count = 1) {\\n    const item = mcData.itemsByName[itemName];\\n    const fuel = mcData.itemsByName[fuelName];\\n    const furnaceBlock = bot.findBlock({\\n        matching: mcData.blocksByName.furnace.id,\\n        maxDistance: 32,\\n    });\\n    await bot.pathfinder.goto(\\n        new GoalLookAtBlock(furnaceBlock.position, bot.world)\\n    );\\n    const furnace = await bot.openFurnace(furnaceBlock);\\n    for (let i = 0; i < count; i++) {\\n        await furnace.putFuel(fuel.id, null, 1);\\n        await furnace.putInput(item.id, null, 1);\\n        // Wait 12 seconds for the furnace to smelt the item\\n        await bot.waitForTicks(12 * 20);\\n        await furnace.takeOutput();\\n    }\\n    await furnace.close();\\n}\\n\\n\\n// Kill a pig and collect the dropped item: killMob(bot, \"pig\", 300);\\nasync function killMob(bot, mobName, timeout = 300) {\\n    const entity = bot.nearestEntity(\\n        (entity) =>\\n            entity.name === mobName &&\\n            entity.position.distanceTo(bot.entity.position) < 32\\n    );\\n    await bot.pvp.attack(entity);\\n    await bot.pathfinder.goto(\\n        new GoalBlock(entity.position.x, entity.position.y, entity.position.z)\\n    );\\n}\\n\\n\\n// Get a torch from chest at (30, 65, 100): getItemFromChest(bot, new Vec3(30, 65, 100), {\"torch\": 1});\\n// This function will work no matter how far the bot is from the chest.\\nasync function getItemFromChest(bot, chestPosition, itemsToGet) {\\n    await moveToChest(bot, chestPosition);\\n    const chestBlock = bot.blockAt(chestPosition);\\n    const chest = await bot.openContainer(chestBlock);\\n    for (const name in itemsToGet) {\\n        const itemByName = mcData.itemsByName[name];\\n        const item = chest.findContainerItem(itemByName.id);\\n        await chest.withdraw(item.type, null, itemsToGet[name]);\\n    }\\n    await closeChest(bot, chestBlock);\\n}\\n// Deposit a torch into chest at (30, 65, 100): depositItemIntoChest(bot, new Vec3(30, 65, 100), {\"torch\": 1});\\n// This function will work no matter how far the bot is from the chest.\\nasync function depositItemIntoChest(bot, chestPosition, itemsToDeposit) {\\n    await moveToChest(bot, chestPosition);\\n    const chestBlock = bot.blockAt(chestPosition);\\n    const chest = await bot.openContainer(chestBlock);\\n    for (const name in itemsToDeposit) {\\n        const itemByName = mcData.itemsByName[name];\\n        const item = bot.inventory.findInventoryItem(itemByName.id);\\n        await chest.deposit(item.type, null, itemsToDeposit[name]);\\n    }\\n    await closeChest(bot, chestBlock);\\n}\\n// Check the items inside the chest at (30, 65, 100): checkItemInsideChest(bot, new Vec3(30, 65, 100));\\n// You only need to call this function once without any action to finish task of checking items inside the chest.\\nasync function checkItemInsideChest(bot, chestPosition) {\\n    await moveToChest(bot, chestPosition);\\n    const chestBlock = bot.blockAt(chestPosition);\\n    await bot.openContainer(chestBlock);\\n    // You must close the chest after opening it if you are asked to open a chest\\n    await closeChest(bot, chestBlock);\\n}\\n\\n\\nawait bot.pathfinder.goto(goal); // A very useful function. This function may change your main-hand equipment.\\n// Following are some Goals you can use:\\nnew GoalNear(x, y, z, range); // Move the bot to a block within the specified range of the specified block. `x`, `y`, `z`, and `range` are `number`\\nnew GoalXZ(x, z); // Useful for long-range goals that don\\'t have a specific Y level. `x` and `z` are `number`\\nnew GoalGetToBlock(x, y, z); // Not get into the block, but get directly adjacent to it. Useful for fishing, farming, filling bucket, and beds. `x`, `y`, and `z` are `number`\\nnew GoalFollow(entity, range); // Follow the specified entity within the specified range. `entity` is `Entity`, `range` is `number`\\nnew GoalPlaceBlock(position, bot.world, {}); // Position the bot in order to place a block. `position` is `Vec3`\\nnew GoalLookAtBlock(position, bot.world, {}); // Path into a position where a blockface of the block at position is visible. `position` is `Vec3`\\n\\n// These are other Mineflayer functions you can use:\\nbot.isABed(bedBlock); // Return true if `bedBlock` is a bed\\nbot.blockAt(position); // Return the block at `position`. `position` is `Vec3`\\n\\n// These are other Mineflayer async functions you can use:\\nawait bot.equip(item, destination); // Equip the item in the specified destination. `item` is `Item`, `destination` can only be \"hand\", \"head\", \"torso\", \"legs\", \"feet\", \"off-hand\"\\nawait bot.consume(); // Consume the item in the bot\\'s hand. You must equip the item to consume first. Useful for eating food, drinking potions, etc.\\nawait bot.fish(); // Let bot fish. Before calling this function, you must first get to a water block and then equip a fishing rod. The bot will automatically stop fishing when it catches a fish\\nawait bot.sleep(bedBlock); // Sleep until sunrise. You must get to a bed block first\\nawait bot.activateBlock(block); // This is the same as right-clicking a block in the game. Useful for buttons, doors, etc. You must get to the block first\\nawait bot.lookAt(position); // Look at the specified position. You must go near the position before you look at it. To fill bucket with water, you must lookAt first. `position` is `Vec3`\\nawait bot.activateItem(); // This is the same as right-clicking to use the item in the bot\\'s hand. Useful for using buckets, etc. You must equip the item to activate first\\nawait bot.useOn(entity); // This is the same as right-clicking an entity in the game. Useful for shearing sheep, equipping harnesses, etc. You must get to the entity first\\n\\n\\n\\n---\\nRAG Context:\\nThe following code snippets are examples retrieved to aid in solving the given task. They are not intended to be executed directly but serve as a reference for understanding how to approach similar problems.\\n\\ntest\\n\\n\\n---\\nTask Communication:\\nAt each round of conversation, I will provide the following updates from the Minecraft world, which will guide the development of the solution.\\n\\nCode from the last round: The JavaScript code generated in the previous interaction. This helps to track changes or improvements across interactions.\\nExecution error: Details of any runtime errors that occurred while executing the code. This is crucial for debugging and refining the code.\\nChat log: Log of in-game chat messages that might include errors, system messages, or player interactions. Useful for understanding events that occurred during execution.\\nBiome: The type of environment around the player in Minecraft, affecting available resources and strategies.\\nTime: The in-game time of day, which can influence gameplay mechanics such as mob spawning and player visibility.\\nNearby blocks: Types of blocks around the player\\'s current location. Important for tasks involving mining, building, or navigation.\\nNearby entities (nearest to farthest): Creatures or players near the player, sorted by proximity. Critical for tasks involving combat, trading, or animal farming.\\nHealth: The current health of the player, indicating how much damage they can take before dying.\\nHunger: The hunger level of the player, affecting stamina and health regeneration.\\nPosition: The player\\'s coordinates in the game world, necessary for navigation and context.\\nEquipment: Items the player is currently wearing or holding, affecting their abilities and interactions.\\nInventory (xx/36): List of items in the player\\'s inventory, with a count of how many slots are occupied, crucial for managing resources and planning tasks.\\nChests: Details about storage chests in the vicinity, which can contain additional resources or task-relevant items.\\nTask: The specific task assigned to the player or AI, defining the goal of the current interaction.\\nContext: Additional background information or clarifications about the task, helping to understand the requirements or conditions.\\nCritique: Feedback on the previous solutions or approaches, aimed at improving future iterations of the code or strategy.\\n\\n\\n---\\nInitial Response Instructions:\\n\\nYou should then respond to me with:\\nExplain (if applicable): Are there any steps missing in your plan? Why does the code not complete the task? What does the chat log and execution error imply?\\nPlan: How to complete the task step by step. You should pay attention to Inventory since it tells what you have. The task completeness check is also based on your final inventory.\\nCode:\\n    1) Write an async function taking the bot as the only argument.\\n    2) Reuse the above useful programs as much as possible.\\n        - Use `mineBlock(bot, name, count)` to collect blocks. Do not use `bot.dig` directly.\\n        - Use `craftItem(bot, name, count)` to craft items. Do not use `bot.craft` or `bot.recipesFor` directly.\\n        - Use `smeltItem(bot, name count)` to smelt items. Do not use `bot.openFurnace` directly.\\n        - Use `placeItem(bot, name, position)` to place blocks. Do not use `bot.placeBlock` directly.\\n        - Use `killMob(bot, name, timeout)` to kill mobs. Do not use `bot.attack` directly.\\n    3) Your function will be reused for building more complex functions. Therefore, you should make it generic and reusable. You should not make strong assumption about the inventory (as it may be changed at a later time), and therefore you should always check whether you have the required items before using them. If not, you should first collect the required items and reuse the above useful programs.\\n    4) Functions in the \"Code from the last round\" section will not be saved or executed. Do not reuse functions listed there.\\n    5) Anything defined outside a function will be ignored, define all your variables inside your functions.\\n    6) Call `bot.chat` to show the intermediate progress.\\n    7) Use `exploreUntil(bot, direction, maxDistance, callback)` when you cannot find something. You should frequently call this before mining blocks or killing mobs. You should select a direction at random every time instead of constantly using (1, 0, 1).\\n    8) `maxDistance` should always be 32 for `bot.findBlocks` and `bot.findBlock`. Do not cheat.\\n    9) Do not write infinite loops or recursive functions.\\n    10) Do not use `bot.on` or `bot.once` to register event listeners. You definitely do not need them.\\n    11) Name your function in a meaningful way (can infer the task from the name).\\n\\n\\nYou should only respond in the format as described below:\\nRESPONSE FORMAT:\\nExplain: ...\\nPlan:\\n1) ...\\n2) ...\\n3) ...\\n...\\nCode:\\n```javascript\\n// helper functions (only if needed, try to avoid them)\\n...\\n// main function after the helper functions\\nasync function yourMainFunctionName(bot) {\\n  // ...\\n}\\n```\\n\\n\\nHuman: ---\\nTask Communication:\\n\\nCode from the last round: No code in the first round\\n\\nExecution error: No error\\n\\nChat log: None\\n\\nBiome: \\n\\nTime: day\\n\\nNearby blocks: grass_block, dirt, dark_oak_log, stone, oak_leaves, oak_log, coal_ore, dark_oak_leaves\\n\\nNearby entities (nearest to farthest): cow\\n\\nHealth: 20.0/20\\n\\nHunger: 20.0/20\\n\\nPosition: x=1248.5, y=94.0, z=-3471.5\\n\\nEquipment: [None, None, None, None, None, None]\\n\\nInventory (0/36): Empty\\n\\nChests: None\\n\\nTask: Mine 1 wood log\\n\\nContext: You can mine one of oak, birch, spruce, jungle, acacia, dark oak, or mangrove logs.\\n\\nCritique: None\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the system message, this one is unique\n",
    "system_message = render_system_message()\n",
    "\n",
    "# Here are sample user messages, containing the state of the Minecraft world.\n",
    "user_messages = read_user_messages()\n",
    "random.seed(422)\n",
    "\n",
    "# Below are some further examples (w/o context) to augment the size of the training set.\n",
    "task_type = 'action'\n",
    "default_config = U.debug_load_prompt(f\"/debugging/{task_type}/user_rag.txt\")\n",
    "\n",
    "# TODO Razvan add new test samples: user_messages = [default_config] + user_messages\n",
    "\n",
    "random.seed(42)\n",
    "# This is the template\n",
    "RAG_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_message.content),\n",
    "        HumanMessagePromptTemplate.from_template(default_config)\n",
    "    ]\n",
    ")\n",
    "\n",
    "RAG_PROMPT_TEMPLATE.format(context=\"test\", question=\"Mine 1 wood log\")\n",
    "# For debugging purposes.\n",
    "# TEMP = RAG_PROMPT_TEMPLATE.format(context=\"here should files\", question='Mine 100 wood log')\n",
    "# outputfile = \"logs/rag_evaluation/test.txt\"\n",
    "# with open(outputfile, \"w\") as f:\n",
    "#    f.write(TEMP)\n",
    "\n",
    "# print(TEMP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker, num_docs_final=num_docs_final, num_retrieved_docs=num_retrieved_docs)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer.content}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer.content,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "            \"description\": example[\"description\"]\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "   \n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "# An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "# 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "# 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "# 3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "# 4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "# ###The instruction to evaluate:\n",
    "# {instruction}\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Is the response correct, accurate, and factual based on the reference answer?]\n",
    "# Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "# Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "# Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "# Score 4: The response is mostly correct, accurate, and factual.\n",
    "# Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "# SYNTAX_ACCURACY_PROMPT = \"\"\"###Task Description:\n",
    "# Will provide JavaScript code response to evaluate based on syntax accuracy. Include a reference answer that scores a 5. Focus solely on the correctness of the syntax, including variable declarations, use of operators, function definitions, and control structures. This format helps ensure that the syntax is assessed without requiring code execution.\n",
    "# 1. Write detailed feedback assessing the quality of the response, focusing specifically on syntax accuracy.\n",
    "# 2. Assign a score from 1 to 5 based on the rubrics after providing feedback.\n",
    "# 3. Format your output as follows: \"Feedback: [write detailed feedback on syntax issues] [RESULT] [score from 1 to 5]\"\n",
    "# 4. Exclude any additional commentary beyond the feedback and score. Ensure the inclusion of [RESULT] in your output.\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Syntax Accuracy]\n",
    "# Score 1: Contains multiple syntax errors, preventing execution.\n",
    "# Score 2: Some syntax errors present, affects overall functionality.\n",
    "# Score 3: Minor syntax errors, do not significantly impact functionality.\n",
    "# Score 4: Very minor syntax inaccuracies, nearly perfect.\n",
    "# Score 5: Perfectly accurate syntax, completely error-free.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "\n",
    "# \"\"\"###Predefined Functions and Practices:\n",
    "# The model has access to several predefined helper functions that are designed to simplify routine operations and enhance code efficiency. While these functions are available for use, their application should be contextually appropriate to the specific task being performed:\n",
    "# - `mineBlock(bot, name, count)`: Collects specified blocks, suitable for tasks involving resource collection.\n",
    "# - `craftItem(bot, name, count)`: Crafts items, applicable for tasks requiring item assembly.\n",
    "# - `smeltItem(bot, name, count)`: Manages item smelting, relevant for tasks involving material processing.\n",
    "# - `placeItem(bot, name, position)`: Places items, useful for building and construction tasks.\n",
    "# - `killMob(bot, name, timeout)`: Handles mob engagement and elimination, necessary for combat-related tasks.\n",
    "\n",
    "# It is not expected that all these functions will be used in every solution; rather, their use should be dictated by the requirements of the specific task at hand. The absence of a function in a solution where it is not relevant should not negatively impact the evaluation. The emphasis of the evaluation should be on the effective use of these functions when they are applicable to the task, ensuring they are not marked as undefined if used and not penalizing their non-use when they are not relevant to the task objectives.\"\"\"\n",
    "\n",
    "# TODO Razvan simplify the prompt\n",
    "# ###Guidelines for Code Writing:\n",
    "# 1. The main function must be an async function that takes `bot` as its sole argument.\n",
    "# 2. Internal variables and setup must be confined within the function to ensure reusability and independence from external state changes.\n",
    "# 3. Progress should be communicated through `bot.chat` to indicate milestones or status updates within the task.\n",
    "# 4. Exploration and item collection must adapt to changing conditions, employing `exploreUntil` with random directions to avoid predictable patterns.\n",
    "# 5. Maintain a strict operational boundary with a maximum distance of 32 blocks for finding blocks or entities.\n",
    "# 6. Avoid the use of infinite loops, recursive functions, and event listeners to prevent performance degradation and unpredictable behavior.\n",
    "# 7. The function's name should reflect its purpose clearly and be indicative of the task it performs.\n",
    "\n",
    "\n",
    "# EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "# An instruction including a JavaScript task, a response to evaluate, a reference answer that scores a 5, and scoring rubrics focused on syntax accuracy and adherence to the task description. This format helps ensure the generated code aligns closely with the intended task without requiring code execution for assessment.\n",
    "# 1. Write detailed feedback assessing the quality of the response, focusing on syntax accuracy and how well the response adheres to the task requirements.\n",
    "# 2. Assign a score from 1 to 5 based on the rubrics after providing feedback.\n",
    "# 3. Format your output as follows: \"Feedback: {{write feedback for each criterion}} [RESULT] {{score from 1 to 5}}\"\n",
    "# 4. Exclude any additional commentary beyond the feedback and score. Ensure the inclusion of [RESULT] in your output.\n",
    "\n",
    "# ###The instruction to evaluate:\n",
    "# {instruction}\n",
    "\n",
    "# ###Response to evaluate:\n",
    "# {response}\n",
    "\n",
    "# ###Reference Answer (Score 5):\n",
    "# {reference_answer}\n",
    "\n",
    "# ###Score Rubrics:\n",
    "# [Syntax Accuracy]\n",
    "# Score 1: Contains multiple syntax errors, preventing execution.\n",
    "# Score 2: Some syntax errors present, affects overall functionality.\n",
    "# Score 3: Minor syntax errors, do not significantly impact functionality.\n",
    "# Score 4: Very minor syntax inaccuracies, nearly perfect.\n",
    "# Score 5: Perfectly accurate syntax, completely error-free.\n",
    "\n",
    "# [Task Adherence]\n",
    "# Score 1: Does not address the task requirements.\n",
    "# Score 2: Partially addresses the task but misses key aspects.\n",
    "# Score 3: Addresses the task adequately, though some aspects could be better aligned.\n",
    "# Score 4: Very closely adheres to the task with minor deviations.\n",
    "# Score 5: Perfectly aligns with the task requirements, fully accomplishing the specified objectives.\n",
    "\n",
    "# ###Feedback:\"\"\"\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The Instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evalute:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "The model has access to the following predefined helper functions: `mineBlock`, `craftItem`, `smeltItem`, `placeItem` and `killMob` which may be used if needed.\n",
    "Is the response correct, accurate, and factual based on the reference answer?\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\n",
    "\"\"\"\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "       \n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        print(\"\\n===========================================\")\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests\n",
    "\n",
    "Parameters to fine-tune:\n",
    "Reader models:\n",
    "- models used: deepseek-coder:6.7b-instruct-q5_K_M, deepseek-coder:33b-instruct-q5_K_M\n",
    "- temperature used\n",
    "- model_kwargs={ \"top_k\":30, \"repetition_penalty\": 1.03 }\n",
    "\n",
    "Splitter chunk size:\n",
    "- 128 is the maximum for st-codesearch-distilroberta-base, see: SentenceTransformer('thenlper/gte-small').max_seq_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m evaluator_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgattipg/prometheus:13b-v1.0-Q5_K_M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m evaluator_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgattipg/prometheus:13b-v1.0-Q5_K_M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "\n",
    "evaluator_name = \"gattipg/prometheus:13b-v1.0-Q5_K_M\"\n",
    "eval_chat_model = ChatOpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    model=evaluator_name)\n",
    "\n",
    "for reader_model in [\"llama3:70b-instruct\", \"deepseek-coder:33b-instruct-q5_K_M\"]: # deepseek-coder:33b-instruct-q5_K_M\n",
    "    READER_LLM = ChatOpenAI(\n",
    "        base_url = 'http://localhost:11434/v1',\n",
    "        api_key='ollama', # required, but unused\n",
    "        temperature=0.0,\n",
    "        streaming=True, \n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        model=reader_model\n",
    "    )\n",
    "    # TODO Razvan Make sure that the prompt generated here and the prompt generated in the simulation are the same\n",
    "    # TODO Razvan Test codebert retriever as well\n",
    "    for num_docs_final in [3, 5]:\n",
    "        for num_retrived_docs in [30]:\n",
    "            for chunk_size in [128]:\n",
    "                for embeddings in [\"flax-sentence-embeddings/st-codesearch-distilroberta-base\", \"microsoft/codebert-base\"]: # \"thenlper/gte-small\",\n",
    "                    for rerank in [True, False]:\n",
    "                        settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_numDocsFinal:{num_docs_final}_numRetrivedDocs:{num_retrived_docs}_reader-model:{reader_model.replace('/', '~').replace('_', '~').replace(':', '~')}_evaluator:{evaluator_name.replace('/', '~').replace('_', '~').replace(':', '~')}\"\n",
    "                        output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "                        print(f\"\\nRunning evaluation for {settings_name}:\")\n",
    "\n",
    "                        print(\"Loading knowledge base embeddings...\")\n",
    "                        knowledge_index = load_embeddings(\n",
    "                            RAW_KNOWLEDGE_BASE,\n",
    "                            chunk_size=chunk_size,\n",
    "                            embedding_model_name=embeddings,\n",
    "                        )\n",
    "\n",
    "                        print(\"Running RAG...\")\n",
    "                        reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "                        run_rag_tests(\n",
    "                            eval_dataset=eval_dataset[:3],\n",
    "                            llm=READER_LLM,\n",
    "                            knowledge_index=knowledge_index,\n",
    "                            output_file=output_file_name,\n",
    "                            reranker=reranker,\n",
    "                            verbose=True,\n",
    "                            test_settings=settings_name,\n",
    "                            num_retrieved_docs=num_retrived_docs,\n",
    "                            num_docs_final=num_docs_final\n",
    "                        )\n",
    "\n",
    "                        print(\"\\nRunning evaluation...\")\n",
    "                        evaluate_answers(\n",
    "                            output_file_name,\n",
    "                            eval_chat_model,\n",
    "                            evaluator_name,\n",
    "                            evaluation_prompt_template\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO to remove\n",
    "# settings_name = \"chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:True_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M\"\n",
    "# answers = []\n",
    "# answer_path = f\"./output/rag_{settings_name}.json\"\n",
    "# if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "#     answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "# for element in answers:\n",
    "#     # Use the pop method to remove the field\n",
    "#     element.pop('eval_score_gattipg/prometheus:13b-v1.0-Q5_K_M', None) \n",
    "#     element.pop('eval_feedback_gattipg/prometheus:13b-v1.0-Q5_K_M', None) \n",
    "\n",
    "# with open(answer_path, 'w') as file:\n",
    "#     json.dump(answers, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114a40e7d4ff4634a3011d6bc556de6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "\n",
      "###Task Description:\n",
      "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
      "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
      "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
      "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
      "4. Please do not generate any other opening, closing, and explanations.\n",
      "\n",
      "###The Instruction to evaluate:\n",
      "How to mine wood log in Minecraft?\n",
      "\n",
      "###Response to evalute:\n",
      "Explain: The code does not complete the task because it doesn't define a function to mine wood logs. It only provides helper functions for other tasks such as mining blocks, crafting items, smelting items, and placing items. \n",
      "\n",
      "Plan:\n",
      "1) Define an async function named `mineWoodLog` that takes the bot as the argument.\n",
      "2) Use the `exploreUntil` function to explore until a wood log is found or the maximum exploration time is reached.\n",
      "3) Once a wood log is found, use the `mineBlock` function to mine it.\n",
      "4) Repeat steps 1-3 until the required number of wood logs are mined.\n",
      "5) After mining all the wood logs, return from the function.\n",
      "\n",
      "Code:\n",
      "```javascript\n",
      "async function mineWoodLog(bot) {\n",
      "    const logNames = [\"oak_log\", \"birch_log\", \"spruce_log\", \"jungle_log\", \"acacia_log\", \"dark_oak_log\", \"mangrove_log\"];\n",
      "    let count = 0; // number of logs mined\n",
      "    while (count < 1) { // change 1 to the required number of wood logs\n",
      "        const logBlock = await bot.findBlock({\n",
      "            matching: block => logNames.includes(block.name),\n",
      "            maxDistance: 32\n",
      "        });\n",
      "        if (!logBlock) {\n",
      "            await exploreUntil(bot, new Vec3(1, 0, 1)); // explore in a random direction\n",
      "            continue;\n",
      "        }\n",
      "        await bot.pathfinder.goto(new GoalNear(logBlock.position.x, logBlock.position.y, logBlock.position.z));\n",
      "        await mineBlock(bot, logNames[0]); // mine any type of wood log\n",
      "        count++;\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "###Reference Answer (Score 5):\n",
      "async function mineWoodLog(bot) {\n",
      "  const logNames = [\"oak_log\", \"birch_log\", \"spruce_log\", \"jungle_log\", \"acacia_log\", \"dark_oak_log\", \"mangrove_log\"];\n",
      "  const logBlock = bot.findBlock({\n",
      "    matching: block => logNames.includes(block.name),\n",
      "    maxDistance: 32\n",
      "  });\n",
      "  if (!logBlock) {\n",
      "    bot.chat(\"No wood log found nearby. Exploring...\");\n",
      "    await exploreUntil(bot, new Vec3(1, 0, 1), 60, () => {\n",
      "      const foundLog = bot.findBlock({\n",
      "        matching: block => logNames.includes(block.name),\n",
      "        maxDistance: 32\n",
      "      });\n",
      "      return foundLog;\n",
      "    });\n",
      "  }\n",
      "  bot.chat(\"Wood log found. Mining...\");\n",
      "  await mineBlock(bot, logBlock.name, 1);\n",
      "  bot.chat(\"Wood log mined.\");\n",
      "}\n",
      "\n",
      "###Score Rubrics:\n",
      "The model has access to the following predefined helper functions: `mineBlock`, `craftItem`, `smeltItem`, `placeItem` and `killMob` which may be used if needed.\n",
      "Is the response correct, accurate, and factual based on the reference answer?\n",
      "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
      "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
      "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
      "Score 4: The response is mostly correct, accurate, and factual.\n",
      "Score 5: The response is completely correct, accurate, and factual.\n",
      "\n",
      "###Feedback:\n",
      "\n",
      "The response provided a detailed explanation of how to mine wood logs in Minecraft using the given helper functions. It correctly identified that the code does not complete the task as it only provides helper functions for other tasks. The plan outlined in the response is mostly correct, accurate, and factual. However, there are minor issues with the code provided. For instance, the `exploreUntil` function should be called after finding a wood log to ensure that the bot doesn't keep exploring after mining all the logs. Also, the `mineBlock` function should take the name of the wood log as an argument instead of using the default value \"oak_log\". So the overall score is 4. [RESULT] 4"
     ]
    }
   ],
   "source": [
    "\"\"\"[The model has access to the following predefined helper functions: `mineBlock`, `craftItem`, `smeltItem`, `placeItem` and `killMob` which may be used if needed.\n",
    "Is the response correct, accurate, and factual based on the reference answer?]\n",
    "\n",
    "Score 1: The code is largely non-functional or irrelevant to the task.\n",
    "Score 2: The code attempts to address the task but is incomplete or incorrect in its approach.\n",
    "Score 3: The code is functional and makes a reasonable attempt at the task.\n",
    "Score 4: The code meets the main objective. Minor errors dont significantly impact the overall functionality.\n",
    "Score 5: The code achieves the task efficiently and handles potential errors well.\n",
    "\"\"\"\n",
    "\n",
    "EVALUATION_PROMPT = \"\"\"\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The Instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evalute:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "The model has access to the following predefined helper functions: `mineBlock`, `craftItem`, `smeltItem`, `placeItem` and `killMob` which may be used if needed.\n",
    "Is the response correct, accurate, and factual based on the reference answer?\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\n",
    "\"\"\"\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")\n",
    "settings_name = \"chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:False_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M\"\n",
    "answers = []\n",
    "answer_path = f\"./output/rag_{settings_name}.json\"\n",
    "if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "    answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "print(len(answers))\n",
    "for experiment in tqdm(answers[:1]):\n",
    "\n",
    "    print(\"\\n===========================================\")\n",
    "\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "        instruction=experiment[\"question\"],\n",
    "        response=experiment[\"generated_answer\"],\n",
    "        reference_answer=experiment[\"true_answer\"],\n",
    "    )\n",
    "    print(eval_prompt[1].content)\n",
    "    eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "    experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "    experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "    with open('logs/rag_evaluation/evaluator_debug.txt', 'w') as file:\n",
    "        # Write some text to the file\n",
    "        file.write(experiment[f\"eval_feedback_{evaluator_name}\"])\n",
    "        file.write(f\"\\n{experiment[f'eval_score_{evaluator_name}']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/rag_chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:False_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M.json    22.395833\n",
       "./output/rag_chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:True_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M.json     25.000000\n",
       "Name: eval_score_gattipg/prometheus:13b-v1.0-Q5_K_M, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)\n",
    "\n",
    "\n",
    "result[f\"eval_score_{evaluator_name}\"] = result[f\"eval_score_{evaluator_name}\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[f\"eval_score_{evaluator_name}\"] = (result[f\"eval_score_{evaluator_name}\"] - 1) / 4\n",
    "\n",
    "\n",
    "average_scores = result.groupby(\"settings\")[f\"eval_score_{evaluator_name}\"].mean()\n",
    "average_scores.sort_values()\n",
    "scaled_values = pd.Series(average_scores * 100)\n",
    "scaled_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Configuration=%{x}<br>Accuracy=%{y}<br>color=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           22.395833333333336,
           25
          ],
          "coloraxis": "coloraxis",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "outside",
         "texttemplate": "%{y:.1f}",
         "type": "bar",
         "x": [
          "./output/rag_chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:False_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M.json",
          "./output/rag_chunk:128_embeddings:flax-sentence-embeddings~st-codesearch-distilroberta-base_rerank:True_numDocsFinal:3_numRetrivedDocs:30_reader-model:deepseek-coder~33b-instruct-q5~K~M_evaluator:gattipg~prometheus~13b-v1.0-Q5~K~M.json"
         ],
         "xaxis": "x",
         "y": [
          22.395833333333336,
          25
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(0,0,255)"
          ],
          [
           1,
           "rgb(255,0,0)"
          ]
         ],
         "showscale": false
        },
        "font": {
         "size": 15
        },
        "height": 600,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>Accuracy of different RAG configurations</b>"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "RAG settings"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          100
         ],
         "ticksuffix": "%",
         "title": {
          "text": "Accuracy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the bar chart\n",
    "fig = px.bar(\n",
    "    scaled_values,\n",
    "    color=scaled_values,\n",
    "    labels={\n",
    "        \"value\": \"Accuracy\",\n",
    "        \"settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 100],\n",
    "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    "    font=dict(size=15),\n",
    ")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
