{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701a4f354c7542b7bf68078147686e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class.\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # the maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=100,  # the number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in RAW_KNOWLEDGE_BASE:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15859388a55d40558c33d4f81d3129f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320002306184438090b16ccaf8b45615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3b6da085574894a070e8e24aff6cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5c64bbebed44868886aa989a572c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d637f78f084d04a8cb5e52d9f28cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=MARKDOWN_SEPARATORS,\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n",
    "\n",
    "\n",
    "docs_processed = split_documents(\n",
    "    512,  # We choose a chunk size adapted to our model\n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    ")\n",
    "\n",
    "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
    "fig = pd.Series(lengths).hist()\n",
    "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12791d0f8e5242fbacd905c935c96255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/pacmap/pacmap.py:828: UserWarning:\n",
      "\n",
      "Warning: random state is set to 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = \"If you are following along\"\n",
    "query_vector = embedding_model.embed_query(user_query)\n",
    "\n",
    "import pacmap\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
    "\n",
    "embeddings_2d = [\n",
    "    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0]) for idx in range(len(docs_processed))\n",
    "] + [query_vector]\n",
    "\n",
    "# fit the data (The index of transformed data corresponds to the index of the original data)\n",
    "documents_projected = embedding_projector.fit_transform(np.array(embeddings_2d), init=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "Create an Endpoint\n\nAfter your first login, you will be directed to the [Endpoint creation page](htt..."
          ],
          [
           "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_cre..."
          ],
          [
           "Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through t..."
          ],
          [
           "Hugging Face Inference Endpoints documentation\n\n## Setup\n\n```bash\npip install hf-doc-builder==0.4.0 ..."
          ],
          [
           "Pricing\n\n<div class=\"flex md:justify-start mb-2 text-gray-400 items-center\">\n  <a href=\"https://ui.e..."
          ],
          [
           "## CPU Instances\n\nThe table below shows currently available CPU instances and their hourly pricing. ..."
          ],
          [
           "## GPU Instances\n\nThe table below shows currently available GPU instances and their hourly pricing. ..."
          ],
          [
           "```\ninstance hourly rate * ((hours * # min replica) + (scale-up hrs * # additional replicas))\n```\n\n#..."
          ],
          [
           "Supported Transformers & Diffusers Tasks\n\nInference Endpoints offers out-of-the-box support for Mach..."
          ],
          [
           "```\n\n### Text Classification\n\n```json\n{\n  \"inputs\": \"This sound track was beautiful! It paints the s..."
          ],
          [
           "```\n\n### Text Generation\n\n```json\n{\n  \"inputs\": \"This sound track was beautiful! It paints the sener..."
          ],
          [
           "```\n\n**Binary**\n```bash\ncurl --request POST \\\n  --url https://{ENDPOINT}/ \\\n  --header 'Content-Type..."
          ],
          [
           "```\n\n**Binary**\n\n```bash\ncurl --request POST \\\n  --url https://{ENDPOINT}/ \\\n  --header 'Content-Typ..."
          ],
          [
           "```\n\n\n### Additional parameters\n\nYou can add additional parameters, which are supported by the `pipe..."
          ],
          [
           "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your..."
          ],
          [
           "# FAQs \n\n\n\n### Q: In which regions are Inference Endpoints available?\n\nA: Inference Endpoints are cu..."
          ],
          [
           "A: Yes, your Endpoint will always stay available/up with the number of min replicas defined in the A..."
          ],
          [
           "### Q: What if I would like to deploy to a different instance type that is not listed?\n\nA: Please co..."
          ],
          [
           "Help & Support \n\nWe have a variety of Inference Endpoints blog posts to help you at https://huggingf..."
          ],
          [
           "Pause and Resume your Endpoint\n\nYou can `pause` & `resume` endpoints to save cost and configurations..."
          ],
          [
           "After clicking the button, you will be asked to confirm the action. Click \"Pause {ENDPOINT-NAME}\" to..."
          ],
          [
           "API Reference (Swagger)\n\nðŸ¤— Inference Endpoints can be used through the [UI](https://ui.endpoints.hug..."
          ],
          [
           "Use a custom Container Image\n\n\nInference Endpoints not only allows you to [customize your inference ..."
          ],
          [
           "Autoscaling\n\nAutoscaling allows you to dynamically adjust the number of endpoint replicas running yo..."
          ],
          [
           "## Scaling to 0\n\nInference Endpoints also supports autoscaling to 0, which means reducing the number..."
          ],
          [
           "Create a Private Endpoint with AWS PrivateLink\n\nSecurity and secure inference are key principles of ..."
          ],
          [
           "Once your Inference Endpoint is created successfully, go to the corresponding AWS account and add th..."
          ],
          [
           "Security & Compliance\n\nðŸ¤— Inference Endpoints is built with security and secure inference at its core..."
          ],
          [
           "Public and Protected Endpoints do not require any additional configuration. For Private Endpoints, y..."
          ],
          [
           "Send Requests to Endpoints\n\nYou can send requests to Inference Endpoints using the UI leveraging the..."
          ],
          [
           "```\n\nThe Endpoints API offers the same API definitions as the [Inference API](https://huggingface.co..."
          ],
          [
           "```\n\n### Custom handler\n\n`@huggingface/inference` supports tasks from https://huggingface.co/tasks, ..."
          ],
          [
           "Change Organization or Account\n\nInference Endpoints uses your [Hugging Face](https://huggingface.co/..."
          ],
          [
           "Update your Endpoint\n\nYou can update `running` Endpoints to change some of the configurations. Howev..."
          ],
          [
           "Advanced Setup (Instance Types, Auto Scaling, Versioning)\n\nWe have seen how fast and easy it is to d..."
          ],
          [
           "_Default: PyTorch if available._\n\n**Revision**\n\nCreate your Endpoint targeting a specific revision c..."
          ],
          [
           "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container..."
          ],
          [
           "### GPU\n\n- `transformers[sklearn,sentencepiece,audio,vision]`: `4.27.2`\n- `diffusers`: `0.14.0`\n- `a..."
          ],
          [
           "Serialization & Deserialization for Requests\n\nHugging Face Inference Endpount comes with a default s..."
          ],
          [
           "| Content-Type           | Payload                        | \n| ---------------------- | ------------..."
          ],
          [
           "| audio/webm             | `{\"inputs\": bytes(body)}`                     |\n| audio/webm;codecs=opus ..."
          ],
          [
           "Below is a list of supported `accept` headers and the serialized payload is returned.\n\n\n| Accept    ..."
          ],
          [
           "ðŸ¤— Inference Endpoints\n\nðŸ¤— Inference Endpoints offers a secure production solution to easily deploy an..."
          ],
          [
           "## Documentation and Examples\n\n* [Security & Compliance](/docs/inference-endpoints/security)\n* [Supp..."
          ],
          [
           "Access ðŸ¤— Inference Endpoints\n\nTo access the [Inference Endpoints web application](https://ui.endpoin..."
          ],
          [
           "Add custom Dependencies\n\nInference Endpointsâ€™ base image includes all required libraries to run infe..."
          ],
          [
           "Create custom Inference Handler\n\nHugging Face Endpoints supports all of the Transformers and Sentenc..."
          ],
          [
           "Included examples are for:\n\n* [Optimum and ONNX Runtime](https://huggingface.co/philschmid/distilber..."
          ],
          [
           "The code can also be found in this [Notebook](https://colab.research.google.com/drive/1hANJeRa1PK1gZ..."
          ],
          [
           "```\n# install git-lfs to interact with the repository\nsudo apt-get update\nsudo apt-get install git-l..."
          ],
          [
           "```\n!cd distilbert-base-uncased-emotion && touch handler.py\n```\n\nIn there, you define your `Endpoint..."
          ],
          [
           "```\n!echo \"holidays\" >> requirements.txt\n!pip install -r requirements.txt\n```\n\nNext, we have to adju..."
          ],
          [
           "```\n\n### 4. Test EndpointHandler\n\nTo test our EndpointHandler, we can simplify import, initialize an..."
          ],
          [
           "```\n# add all our new files\n!git add *\n# commit our files\n!git commit -m \"add custom handler\"\n# push..."
          ]
         ],
         "hovertemplate": "source=hf-endpoints-documentation<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "hf-endpoints-documentation, circle",
         "marker": {
          "color": "#EF553B",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "hf-endpoints-documentation, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          5.297929,
          4.815523,
          5.098337,
          4.77285,
          4.5355716,
          -1.9629714,
          -1.5662528,
          -2.498989,
          -7.071041,
          -4.0746827,
          -7.240684,
          7.574142,
          6.947238,
          5.176873,
          5.1584125,
          4.7304983,
          4.741742,
          5.5761743,
          4.778773,
          4.2456813,
          4.928527,
          5.044704,
          -0.5956259,
          -2.0234919,
          -2.50942,
          5.176421,
          5.1317315,
          4.912038,
          5.2908974,
          5.0242066,
          4.8110566,
          5.023294,
          5.190178,
          4.439669,
          4.4231834,
          3.6023793,
          4.285324,
          -1.666322,
          4.6213846,
          7.617654,
          7.6681404,
          7.538601,
          4.7438393,
          4.6706653,
          4.9137807,
          4.31712,
          4.5680413,
          -6.0913544,
          4.779878,
          3.9573638,
          3.4436536,
          0.7199611,
          3.2479582,
          4.361678
         ],
         "xaxis": "x",
         "y": [
          0.026428696,
          0.48340696,
          -0.28609747,
          -0.25550175,
          0.93083054,
          3.1019263,
          2.8982341,
          3.0860727,
          -1.152549,
          -3.3355346,
          -5.733697,
          -3.8256104,
          -3.3540123,
          -3.2588422,
          0.31033605,
          0.7204437,
          0.49160025,
          -1.7245802,
          0.8442769,
          0.7733391,
          0.2297154,
          0.36893624,
          -0.2434337,
          3.003668,
          3.3088608,
          0.20683692,
          0.30561927,
          0.7635272,
          0.21304886,
          0.30239508,
          0.2680531,
          0.12526117,
          -0.27101848,
          0.50341445,
          0.7106177,
          -0.038805343,
          1.2016736,
          1.6988934,
          0.295839,
          -3.6825447,
          -3.5950074,
          -3.4793384,
          1.0757699,
          0.6663195,
          0.7589808,
          0.6033254,
          0.86490715,
          -0.2749218,
          0.4621286,
          -0.33645767,
          -0.40687838,
          -2.817591,
          -1.2741976,
          -0.01111821
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Choosing a metric for your task\n\n**So you've trained your model and want to see how well itâ€™s doing ..."
          ],
          [
           "```\n>>> precision_metric = evaluate.load(\"precision\")\n>>> results = precision_metric.compute(referen..."
          ],
          [
           "```\n\n### Task-specific metrics\n\nPopular ML tasks like Machine Translation and Named Entity Recogniti..."
          ],
          [
           "<Tip warning={true}>\nðŸ’¡\nGLUE is actually a collection of different subsets on different tasks, so fir..."
          ],
          [
           "```\n>>> from evaluate import load\n>>> squad_metric = load(\"squad\")\n>>> predictions = [{'prediction_t..."
          ],
          [
           "--\ntitle: poseval\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: a..."
          ],
          [
           "`references`: a list of lists of reference labels, i.e. the ground truth/target values.\n\nIt can also..."
          ],
          [
           "```\n\n## Output values\n\nThis metric returns a a classification report as a dictionary with a summary ..."
          ],
          [
           "`f1`: the average [F1 score](https://huggingface.co/metrics/f1), on a scale between 0.0 and 1.0.\n\n\n#..."
          ],
          [
           "```\n\n## Limitations and bias\n\nIn contrast to [seqeval](https://github.com/chakki-works/seqeval), the..."
          ],
          [
           "--\ntitle: MAPE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "```\n\n### Inputs\n\nMandatory inputs: \n- `predictions`: numeric array-like of shape (`n_samples,`) or (..."
          ],
          [
           "```\n\nIf `multioutput=\"raw_values\"`:\n```python\n{'mape': array([0.5, 1. ])}\n```\n\n#### Values from Popu..."
          ],
          [
           "```\n\n## Limitations and Bias\nOne limitation of MAPE is that it cannot be used if the ground truth is..."
          ],
          [
           "--\ntitle: ROUGE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app..."
          ],
          [
           "```\n\nOne can also pass a custom tokenizer which is especially useful for non-latin languages.\n```pyt..."
          ],
          [
           "```\n```\n\n### Inputs\n- **predictions** (`list`): list of predictions to score. Each prediction\n      ..."
          ],
          [
           "```\n\nThe ROUGE values are in the range of 0 to 1.\n\n\n#### Values from Popular Papers\n\n\n### Examples\nA..."
          ],
          [
           "```\n\n## Limitations and Bias\nSee [Schluter (2017)](https://aclanthology.org/E17-2007/) for an in-dep..."
          ],
          [
           "--\ntitle: Word Length\nemoji: ðŸ¤—\ncolorFrom: green\ncolorTo: purple\nsdk: gradio\nsdk_version: 3.0.2\napp_f..."
          ],
          [
           "```\n\nThis metric outputs a dictionary containing the number of words in the input string (`word leng..."
          ],
          [
           "Working with Keras and Tensorflow\n\n\n\nEvaluate can be easily intergrated into your Keras and Tensorfl..."
          ],
          [
           "x_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n\n\nmodel = keras.Sequential..."
          ],
          [
           "```\n\n## Callbacks\n\nSuppose we want to keep track of model metrics while a model is training. We can ..."
          ],
          [
           "```\n\n## Using an Evaluate Metric for... Evaluation!\n\nWe can also use the same metric after model tra..."
          ],
          [
           "--\ntitle: CharCut\nemoji: ðŸ”¤\ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: ap..."
          ],
          [
           "```\n\n## Citation\n```bibtex\n@inproceedings{lardilleux-lepage-2017-charcut,\n    title = \"{CHARCUT}: Hu..."
          ],
          [
           "--\ntitle: IndicGLUE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the mo..."
          ],
          [
           "```\n    \n## Output values\n\nThe output of the metric depends on the IndicGLUE subset chosen, consisti..."
          ],
          [
           "```\n\nMinimal values for the Wiki-NER subset (which outputs `accuracy` and `f1`):\n\n```python\n>>> indi..."
          ],
          [
           "```\n    \n## Further References \n- [IndicNLP website](https://indicnlp.ai4bharat.org/home/)..."
          ],
          [
           "--\ntitle: Google BLEU\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_fil..."
          ],
          [
           "The minimum value of precision and recall is then returned as the score.\n\n\n## Intended Uses\nThis met..."
          ],
          [
           "```\n\n### Inputs\n- **predictions** (list of str): list of translations to score.\n- **references** (li..."
          ],
          [
           "```\n\n#### Values from Popular Papers\n\n\n### Examples\nExample with one reference per sample:\n```python..."
          ],
          [
           "```\n\nExample with multiple references for the first sample, and with `min_len` adjusted to `2`, inst..."
          ],
          [
           "```\n\nExample with multiple references for the first sample, with `min_len` adjusted to `2`, instead ..."
          ],
          [
           "```\n\n## Limitations and Bias\n\nThe GoogleBLEU metric does not come with a predefined tokenization fun..."
          ],
          [
           "--\ntitle: \nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.py\np..."
          ],
          [
           "```\n\nFrugalScore calculates how good are the predictions given some references, based on a set of sc..."
          ],
          [
           "```\n\nPartial values: \n\n```python\n>>> frugalscore = evaluate.load(\"frugalscore\")\n>>> results = frugal..."
          ],
          [
           "```\n\n## Limitations and bias\n\nFrugalScore is based on [BertScore](https://huggingface.co/metrics/ber..."
          ],
          [
           "| FrugalScore                                        | Student     | Teacher        | Method     |\n|..."
          ],
          [
           "| [moussaKam/frugalscore_medium_roberta_bert-score](https://huggingface.co/moussaKam/frugalscore_med..."
          ],
          [
           "Depending on the size of the model picked, the loading time will vary: the `tiny` models will load v..."
          ],
          [
           "```\n\n## Further References\n- [Original FrugalScore code](https://github.com/moussaKam/FrugalScore)\n-..."
          ],
          [
           "--\ntitle: Mean IoU\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: ..."
          ],
          [
           "```\n\n### Inputs\n**Mandatory inputs**\n- `predictions` (`List[ndarray]`): List of predicted segmentati..."
          ],
          [
           "The values of all of the scores reported range from from `0.0` (minimum) and `1.0` (maximum).\n\nOutpu..."
          ],
          [
           "```\n\n#### Values from Popular Papers\n\nThe [leaderboard for the CityScapes dataset](https://paperswit..."
          ],
          [
           "### Examples\n\n```python\n>>> import numpy as np\n>>> mean_iou = evaluate.load(\"mean_iou\")\n>>> # suppos..."
          ],
          [
           "## Limitations and Bias\nMean IOU is an average metric, so it will not show you where model predictio..."
          ],
          [
           "```\n\n\n## Further References\n- [Wikipedia article - Jaccard Index](https://en.wikipedia.org/wiki/Jacc..."
          ],
          [
           "--\ntitle: SuperGLUE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the mo..."
          ],
          [
           "```\n## Output values\n\nThe output of the metric depends on the SuperGLUE subset chosen, consisting of..."
          ],
          [
           "```\n\nMinimal values for the MultiRC subset (which outputs `pearson` and `spearmanr`):\n\n```python\nfro..."
          ],
          [
           "```\n\n## Limitations and bias\nThis metric works only with datasets that have the same format as the [..."
          ],
          [
           "ðŸ¤— Transformers\n\nTo run the ðŸ¤— Transformers examples make sure you have installed the following librar..."
          ],
          [
           "```\n\n## Trainer\n\nThe metrics in `evaluate` can be easily integrated with the [`~transformers.Trainer..."
          ],
          [
           "trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n ..."
          ],
          [
           "```\n\n## Seq2SeqTrainer\n\nWe can use the [`~transformers.Seq2SeqTrainer`] for sequence-to-sequence tas..."
          ],
          [
           "model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_billsum = billsum.ma..."
          ],
          [
           "trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_bills..."
          ],
          [
           "```\n\nYou can use any `evaluate` metric with the `Trainer` and `Seq2SeqTrainer` as long as they are c..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nYou can adapt the `--build_dir` to set any temporary folder that you prefer. This command will ..."
          ],
          [
           "```\n\nUse the relative style to link to the new file so that the versioned docs continue to work.\n\nFo..."
          ],
          [
           "```\n## XXXConfig\n\n[[autodoc]] XXXConfig\n```\n\nThis will include every public method of the configurat..."
          ],
          [
           "```\n## XXXTokenizer\n\n[[autodoc]] XXXTokenizer\n    - all\n    - __call__\n```\n\n### Writing source docum..."
          ],
          [
           "```\n\nIf the description is too long to fit in one line, another indentation is necessary before writ..."
          ],
          [
           "```\n```\n# first line of code\n# second line\n# etc\n```\n````\n\nWe follow the [doctest](https://docs.pyth..."
          ],
          [
           "```\n\n#### Adding an image\n\nDue to the rapidly growing repository, it is important to make sure that ..."
          ],
          [
           "--\ntitle: Spearman Correlation Coefficient Metric \nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradi..."
          ],
          [
           "## How to Use\nAt minimum, this metric only requires a `list` of predictions and a `list` of referenc..."
          ],
          [
           "```\n\n### Inputs\n- **`predictions`** (`list` of `float`): Predicted labels, as returned by a model.\n-..."
          ],
          [
           "```\n\nThe same example, but that also returns the pvalue:\n```python\n>>> spearmanr_metric = evaluate.l..."
          ],
          [
           "```\n\n## Limitations and Bias\n\n\n## Citation\n```bibtex\n@book{kokoska2000crc,\n  title={CRC standard pro..."
          ],
          [
           "--\ntitle: TREC Eval\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "```\n\n### Inputs\n- **predictions** *(dict): a single retrieval run.*\n    - **query** *(int): Query ID..."
          ],
          [
           "### Output Values\n- **runid** *(str): Run name.*  \n- **num_ret** *(int): Number of retrieved documen..."
          ],
          [
           "```\n\nA more realistic use case with an examples from [`trectools`](https://github.com/joaopalotti/tr..."
          ],
          [
           "```\n\n```python\nresult\n\n{'runid': 'InexpC2',\n 'num_ret': 100000,\n 'num_rel': 6074,\n 'num_rel_ret': 31..."
          ],
          [
           "```\n\n## Limitations and Bias\nThe `trec_eval` metric requires the inputs to be in the TREC run and qr..."
          ],
          [
           "A quick tour\n\nðŸ¤— Evaluate provides access to a wide range of evaluation tools. It covers a range of m..."
          ],
          [
           "```\n\nIf you want to make sure you are loading the right type of evaluation (especially if there are ..."
          ],
          [
           "```\n\n## Module attributes\n\nAll evalution modules come with a range of useful attributes that help to..."
          ],
          [
           "```\n\nYou can see that it describes how the metric works in theory. If you use this metric for your w..."
          ],
          [
           "```\n\n<Tip>\n\nNote that features always describe the type of a single input element. In general we wil..."
          ],
          [
           "```\nEvaluation modules return the results in a dictionary. However, in some instances you build up t..."
          ],
          [
           "```\n\n### Distributed evaluation\n\nComputing metrics in a distributed environment can be tricky. Metri..."
          ],
          [
           "```\n\nThe `combine` function accepts both the list of names of the metrics as well as an instantiated..."
          ],
          [
           "```\n\nThe content of the JSON file look like the following:\n\n```json\n{\n    \"experiment\": \"run 42\",\n  ..."
          ],
          [
           "```\n\n## Evaluator\n\nThe [`evaluate.evaluator`] provides automated evaluation and only requires a mode..."
          ],
          [
           "```\n\nCalculating the value of the metric alone is often not enough to know if a model performs signi..."
          ],
          [
           "```\n\nThe evaluator expects a `\"text\"` and `\"label\"` column for the data input. If your dataset diffe..."
          ],
          [
           "```\n\nWhich lets you visually compare the 4 models and choose the optimal one for you, based on one o..."
          ],
          [
           "```\n\nEvaluation can be run by loading the `EvaluationSuite` and calling the `run()` method with a mo..."
          ],
          [
           "Using the `evaluator` with custom pipelines\n\nThe evaluator is designed to work with `transformer` pi..."
          ],
          [
           "```\n\nFollowing the convention in the `TextClassificationPipeline` of `transformers` our pipeline sho..."
          ],
          [
           "```\n\nThis snippet shows how we can use the `polarity` feature added with `spacytextblob` to get the ..."
          ],
          [
           "--\ntitle: Matthews Correlation Coefficient\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_ve..."
          ],
          [
           "```\n\n### Inputs\n- **`predictions`** (`list` of `int`s): Predicted class labels.\n- **`references`** (..."
          ],
          [
           "```\n\nThe same example as above, with sample weights that cause a negative correlation:\n```python\n>>>..."
          ],
          [
           "Evaluator\n\nThe evaluator classes for automatic evaluation.\n\n## Evaluator classes\n\nThe main entry poi..."
          ],
          [
           "Using the `evaluator`\n\nThe `Evaluator` classes allow to evaluate a  triplet of model, dataset, and m..."
          ],
          [
           "## Text classification\n\nThe text classification evaluator can be used to evaluate text models on cla..."
          ],
          [
           "# 2. Pass an instantiated model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/..."
          ],
          [
           "```\n<Tip>\n\nWithout specifying a device, the default for model inference will be the first GPU on the..."
          ],
          [
           "```\n\nNext let's have a look at token classification.\n\n## Token Classification\n\nWith the token classi..."
          ],
          [
           "```python\nimport pandas as pd\nfrom datasets import load_dataset\nfrom evaluate import evaluator\nfrom ..."
          ],
          [
           "```\n\nThe result is a table that looks like this:..."
          ],
          [
           "|   model                                                            |   overall_f1 |   overall_accu..."
          ],
          [
           "| philschmid/distilroberta-base-ner-conll2003                        |        0.961 |              0..."
          ],
          [
           "### Visualizing results\n\nYou can feed in the `results` list above into the `plot_radar()` function t..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/evaluate/media/..."
          ],
          [
           "```python\nfrom datasets import load_dataset\nfrom evaluate import evaluator\n\ntask_evaluator = evaluat..."
          ],
          [
           "```\n\nResults include confidence intervals as well as error estimates as follows:\n\n```python\n{\n    'e..."
          ],
          [
           "```\n\nSince we are using `datasets` to store data we make use of a technique called memory mappings. ..."
          ],
          [
           "--\ntitle: Exact Match\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_fil..."
          ],
          [
           "```\n\n### Inputs\n- **`predictions`** (`list` of `str`): List of predicted texts.\n- **`references`** (..."
          ],
          [
           "```\n\nThis metric's range is 0-1, inclusive. Here, 0.0 means no prediction/reference pairs were match..."
          ],
          [
           "```\nNote that in the example above, because the regexes are ignored before the case is normalized, \"..."
          ],
          [
           "```\n\nAn example that includes sentences:\n```python\n>>> exact_match = evaluate.load(\"exact_match\")\n>>..."
          ],
          [
           "--\ntitle: Wilcoxon\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: green\nsdk: gradio\nsdk_version: 3.0.2\napp_file:..."
          ],
          [
           "--\ntitle: SQuAD v2\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: ..."
          ],
          [
           "## How to use \n\nThe metric takes two files or two lists - one representing model predictions and the..."
          ],
          [
           "```\n## Output values\n\nThis metric outputs a dictionary with 13 values: \n* `'exact'`: Exact match (th..."
          ],
          [
           "The range of `total` depends on the length of predictions/references: its minimal value is 0, and ma..."
          ],
          [
           "```\n\nMinimal values for both exact match and F1 (no match):\n\n```python\nfrom evaluate import load\nsqu..."
          ],
          [
           "```\n\nPartial match (2 out of 3 answers correct) : \n\n```python\nfrom evaluate import load\nsquad_metric..."
          ],
          [
           "```\n\n## Limitations and bias\nThis metric works only with the datasets in the same format as the [SQu..."
          ],
          [
           "--\ntitle: BLEU\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "## How to Use\n\nThis metric takes as input a list of predicted sentences and a list of lists of refer..."
          ],
          [
           "```\n\n### Inputs\n- **predictions** (`list` of `str`s): Translations to score.\n- **references** (`list..."
          ],
          [
           "Output Example:\n```python\n{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, ..."
          ],
          [
           "```\n\nBLEU's output is always a number between 0 and 1. This value indicates how similar the candidat..."
          ],
          [
           "```\n\nExample where the second prediction has 2 references:\n```python\n>>> predictions = [\n...     [\"h..."
          ],
          [
           "```\n\n## Limitations and Bias\nThis metric has multiple known limitations:\n- BLEU compares overlap in ..."
          ],
          [
           "--\ntitle: Pearson Correlation Coefficient \nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_ve..."
          ],
          [
           "```\n\n\n### Inputs\n- **predictions** (`list` of `int`): Predicted class labels, as returned by a model..."
          ],
          [
           "```\n\nExample 2-The same as Example 1, but that also returns the `p-value`.\n```python\n>>> pearsonr_me..."
          ],
          [
           "```\n\n\n## Limitations and Bias\n\nAs stated above, the calculation of the p-value relies on the assumpt..."
          ],
          [
           "--\ntitle: Code Eval\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "```\n\nN.B.\nThis metric exists to run untrusted model-generated code. Users are strongly encouraged no..."
          ],
          [
           "```\n\nNo match for k = 1:\n\n```python\nfrom evaluate import load\ncode_eval = load(\"code_eval\")\ntest_cas..."
          ],
          [
           "```\n\n## Limitations and bias\n\nAs per the warning included in the metric code itself:\n> This program ..."
          ],
          [
           "More information about the limitations of the code can be found on the [Human Eval Github repository..."
          ],
          [
           "```\n    \n## Further References \n\n- [Human Eval Github repository](https://github.com/openai/human-ev..."
          ],
          [
           "p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/evaluate/media/resolve/main..."
          ],
          [
           "ðŸ¤— Evaluate is a library that makes evaluating and comparing models and reporting their performance e..."
          ],
          [
           "# Installation\n\n## With pip\n\nðŸ¤— Evaluate can be installed from PyPi and has to be installed in a virt..."
          ],
          [
           "```\n\n# Usage\n\nðŸ¤— Evaluate's main methods are:\n\n- `evaluate.list_evaluation_modules()` to list the ava..."
          ],
          [
           "Types of Evaluations in ðŸ¤— Evaluate\n\nThe goal of the ðŸ¤— Evaluate library is to support different types..."
          ],
          [
           "Comparisons have yet to be systematically used when comparing and reporting model performance, howev..."
          ],
          [
           "--\ntitle: WER\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.p..."
          ],
          [
           "This problem is solved by first aligning the recognized word sequence with the reference (spoken) wo..."
          ],
          [
           "```\n## Output values\n\nThis metric outputs a float representing the word error rate.\n\n```\nprint(wer_s..."
          ],
          [
           "```\n\nNo match between prediction and reference:\n\n```python\nfrom evaluate import load\nwer = load(\"wer..."
          ],
          [
           "--\ntitle: chrF\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "See the [sacreBLEU README.md](https://github.com/mjpost/sacreBLEU#chrf--chrf) for more information.\n..."
          ],
          [
           "```\n\nThe chrF(++) score can be any value between `0.0` and `100.0`, inclusive.\n\n#### Values from Pop..."
          ],
          [
           "```\n\nThe same chrF++ example as above, but with `lowercase=True` to normalize all case:\n```python\n>>..."
          ],
          [
           "--\ntitle: Regard\nemoji: ðŸ¤—\ncolorFrom: green\ncolorTo: purple\nsdk: gradio\nsdk_version: 3.0.2\napp_file: ..."
          ],
          [
           "```\n\n### Inputs\n- **data** (list of `str`): prediction/candidate sentences, e.g. sentences describin..."
          ],
          [
           "```\n\nWith the `aggregation='maximum'` option, this measurement will output the maximum regard for ea..."
          ],
          [
           "```\n\nExample 3 (returns the maximum regard score):\n```python\n>>> regard = evaluate.load(\"regard\", \"c..."
          ],
          [
           "```\n\nExample 4 (returns the average regard score):\n```python\n>>> regard = evaluate.load(\"regard\", \"c..."
          ],
          [
           "--\ntitle: Honest\nemoji: ðŸ¤—\ncolorFrom: blue\ncolorTo: green\nsdk: gradio\nsdk_version: 3.0.2\napp_file: ap..."
          ],
          [
           "```\n\nArguments:\n    **predictions** (list of list of `str`): a list of completions to [HONEST prompt..."
          ],
          [
           "| Model Name       | Top K =1 | Top K =5 |Top K =20 |\n| ---------------- | -------- | -------- | ---..."
          ],
          [
           "## Examples\n\nExample 1: Calculating HONEST without groups\n\n```python\n>>> honest = evaluate.load('hon..."
          ],
          [
           "```\n\nExample 2: Calculating HONEST with 2 groups (e.g. male/female)\n```python\n>>> honest = evaluate...."
          ],
          [
           "```\n\n## Citation\n\n```bibtex\n@inproceedings{nozza-etal-2021-honest,\n    title = {\"{HONEST}: Measuring..."
          ],
          [
           "--\ntitle: SQuAD\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app..."
          ],
          [
           "```\n{'exact_match': 100.0, 'f1': 100.0}\n```\n\nThe range of `exact_match` is 0-100, where 0.0 means no..."
          ],
          [
           "```\n\nMinimal values for both exact match and F1 (no match):\n\n```python\nfrom evaluate import load\nsqu..."
          ],
          [
           "```\n\nPartial match (2 out of 3 answers correct) : \n\n```python\nfrom evaluate import load\nsquad_metric..."
          ],
          [
           "```\n\n## Limitations and bias\nThis metric works only with datasets that have the same format as [SQuA..."
          ],
          [
           "--\ntitle: Recall\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: ap..."
          ],
          [
           "```\n```python\n{'recall': array([1., 0., 0.])}\n```\n\nThis metric outputs a dictionary with one entry, ..."
          ],
          [
           "```\n\nExample 4-A multiclass example, using different averages.\n```python\n>>> recall_metric = evaluat..."
          ],
          [
           "--\ntitle: BERT Score\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file..."
          ],
          [
           "```python\nfrom evaluate import load\nbertscore = load(\"bertscore\")\npredictions = [\"hello there\", \"gen..."
          ],
          [
           "`recall`: The [recall](https://huggingface.co/metrics/recall) for each sentence from the `prediction..."
          ],
          [
           "```\n\nPartial match with the `distilbert-base-uncased` model:\n\n```python\nfrom evaluate import load\nbe..."
          ],
          [
           "--\ntitle: Competition MATH\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\nap..."
          ],
          [
           "```\n\nN.B. To be able to use Competition MATH, you need to install the `math_equivalence` dependency ..."
          ],
          [
           "```\n\nPartial match:\n\n```python\n>>> from evaluate import load\n>>> math = load(\"competition_math\")\n>>>..."
          ],
          [
           "--\ntitle: MSE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.p..."
          ],
          [
           "```\n\nIf `multioutput=\"raw_values\"`:\n```python\n{'mse': array([0.41666667, 1. ])}\n```\n\n#### Values fro..."
          ],
          [
           "--\ntitle: WikiSplit\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "```\n\n### Values from popular papers\n\nThis metric was initially used by [Rothe et al.(2020)](https://..."
          ],
          [
           "```\n\nNo match between prediction and reference:\n\n```python\n>>> wiki_split = evaluate.load(\"wiki_spli..."
          ],
          [
           "--\ntitle: r_squared\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.0.2\napp_file: ..."
          ],
          [
           "```\n\n### How to Use Examples:\n\nThe R2 class in the evaluate module can be used to compute the R^2 va..."
          ],
          [
           "```\n\n## Further References\n\n- [The Open University: R-Squared](https://www.open.edu/openlearn/ocw/mo..."
          ],
          [
           "--\ntitle: Precision\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "```\n```python\n{'precision': array([0.66666667, 0.0, 0.0])}\n```\n\n\n\n\n#### Values from Popular Papers\n\n..."
          ],
          [
           "--\ntitle: XTREME-S\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: ..."
          ],
          [
           "```\n\n2. **Calculating the metric**: the metric takes two inputs : \n\n- `predictions`: a list of predi..."
          ],
          [
           "```\n\nIt also has two optional arguments: \n\n- `bleu_kwargs`: a `dict` of keywords to be passed when c..."
          ],
          [
           "- `cer`:  Character error rate (CER) is similar to WER, but operates on character instead of word. T..."
          ],
          [
           "```\n\nFor the `covost2` subset (which outputs `bleu`):\n\n```python\n>>> xtreme_s_metric = evaluate.load..."
          ],
          [
           "```\n\n## Limitations and bias\nThis metric works only with datasets that have the same format as the [..."
          ],
          [
           "--\ntitle: CER\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.p..."
          ],
          [
           "```\n## Output values\n\nThis metric outputs a float representing the character error rate.\n\n```\nprint(..."
          ],
          [
           "```\n\nNo match between prediction and reference:\n\n```python\nfrom evaluate import load\ncer = load(\"cer..."
          ],
          [
           "--\ntitle: {{ cookiecutter.module_name }}\ndatasets:\n- {{ cookiecutter.dataset_name }} \ntags:\n- evalua..."
          ],
          [
           "#### Values from Popular Papers\n*Give examples, preferrably with links to leaderboards or publicatio..."
          ],
          [
           "--\ntitle: McNemar\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: green\nsdk: gradio\nsdk_version: 3.0.2\napp_file: ..."
          ],
          [
           "`stat`: The McNemar statistic.\n\n`p`: The p value.\n\n## Examples \n\nExample comparison:\n\n```python\nmcne..."
          ],
          [
           "```\n\n## Limitations and bias\n\nThe McNemar test is a non-parametric test, so it has relatively few as..."
          ],
          [
           "--\ntitle: F1\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.py..."
          ],
          [
           "```\n```python\n{'f1': array([0.8, 0.0, 0.0])}\n```\n\nThis metric outputs a dictionary, with either a si..."
          ],
          [
           "Considerations for model evaluation\n\nDeveloping an ML model is rarely a one-shot deal: it often invo..."
          ],
          [
           "## The impact of class imbalance\n\nWhile many academic datasets, such as the [IMDb dataset](https://h..."
          ],
          [
           "In cases where there is an imbalance, using [F1 score](https://huggingface.co/metrics/f1) can be a b..."
          ],
          [
           "### Interpretability\n\nWhen evaluating models, **interpretability** (i.e. the ability to *interpret* ..."
          ],
          [
           "Memory footprint refers to the size of the model weights and how much hardware memory they occupy. I..."
          ],
          [
           "--\ntitle: Mahalanobis Distance\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19...."
          ],
          [
           "```\n\n#### Values from Popular Papers\n*N/A*\n\n### Example\n\n```python\n>>> mahalanobis_metric = evaluate..."
          ],
          [
           "--\ntitle: MAUVE\nemoji: ðŸ¤—\ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "```\n\nIt also has several optional arguments:\n\n`num_buckets`: the size of the histogram to quantize P..."
          ],
          [
           "`seed`: random seed to initialize k-means cluster assignments, randomly assigned by default.\n    \n\n\n..."
          ],
          [
           "```\n\nPartial match between prediction and reference:\n\n```python\nfrom evaluate import load\nmauve = lo..."
          ],
          [
           "```\n\n## Limitations and bias\n\nThe [original MAUVE paper](https://arxiv.org/abs/2102.01454) did not a..."
          ],
          [
           "For MAUVE to be large, the model distribution must be close to the human text distribution as seen b..."
          ],
          [
           "```\n\n## Further References\n- [Official MAUVE implementation](https://github.com/krishnap25/mauve)\n- ..."
          ],
          [
           "--\ntitle: BLEURT\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: ap..."
          ],
          [
           "```\n\n### Inputs\n- **predictions** (`list` of `str`s): List of generated sentences to score.\n- **refe..."
          ],
          [
           "```\n\nBLEURT's output is always a number between 0 and (approximately 1). This value indicates how si..."
          ],
          [
           "```\n\n## Limitations and Bias\nThe [original BLEURT paper](https://arxiv.org/pdf/2004.04696.pdf) showe..."
          ],
          [
           "--\ntitle: Label Distribution\nemoji: ðŸ¤—\ncolorFrom: green\ncolorTo: purple\nsdk: gradio\nsdk_version: 3.0...."
          ],
          [
           "```\n\nIf skewness is 0, the dataset is perfectly balanced; if it is less than -1 or greater than 1, t..."
          ],
          [
           "```\n\n## Limitations and Bias\nWhile label distribution can be a useful signal for analyzing datasets ..."
          ],
          [
           "--\ntitle: XNLI\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "```\n\n## Output values\n\nThe output of the XNLI metric is simply the `accuracy`, i.e. the proportion o..."
          ],
          [
           "```\n\n## Limitations and bias\n\nWhile accuracy alone does give a certain indication of performance, it..."
          ],
          [
           "--\ntitle: Text Duplicates\nemoji: ðŸ¤—\ncolorFrom: green\ncolorTo: purple\nsdk: gradio\nsdk_version: 3.0.2\na..."
          ],
          [
           "```\n\nWarning: the `list_duplicates=True` function can be memory-intensive for large datasets.\n\n### E..."
          ],
          [
           "Creating an EvaluationSuite\n\nIt can be useful to evaluate models on a variety of different tasks to ..."
          ],
          [
           "The mandatory attributes for a new `SubTask` are `task_type` and `data`.\n1. [`task_type`] maps to th..."
          ],
          [
           "```\n\nAn `EvaluationSuite` can be loaded by name from the Hugging Face Hub, or locally by providing a..."
          ],
          [
           "--\ntitle: MAE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.p..."
          ],
          [
           "```\n\n### Inputs\n\nMandatory inputs: \n- `predictions`: numeric array-like of shape (`n_samples,`) or (..."
          ],
          [
           "```\n\nIf `multioutput=\"raw_values\"`:\n```python\n{'mae': array([0.5, 1. ])}\n```\n\n#### Values from Popul..."
          ],
          [
           "```\n\n## Limitations and Bias\nOne limitation of MAE is that the relative size of the error is not alw..."
          ],
          [
           "--\ntitle: GLUE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "### Values from popular papers\nThe [original GLUE paper](https://huggingface.co/datasets/glue) repor..."
          ],
          [
           "```\n\nMinimal values for the STSB subset (which outputs `pearson` and `spearmanr`):\n\n```python\nfrom e..."
          ],
          [
           "Scikit-Learn\n\nTo run the scikit-learn examples make sure you have installed the following library:\n\n..."
          ],
          [
           "```\n\nAlternatively X and y can be obtained directly from the frame attribute:\n\n```python\nX = titanic..."
          ],
          [
           "```\n\nYou can use any suitable `evaluate` metric with the estimators as long as they are compatible w..."
          ],
          [
           "Logging methods\n\nðŸ¤— Evaluate strives to be transparent and explicit about how it works, but this can ..."
          ],
          [
           "```\n\nAll the methods of this logging module are documented below. The main ones are:\n\n- [`logging.ge..."
          ],
          [
           "[[autodoc]] evaluate.logging.enable_progress_bar\n\n[[autodoc]] evaluate.logging.disable_progress_bar\n..."
          ],
          [
           "Installation\n\nBefore you start, you will need to setup your environment and install the appropriate ..."
          ],
          [
           "--\ntitle: Word Count\nemoji: ðŸ¤—\ncolorFrom: green\ncolorTo: purple\nsdk: gradio\nsdk_version: 3.0.2\napp_fi..."
          ],
          [
           "```\n\nExample for a multiple strings\n```python\n>>> data = [\"hello sun and goodbye moon\", \"foo bar foo..."
          ],
          [
           "--\ntitle: seqeval\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: a..."
          ],
          [
           "`scheme`: the target tagging scheme, which can be one of [`IOB1`, `IOB2`, `IOE1`, `IOE2`, `IOBES`, `..."
          ],
          [
           "More recently, seqeval continues being used for reporting performance on tasks such as [named entity..."
          ],
          [
           "```\n\nMinimal values (no match):\n\n```python\n>>> seqeval = evaluate.load('seqeval')\n>>> predictions = ..."
          ],
          [
           "```\n\nPartial match:\n\n```python\n>>> seqeval = evaluate.load('seqeval')\n>>> predictions = [['O', 'O', ..."
          ],
          [
           "p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/evaluate/media/resolve/main..."
          ],
          [
           "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./t..."
          ],
          [
           "--\ntitle: SARI\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "SARI can be computed as:\n\n`sari = ( F1_add + F1_keep + P_del) / 3`\n\nwhere \n\n`F1_add` is the n-gram F..."
          ],
          [
           "```\n## Output values\n\nThis metric outputs a dictionary with the SARI score:\n\n```\nprint(sari_score)\n{..."
          ],
          [
           "```\n\nPartial match between prediction and reference:\n\n```python\nfrom evaluate import load\nsari = loa..."
          ],
          [
           "--\ntitle: METEOR\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: ap..."
          ],
          [
           "## How to use \n\nMETEOR has two mandatory arguments:\n\n`predictions`: a `list` of predictions to score..."
          ],
          [
           "```\n\n## Output values\n\nThe metric outputs a dictionary containing the METEOR score. Its values range..."
          ],
          [
           "```\n\nMultiple `references` per `prediction`, partial match:\n\n```python\n>>> meteor = evaluate.load('m..."
          ],
          [
           "```\n    \n## Further References \n- [METEOR -- Wikipedia](https://en.wikipedia.org/wiki/METEOR)\n- [MET..."
          ],
          [
           "--\ntitle: CharacTER\nemoji: ðŸ”¤\ncolorFrom: orange\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file..."
          ],
          [
           "```\n\n### Inputs\n- **predictions**: a single prediction or a list of predictions to score. Each predi..."
          ],
          [
           "```\n\n## Citation\n```bibtex\n@inproceedings{wang-etal-2016-character,\n    title = \"{C}harac{T}er: Tran..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "## Submitting a new issue or feature request\n\nFollowing these guidelines when submitting an issue or..."
          ],
          [
           "### Did you find a bug?\n\nThank you for reporting an issue. If the bug is related to a community metr..."
          ],
          [
           "```\n\n3. Create a new branch to hold your development changes:\n\n   ```bash\n   $ git checkout -b a-des..."
          ],
          [
           "```\n\n   ðŸ¤— Evaluate also uses `flake8` and a few custom scripts to check for coding mistakes. Quality..."
          ],
          [
           "```\n\n6. Once you are satisfied, go to the webpage of your fork on GitHub. Click on 'Pull request' to..."
          ],
          [
           "### Develop on Windows\n\nOn Windows, you need to configure git to transform Windows `CRLF` line endin..."
          ],
          [
           "```\n$ git checkout -b your-branch-for-syncing\n$ git pull --squash --no-commit upstream main\n$ git co..."
          ],
          [
           "--\ntitle: MASE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "```\n\n### Inputs\n\nMandatory inputs: \n- `predictions`: numeric array-like of shape (`n_samples,`) or (..."
          ],
          [
           "```\n\nIf `multioutput=\"raw_values\"`:\n```python\n{'mase': array([0.5, 1. ])}\n```\n\n#### Values from Popu..."
          ],
          [
           "```\n\n## Limitations and Bias\n\n\n## Citation(s)\n\n```bibtex\n@article{HYNDMAN2006679,\n    title = {Anoth..."
          ],
          [
           "--\ntitle: sMAPE\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app..."
          ],
          [
           "```\n\n### Inputs\n\nMandatory inputs: \n- `predictions`: numeric array-like of shape (`n_samples,`) or (..."
          ],
          [
           "```\n\nIf `multioutput=\"raw_values\"`:\n```python\n{'smape': array([0.5, 1.5 ])}\n```\n\n#### Values from Po..."
          ],
          [
           "```\n\n## Further References\n- [Symmetric Mean absolute percentage error - Wikipedia](https://en.wikip..."
          ],
          [
           "Saving methods\n\nMethods for saving evaluations results:\n\n## Save\n\n[[autodoc]] evaluate.save..."
          ],
          [
           "--\ntitle: Exact Match \nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: green\nsdk: gradio\nsdk_version: 3.0.2\napp_f..."
          ],
          [
           "```\n\n```python\n>>> exact_match = evaluate.load(\"exact_match\", module_type=\"comparison\")\n>>> results ..."
          ],
          [
           "--\ntitle: Perplexity\nemoji: ðŸ¤—\ncolorFrom: green\ncolorTo: purple\nsdk: gradio\nsdk_version: 3.0.2\napp_fi..."
          ],
          [
           "```\n\n### Inputs\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on..."
          ],
          [
           "```\n\nThe range of this metric is [0, inf). A lower score is better.\n\n#### Values from Popular Papers..."
          ],
          [
           "--\ntitle: Accuracy\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: ..."
          ],
          [
           "```\n\nThis metric outputs a dictionary, containing the accuracy score.\n\n\n#### Values from Popular Pap..."
          ],
          [
           "--\ntitle: RL Reliability\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_..."
          ],
          [
           "```\n\n\n### Inputs\n- **timesteps** *(List[int]): For each run a an list/array with its timesteps.*\n- *..."
          ],
          [
           "### Output Values\n\nIn `\"online\"` mode:\n- HighFreqEnergyWithinRuns: High Frequency across Time (DT)\n-..."
          ],
          [
           "```\n\nLoad the sample data:\n```python\ndfs = [pd.read_csv(f\"./csv_data/sac_humanoid_{i}_train.csv\") fo..."
          ],
          [
           "--\ntitle: \nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.py\np..."
          ],
          [
           "5\tPart-of-Speech\n  6\tParse bit\tThis is the bracketed structure broken before the first open parenthe..."
          ],
          [
           "## Metric description\n\nCoVal is a coreference evaluation tool for the [CoNLL](https://huggingface.co..."
          ],
          [
           "```python\nfrom evaluate import load\ncoval = load('coval')\nwords = ['bc/cctv/00/cctv_0005   0   0    ..."
          ],
          [
           "## Examples \n\nMaximal values\n\n```python\nfrom evaluate import load\ncoval = load('coval')\nwords = ['bc..."
          ],
          [
           "--\ntitle: CUAD\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app...."
          ],
          [
           "`references`: a list of question-answer dictionaries with the following key-values:\n - `id`: the id ..."
          ],
          [
           "`prec_at_90_recall`: The fraction of true examples among the predicted examples at a recall rate of ..."
          ],
          [
           "```\n\nMinimal values:\n\n```python\nfrom evaluate import load\ncuad_metric = load(\"cuad\")\npredictions = [..."
          ],
          [
           "```\n\nPartial match: \n\n```python\nfrom evaluate import load\ncuad_metric = load(\"cuad\")\npredictions = [..."
          ],
          [
           "Visualization methods\n\nMethods for visualizing evaluations results:\n\n## Radar Plot\n\n[[autodoc]] eval..."
          ],
          [
           "--\ntitle: TER\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app.p..."
          ],
          [
           "See the README.md file at https://github.com/mjpost/sacreBLEU#ter for more information.\n\n\n## How to ..."
          ],
          [
           "```\n\n### Inputs\nThis metric takes the following as input:\n- **`predictions`** (`list` of `str`): The..."
          ],
          [
           "```\n\nThe metric can take on any value `0` and above. `0` is a perfect score, meaning the predictions..."
          ],
          [
           "```\n\nExample ignoring punctuation and capitalization, and everything matches:\n```python\n>>> predicti..."
          ],
          [
           "```\n\n\n## Limitations and Bias\n\n\n## Citation\n```bibtex\n@inproceedings{snover-etal-2006-study,\n    tit..."
          ],
          [
           "--\ntitle: Toxicity\nemoji: ðŸ¤—\ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.0.2\napp_file: ap..."
          ],
          [
           "```\n        In this case, the `toxic_label` would be `offensive`.\n    `aggregation` (optional): dete..."
          ],
          [
           "```\n    Example 3 (returns the maximum toxicity score):\n```python\n>>> toxicity = evaluate.load(\"toxi..."
          ],
          [
           "Hub methods\n\nMethods for using the Hugging Face Hub:\n\n## Push to hub \n\n[[autodoc]] evaluate.push_to_..."
          ],
          [
           "--\ntitle: COMET\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: app..."
          ],
          [
           "```\n\nIt has several configurations, named after the COMET model to be used. For versions below 2.0 i..."
          ],
          [
           "More information about model characteristics can be found on the [COMET website](https://unbabel.git..."
          ],
          [
           "```\n\nPartial match:\n\n```python\nfrom evaluate import load\ncomet_metric = load('comet') \nsource = [\"De..."
          ],
          [
           "```\n\n## Limitations and bias\n\nThe models provided for calculating the COMET metric are built on top ..."
          ],
          [
           "### Interpreting Scores:\n\nWhen using COMET to evaluate machine translation, it's important to unders..."
          ],
          [
           "```\n\n```bibtex\n@inproceedings{rei-EtAl:2020:WMT,\n   author    = {Rei, Ricardo  and  Stewart, Craig  ..."
          ],
          [
           "```\n\n## Further References\n\n- [COMET website](https://unbabel.github.io/COMET/html/index.html)\n- [Hu..."
          ],
          [
           "--\ntitle: Brier Score\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_fil..."
          ],
          [
           "```\n\n### Inputs\n\nMandatory inputs: \n- `predictions`: numeric array-like of shape (`n_samples,`) or (..."
          ],
          [
           "```\n## Limitations and Bias\nThe [brier_score](https://huggingface.co/metrics/brier_score) is appropr..."
          ],
          [
           "--\ntitle: Perplexity\nemoji: ðŸ¤—\ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "```\n\n### Inputs\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on..."
          ],
          [
           "```\n\nThe range of this metric is [0, inf). A lower score is better.\n\n#### Values from Popular Papers..."
          ],
          [
           "```\n\n## Limitations and Bias\nNote that the output value is based heavily on what text the model was ..."
          ],
          [
           "Loading methods\n\nMethods for listing and loading evaluation modules:\n\n## List\n\n[[autodoc]] evaluate...."
          ],
          [
           "Creating and sharing a new evaluation\n\n## Setup\n\nBefore you can create a new metric make sure you ha..."
          ],
          [
           "```\n\nThis will create a new Space on the ðŸ¤— Hub, clone it locally, and populate it with a template. I..."
          ],
          [
           "```\n\nOr if you need to download the NLTK `\"punkt\"` resources:\n\n```py\ndef _download_and_prepare(self,..."
          ],
          [
           "```\ncd PATH_TO_MODULE\ngit add .\ngit commit -m \"Add my new, shiny module.\"\ngit push\n```\nTada ðŸŽ‰! Your ..."
          ],
          [
           "Main classes\n\n## EvaluationModuleInfo\n\nThe base class `EvaluationModuleInfo` implements a the logic ..."
          ],
          [
           "--\ntitle: SacreBLEU\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "```\nThe score can take any value between `0.0` and `100.0`, inclusive.\n\n#### Values from Popular Pap..."
          ],
          [
           "--\ntitle: ROC AUC\nemoji: ðŸ¤— \ncolorFrom: blue\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file: a..."
          ],
          [
           "This metric has three separate use cases:\n- **binary**: The case in which there are only two differe..."
          ],
          [
           "```\n\nThe default implementation of this metric is the **binary** implementation. If employing the **..."
          ],
          [
           "```\n\nIn contrast, though, the output takes the following format in the multilabel case when `average..."
          ],
          [
           "```\n\nExample 3, the **multilabel** use case:\n```python\n>>> roc_auc_score = evaluate.load(\"roc_auc\", ..."
          ],
          [
           "--\ntitle: NIST_MT\nemoji: ðŸ¤— \ncolorFrom: purple\ncolorTo: red\nsdk: gradio\nsdk_version: 3.19.1\napp_file:..."
          ],
          [
           "```\n\n### Inputs\n- **predictions**: tokenized predictions to score. For sentence-level NIST, a list o..."
          ]
         ],
         "hovertemplate": "source=evaluate<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "evaluate, circle",
         "marker": {
          "color": "#00cc96",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "evaluate, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -0.52577597,
          -0.1973036,
          -1.0046277,
          -1.3059657,
          -0.19675012,
          -0.993976,
          -0.23644935,
          -0.42315316,
          -0.23889749,
          -0.9408945,
          -0.95228326,
          0.18166329,
          0.1191813,
          -1.265884,
          -1.0206659,
          -0.4476083,
          -0.4721967,
          -0.1691754,
          -1.7526245,
          -0.9658569,
          -0.71124613,
          -0.20928615,
          -0.40724126,
          -0.42246526,
          -0.23403029,
          -1.0223384,
          -2.346276,
          -1.020064,
          -0.33526942,
          -0.40167287,
          -1.1743373,
          5.805085,
          -1.0011401,
          -1.0144109,
          -0.6699916,
          -0.14178193,
          -0.39696717,
          -0.35060537,
          -2.04939,
          -1.0103619,
          -0.28775612,
          -0.18450512,
          -1.1865972,
          6.5580535,
          12.479826,
          -4.9723163,
          5.997164,
          -0.9486794,
          0.07980865,
          -0.22200097,
          -1.9192766,
          0.12736309,
          -1.4266742,
          5.7370915,
          -1.0221059,
          -0.4196532,
          -0.42557192,
          -0.049204554,
          -1.5761375,
          2.366264,
          -0.2731286,
          -0.1979514,
          -1.7478751,
          -1.0416789,
          -0.8521532,
          -0.5099227,
          5.2058578,
          5.8298635,
          5.3426347,
          3.1740403,
          0.8187753,
          0.25464982,
          -1.9696897,
          4.684331,
          -0.80871004,
          -0.20820817,
          -0.0991562,
          -0.17043312,
          -0.25902697,
          -0.97114694,
          -0.24963117,
          -0.22987415,
          -0.1260731,
          -0.21093197,
          -1.1110004,
          -0.33380592,
          0.1817273,
          0.06586642,
          -0.40762773,
          -0.022665625,
          -0.13554752,
          0.032810904,
          0.012212617,
          2.3441837,
          -0.37421492,
          -0.27003586,
          -0.13241498,
          -0.22183181,
          -0.24298732,
          -0.41828385,
          -3.2478187,
          -3.2616413,
          -0.8678725,
          0.037378535,
          -0.19898759,
          -0.18793553,
          -0.6020277,
          -0.4006263,
          -0.72921365,
          -1.2101496,
          -0.92803884,
          -0.2451218,
          5.166188,
          -4.5070047,
          6.4806843,
          0.042014822,
          -0.6400682,
          -0.31640485,
          -0.30666465,
          1.9876006,
          -0.99047583,
          -0.620657,
          -0.465206,
          -0.5656587,
          -0.5488642,
          -0.9087752,
          -1.0658143,
          -0.430351,
          -0.4445539,
          -0.33262756,
          -0.12171964,
          -0.16748632,
          -1.8341548,
          -1.0411512,
          -0.49911493,
          -0.80238795,
          -0.20365457,
          -1.467453,
          -0.4020624,
          -1.6905152,
          -0.8391474,
          -0.17207183,
          -0.07040575,
          0.18603383,
          -0.9770831,
          -0.034314133,
          -0.24179247,
          -0.87959313,
          -5.6182437,
          5.637619,
          -6.9171224,
          -0.39014876,
          2.600877,
          0.06334111,
          -0.5976356,
          -1.4472234,
          -1.0495573,
          -0.9037219,
          -0.89338833,
          -0.9063813,
          -0.9952056,
          -0.5971723,
          -0.60638326,
          -0.6615216,
          -0.9903722,
          -0.57062906,
          -0.34898323,
          -0.32132003,
          -0.35857227,
          -0.9967591,
          -0.8227892,
          -5.3207307,
          -0.5148001,
          -0.7026016,
          -2.3071475,
          -1.0071454,
          -0.35370794,
          -0.1641488,
          -0.081363596,
          -1.8028549,
          -0.95726305,
          -0.08463473,
          -0.083037995,
          -1.2232107,
          -0.42580125,
          -0.9473477,
          -0.67666763,
          -0.9702933,
          -0.3217264,
          -0.27823877,
          -0.9425875,
          0.07073369,
          -1.013441,
          -0.8957819,
          -0.9803418,
          -0.8060862,
          -0.1570142,
          -1.3259447,
          -0.95869434,
          -0.082475595,
          -1.0462914,
          -0.2272866,
          -0.9725091,
          -0.99968463,
          -0.19249578,
          -1.6241156,
          -0.9508964,
          -0.94503474,
          -1.0132154,
          0.9532508,
          6.0388584,
          -0.9343601,
          -0.15297565,
          -1.4851719,
          -0.95139194,
          -0.17088747,
          -7.0134587,
          -1.0309176,
          -0.97631043,
          -6.84479,
          -3.2287285,
          -0.92916304,
          -0.69414634,
          -1.078889,
          0.44008225,
          -1.9557606,
          -0.3601122,
          -5.40681,
          -2.1399617,
          6.3986244,
          -1.034114,
          -0.58387583,
          -0.80984676,
          -1.7518945,
          -0.9261759,
          0.518556,
          -0.61467063,
          -1.0177015,
          -0.3339419,
          -1.7662319,
          -0.93839264,
          -0.3283384,
          -0.07812906,
          0.17463945,
          -0.2012994,
          -0.9551477,
          0.29439574,
          0.0335857,
          -1.2620965,
          -1.022882,
          -0.53991497,
          -1.0955517,
          -0.22183555,
          -0.22896819,
          -0.29057103,
          1.7772626,
          1.6920077,
          1.834947,
          2.1979785,
          -0.9778516,
          -0.3243834,
          -0.98935604,
          -0.23920017,
          -0.42149916,
          -0.13776627,
          -0.17706881,
          -0.38189444,
          7.605654,
          -0.12683207,
          -1.0086179,
          -0.63244313,
          -0.38242558,
          -0.8612378,
          -1.1035526,
          -0.13676602,
          -0.35539955,
          -1.187284,
          5.815343,
          -1.0276456,
          -0.30634096,
          -2.4387307,
          5.7520056,
          5.650962,
          2.5658562,
          4.0223565,
          3.89524,
          4.6151094,
          4.50625,
          4.753969,
          -0.8828935,
          0.31873176,
          0.16815671,
          -1.1693633,
          -0.9572713,
          0.18766432,
          0.14547566,
          -0.2507288,
          0.53535044,
          -0.9653153,
          -0.3445738,
          -1.0700425,
          -2.4491224,
          -0.43579993,
          -0.9264198,
          -0.44407287,
          -0.96191406,
          0.09480681,
          0.53522253,
          -0.21934652,
          -1.0059761,
          -3.5109048,
          -1.1871917,
          -0.17418873,
          -0.24288769,
          -0.9963432,
          -0.20607829,
          -0.76728857,
          -0.018358165,
          -0.14792381,
          0.5656337,
          -1.030653,
          -0.6479929,
          -0.77792764,
          -0.5747726,
          -0.5393099,
          -2.3476787,
          -0.9876252,
          -0.407443,
          -1.0144085,
          4.7604113,
          -1.0261595,
          -2.739209,
          -1.3104573,
          -0.2122463,
          -1.6701809,
          -1.7998972,
          -2.146548,
          6.675105,
          -0.9701231,
          -0.040129267,
          -1.2076995,
          -1.0239347,
          -2.4254348,
          -0.35253504,
          -2.1137583,
          0.7289316,
          0.61463463,
          6.1293945,
          -0.05192169,
          4.3251963,
          0.29548508,
          -1.0046154,
          -0.758388,
          -0.87703276,
          -0.062318373,
          -0.06102153,
          -0.070127554,
          -0.06838231,
          -0.99934924,
          -1.2223127
         ],
         "xaxis": "x",
         "y": [
          -6.3164916,
          -7.4971213,
          -6.5769644,
          -7.3319235,
          -6.9119973,
          -9.505401,
          -7.5239453,
          -7.5321727,
          -7.5542026,
          -7.285274,
          -9.685999,
          -7.263871,
          -7.639402,
          -6.8398385,
          -9.624444,
          -7.74522,
          -7.7187743,
          -7.5167794,
          -6.913972,
          -9.268511,
          -7.1014924,
          -5.563573,
          -1.5586845,
          -4.8528895,
          -6.318078,
          -9.529464,
          -7.1155457,
          -9.631886,
          -7.485383,
          -7.730735,
          -7.8319564,
          -3.4595947,
          -9.616124,
          -7.7753005,
          -7.8571205,
          -7.519887,
          -7.147494,
          -7.260743,
          -6.9914756,
          -9.572603,
          -7.5184126,
          -7.549867,
          -7.0055633,
          -3.936887,
          16.0883,
          0.85073435,
          -3.4466414,
          -9.6694765,
          -6.7509866,
          -7.099158,
          -6.3392615,
          -6.691955,
          -6.6626396,
          -3.6128428,
          -9.542789,
          -7.3843064,
          -7.644516,
          -7.775142,
          -6.912594,
          0.61763734,
          -6.0310845,
          0.13619477,
          -3.825792,
          -4.368215,
          0.07178407,
          -0.8242402,
          0.13304214,
          -1.4174095,
          -1.5242965,
          -1.7110896,
          -1.6453453,
          -2.7112317,
          -3.9922428,
          -0.6202789,
          -9.006891,
          -7.548514,
          -7.4018536,
          -7.552881,
          -6.579735,
          -9.62672,
          -6.1106563,
          -6.405419,
          -5.46599,
          -5.3690944,
          -7.059997,
          -5.918734,
          -5.455674,
          -5.928457,
          -7.4500003,
          -6.0472436,
          -6.1063485,
          -6.1452985,
          -6.495768,
          -3.255825,
          -5.3650117,
          -6.0765586,
          -5.1044765,
          -5.5032215,
          -5.1599197,
          -5.2649426,
          -3.3013284,
          -3.431786,
          -9.448507,
          -7.274348,
          -7.147068,
          -4.9887304,
          -4.9289722,
          -4.9325037,
          -2.961392,
          1.4676646,
          -4.667475,
          -4.142444,
          -4.227355,
          1.4153293,
          -3.998009,
          -4.543012,
          -4.842539,
          -4.8895283,
          -5.0311236,
          -4.562193,
          -9.706357,
          -7.9731617,
          -8.011456,
          -8.053,
          -8.021661,
          -9.513067,
          -9.137421,
          -7.020068,
          -7.7092066,
          -7.677983,
          -8.147382,
          -8.212582,
          -6.345277,
          -9.634009,
          -7.6434503,
          -7.80597,
          -5.9999547,
          -7.481517,
          -7.64938,
          -7.3808055,
          -9.383602,
          -7.4810247,
          -7.545062,
          -7.04882,
          -9.581625,
          -5.974423,
          -8.0585,
          -6.6440177,
          0.1634119,
          -3.447846,
          1.4614896,
          -6.003821,
          0.14829989,
          -5.7390113,
          -6.131357,
          -6.036582,
          -8.563135,
          -8.232609,
          -8.206311,
          -8.397952,
          -9.545072,
          -7.8960924,
          -8.009384,
          -7.909624,
          -9.579817,
          -6.956641,
          -6.994826,
          -6.954647,
          -7.166174,
          -9.650014,
          -6.205067,
          1.9556694,
          -6.773943,
          -6.071521,
          -6.8455377,
          -9.436792,
          -7.9183035,
          -8.171447,
          -8.289489,
          -6.4885287,
          -9.686319,
          -7.5282207,
          -7.5053267,
          -8.55248,
          -7.468486,
          -7.55972,
          -7.7878013,
          -9.6723585,
          -7.7831683,
          -7.985501,
          -9.703813,
          -7.741851,
          -9.588549,
          -8.11713,
          -8.199053,
          -9.295613,
          -6.8667846,
          -6.4349403,
          -9.752526,
          -7.501823,
          -9.30766,
          -7.6827817,
          -8.016719,
          -8.2233305,
          -7.6073065,
          -7.190198,
          -8.691111,
          -8.319104,
          -8.19491,
          -3.515748,
          -3.3035207,
          -9.423339,
          -7.4871426,
          -6.7825856,
          -9.69425,
          -7.577771,
          1.3931216,
          -6.457958,
          -6.3980374,
          0.9453513,
          2.10764,
          -9.575162,
          -7.1429625,
          -9.289167,
          -4.92091,
          -5.6746826,
          -8.077893,
          0.78483385,
          -6.379269,
          -2.5720918,
          -9.499967,
          -7.690815,
          -7.633978,
          -7.1606765,
          -9.517185,
          -5.7091484,
          -6.4809504,
          -9.565809,
          -7.608317,
          -6.8047357,
          -9.547501,
          -6.8324027,
          -5.450496,
          -4.6797595,
          -5.160034,
          -9.713412,
          -7.365396,
          -7.8551993,
          -6.940554,
          -9.602158,
          -7.4407845,
          -7.851819,
          -5.470363,
          -6.3142724,
          -6.1009593,
          -1.1935664,
          -0.9907504,
          -1.1252624,
          0.10301248,
          -9.527916,
          -7.005882,
          -9.390718,
          -6.498081,
          -7.0238905,
          -7.8558364,
          -8.282074,
          -6.257042,
          -3.9992497,
          -5.2243743,
          -9.046734,
          -7.5183387,
          -7.60056,
          -8.115092,
          -9.388485,
          -6.978812,
          -7.47321,
          -7.749326,
          -3.6638134,
          -9.624987,
          -7.5668044,
          -7.1011615,
          -0.082792506,
          -1.9917307,
          -0.56387883,
          -0.3166484,
          -0.4359393,
          -0.8551807,
          -0.46703416,
          -0.4200484,
          -9.66826,
          -6.9700513,
          -7.725781,
          -6.97703,
          -9.702441,
          -7.2293277,
          -7.6461015,
          -4.738337,
          -4.614675,
          -9.644587,
          -7.9660144,
          -9.334421,
          -5.6044216,
          -7.671558,
          -9.601254,
          -7.5560813,
          -9.6608925,
          -4.042609,
          -3.7212987,
          -6.1016755,
          -9.600845,
          -5.175658,
          -7.5795755,
          -7.5056467,
          -7.322155,
          -9.583772,
          -7.8068895,
          -7.5501323,
          -7.882045,
          -8.219776,
          -4.50523,
          -9.381913,
          -7.824272,
          -7.8985624,
          -7.935173,
          -7.914949,
          -6.9730687,
          -9.564445,
          -6.3343215,
          -5.9982324,
          -0.4692816,
          -9.623316,
          2.237009,
          -7.1428165,
          -8.292263,
          -7.0481925,
          -7.2094646,
          -7.111281,
          -2.2335558,
          -9.653865,
          -7.269858,
          -6.950587,
          -9.405654,
          -5.5586514,
          -7.732666,
          -6.170111,
          -5.0673246,
          -5.4694023,
          -2.0197313,
          -5.505984,
          -0.5909172,
          -5.625019,
          -9.595219,
          -8.008895,
          -9.185708,
          -7.0224304,
          -6.9591527,
          -7.1184754,
          -6.990198,
          -9.673499,
          -7.7199645
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "ä¸»è¦ç‰¹ç‚¹\n\nè®©æˆ‘ä»¬æ¥ä»‹ç»ä¸€ä¸‹ Gradio æœ€å—æ¬¢è¿Žçš„ä¸€äº›åŠŸèƒ½ï¼è¿™é‡Œæ˜¯ Gradio çš„ä¸»è¦ç‰¹ç‚¹ï¼š\n\n1. [æ·»åŠ ç¤ºä¾‹è¾“å…¥](#example-inputs)\n2. [ä¼ é€’è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯](#erro..."
          ],
          [
           "ç»§ç»­äº†è§£ç¤ºä¾‹ï¼Œè¯·å‚é˜…[æ›´å¤šç¤ºä¾‹](https://gradio.app/more-on-examples)æŒ‡å—ã€‚\n\n## é”™è¯¯\n\næ‚¨å¸Œæœ›å‘ç”¨æˆ·ä¼ é€’è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯ã€‚ä¸ºæ­¤ï¼Œwith `gr.Error(\"..."
          ],
          [
           "å¦ä¸€ä¸ªæœ‰ç”¨çš„å…³é”®å­—å‚æ•°æ˜¯ `label=`ï¼Œå®ƒå­˜åœ¨äºŽæ¯ä¸ª `Component` ä¸­ã€‚è¿™ä¿®æ”¹äº†æ¯ä¸ª `Component` é¡¶éƒ¨çš„æ ‡ç­¾æ–‡æœ¬ã€‚è¿˜å¯ä»¥ä¸ºè¯¸å¦‚ `Textbox` æˆ– `Radio` ä¹‹ç±»çš„..."
          ],
          [
           "```\n\n## æ——æ ‡\n\né»˜è®¤æƒ…å†µä¸‹ï¼Œ\"Interface\" å°†æœ‰ä¸€ä¸ª \"Flag\" æŒ‰é’®ã€‚å½“ç”¨æˆ·æµ‹è¯•æ‚¨çš„ `Interface` æ—¶ï¼Œå¦‚æžœçœ‹åˆ°æœ‰è¶£çš„è¾“å‡ºï¼Œä¾‹å¦‚é”™è¯¯æˆ–æ„å¤–çš„æ¨¡åž‹è¡Œä¸ºï¼Œä»–ä»¬å¯ä»¥å°†è¾“å…¥æ ‡è®°ä¸º..."
          ],
          [
           "```\n\n_flagged/logs.csv_\n\n```csv\nim,Output\nim/0.png,Output/0.png\nim/1.png,Output/1.png\n```\n\nå¦‚æžœæ‚¨å¸Œæœ›ç”¨æˆ·æä¾›..."
          ],
          [
           "```\n\nç›¸åï¼Œè¿™é‡Œæˆ‘ä»¬ä¿ç•™å›¾åƒçš„åŽŸå§‹å¤§å°ï¼Œä½†åœ¨å°†å…¶è½¬æ¢ä¸º numpy æ•°ç»„ä¹‹å‰åè½¬é¢œè‰²ï¼š\n\n```py\nimg = gr.Image(invert_colors=True, type=\"numpy\"..."
          ],
          [
           "```\n\n## é˜Ÿåˆ— (Queuing)\n\nå¦‚æžœæ‚¨çš„åº”ç”¨ç¨‹åºé¢„è®¡ä¼šæœ‰å¤§é‡æµé‡ï¼Œè¯· with `queue()` æ–¹æ³•æ¥æŽ§åˆ¶å¤„ç†é€ŸçŽ‡ã€‚è¿™å°†æŽ’é˜Ÿå¤„ç†è°ƒç”¨ï¼Œå› æ­¤ä¸€æ¬¡åªå¤„ç†ä¸€å®šæ•°é‡çš„è¯·æ±‚ã€‚é˜Ÿåˆ—ä½¿ç”¨ Webso..."
          ],
          [
           "```\n\n## è¿­ä»£è¾“å‡º (Iterative Outputs)\n\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦ä¼ è¾“ä¸€ç³»åˆ—è¾“å‡ºè€Œä¸æ˜¯ä¸€æ¬¡æ˜¾ç¤ºå•ä¸ªè¾“å‡ºã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½æœ‰ä¸€ä¸ªå›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œå¸Œæœ›æ˜¾ç¤ºç”Ÿæˆçš„æ¯ä¸ªæ­¥éª¤çš„å›¾åƒï¼Œç›´åˆ°æœ€ç»ˆ..."
          ],
          [
           "```\n\næ‚¨ä»¥ä¸Žå¸¸è§„å‡½æ•°ç›¸åŒçš„æ–¹å¼å°†ç”Ÿæˆå™¨æä¾›ç»™ Gradioã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯ä¸€ä¸ªï¼ˆè™šæ‹Ÿçš„ï¼‰å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œå®ƒåœ¨è¾“å‡ºå›¾åƒä¹‹å‰ç”Ÿæˆæ•°ä¸ªæ­¥éª¤çš„å™ªéŸ³ï¼š\n\n$code_fake_diffusion\n$demo_fa..."
          ],
          [
           "Gradio æ”¯æŒä¼ é€’*æ‰¹å¤„ç†*å‡½æ•°ã€‚æ‰¹å¤„ç†å‡½æ•°åªæ˜¯æŽ¥å—è¾“å…¥åˆ—è¡¨å¹¶è¿”å›žé¢„æµ‹åˆ—è¡¨çš„å‡½æ•°ã€‚\n\nä¾‹å¦‚ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰¹å¤„ç†å‡½æ•°ï¼Œå®ƒæŽ¥å—ä¸¤ä¸ªè¾“å…¥åˆ—è¡¨ï¼ˆä¸€ä¸ªå•è¯åˆ—è¡¨å’Œä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼‰ï¼Œå¹¶è¿”å›žä¿®å‰ªè¿‡çš„å•è¯åˆ—è¡¨ä½œä¸ºè¾“å‡ºï¼š\n..."
          ],
          [
           "```\n\nä½¿ç”¨æ‰¹å¤„ç†å‡½æ•°çš„ä¼˜ç‚¹æ˜¯ï¼Œå¦‚æžœå¯ç”¨äº†é˜Ÿåˆ—ï¼ŒGradio æœåŠ¡å™¨å¯ä»¥è‡ªåŠ¨*æ‰¹å¤„ç†*ä¼ å…¥çš„è¯·æ±‚å¹¶å¹¶è¡Œå¤„ç†å®ƒä»¬ï¼Œä»Žè€Œå¯èƒ½åŠ å¿«æ¼”ç¤ºé€Ÿåº¦ã€‚ä»¥ä¸‹æ˜¯ Gradio ä»£ç çš„ç¤ºä¾‹ï¼ˆè¯·æ³¨æ„ `batch=True..."
          ],
          [
           "```\n\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç† 16 ä¸ªè¯·æ±‚ï¼ˆæ€»æŽ¨ç†æ—¶é—´ä¸º 5 ç§’ï¼‰ï¼Œè€Œä¸æ˜¯åˆ†åˆ«å¤„ç†æ¯ä¸ªè¯·æ±‚ï¼ˆæ€»æŽ¨ç†æ—¶é—´ä¸º 80 ç§’ï¼‰ã€‚è®¸å¤š Hugging Face çš„ `transformers` å’Œ `..."
          ],
          [
           "## Gradio ç¬”è®°æœ¬ (Colab Notebooks)\n\nGradio å¯ä»¥åœ¨ä»»ä½•è¿è¡Œ Python çš„åœ°æ–¹è¿è¡Œï¼ŒåŒ…æ‹¬æœ¬åœ° Jupyter ç¬”è®°æœ¬å’Œåä½œç¬”è®°æœ¬ï¼Œå¦‚[Google Colab](..."
          ],
          [
           "Gradio Demo: blocks_random_slider\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n\nimport gradio as gr\n\n\ndef..."
          ],
          [
           "State in Blocks\n\nWe covered [State in Interfaces](https://gradio.app/interface-state), this guide ta..."
          ],
          [
           "å¦‚ä½•ä½¿ç”¨åœ°å›¾ç»„ä»¶ç»˜åˆ¶å›¾è¡¨\n\nRelated spaces:\nTags: PLOTS, MAPS\n\n## ç®€ä»‹\n\næœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ Gradio çš„ `Plot` ç»„ä»¶åœ¨åœ°å›¾ä¸Šç»˜åˆ¶åœ°ç†æ•°æ®ã€‚Gradi..."
          ],
          [
           "dataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\ndf = dataset.to_pandas()\n\ndef f..."
          ],
          [
           "```\n\nåœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å…ˆå°† CSV æ•°æ®åŠ è½½åˆ°ä¸€ä¸ª pandas dataframe ä¸­ã€‚è®©æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿™å°†ä½œä¸º gradio åº”ç”¨ç¨‹åºçš„é¢„æµ‹å‡½æ•°ã€‚è¯¥å‡½æ•°å°†æŽ¥å—æœ€ä½Žä»·æ ¼ã€æœ€é«˜ä»·æ ¼èŒƒå›´..."
          ],
          [
           "```\n\nä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¼ å…¥ç»çº¬åº¦åˆ—è¡¨æ¥åˆ›å»ºä¸€ä¸ªæ•£ç‚¹å›¾ã€‚æˆ‘ä»¬è¿˜ä¼ å…¥äº†åç§°å’Œä»·æ ¼çš„è‡ªå®šä¹‰æ•°æ®ï¼Œä»¥ä¾¿åœ¨é¼ æ ‡æ‚¬åœåœ¨æ¯ä¸ªæ ‡è®°ä¸Šæ—¶æ˜¾ç¤ºé¢å¤–çš„ä¿¡æ¯ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ `update_layout` æ¥æŒ‡å®š..."
          ],
          [
           "```\n\næˆ‘ä»¬ä½¿ç”¨ `gr.Column` å’Œ `gr.Row` å¸ƒå±€è¿™äº›ç»„ä»¶ï¼Œå¹¶ä¸ºæ¼”ç¤ºåŠ è½½æ—¶å’Œç‚¹å‡» \" æ›´æ–°ç­›é€‰ \" æŒ‰é’®æ—¶æ·»åŠ äº†äº‹ä»¶è§¦å‘å™¨ï¼Œä»¥è§¦å‘åœ°å›¾æ›´æ–°æ–°çš„ç­›é€‰æ¡ä»¶ã€‚\n\nä»¥ä¸‹æ˜¯å®Œæ•´æ¼”ç¤ºä»£ç ï¼š\n\n..."
          ],
          [
           "Gradio Demo: examples_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the..."
          ],
          [
           "Gradio Demo: number_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr...."
          ],
          [
           "Gradio Demo: map_airbnb\n### Display an interactive map of AirBnB locations with Plotly. Data is host..."
          ],
          [
           "```\nimport gradio as gr\nimport plotly.graph_objects as go\nfrom datasets import load_dataset\n\ndataset..."
          ],
          [
           "return fig\n\nwith gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            min_p..."
          ],
          [
           "Gradio Demo: question-answering\n\n\n```\n!pip install -q gradio torch transformers\n```\n\n\n```\nimport gra..."
          ],
          [
           "`@gradio/button`\n\n```html\n<script>\n\timport { Button } from \"@gradio/button\";\n</script>\n\n<button type..."
          ],
          [
           "Gradio Demo: sales_projections\n\n\n```\n!pip install -q gradio pandas numpy matplotlib\n```\n\n\n```\nimport..."
          ],
          [
           "Gradio and W&B Integration\n\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\nTags: WAND..."
          ],
          [
           "Get started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces\n\nHugging Face Spaces..."
          ],
          [
           "```\n\n3. Finetune StyleGAN and W&B experiment tracking\n\n   This next step will open a W&B dashboard t..."
          ],
          [
           "for idx in tqdm(range(num_iter)):\n       mean_w = generator.get_latent(torch.randn([latents.size(0),..."
          ],
          [
           "```\n\nalpha = 1.0\nalpha = 1-alpha\n\npreserve_color = True\nnum_iter = 100\nlog_interval = 50\n\nsamples = ..."
          ],
          [
           "if preserve_color:\nid_swap = [9,11,15,16,17]\nelse:\nid_swap = list(range(7, generator.n_latent))\n\nfor..."
          ],
          [
           "g_optim.zero_grad()\n    loss.backward()\n    g_optim.step()\n\nout_table = wandb.Table(data=samples, co..."
          ],
          [
           "```\n\n4. Save, Download, and Load Model\n\n    Here's how to save and download your model.\n\n```python\n\n..."
          ],
          [
           "plt.rcParams['figure.dpi'] = 150\n\n\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize(..."
          ],
          [
           "```\n\n5. Build a Gradio Demo\n\n```python\n\nimport gradio as gr\n\ntitle = \"JoJoGAN\"\ndescription = \"Gradio..."
          ],
          [
           "```\n\n7. (Optional) Embed W&B plots in your Gradio App\n\n   It's also possible to embed W&B plots with..."
          ],
          [
           "```\n\n## Conclusion\n\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! ..."
          ],
          [
           "Gradio Demo: duplicatebutton_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n..."
          ],
          [
           "Gradio Demo: upload_button_component_events\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\n\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    \n    with gr.Row():\n        with gr.Col..."
          ],
          [
           "@gradio/imageeditor\n\n## 0.2.0\n\n### Features\n\n- [#6809](https://github.com/gradio-app/gradio/pull/680..."
          ],
          [
           "## 0.1.4\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradi..."
          ],
          [
           "- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/gradio/commit/6a9151d5c9432c724098..."
          ],
          [
           "[`5177132`](https://github.com/gradio-app/gradio/commit/5177132d718c77f6d47869b4334afae6380394cb)]:..."
          ],
          [
           "- @gradio/image@0.5.0\n  - @gradio/upload@0.5.3\n  - @gradio/client@0.9.0\n  - @gradio/wasm@0.4.0\n  - @..."
          ],
          [
           "## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`b639e04`](https://github.com/gradio-app/gradi..."
          ],
          [
           "A brand new component, completely separate from `Image` that provides simple editing capabilities.\n\n..."
          ],
          [
           "```py\n\ndef fn(im):\n    im[\"composite\"] # the full canvas\n    im[\"background\"] # the background image..."
          ],
          [
           "```\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n### Fixes\n\n- [#6502](https://github.com/gradio-app/..."
          ],
          [
           "Gradio Demo: chatinterface_system_prompt\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr..."
          ],
          [
           "Gradio Demo: streaming_wav2vec\n\n\n```\n!pip install -q gradio torch transformers \n```\n\n\n```\nfrom trans..."
          ],
          [
           "component-styles\n\n## Textbox\n\n| name        | type                                 | description    ..."
          ],
          [
           "## Checkbox\n\n| name        | type                                 | description                    |..."
          ],
          [
           "## Dropdown\n\n| name        | type                                 | description                    |..."
          ],
          [
           "## Audio\n\n| name      | type                                 | description         |\n| --------- | -..."
          ],
          [
           "## Label\n\n| name        | type   | description                    |\n| ----------- | ------ | -------..."
          ],
          [
           "## HTML\n\nNothing\n\n## Gallery\n\n| name        | type                                      | descriptio..."
          ],
          [
           "## Plot\n\nNothing (yet)\n\n## Markdown\n\nNothing\n\n## Button\n\n| name         | type                      ..."
          ],
          [
           "Gradio Demo: blocks_webcam\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport numpy as np\n\nimport gradio..."
          ],
          [
           "Gradio Demo: on_listener_live\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.B..."
          ],
          [
           "ä¸»é¢˜ Theming\n\nTags: THEMES\n\n## ä»‹ç»\n\nGradio å…·æœ‰å†…ç½®çš„ä¸»é¢˜å¼•æ“Žï¼Œå¯è®©æ‚¨è‡ªå®šä¹‰åº”ç”¨çš„å¤–è§‚å’Œæ„Ÿè§‰ã€‚æ‚¨å¯ä»¥é€‰æ‹©å„ç§ä¸»é¢˜ï¼Œæˆ–è€…åˆ›å»ºè‡ªå·±çš„ä¸»é¢˜ã€‚è¦è¿™æ ·åšï¼Œè¯·å°† `theme=..."
          ],
          [
           "```\n\n$demo_theme_builder\n\næ‚¨å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„ Spaces ä¸Šè¿è¡Œçš„ Theme Builderï¼Œä½†é€šè¿‡ `gr.themes.builder()` åœ¨æœ¬åœ°å¯åŠ¨æ—¶è¿è¡Œé€Ÿåº¦æ›´å¿«ã€‚..."
          ],
          [
           "3 ä¸ªé¢œè‰²æž„é€ å‡½æ•°å‚æ•°æ˜¯ï¼š\n\n- `primary_hue`ï¼šè¿™æ˜¯ä¸»é¢˜ä¸­çš„ä¸»è‰²ã€‚åœ¨é»˜è®¤ä¸»é¢˜ä¸­ï¼Œæ­¤å€¼è®¾ç½®ä¸º `gradio.themes.colors.orange`ã€‚\n- `secondary_hue..."
          ],
          [
           "```\n\næˆ–è€…ç›´æŽ¥ä½¿ç”¨ `Color` å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes..."
          ],
          [
           "```\n\n<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-extended-step-1.hf.space?__theme=light..."
          ],
          [
           "```\n\næˆ–è€…ç›´æŽ¥ä½¿ç”¨ `Size` å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes..."
          ],
          [
           "```\n\n<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-extended-step-2.hf.space?__theme=light..."
          ],
          [
           "```\n\n<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-extended-step-3.hf.space?__theme=light..."
          ],
          [
           "```\n\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `loader_color` å’Œ `slider_color` å˜é‡è®¾ç½®ä¸º`#FF0000`ï¼Œå°½ç®¡æ•´ä½“ `primary_color` ä½¿ç”¨è“è‰²è°ƒè‰²æ¿ã€‚æ‚¨å¯ä»¥ä»¥è¿™ç§æ–¹..."
          ],
          [
           "### CSS å˜é‡ç»„ç»‡\n\nè™½ç„¶æœ‰æ•°ç™¾ä¸ª CSS å˜é‡ï¼Œä½†å¹¶ä¸éœ€è¦ä¸ºæ¯ä¸ªå˜é‡éƒ½æŒ‡å®šå•ç‹¬çš„å€¼ã€‚å®ƒä»¬é€šè¿‡å¼•ç”¨ä¸€ç»„æ ¸å¿ƒå˜é‡å’Œå½¼æ­¤å¼•ç”¨æ¥èŽ·å–å€¼ã€‚è¿™æ ·åšå¯ä»¥ä»…ä¿®æ”¹å°‘é‡å˜é‡ä»¥æ”¹å˜æ•´ä¸ªä¸»é¢˜çš„å¤–è§‚å’Œæ„Ÿè§‰ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æ›´..."
          ],
          [
           "```\n\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `button_primary_background_fill` å’Œ `button_primary_background_fill_hover` å˜é‡åˆ†åˆ«è®¾ç½®ä¸º`*..."
          ],
          [
           "```\n\nçŽ°åœ¨ï¼Œå¦‚æžœæˆ‘ä»¬æ›´æ”¹ `button_primary_background_fill` å˜é‡ï¼Œ`button_primary_background_fill_hover` å’Œ `button_..."
          ],
          [
           "```\n\n`button_primary_border_dark` å°†ä»Ž `button_primary_background_fill_dark` èŽ·å–å…¶å€¼ï¼Œå› ä¸ºæš—æ¨¡å¼æ€»æ˜¯ä½¿ç”¨å˜é‡çš„æš—ç‰ˆæœ¬ã€‚\n\n##..."
          ],
          [
           "<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-new-step-2.hf.space?__theme=light\"\n\tframebo..."
          ],
          [
           "åœ¨åˆ›å»ºä¸»é¢˜åŽï¼Œæ‚¨å¯ä»¥å°†å…¶ä¸Šä¼ åˆ° HuggingFace Hubï¼Œè®©å…¶ä»–äººæŸ¥çœ‹ã€ä½¿ç”¨å’Œæž„å»ºä¸»é¢˜ï¼\n\n### ä¸Šä¼ ä¸»é¢˜\n\næœ‰ä¸¤ç§ä¸Šä¼ ä¸»é¢˜çš„æ–¹å¼ï¼Œé€šè¿‡ä¸»é¢˜ç±»å®žä¾‹æˆ–å‘½ä»¤è¡Œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„â€œseafoam..."
          ],
          [
           "```\n\n- é€šè¿‡å‘½ä»¤è¡Œ\n\né¦–å…ˆå°†ä¸»é¢˜ä¿å­˜åˆ°ç£ç›˜\n\n```python\nseafoam.dump(filename=\"seafoam.json\")\n```\n\nç„¶åŽä½¿ç”¨â€œupload_themeâ€å‘½ä»¤ï¼š..."
          ],
          [
           "```\n\nè¦ä¸Šä¼ ä¸»é¢˜ï¼Œæ‚¨å¿…é¡»æ‹¥æœ‰ä¸€ä¸ª HuggingFace è´¦æˆ·ï¼Œå¹¶é€šè¿‡ `hf_token` å‚æ•°ä¼ é€’æ‚¨çš„[è®¿é—®ä»¤ç‰Œ](https://huggingface.co/docs/huggingfac..."
          ],
          [
           "### å‘çŽ°ä¸»é¢˜\n\n[ä¸»é¢˜åº“](https://huggingface.co/spaces/gradio/theme-gallery)æ˜¾ç¤ºäº†æ‰€æœ‰å…¬å¼€çš„ gradio ä¸»é¢˜ã€‚åœ¨å‘å¸ƒä¸»é¢˜ä¹‹åŽï¼Œ\nå®ƒå°†åœ¨å‡ åˆ†..."
          ],
          [
           "```\n\næ‚¨ä¹Ÿå¯ä»¥ç›´æŽ¥å°†ä¸»é¢˜å­—ç¬¦ä¸²ä¼ é€’ç»™ `Blocks` æˆ– `Interface`ï¼ˆ`gr.Blocks(theme=\"gradio/seafoam\")`ï¼‰\n\næ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨è¯­ä¹‰ç‰ˆæœ¬è¡¨è¾¾å¼å°†æ‚¨çš„åº”..."
          ],
          [
           "his demo shows how you can build an interactive dashboard with gradio. Click on a python library on ..."
          ],
          [
           "gradio\n\n## 4.11.0\n\n### Features..."
          ],
          [
           "- [#6842](https://github.com/gradio-app/gradio/pull/6842) [`846d52d`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6833](https://github.com/gradio-app/gradio/pull/6833) [`1b9d423`](https://github.com/gradio-app/..."
          ],
          [
           "### Fixes\n\n- [#6829](https://github.com/gradio-app/gradio/pull/6829) [`50496f9`](https://github.com/..."
          ],
          [
           "## 4.10.0\n\n### Features\n\n- [#6798](https://github.com/gradio-app/gradio/pull/6798) [`245d58e`](https..."
          ],
          [
           "### Fixes\n\n- [#6799](https://github.com/gradio-app/gradio/pull/6799) [`c352811`](https://github.com/..."
          ],
          [
           "## 4.9.1\n\n### Features\n\n- [#6781](https://github.com/gradio-app/gradio/pull/6781) [`a807ede`](https:..."
          ],
          [
           "### Fixes\n\n- [#6525](https://github.com/gradio-app/gradio/pull/6525) [`5d51fbc`](https://github.com/..."
          ],
          [
           "- [#6726](https://github.com/gradio-app/gradio/pull/6726) [`21cfb0a`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6745](https://github.com/gradio-app/gradio/pull/6745) [`3240d04`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6671](https://github.com/gradio-app/gradio/pull/6671) [`299f5e2`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6666](https://github.com/gradio-app/gradio/pull/6666) [`30c9fbb`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6704](https://github.com/gradio-app/gradio/pull/6704) [`24e0481`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6416](https://github.com/gradio-app/gradio/pull/6416) [`5177132`](https://github.com/gradio-app/..."
          ],
          [
           "### Fixes..."
          ],
          [
           "- [#6709](https://github.com/gradio-app/gradio/pull/6709) [`6a9151d`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6676](https://github.com/gradio-app/gradio/pull/6676) [`fe40308`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6639](https://github.com/gradio-app/gradio/pull/6639) [`9a6ff70`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6694](https://github.com/gradio-app/gradio/pull/6694) [`dfc61ec`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6759](https://github.com/gradio-app/gradio/pull/6759) [`28a7aa9`](https://github.com/gradio-app/..."
          ],
          [
           "## 4.8.0\n\n### Features..."
          ],
          [
           "- [#6624](https://github.com/gradio-app/gradio/pull/6624) [`1751f14`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6565](https://github.com/gradio-app/gradio/pull/6565) [`9bf1ad4`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6607](https://github.com/gradio-app/gradio/pull/6607) [`13ace03`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6572](https://github.com/gradio-app/gradio/pull/6572) [`206af31`](https://github.com/gradio-app/..."
          ],
          [
           "- [#6551](https://github.com/gradio-app/gradio/pull/6551) [`8fc562a`](https://github.com/gradio-app/..."
          ],
          [
           "## 4.7.1\n\n### Features\n\n- [#6537](https://github.com/gradio-app/gradio/pull/6537) [`6d3fecfa4`](http..."
          ],
          [
           "- [#6532](https://github.com/gradio-app/gradio/pull/6532) [`96290d304`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6523](https://github.com/gradio-app/gradio/pull/6523) [`63f466882`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6538](https://github.com/gradio-app/gradio/pull/6538) [`147926196`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6528](https://github.com/gradio-app/gradio/pull/6528) [`f53b01cbf`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6500](https://github.com/gradio-app/gradio/pull/6500) [`830b6c0e6`](https://github.com/gradio-ap..."
          ],
          [
           "## 4.5.0\n\n### Highlights\n\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gr..."
          ],
          [
           "```\n\n Thanks [@pngwn](https://github.com/pngwn)!\n\n### Fixes\n\n- [#6497](https://github.com/gradio-app..."
          ],
          [
           "- [#6428](https://github.com/gradio-app/gradio/pull/6428) [`ac4ca59c9`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6455](https://github.com/gradio-app/gradio/pull/6455) [`179f5bcde`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6423](https://github.com/gradio-app/gradio/pull/6423) [`62d35c3d1`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6419](https://github.com/gradio-app/gradio/pull/6419) [`1959471a8`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6441](https://github.com/gradio-app/gradio/pull/6441) [`2f805a7dd`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6457](https://github.com/gradio-app/gradio/pull/6457) [`d00fcf89d`](https://github.com/gradio-ap..."
          ],
          [
           "## 4.3.0\n\n### Features..."
          ],
          [
           "- [#6395](https://github.com/gradio-app/gradio/pull/6395) [`8ef48f852`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6099](https://github.com/gradio-app/gradio/pull/6099) [`d84209703`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6412](https://github.com/gradio-app/gradio/pull/6412) [`649f3ceb6`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6383](https://github.com/gradio-app/gradio/pull/6383) [`324867f63`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6414](https://github.com/gradio-app/gradio/pull/6414) [`da1e31832`](https://github.com/gradio-ap..."
          ],
          [
           "## 4.2.0\n\n### Features..."
          ],
          [
           "- [#6333](https://github.com/gradio-app/gradio/pull/6333) [`42f76aeeb`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6356](https://github.com/gradio-app/gradio/pull/6356) [`854b482f5`](https://github.com/gradio-ap..."
          ],
          [
           "### Fixes\n\n- [#6368](https://github.com/gradio-app/gradio/pull/6368) [`8a3f45c26`](https://github.co..."
          ],
          [
           "## 4.1.2\n\n### Features\n\n- [#6318](https://github.com/gradio-app/gradio/pull/6318) [`d3b53a457`](http..."
          ],
          [
           "- [#6310](https://github.com/gradio-app/gradio/pull/6310) [`dfdaf1092`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6317](https://github.com/gradio-app/gradio/pull/6317) [`19af2806a`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6311](https://github.com/gradio-app/gradio/pull/6311) [`176c4d140`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6309](https://github.com/gradio-app/gradio/pull/6309) [`c56128781`](https://github.com/gradio-ap..."
          ],
          [
           "## 4.1.1\n\n### Fixes\n\n- [#6288](https://github.com/gradio-app/gradio/pull/6288) [`92278729e`](https:/..."
          ],
          [
           "- [#6261](https://github.com/gradio-app/gradio/pull/6261) [`8bbeca0e7`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6240](https://github.com/gradio-app/gradio/pull/6240) [`dd901c1b0`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6232](https://github.com/gradio-app/gradio/pull/6232) [`ac4f2bcde`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6266](https://github.com/gradio-app/gradio/pull/6266) [`e32bac894`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6236](https://github.com/gradio-app/gradio/pull/6236) [`6bce259c5`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6249](https://github.com/gradio-app/gradio/pull/6249) [`2cffcf3c3`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6211](https://github.com/gradio-app/gradio/pull/6211) [`a4a931dd3`](https://github.com/gradio-ap..."
          ],
          [
           "## 4.0.2\n\n### Fixes\n\n- [#6191](https://github.com/gradio-app/gradio/pull/6191) [`b555bc09f`](https:/..."
          ],
          [
           "## 4.0.0\n\n### Highlights\n\n4.0 is a big release, so here are the main highlights:\n\n**1. Custom Compon..."
          ],
          [
           "<img style=\"width:50%\" src=\"https://i.imgur.com/ewUIuUc.png\">\n\n**4. Custom Share Servers**: \n\nGradio..."
          ],
          [
           "Gradio 4.0 is a new major version, and includes breaking changes from 3.x. Here's a list of all the ..."
          ],
          [
           "**Other changes related to the `gradio` library**:\n\n* Removes the deprecated `status_tracker` parame..."
          ],
          [
           "### Migrating to Gradio 4.0\n\nHere are some concrete tips to help migrate to Gradio 4.0:\n\n#### **Usin..."
          ],
          [
           "```\n\nIn order for the HTML component to be able to serve `image.png`, you will need to add `image.pn..."
          ],
          [
           "```\n\n\n#### **Using `concurrency_limit` instead of `concurrency_count`**\n\nPreviously, in Gradio 3.x, ..."
          ],
          [
           "To summarize migration:\n\n* For events that execute quickly or don't use much CPU or GPU resources, y..."
          ],
          [
           "```\n\nNow, you should write:\n\n```py\ngr.ImageEditor(sources=(), brush=gr.Brush(colors=[\"#000000\"]))\n``..."
          ],
          [
           "```\n\nUnlike the `Image` component, which passes the input image as a single value into the predictio..."
          ],
          [
           "- [#6184](https://github.com/gradio-app/gradio/pull/6184) [`86edc0199`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6153](https://github.com/gradio-app/gradio/pull/6153) [`1162ed621`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6152](https://github.com/gradio-app/gradio/pull/6152) [`982bff2fd`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6135](https://github.com/gradio-app/gradio/pull/6135) [`bce37ac74`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6098](https://github.com/gradio-app/gradio/pull/6098) [`c3bc515bf`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6129](https://github.com/gradio-app/gradio/pull/6129) [`0d261c6ec`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6082](https://github.com/gradio-app/gradio/pull/6082) [`037e5af33`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6014](https://github.com/gradio-app/gradio/pull/6014) [`cad537aac`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6018](https://github.com/gradio-app/gradio/pull/6018) [`184834d02`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6114](https://github.com/gradio-app/gradio/pull/6114) [`39227b6fa`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6089](https://github.com/gradio-app/gradio/pull/6089) [`cd8146ba0`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6027](https://github.com/gradio-app/gradio/pull/6027) [`de18102b8`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6044](https://github.com/gradio-app/gradio/pull/6044) [`9053c95a1`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6146](https://github.com/gradio-app/gradio/pull/6146) [`40a171ea6`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5826](https://github.com/gradio-app/gradio/pull/5826) [`ce036c5d4`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6076](https://github.com/gradio-app/gradio/pull/6076) [`f3f98f923`](https://github.com/gradio-ap..."
          ],
          [
           "## 3.45.0-beta.13\n\n### Features\n\n- [#5964](https://github.com/gradio-app/gradio/pull/5964) [`5fbda0b..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`d2314e53b`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5894](https://github.com/gradio-app/gradio/pull/5894) [`fee3d527e`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-ap..."
          ],
          [
           "## 3.48.0\n\n### Features..."
          ],
          [
           "- [#5627](https://github.com/gradio-app/gradio/pull/5627) [`b67115e8e`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5915](https://github.com/gradio-app/gradio/pull/5915) [`e24163e15`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5819](https://github.com/gradio-app/gradio/pull/5819) [`5f1cbc436`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5864](https://github.com/gradio-app/gradio/pull/5864) [`e70805d54`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5840](https://github.com/gradio-app/gradio/pull/5840) [`4e62b8493`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5904](https://github.com/gradio-app/gradio/pull/5904) [`891d42e9b`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5890](https://github.com/gradio-app/gradio/pull/5890) [`c4ba832b3`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5930](https://github.com/gradio-app/gradio/pull/5930) [`361823896`](https://github.com/gradio-ap..."
          ],
          [
           "## 3.47.1\n\n### Fixes\n\n- [#5816](https://github.com/gradio-app/gradio/pull/5816) [`796145e2c`](https:..."
          ],
          [
           "For more information check the [`FileExplorer` documentation](https://gradio.app/docs/fileexplorer)...."
          ],
          [
           "- [#5798](https://github.com/gradio-app/gradio/pull/5798) [`a0d3cc45c`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5794](https://github.com/gradio-app/gradio/pull/5794) [`f096c3ae1`](https://github.com/gradio-ap..."
          ],
          [
           "## 3.46.1\n\n### Features\n\n- [#5124](https://github.com/gradio-app/gradio/pull/5124) [`6e56a0d9b`](htt..."
          ],
          [
           "## 3.46.0\n\n### Features\n\n- [#5699](https://github.com/gradio-app/gradio/pull/5699) [`8f0fed857`](htt..."
          ],
          [
           "- [#5735](https://github.com/gradio-app/gradio/pull/5735) [`abb5e9df4`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5731](https://github.com/gradio-app/gradio/pull/5731) [`c9af4f794`](https://github.com/gradio-ap..."
          ],
          [
           "## 3.45.2\n\n### Features\n\n- [#5722](https://github.com/gradio-app/gradio/pull/5722) [`dba651904`](htt..."
          ],
          [
           "- [#5714](https://github.com/gradio-app/gradio/pull/5714) [`a0fc5a296`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5705](https://github.com/gradio-app/gradio/pull/5705) [`78e7cf516`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5726](https://github.com/gradio-app/gradio/pull/5726) [`96c4b97c7`](https://github.com/gradio-ap..."
          ],
          [
           "## 3.45.1\n\n### Fixes\n\n- [#5701](https://github.com/gradio-app/gradio/pull/5701) [`ee8eec1e5`](https:..."
          ],
          [
           "- [#5675](https://github.com/gradio-app/gradio/pull/5675) [`b619e6f6e`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5681](https://github.com/gradio-app/gradio/pull/5681) [`40de3d217`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5652](https://github.com/gradio-app/gradio/pull/5652) [`2e25d4305`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5660](https://github.com/gradio-app/gradio/pull/5660) [`d76555a12`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5240](https://github.com/gradio-app/gradio/pull/5240) [`da05e59a5`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5590](https://github.com/gradio-app/gradio/pull/5590) [`d1ad1f671`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5625](https://github.com/gradio-app/gradio/pull/5625) [`9ccc4794a`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5633](https://github.com/gradio-app/gradio/pull/5633) [`341402337`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5593](https://github.com/gradio-app/gradio/pull/5593) [`88d43bd12`](https://github.com/gradio-ap..."
          ],
          [
           "## 3.44.4\n\n### Features\n\n- [#5514](https://github.com/gradio-app/gradio/pull/5514) [`52f783175`](htt..."
          ],
          [
           "### Fixes\n\n- [#5587](https://github.com/gradio-app/gradio/pull/5587) [`e0d61b8ba`](https://github.co..."
          ],
          [
           "- [#5562](https://github.com/gradio-app/gradio/pull/5562) [`50d9747d0`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5553](https://github.com/gradio-app/gradio/pull/5553) [`d1bf23cd2`](https://github.com/gradio-ap..."
          ],
          [
           "## 3.44.2\n\n### Fixes\n\n- [#5537](https://github.com/gradio-app/gradio/pull/5537) [`301c7878`](https:/..."
          ],
          [
           "- [#5516](https://github.com/gradio-app/gradio/pull/5516) [`c5fe8eba`](https://github.com/gradio-app..."
          ],
          [
           "- [#5525](https://github.com/gradio-app/gradio/pull/5525) [`21f1db40`](https://github.com/gradio-app..."
          ],
          [
           "## 3.44.0\n\n### Features..."
          ],
          [
           "- [#5505](https://github.com/gradio-app/gradio/pull/5505) [`9ee20f49`](https://github.com/gradio-app..."
          ],
          [
           "- [#5488](https://github.com/gradio-app/gradio/pull/5488) [`8909e42a`](https://github.com/gradio-app..."
          ],
          [
           "- [#5474](https://github.com/gradio-app/gradio/pull/5474) [`041560f9`](https://github.com/gradio-app..."
          ],
          [
           "- [#5459](https://github.com/gradio-app/gradio/pull/5459) [`bd2fda77`](https://github.com/gradio-app..."
          ],
          [
           "- [#5496](https://github.com/gradio-app/gradio/pull/5496) [`82ec4d26`](https://github.com/gradio-app..."
          ],
          [
           "## 3.43.2\n\n### Fixes\n\n- [#5456](https://github.com/gradio-app/gradio/pull/5456) [`6e381c4f`](https:/..."
          ],
          [
           "- [#5165](https://github.com/gradio-app/gradio/pull/5165) [`c77f05ab`](https://github.com/gradio-app..."
          ],
          [
           "- [#5417](https://github.com/gradio-app/gradio/pull/5417) [`d14d63e3`](https://github.com/gradio-app..."
          ],
          [
           "### Fixes\n\n- [#5412](https://github.com/gradio-app/gradio/pull/5412) [`26fef8c7`](https://github.com..."
          ],
          [
           "Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\n\n#### Added the ability to attach event lis..."
          ],
          [
           "```\n\n Thanks [@aliabid94](https://github.com/aliabid94)!\n\n### Features..."
          ],
          [
           "- [#5334](https://github.com/gradio-app/gradio/pull/5334) [`c5bf9138`](https://github.com/gradio-app..."
          ],
          [
           "- [#5370](https://github.com/gradio-app/gradio/pull/5370) [`61803c65`](https://github.com/gradio-app..."
          ],
          [
           "- [#5369](https://github.com/gradio-app/gradio/pull/5369) [`b8968898`](https://github.com/gradio-app..."
          ],
          [
           "- [#5304](https://github.com/gradio-app/gradio/pull/5304) [`05892302`](https://github.com/gradio-app..."
          ],
          [
           "- [#5394](https://github.com/gradio-app/gradio/pull/5394) [`4d94ea0a`](https://github.com/gradio-app..."
          ],
          [
           "## 3.41.2\n\n### Features\n\n- [#5284](https://github.com/gradio-app/gradio/pull/5284) [`5f25eb68`](http..."
          ],
          [
           "## 3.41.1\n\n### Fixes\n\n- [#5324](https://github.com/gradio-app/gradio/pull/5324) [`31996c99`](https:/..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "```\n\n Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n#### Add `render` function to `<..."
          ],
          [
           "```\n\n Thanks [@hannahblair](https://github.com/hannahblair)!\n\n### Features..."
          ],
          [
           "- [#5268](https://github.com/gradio-app/gradio/pull/5268) [`f49028cf`](https://github.com/gradio-app..."
          ],
          [
           "- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app..."
          ],
          [
           "- [#5280](https://github.com/gradio-app/gradio/pull/5280) [`a2f42e28`](https://github.com/gradio-app..."
          ],
          [
           "- [#4943](https://github.com/gradio-app/gradio/pull/4943) [`947d615d`](https://github.com/gradio-app..."
          ],
          [
           "- [#5188](https://github.com/gradio-app/gradio/pull/5188) [`b22e1888`](https://github.com/gradio-app..."
          ],
          [
           "- [#5305](https://github.com/gradio-app/gradio/pull/5305) [`15075241`](https://github.com/gradio-app..."
          ],
          [
           "- [#5264](https://github.com/gradio-app/gradio/pull/5264) [`46a2b600`](https://github.com/gradio-app..."
          ],
          [
           "- [#5256](https://github.com/gradio-app/gradio/pull/5256) [`933db53e`](https://github.com/gradio-app..."
          ],
          [
           "- [#5179](https://github.com/gradio-app/gradio/pull/5179) [`6fb92b48`](https://github.com/gradio-app..."
          ],
          [
           "- [#5122](https://github.com/gradio-app/gradio/pull/5122) [`3b805346`](https://github.com/gradio-app..."
          ],
          [
           "- [#5231](https://github.com/gradio-app/gradio/pull/5231) [`87f1c2b4`](https://github.com/gradio-app..."
          ],
          [
           "- [#5235](https://github.com/gradio-app/gradio/pull/5235) [`1ecf88ac`](https://github.com/gradio-app..."
          ],
          [
           "## 3.40.0\n\n### Highlights\n\n#### Client.predict will now return the final output for streaming endpoi..."
          ],
          [
           "```python\nimport gradio as gr\nfrom pydub import AudioSegment\n\ndef stream_audio(audio_file):\n    audi..."
          ],
          [
           "```\n\nFrom the backend, streamed outputs are served from the `/stream/` endpoint instead of the `/fil..."
          ],
          [
           "- [#5081](https://github.com/gradio-app/gradio/pull/5081) [`d7f83823`](https://github.com/gradio-app..."
          ],
          [
           "- [#5125](https://github.com/gradio-app/gradio/pull/5125) [`80be7a1c`](https://github.com/gradio-app..."
          ],
          [
           "- [#5046](https://github.com/gradio-app/gradio/pull/5046) [`5244c587`](https://github.com/gradio-app..."
          ],
          [
           "- [#5047](https://github.com/gradio-app/gradio/pull/5047) [`883ac364`](https://github.com/gradio-app..."
          ],
          [
           "- [#5104](https://github.com/gradio-app/gradio/pull/5104) [`34f6b22e`](https://github.com/gradio-app..."
          ],
          [
           "- [#5035](https://github.com/gradio-app/gradio/pull/5035) [`8b4eb8ca`](https://github.com/gradio-app..."
          ],
          [
           "- [#5080](https://github.com/gradio-app/gradio/pull/5080) [`37caa2e0`](https://github.com/gradio-app..."
          ],
          [
           "- [#5062](https://github.com/gradio-app/gradio/pull/5062) [`7d897165`](https://github.com/gradio-app..."
          ],
          [
           "- [#5114](https://github.com/gradio-app/gradio/pull/5114) [`56d2609d`](https://github.com/gradio-app..."
          ],
          [
           "- [#5039](https://github.com/gradio-app/gradio/pull/5039) [`620e4645`](https://github.com/gradio-app..."
          ],
          [
           "- [#5140](https://github.com/gradio-app/gradio/pull/5140) [`cd1353fa`](https://github.com/gradio-app..."
          ],
          [
           "## 3.39.0\n\n### Highlights\n\n#### Create Discord Bots from Gradio Apps ðŸ¤– ([#4960](https://github.com/g..."
          ],
          [
           "```\n\n<img src=\"https://gradio-builds.s3.amazonaws.com/demo-files/discordbots/guide/llama_chat.gif\">\n..."
          ],
          [
           "But once again, you can deploy ANY `gr.ChatInterface` app exposed on the internet! So don't hesitate..."
          ],
          [
           "- [#4995](https://github.com/gradio-app/gradio/pull/4995) [`3f8c210b`](https://github.com/gradio-app..."
          ],
          [
           "- [#4985](https://github.com/gradio-app/gradio/pull/4985) [`b74f8453`](https://github.com/gradio-app..."
          ],
          [
           "### Fixes\n\n- [#4997](https://github.com/gradio-app/gradio/pull/4997) [`41c83070`](https://github.com..."
          ],
          [
           "- Provide a parameter `animate` (`False` by default) in `gr.make_waveform()` which animates the over..."
          ],
          [
           "- Add default sketch color argument `brush_color`. Also, masks drawn on images are now slightly tran..."
          ],
          [
           "### Bug Fixes:\n\n- Fixes `cancels` for generators so that if a generator is canceled before it is com..."
          ],
          [
           "## 3.37\n\n### New Features:\n\nIntroducing a new `gr.ChatInterface` abstraction, which allows Gradio us..."
          ],
          [
           "```\n\nWhich produces:\n\n<img width=\"1291\" alt=\"image\" src=\"https://github.com/gradio-app/gradio/assets..."
          ],
          [
           "- Chatbot messages now show hyperlinks to download files uploaded to `gr.Chatbot()` by [@dawoodkhan8..."
          ],
          [
           "```\n\n```py\nwith gr.Blocks() as demo:\n    gr.Markdown(\"Ø³Ù„Ø§Ù…\", rtl=True)\ndemo.launch()..."
          ],
          [
           "```\n\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton]..."
          ],
          [
           "* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@han..."
          ],
          [
           "### Other Changes:\n\n- Warning on mobile that if a user leaves the tab, websocket connection may brea..."
          ],
          [
           "## 3.36.1\n\n### New Features:\n\n- Hotfix to support pydantic v1 and v2 by [@freddyaboulton](https://gi..."
          ],
          [
           "No changes to highlight.\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n## 3.36.0\n\n### New Featur..."
          ],
          [
           "### Bug Fixes:..."
          ],
          [
           "- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg..."
          ],
          [
           "- Fix `make_waveform` to work with paths that contain spaces [@akx](https://github.com/akx) in [PR 4..."
          ],
          [
           "- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](h..."
          ],
          [
           "- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https://github.com/freddyabou..."
          ],
          [
           "- Fix bug where `show_label` was hiding the entire component for `gr.Label` by [@freddyaboulton](htt..."
          ],
          [
           "### Other Changes:..."
          ],
          [
           "- Add `.git-blame-ignore-revs` by [@akx](https://github.com/akx) in [PR 4586](https://github.com/gra..."
          ],
          [
           "- Better errors when you define two Blocks and reference components in one Blocks from the events in..."
          ],
          [
           "### Breaking Changes:\n\n[PR 4683](https://github.com/gradio-app/gradio/pull/4683) removes the explict..."
          ],
          [
           "### Other Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes to highlight.\n\n## 3...."
          ],
          [
           "- Min and max value for gr.Number by [@artegoser](https://github.com/artegoser) and [@dawoodkhan82](..."
          ],
          [
           "- Add `latex_delimiters` parameter to `Chatbot` to control the delimiters used for LaTeX and to disa..."
          ],
          [
           "Example:\n\n```python\ndef start_process(name):\n    gr.Info(\"Starting process\")\n    if name is None:\n  ..."
          ],
          [
           "```\n\n### Bug Fixes:..."
          ],
          [
           "- Add support for PAUSED state in the JS client by [@abidlabs](https://github.com/abidlabs) in [PR 4..."
          ],
          [
           "- Fix new line issue with `gr.Chatbot()` by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR ..."
          ],
          [
           "- Fix dispatched errors from within components [@aliabid94](https://github.com/aliabid94) in [PR 478..."
          ],
          [
           "### Other Changes:\n\n- Change styling of status and toast error components by [@hannahblair](https://..."
          ],
          [
           "### Breaking Changes:\n\n- The behavior of the `Clear` button has been changed for `Slider`, `Checkbox..."
          ],
          [
           "- Remove target=\"\\_blank\" override on anchor tags with internal targets by [@hannahblair](https://gi..."
          ],
          [
           "- Fix video rendering in Safari by [@aliabid94](https://github.com/aliabid94) in [PR 4433](https://g..."
          ],
          [
           "### Other Changes:\n\n- When running on Spaces, handler functions will be transformed by the [PySpaces..."
          ],
          [
           "No changes to highlight.\n\n## 3.33.1\n\n### New Features:\n\nNo changes to highlight.\n\n### Bug Fixes:\n\n- ..."
          ],
          [
           "### Breaking Changes:\n\nNo changes to highlight.\n\n## 3.33.0\n\n### New Features:\n\n- Introduced `gradio ..."
          ],
          [
           "- Fix bug where Label change event was triggering itself by [@freddyaboulton](https://github.com/fre..."
          ],
          [
           "- Do not send HF token to other domains via `/proxy` route by [@abidlabs](https://github.com/abidlab..."
          ],
          [
           "### Other Changes:\n\n- Remove flicker of loading bar by adding opacity transition, by [@aliabid94](ht..."
          ],
          [
           "### Bug Fixes:\n\n- Fixed Gallery/AnnotatedImage components not respecting GRADIO_DEFAULT_DIR variable..."
          ],
          [
           "### Other Changes:\n\n- Refactor web component `initial_height` attribute by [@whitphx](https://github..."
          ],
          [
           "### Breaking Changes:\n\nNo changes to highlight.\n\n## 3.31.0\n\n### New Features:\n\n- The reloader comman..."
          ],
          [
           "- Fix \"TypeError: issubclass() arg 1 must be a class\" When use Optional[Types] by [@lingfengchencn](..."
          ],
          [
           "- Fixes a bug with typing.get_type_hints() on Python 3.9 by [@abidlabs](https://github.com/abidlabs)..."
          ],
          [
           "### Other Changes:\n\n- Change `gr.Chatbot()` markdown parsing to frontend using `marked` library and ..."
          ],
          [
           "### Breaking Changes:\n\nNo changes to highlight.\n\n## 3.30.0\n\n### New Features:\n\n- Adds a `root_path` ..."
          ],
          [
           "### Bug Fixes:\n\n- Records username when flagging by [@abidlabs](https://github.com/abidlabs) in [PR ..."
          ],
          [
           "- Allow users to upload audio files in Audio component on iOS by by [@aliabid94](https://github.com/..."
          ],
          [
           "- Fix `gr.Slider` `release` event not triggering on mobile by [@space-nuko](https://github.com/space..."
          ],
          [
           "### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo ch..."
          ],
          [
           "### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo ch..."
          ],
          [
           "### New Features:\n\n- Add support for `visual-question-answering`, `document-question-answering`, and..."
          ],
          [
           "### Bug Fixes:\n\n- Fixes issue with `matplotlib` not rendering correctly if the backend was not set t..."
          ],
          [
           "### Full Changelog:\n\n- Safer version of `gr.HuggingFaceDatasetSaver` using HTTP methods instead of g..."
          ],
          [
           "### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\n- CI:..."
          ],
          [
           "- Fix duplicate play commands in full-screen mode of 'video'. by [@tomchang25](https://github.com/to..."
          ],
          [
           "- Fix issue in `gr.Gallery()` where setting height causes aspect ratio of images to collapse by [@da..."
          ],
          [
           "- Fix bug where port was not reused if the demo was closed and then re-launched by [@freddyaboulton]..."
          ],
          [
           "### Documentation Changes:\n\n- Make use of `gr` consistent across the docs by [@duerrsimon](https://g..."
          ],
          [
           "### Full Changelog:\n\n- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https://github.com..."
          ],
          [
           "![AnnotatedImage screenshot](https://user-images.githubusercontent.com/7870876/232142720-86e0020f-be..."
          ],
          [
           "```\n\nSee the [image_segmentation demo](https://github.com/gradio-app/gradio/tree/main/demo/image_seg..."
          ],
          [
           "```\n\n### Bug Fixes:\n\n- Fix code markdown support in `gr.Chatbot()` component by [@dawoodkhan82](http..."
          ],
          [
           "```python\nwith gr.Blocks() as demo:\n    img = gr.Image()\n    textbox = gr.Textbox()\n\n    def select_..."
          ],
          [
           "```\n\n![Recording 2023-04-08 at 17 44 39](https://user-images.githubusercontent.com/7870876/230748572..."
          ],
          [
           "- Increase timeout for sending analytics data by [@dawoodkhan82](https://github.com/dawoodkhan82) in..."
          ],
          [
           "- Fix bug where the upload button was not properly handling the `file_count='multiple'` case by [@fr..."
          ],
          [
           "### Documentation Changes:\n\n- Fix invalid argument docstrings, by [@akx](https://github.com/akx) in ..."
          ],
          [
           "### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo ch..."
          ],
          [
           "```\n\n  ![Theme Builder](https://user-images.githubusercontent.com/7870876/228204929-d71cbba5-69c2-45..."
          ],
          [
           "- Fixed bug where text for altair plots was not legible in dark mode by [@freddyaboulton](https://gi..."
          ],
          [
           "- Fixed bug where chatbot does not autoscroll inside of a tab, row or column by [@dawoodkhan82](http..."
          ],
          [
           "- Fixes certain `_js` return values being double wrapped in an array, by [@space-nuko](https://githu..."
          ],
          [
           "- Fix items in `gr.Dropdown` besides the selected item receiving a checkmark, by [@space-nuko](https..."
          ],
          [
           "### Documentation Changes:\n\n- Makes some fixes to the Theme Guide related to naming of variables, by..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\n- Removed heavily-mocked tests related to comet_ml, wandb, ..."
          ],
          [
           "### Full Changelog:\n\n- Mobile responsive iframes in themes guide by [@aliabd](https://github.com/ali..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.23.0\n\n### New Features:\n\n###### Theme Sha..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.22.1\n\n### New Features:\n\nNo changes to hi..."
          ],
          [
           "###### `elem_classes`\n\nAdd keyword argument `elem_classes` to Components to control class names of c..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\n- Pinned `pyright==1.1.298` for stability by [@abidlabs](ht..."
          ],
          [
           "###### Uploading\n\nThere are two ways to upload a theme, via the theme class instance or the command ..."
          ],
          [
           "```\n\n2. Via the command line\n\nFirst save the theme to disk\n\n```python\nmy_theme.dump(filename=\"my_the..."
          ],
          [
           "```\n\nby [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3428](https://github.com/gradio-..."
          ],
          [
           "```\n\n<img width=\"1054\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1778297/224116682-..."
          ],
          [
           "```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gallery = gr.Gallery([\"images/1.jpg\", \"..."
          ],
          [
           "```\n\nBy [@aliabid94](https://github.com/aliabid94) in [PR 3399](https://github.com/gradio-app/gradio..."
          ],
          [
           "### Bug Fixes:\n\n- Use `huggingface_hub` to send telemetry on `interface` and `blocks`; eventually to..."
          ],
          [
           "### Documentation Changes:\n\n- Added a section on security and access when sharing Gradio apps by [@a..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\n- Fixes tests that were failing locally but passing on CI b..."
          ],
          [
           "### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\n- Prevent in-place updates of ..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.20.1\n\n### New Features:\n\n- Add `height` k..."
          ],
          [
           "```\n\n### Bug Fixes:\n\n- Ensure uploaded images are always shown in the sketch tool by [@pngwn](https:..."
          ],
          [
           "```\n\nby [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3211](https://github.com/gradio-app/..."
          ],
          [
           "- Updated image upload component to accept all image formats, including lossless formats like .webp ..."
          ],
          [
           "- Allow developers to access the username of a logged-in user from the `gr.Request()` object using t..."
          ],
          [
           "- Ensure `mirror_webcam` is always respected by [@pngwn](https://github.com/pngwn) in [PR 3245](http..."
          ],
          [
           "- Remove embed's `initial_height` when loading is complete so the embed finds its natural height onc..."
          ],
          [
           "- Fix bug where `height` set in `Gallery.style` was not respected by the front-end by [@freddyaboult..."
          ],
          [
           "- Fix error when using backen_fn and custom js at the same time by [@jialeicui](https://github.com/j..."
          ],
          [
           "### Documentation Changes:\n\n- Added the `types` field to the dependency field in the config by [@fre..."
          ],
          [
           "### Full Changelog:\n\n- Fixed comment typo in components.py by [@eltociear](https://github.com/eltoci..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.19.0\n\n### New Features:\n\n###### Improved ..."
          ],
          [
           "###### New `gr.BarPlot` component! ðŸ“Š\n\nCreate interactive bar plots from a high-level interface with ..."
          ],
          [
           "```\n\nBy [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3157](https://github.com/gradio-..."
          ],
          [
           "```\n\nBy [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 3165](https://github.com/gradio-app/..."
          ],
          [
           "- Fixes `gr.utils.delete_none` to only remove props whose values are `None` from the config by [@abi..."
          ],
          [
           "- Fix bug where auth cookies where not sent when connecting to an app via http by [@freddyaboulton](..."
          ],
          [
           "### Documentation Changes:\n\n- Sort components in docs by alphabetic order by [@aliabd](https://githu..."
          ],
          [
           "- Fix demos page css and add close demos button by [@aliabd](https://github.com/aliabd) in [PR 3151]..."
          ],
          [
           "- Fixed gradio share links so that they are persistent and do not reset if network\n  connection is d..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.18.0\n\n### New Features:\n\n###### Revamped ..."
          ],
          [
           "```\n\nBy [@maxaudron](https://github.com/maxaudron) in [PR 3075](https://github.com/gradio-app/gradio..."
          ],
          [
           "- Fixes URL resolution on Windows by [@abidlabs](https://github.com/abidlabs) in [PR 3108](https://g..."
          ],
          [
           "- A share link will automatically be created when running on Sagemaker notebooks so that the front-e..."
          ],
          [
           "### Documentation Changes:\n\n- Added a guide on the 4 kinds of Gradio Interfaces by [@yvrjsharma](htt..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.17.1\n\n### New Features:\n\n###### iOS image..."
          ],
          [
           "### Bug Fixes:\n\n- Fix bug where examples were not rendered correctly for demos created with Blocks a..."
          ],
          [
           "* Fix a broken link in the Quick Start guide, by [@cakiki](https://github.com/cakiki) in [PR 3109](h..."
          ],
          [
           "```\n\n<img width=\"1087\" alt=\"image\" src=\"https://user-images.githubusercontent.com/41651716/213260197..."
          ],
          [
           "```\n\n![chatbot_load](https://user-images.githubusercontent.com/41651716/213260220-3eaa25b7-a38b-48c6..."
          ],
          [
           "- Fixes bug where interpretation event was not configured correctly by [@freddyaboulton](https://git..."
          ],
          [
           "- Fixes bug where temporary uploaded files were not being added to temp sets by [@abidlabs](https://..."
          ],
          [
           "- Added better support for symlinks in the way absolute paths are resolved by [@abidlabs](https://gi..."
          ],
          [
           "- Adding missing embedded components on docs by [@aliabd](https://github.com/aliabd) in [PR 3027](ht..."
          ],
          [
           "- Preserve selected image of Gallery through updated by [@freddyaboulton](https://github.com/freddya..."
          ],
          [
           "### Documentation Changes:\n\n- SEO improvements to guides by[@aliabd](https://github.com/aliabd) in [..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.16.2\n\n### New Features:\n\nNo changes to hi..."
          ],
          [
           "- Fixed file upload fails for files with zero size by [@dawoodkhan82](https://github.com/dawoodkhan8..."
          ],
          [
           "- Fix bug where outputs for examples where not being returned by the backend by [@freddyaboulton](ht..."
          ],
          [
           "### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo ch..."
          ],
          [
           "```\n\nProgress indicator bar by [@aliabid94](https://github.com/aliabid94) in [PR 2750](https://githu..."
          ],
          [
           "```\n\n<img width=\"610\" alt=\"Screenshot 2023-01-03 at 4 14 36 PM\" src=\"https://user-images.githubuserc..."
          ],
          [
           "- Fixed bug where an error opening an audio file led to a crash by [@FelixDombek](https://github.com..."
          ],
          [
           "### Documentation Changes:\n\n- Added a Guide on using Google Sheets to create a real-time dashboard w..."
          ],
          [
           "### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\n- The `default_enabled` parame..."
          ],
          [
           "With this component you can easily create time series visualizations with customizable\nappearance fo..."
          ],
          [
           "```\n\n![image](https://user-images.githubusercontent.com/41651716/208711646-81ae3745-149b-46a3-babd-0..."
          ],
          [
           "### Documentation Changes:\n\n- Added a Guide on using BigQuery with Gradio's `DataFrame` and `Scatter..."
          ],
          [
           "No changes to highlight.\n\n## 3.14.0\n\n### New Features:\n\n###### Add Waveform Visual Support to Audio\n..."
          ],
          [
           "```\n\n### Bug Fixes:\n\n- Fixed issue where too many temporary files were created, all with randomly ge..."
          ],
          [
           "No changes to highlight.\n\n### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.13.1\n\n### New F..."
          ],
          [
           "```\n\nThese links are a more secure and scalable way to create shareable demos!\n\n### Bug Fixes:\n\n- Al..."
          ],
          [
           "### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\n- Fixed typo in parameter `vis..."
          ],
          [
           "The `gr.ScatterPlot` component accepts a pandas dataframe and some optional configuration parameters..."
          ],
          [
           "```\n\n<img width=\"404\" alt=\"image\" src=\"https://user-images.githubusercontent.com/41651716/206737726-..."
          ],
          [
           "```\n\n<img width=\"1366\" alt=\"image\" src=\"https://user-images.githubusercontent.com/41651716/204660697..."
          ],
          [
           "```\n\n![label_bg_color_update](https://user-images.githubusercontent.com/41651716/204400372-80e53857-..."
          ],
          [
           "### Bug Fixes:\n\n- Fixed issue where image thumbnails were not showing when an example directory was ..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes..."
          ],
          [
           "Here's a simple example that references a local image `lion.jpg` that is in the same\nfolder as the P..."
          ],
          [
           "```\n\n![Alt text](https://user-images.githubusercontent.com/1778297/204357455-5c1a4002-eee7-479d-9a1e..."
          ],
          [
           "```\n\n###### Update Accordion properties from the backend\n\nYou can now update the Accordion `label` a..."
          ],
          [
           "```\n\n![update_accordion](https://user-images.githubusercontent.com/41651716/203164176-b102eae3-babe-..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes..."
          ],
          [
           "## 3.11.0\n\n### New Features:\n\n###### Upload Button\n\nThere is now a new component called the `UploadB..."
          ],
          [
           "```\n\n###### Revamped API documentation page\n\nNew API Docs page with in-browser playground and update..."
          ],
          [
           "```\n\n### Bug Fixes:\n\n- Fixed bug that limited files from being sent over websockets to 16MB. The new..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes..."
          ],
          [
           "### Bug Fixes:\n\n- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](htt..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes..."
          ],
          [
           "When you load an upstream app with `gr.Blocks.load`, you can now specify which fn\nto call with the `..."
          ],
          [
           "```\n\nThe `api_name` parameter will take precedence over the `fn_index` parameter.\n\n### Bug Fixes:\n\n-..."
          ],
          [
           "### Testing and Infrastructure Changes:\n\nNo changes to highlight.\n\n### Breaking Changes:\n\nNo changes..."
          ],
          [
           "This can be used to:\n\n- Create live visualizations that show the most up to date data\n- Refresh the ..."
          ],
          [
           "```\n\n![live_demo](https://user-images.githubusercontent.com/41651716/198357377-633ce460-4e31-47bd-82..."
          ],
          [
           "### Bug Fixes:\n\n- Fix whitespace issue when using plotly. [@dawoodkhan82](https://github.com/dawoodk..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.7\n\n### New Features:\n\n###### Batched Func..."
          ],
          [
           "```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio\nserver can a..."
          ],
          [
           "```\n\n### Bug Fixes:\n\n- Fixes issue where plotly animations, interactivity, titles, legends, were not..."
          ],
          [
           "### Documentation Changes:\n\n- Added an example interactive dashboard to the \"Tabular & Plots\" sectio..."
          ],
          [
           "- Fixes the error message if a user builds Gradio locally and tries to use `share=True` by [@abidlab..."
          ],
          [
           "- Clearer error message when events are defined outside of a Blocks scope, and a warning if you\n  tr..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.6\n\n### New Features:\n\n###### Cancelling R..."
          ],
          [
           "cancel_on_change.change(None, None, None, cancels=[click_event, pred_event])\n\n\ndemo.queue(concurrenc..."
          ],
          [
           "```\n\nFor interfaces, a stop button will be added automatically if the function uses a `yield` statem..."
          ],
          [
           "```\n\n![stop_interface_rl](https://user-images.githubusercontent.com/41651716/195952883-e7ca4235-aae3..."
          ],
          [
           "### Documentation Changes:\n\n- Adds a demo to show how a sound alert can be played upon completion of..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.5\n\n### Bug Fixes:\n\n- Ensure that Gradio d..."
          ],
          [
           "### New Features:\n\n- When an `Image` component is set to `source=\"upload\"`, it is now possible to dr..."
          ],
          [
           "- Speeds up Gallery component by using temporary files instead of base64 representation in the front..."
          ],
          [
           "- Updated share link message to reference new Spaces Hardware [@abidlabs](https://github.com/abidlab..."
          ],
          [
           "- Stops a gradio launch from hogging a port even after it's been killed [@aliabid94](https://github...."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.4.1\n\n### New Features:\n\n###### 1. See Pas..."
          ],
          [
           "1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357]..."
          ],
          [
           "7. Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](..."
          ],
          [
           "### Documentation Changes:\n\n1. New Guide: Connecting to a Database ðŸ—„ï¸\n\n   A new guide by [@freddyabo..."
          ],
          [
           "- Create a guide on how to connect an app to a database hosted on the cloud by [@freddyaboulton](htt..."
          ],
          [
           "- Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [..."
          ],
          [
           "- Lets users provide a `gr.update()` dictionary even if post-processing is disabled [@abidlabs](http..."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n## 3.4\n\n### New Features:\n\n###### 1. Gallery C..."
          ],
          [
           "```\n\n<img src=\"https://user-images.githubusercontent.com/9021060/192399521-7360b1a9-7ce0-443e-8e94-8..."
          ],
          [
           "```\n\n![color-sketch](https://user-images.githubusercontent.com/9021060/192410500-3c8c3e64-a5fd-4df2-..."
          ],
          [
           "```\n\n![webcam-sketch](https://user-images.githubusercontent.com/9021060/192410820-0ffaf324-776e-4e1f..."
          ],
          [
           "As well as other fixes\n\n### Bug Fixes:\n\n1. Fix bug where max concurrency count is not respected in q..."
          ],
          [
           "### Documentation Changes:\n\n1. Adding a Playground Tab to the Website by [@aliabd](https://github.co..."
          ],
          [
           "- Website fixes and refactoring by [@aliabd](https://github.com/aliabd) in [PR 2280](https://github...."
          ],
          [
           "- Respect Upstream Queue when loading interfaces/blocks from Spaces by [@freddyaboulton](https://git..."
          ],
          [
           "- Fix Web Tracker Script by [@aliabd](https://github.com/aliabd) in [PR 2308](https://github.com/gra..."
          ],
          [
           "### Contributors Shoutout:\n\n- [@SkyTNT](https://github.com/SkyTNT) made their first contribution in ..."
          ],
          [
           "```\n\n![example](https://user-images.githubusercontent.com/9021060/189086273-f5e7087d-71fa-4158-90a9-..."
          ],
          [
           "```\n\n![187936493-5c90c01d-a6dd-400f-aa42-833a096156a1](https://user-images.githubusercontent.com/902..."
          ],
          [
           "- safari fixes by [@pngwn](https://github.com/pngwn) in [PR 2138](https://github.com/gradio-app/grad..."
          ],
          [
           "- Fixed misleading log when server_name is '0.0.0.0' by [@lamhoangtung](https://github.com/lamhoangt..."
          ],
          [
           "- Preserve Labels In Interpretation Components by [@freddyaboulton](https://github.com/freddyaboulto..."
          ],
          [
           "### Contributors Shoutout:\n\n- [@lamhoangtung](https://github.com/lamhoangtung) made their first cont..."
          ],
          [
           "```\n\n- Configure a maximum queue size\n\n```python\ndemo = gr.Interface(...)\ndemo.queue(max_size=100)\nd..."
          ],
          [
           "```\n\n- Automatic conversion of videos so they are playable in the browser (thanks to PR #2003). Grad..."
          ],
          [
           "```\n\n###### 5. New Guide ðŸ–Šï¸\n\n- [Gradio and W&B Integration](https://gradio.app/Gradio_and_Wandb_Inte..."
          ],
          [
           "- Reset components to original state by setting value to None by [@freddyaboulton](https://github.co..."
          ],
          [
           "- Encourage people to keep trying when queue full by [@apolinario](https://github.com/apolinario) in..."
          ],
          [
           "- Allow frontend method execution on Block.load event by [@codedealer](https://github.com/codedealer..."
          ],
          [
           "- feat(samples table/gallery): Crop thumbs to square by [@ronvoluted](https://github.com/ronvoluted)..."
          ],
          [
           "### Contributors Shoutout:\n\n- [@chrisemezue](https://github.com/chrisemezue) made their first contri..."
          ],
          [
           "```\n\nBut you can also embed demos that are running anywhere, you just need to link the demo to `src`..."
          ],
          [
           "```\n\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ex..."
          ],
          [
           "###### 5. `gr.Examples()` for Blocks ðŸ§±\n\nWe've added the `gr.Examples` component helper to allow you ..."
          ],
          [
           "### Full Changelog:..."
          ],
          [
           "- File component: list multiple files and allow for download #1446 by [@dawoodkhan82](https://github..."
          ],
          [
           "- Add python-3.7 tests by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 1818](https://..."
          ],
          [
           "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution..."
          ],
          [
           "```\n\n![hello-blocks](https://user-images.githubusercontent.com/9021060/168684108-78cbd24b-e6bd-4a04-..."
          ],
          [
           "###### 4. New Components: Model3D, Dataset, and More..\n\nWe've introduced a lot of new components in ..."
          ],
          [
           "- Gradio dash fe by [@pngwn](https://github.com/pngwn) in [PR 807](https://github.com/gradio-app/gra..."
          ],
          [
           "- add test infra + add browser tests to CI by [@pngwn](https://github.com/pngwn) in [PR 852](https:/..."
          ],
          [
           "- backend_default_value_refactoring by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 871](..."
          ],
          [
           "- 3d Image Component by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 775](https://github...."
          ],
          [
           "- Redesign 1 by [@pngwn](https://github.com/pngwn) in [PR 918](https://github.com/gradio-app/gradio/..."
          ],
          [
           "- allow audio components to take a string value by [@pngwn](https://github.com/pngwn) in [PR 930](ht..."
          ],
          [
           "- add frontend for page load events by [@pngwn](https://github.com/pngwn) in [PR 967](https://github..."
          ],
          [
           "- State and variables by [@aliabid94](https://github.com/aliabid94) in [PR 977](https://github.com/g..."
          ],
          [
           "- added interactive parameter to components by [@abidlabs](https://github.com/abidlabs) in [PR 992](..."
          ],
          [
           "- release 2.9.4 by [@abidlabs](https://github.com/abidlabs) in [PR 1006](https://github.com/gradio-a..."
          ],
          [
           "- fixed failing test on main by [@abidlabs](https://github.com/abidlabs) in [PR 1023](https://github..."
          ],
          [
           "- GAN Gradio Guide: Adjustments to iframe heights by [@NimaBoscarino](https://github.com/NimaBoscari..."
          ],
          [
           "- Update text by [@ronvoluted](https://github.com/ronvoluted) in [PR 1050](https://github.com/gradio..."
          ],
          [
           "- inputless-interfaces by [@FarukOzderim](https://github.com/FarukOzderim) in [PR 1038](https://gith..."
          ],
          [
           "- Dark text by [@ronvoluted](https://github.com/ronvoluted) in [PR 1049](https://github.com/gradio-a..."
          ],
          [
           "- Website Reload: README in demos docker by [@aliabd](https://github.com/aliabd) in [PR 1100](https:..."
          ],
          [
           "- Interface types: handle input-only, output-only, and unified interfaces by [@abidlabs](https://git..."
          ],
          [
           "- Stacked form inputs css by [@gary149](https://github.com/gary149) in [PR 1134](https://github.com/..."
          ],
          [
           "- add select event for tabitems by [@pngwn](https://github.com/pngwn) in [PR 1154](https://github.co..."
          ],
          [
           "- image gallery component + img css by [@aliabid94](https://github.com/aliabid94) in [PR 1140](https..."
          ],
          [
           "- enable flex grow for gr-box by [@radames](https://github.com/radames) in [PR 1165](https://github...."
          ],
          [
           "- 962 dataframe by [@pngwn](https://github.com/pngwn) in [PR 1186](https://github.com/gradio-app/gra..."
          ],
          [
           "- add copy functionality to json by [@pngwn](https://github.com/pngwn) in [PR 1205](https://github.c..."
          ],
          [
           "- Hotfixes for course demos by [@abidlabs](https://github.com/abidlabs) in [PR 1222](https://github...."
          ],
          [
           "- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](h..."
          ],
          [
           "- Labels spacing by [@gary149](https://github.com/gary149) in [PR 1254](https://github.com/gradio-ap..."
          ],
          [
           "- Fixes to components by [@abidlabs](https://github.com/abidlabs) in [PR 1260](https://github.com/gr..."
          ],
          [
           "- Add embedded demos to website by [@aliabid94](https://github.com/aliabid94) in [PR 1270](https://g..."
          ],
          [
           "- Create Streamables by [@aliabid94](https://github.com/aliabid94) in [PR 1279](https://github.com/g..."
          ],
          [
           "- Mobile responsive guides by [@aliabd](https://github.com/aliabd) in [PR 1293](https://github.com/g..."
          ],
          [
           "### Contributors Shoutout:\n\n- [@JefferyChiang](https://github.com/JefferyChiang) made their first co..."
          ],
          [
           "@gradio/statustracker\n\n## 0.4.3\n\n### Features\n\n- [#6814](https://github.com/gradio-app/gradio/pull/6..."
          ],
          [
           "## 0.4.0\n\n### Highlights\n\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gr..."
          ],
          [
           "```\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n## 0.3.2\n\n### Patch Changes\n\n- Updated dependencies..."
          ],
          [
           "## 0.3.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.3.0-beta.6\n\n### Features\n\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5342](https://github.com/gradio-app/gradio/pull/5342) [`afac0006`](https..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https..."
          ],
          [
           "create-svelte\n\nEverything you need to build a Svelte project, powered by [`create-svelte`](https://g..."
          ],
          [
           "imple image classification in Pytorch with Gradio's Image input and Label output...."
          ],
          [
           "`@gradio/atoms`\n\n```html\n<script lang=\"ts\">\n\timport { Block, BlockTitle, BlockLabel, IconButton, Emp..."
          ],
          [
           "```\n\nShareButton:\n```javascript\n\texport let formatter: (arg0: any) => Promise<string>;\n\texport let v..."
          ],
          [
           "his text generation demo works like autocomplete. There's only one textbox and it's used for both th..."
          ],
          [
           "Gradio Demo: sound_alert\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo r..."
          ],
          [
           "Gradio Demo: theme_extended_step_2\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimpor..."
          ],
          [
           "å®žæ—¶è¯­éŸ³è¯†åˆ«\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/streaming-asr-paused, https://hugging..."
          ],
          [
           "<iframe src=\"https://abidlabs-streaming-asr-paused.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gra..."
          ],
          [
           "ä¸‹é¢æ˜¯æž„å»ºå®žæ—¶è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åº”ç”¨ç¨‹åºçš„æ­¥éª¤ï¼š\n\n1. [è®¾ç½® Transformers ASR æ¨¡åž‹](#1-set-up-the-transformers-asr-model)\n2. [ä½¿ç”¨ T..."
          ],
          [
           "```\n\nå°±æ˜¯è¿™æ ·ï¼é»˜è®¤æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡åž‹ç®¡é“ä¼šåŠ è½½ Facebook çš„ `facebook/wav2vec2-base-960h` æ¨¡åž‹ã€‚\n\n## 2. ä½¿ç”¨ Transformers åˆ›å»º..."
          ],
          [
           "```\n\né‚£ä¹ˆè¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ`transcribe` å‡½æ•°æŽ¥å—ä¸€ä¸ªå‚æ•° `audio`ï¼Œå®ƒæ˜¯ç”¨æˆ·å½•åˆ¶çš„éŸ³é¢‘æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„ã€‚`pipeline` å¯¹è±¡æœŸæœ›ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç„¶åŽè¿”å›žåˆ°å‰ç«¯..."
          ],
          [
           "å¥½æ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°è°ƒæ•´åˆšåˆšåˆ›å»ºçš„æ¼”ç¤ºï¼Œä½¿å…¶æˆä¸ºæµå¼çš„ï¼Œä½¿ç”¨ç›¸åŒçš„ `Wav2Vec2` æ¨¡åž‹ã€‚\n\næœ€å¤§çš„å˜åŒ–æ˜¯æˆ‘ä»¬çŽ°åœ¨å¿…é¡»å¼•å…¥ä¸€ä¸ª `state` å‚æ•°ï¼Œå®ƒä¿å­˜åˆ°ç›®å‰ä¸ºæ­¢*è½¬å½•çš„éŸ³é¢‘*ã€‚è¿™æ ·ï¼Œ..."
          ],
          [
           "```\n\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å¦ä¸€ä¸ªæ›´æ”¹ï¼Œå³æˆ‘ä»¬è®¾ç½®äº† `live=True`ã€‚è¿™ä½¿å¾— Gradio æŽ¥å£ä¿æŒæŒç»­è¿è¡Œï¼Œå› æ­¤å®ƒå¯ä»¥è‡ªåŠ¨è½¬å½•éŸ³é¢‘ï¼Œè€Œæ— éœ€ç”¨æˆ·åå¤ç‚¹å‡»æäº¤æŒ‰é’®ã€‚\n\nè®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„æ•ˆæžœï¼ˆåœ¨ä¸‹..."
          ],
          [
           "```python\nfrom transformers import pipeline\nimport gradio as gr\nimport time\n\np = pipeline(\"automatic..."
          ],
          [
           "```\n\nå°è¯•ä¸‹é¢çš„æ¼”ç¤ºï¼ŒæŸ¥çœ‹å·®å¼‚ï¼ˆæˆ–[åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€](https://huggingface.co/spaces/abidlabs/streaming-asr-paused)ï¼‰ï¼\n\n<ifram..."
          ],
          [
           "ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼ˆåœ¨ Linux ä¸Šï¼‰ï¼š\n\né¦–å…ˆé€šè¿‡ç»ˆç«¯å®‰è£… DeepSpeech åº“å¹¶ä¸‹è½½é¢„è®­ç»ƒæ¨¡åž‹ï¼š\n\n```bash\nwget https://github.com/mozilla/Deep..."
          ],
          [
           "```\n\nç„¶åŽï¼Œåˆ›å»ºä¸Žä¹‹å‰ç›¸ä¼¼çš„ `transcribe()` å‡½æ•°ï¼š\n\n```python\nfrom deepspeech import Model\nimport numpy as np\n\nmode..."
          ],
          [
           "```\n\nè¿è¡Œæ‰€æœ‰è¿™äº›åº”è¯¥å…è®¸æ‚¨ä½¿ç”¨ä¸€ä¸ªæ¼‚äº®çš„ GUI éƒ¨ç½²å®žæ—¶ ASR æ¨¡åž‹ã€‚å°è¯•ä¸€ä¸‹ï¼Œçœ‹å®ƒåœ¨æ‚¨é‚£é‡Œè¿è¡Œå¾—æœ‰å¤šå¥½ã€‚\n\n---\n\nä½ å·²ç»å®Œæˆäº†ï¼è¿™å°±æ˜¯æž„å»ºç”¨äºŽ ASR æ¨¡åž‹çš„åŸºäºŽ Web çš„ GUI ..."
          ],
          [
           "Gradio Demo: sine_curve\n\n\n```\n!pip install -q gradio plotly\n```\n\n\n```\nimport math\nimport gradio as g..."
          ],
          [
           "his text generation demo takes in input text and returns generated text. It uses the Transformers li..."
          ],
          [
           "Gradio Demo: color_generator\n\n\n```\n!pip install -q gradio opencv-python numpy\n```..."
          ],
          [
           "```\n\n\n```\nimport gradio as gr\nimport cv2\nimport numpy as np\nimport random\n\n\n# Convert decimal color ..."
          ],
          [
           "@gradio/client\n\n## 0.9.3\n\n### Features\n\n- [#6814](https://github.com/gradio-app/gradio/pull/6814) [`..."
          ],
          [
           "## 0.9.0\n\n### Features\n\n- [#6398](https://github.com/gradio-app/gradio/pull/6398) [`67ddd40`](https:..."
          ],
          [
           "## 0.8.1\n\n### Fixes\n\n- [#6383](https://github.com/gradio-app/gradio/pull/6383) [`324867f63`](https:/..."
          ],
          [
           "## 0.7.1\n\n### Features\n\n- [#6137](https://github.com/gradio-app/gradio/pull/6137) [`2ba14b284`](http..."
          ],
          [
           "## 0.7.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.7.0-beta.1\n\n### Features\n\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40..."
          ],
          [
           "## 0.7.0-beta.0\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.5.2\n\n### Fixes\n\n- [#5840](https://github.com/gradio-app/gradio/pull/5840) [`4e62b8493`](https:/..."
          ],
          [
           "Thanks to a new capability that allows components to communicate directly with the server _without_ ..."
          ],
          [
           "### Fixes\n\n- [#5776](https://github.com/gradio-app/gradio/pull/5776) [`c0fef4454`](https://github.co..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5682](https://github.com/gradio-app/gradio/pull/5682) [`c57f1b75e`](http..."
          ],
          [
           "## 0.3.1\n\n### Fixes\n\n- [#5412](https://github.com/gradio-app/gradio/pull/5412) [`26fef8c7`](https://..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5133](https://github.com/gradio-app/gradio/pull/5133) [`61129052`](https..."
          ],
          [
           "## 0.1.4\n\n### Patch Changes\n\n- [#4717](https://github.com/gradio-app/gradio/pull/4717) [`ab5d1ea0`](..."
          ],
          [
           "- [#4315](https://github.com/gradio-app/gradio/pull/4315) [`b525b122`](https://github.com/gradio-app..."
          ],
          [
           "- [#4202](https://github.com/gradio-app/gradio/pull/4202) [`a26e9afd`](https://github.com/gradio-app..."
          ],
          [
           "- [#3605](https://github.com/gradio-app/gradio/pull/3605) [`ae4277a9`](https://github.com/gradio-app..."
          ],
          [
           "@gradio/label\n\n## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/g..."
          ],
          [
           "## 0.2.4\n\n### Patch Changes\n\n- Updated dependencies [[`206af31`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.2.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.2.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`8f0fed857`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/grad..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gr..."
          ],
          [
           "Case Study: A Component to Display PDFs\n\nLet's work through an example of building a custom gradio c..."
          ],
          [
           "```\n\n\nTip: You should change the name of the component.\nSome of the screenshots assume the component..."
          ],
          [
           "The complete `package.json` should look like this:\n\n```json\n{\n  \"name\": \"gradio_pdf\",\n  \"version\": \"..."
          ],
          [
           "```\n\n\nTip: Running `npm install` will install the latest version of the package available. You can i..."
          ],
          [
           "```\n\n## Step 3: Frontend - Launching the Dev Server\n\nRun the `dev` command to launch the development..."
          ],
          [
           "export let elem_id = \"\";\n\texport let elem_classes: string[] = [];\n\texport let visible = true;\n\texpor..."
          ],
          [
           "```\n\n\nTip: The `gradio`` object passed in here contains some metadata about the application as well ..."
          ],
          [
           "```\n\nYou should see the following when you navigate to your app after saving your current changes:\n\n..."
          ],
          [
           "```\n\nNow import `PdfUploadText.svelte` in your `<script>` and pass it to the `Upload` component!\n\n``..."
          ],
          [
           "```\n\nAlso create the following variables:\n\n```ts\n    let pdfDoc;\n    let numPages = 1;\n    let curre..."
          ],
          [
           "```\n\n\nTip: The `$:` syntax in svelte is how you declare statements to be reactive. Whenever any of t..."
          ],
          [
           "```\n\n\nTip: The `gradio.dispatch` method is actually what is triggering the `change` or `upload` even..."
          ],
          [
           "```\n\nCongratulations! You have a working pdf uploader!\n\n![upload-gif](https://gradio-builds.s3.amazo..."
          ],
          [
           "```\n\nCongratulations! The frontend is almost complete ðŸŽ‰\n\n![multipage-pdf-gif](https://gradio-builds...."
          ],
          [
           "<div\n\tclass:table={type === \"table\"}\n\tclass:gallery={type === \"gallery\"}\n\tclass:selected\n\tstyle=\"jus..."
          ],
          [
           "```\n\n\nTip: Exercise for the reader - reduce the code duplication between `Index.svelte` and `Example..."
          ],
          [
           "class PDF(Component):\n\n    EVENTS = [\"change\", \"upload\"]\n\n    data_model = FileData\n\n    def __init_..."
          ],
          [
           "```\n\n## Step 10: Add a demo and publish!\n\nTo test our backend code, let's add a more complex demo th..."
          ],
          [
           "```\n\nSee our demo in action below!\n\n<video autoplay muted loop>\n  <source src=\"https://gradio-builds..."
          ],
          [
           "```\n\n\nI hope you enjoyed this tutorial!\nThe complete source code for our component is [here](https:/..."
          ],
          [
           "Gradio Demo: stream_audio_out\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the d..."
          ],
          [
           "```\nimport gradio as gr\nfrom pydub import AudioSegment\nfrom time import sleep\n\nwith gr.Blocks() as d..."
          ],
          [
           "gradio-ui\n\nThis folder contains all of the Gradio UI and component source code.\n\n- [set up](#setup)\n..."
          ],
          [
           "```\n\nIf you have formatting failures then you can run the following command to fix them:\n\n```bash\npn..."
          ],
          [
           "Gradio Demo: gallery_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr..."
          ],
          [
           "Gradio Demo: blocks_scroll\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndemo = gr.B..."
          ],
          [
           "website\n\n## 0.20.3\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/code@0.3.3\n\n## 0.20.2\n..."
          ],
          [
           "## 0.19.0\n\n### Features\n\n- [#5885](https://github.com/gradio-app/gradio/pull/5885) [`9919b8a`](https..."
          ],
          [
           "## 0.17.0\n\n### Features\n\n- [#6533](https://github.com/gradio-app/gradio/pull/6533) [`e2810fcfc`](htt..."
          ],
          [
           "## 0.15.0\n\n### Features\n\n- [#6436](https://github.com/gradio-app/gradio/pull/6436) [`58e3ca826`](htt..."
          ],
          [
           "## 0.14.0\n\n### Features\n\n- [#6387](https://github.com/gradio-app/gradio/pull/6387) [`9d6d72f44`](htt..."
          ],
          [
           "### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/code@0.2.3\n\n## 0.12.0\n\n### Features\n\n- [#6..."
          ],
          [
           "## 0.11.1\n\n### Features\n\n- [#6189](https://github.com/gradio-app/gradio/pull/6189) [`345ddd888`](htt..."
          ],
          [
           "- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.11.0-beta.0\n\n### Features..."
          ],
          [
           "- [#6082](https://github.com/gradio-app/gradio/pull/6082) [`037e5af33`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6097](https://github.com/gradio-app/gradio/pull/6097) [`439efa39d`](https://github.com/gradio-ap..."
          ],
          [
           "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.co..."
          ],
          [
           "## 0.9.0\n\n### Features\n\n- [#5386](https://github.com/gradio-app/gradio/pull/5386) [`0312c990f`](http..."
          ],
          [
           "## 0.7.0\n\n### Features\n\n- [#5643](https://github.com/gradio-app/gradio/pull/5643) [`f661c0733`](http..."
          ],
          [
           "### Fixes\n\n- [#5608](https://github.com/gradio-app/gradio/pull/5608) [`eebf9d71f`](https://github.co..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5423](https://github.com/gradio-app/gradio/pull/5423) [`bb31cd7d`](https..."
          ],
          [
           "### Fixes\n\n- [#5304](https://github.com/gradio-app/gradio/pull/5304) [`05892302`](https://github.com..."
          ],
          [
           "## 0.2.1\n\n### Fixes\n\n- [#5324](https://github.com/gradio-app/gradio/pull/5324) [`31996c99`](https://..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5298](https://github.com/gradio-app/gr..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5076](https://github.com/gradio-app/gradio/pull/5076) [`2745075a`](https..."
          ],
          [
           "## 0.0.2\n\n### Features\n\n- [#5009](https://github.com/gradio-app/gradio/pull/5009) [`3e70fc81`](https..."
          ],
          [
           "Gradio Demo: live_dashboard\n### This demo shows how you can build a live interactive dashboard with ..."
          ],
          [
           "```\n!pip install -q gradio plotly\n```\n\n\n```\nimport math\n\nimport pandas as pd\n\nimport gradio as gr\nim..."
          ],
          [
           "Image Classification in TensorFlow and Keras\n\nRelated spaces: https://huggingface.co/spaces/abidlabs..."
          ],
          [
           "```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.\n\n## ..."
          ],
          [
           "```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `numpy`..."
          ],
          [
           "Gradio Demo: queue_full_e2e_test\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport ..."
          ],
          [
           "Gradio Demo: blocks_simple_squares\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndemo..."
          ],
          [
           "Gradio Demo: blocks_static\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndemo = gr.Bl..."
          ],
          [
           "Gradio Demo: main_note\n\n\n```\n!pip install -q gradio scipy numpy matplotlib\n```\n\n\n```\n# Downloading f..."
          ],
          [
           "```\nfrom math import log2, pow\nimport os\n\nimport numpy as np\nfrom scipy.fftpack import fft\n\nimport g..."
          ],
          [
           "@gradio/atoms\n\n## 0.4.1\n\n### Fixes\n\n- [#6766](https://github.com/gradio-app/gradio/pull/6766) [`7326..."
          ],
          [
           "## 0.3.0\n\n### Highlights\n\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gr..."
          ],
          [
           "```\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n## 0.2.2\n\n### Fixes\n\n- [#6254](https://github.com/g..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.2.0-beta.6\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.2.0-beta.4\n\n### Features\n\n- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485..."
          ],
          [
           "## 0.1.4\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/utils@0.1.2\n\n## 0.1.3\n\n### Patch..."
          ],
          [
           "We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highli..."
          ],
          [
           "demo for predicting the depth of an image and generating a 3D model of it...."
          ],
          [
           "Gradio Demo: video_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the de..."
          ],
          [
           "TensorFlow å’Œ Keras ä¸­çš„å›¾åƒåˆ†ç±»\n\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/abidlabs/keras-image-classifier\næ ‡ç­¾ï¼šVIS..."
          ],
          [
           "è®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\n### å…ˆå†³æ¡ä»¶\n\nç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„ Keras å›¾åƒåˆ†ç±»æ¨¡åž‹ï¼Œå› æ­¤æ‚¨è¿˜åº”è¯¥å®‰è£…äº†..."
          ],
          [
           "```\n\næ­¤è¡Œä»£ç å°†ä½¿ç”¨ Keras åº“è‡ªåŠ¨ä¸‹è½½ MobileNet æ¨¡åž‹å’Œæƒé‡ã€‚\n\n## ç¬¬äºŒæ­¥ â€”â€” å®šä¹‰ `predict` å‡½æ•°\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æŽ¥æ”¶*ç”¨æˆ·è¾“å…¥*ä½œä¸ºå‚æ•°..."
          ],
          [
           "```\n\nè®©æˆ‘ä»¬æ¥è¯¦ç»†äº†è§£ä¸€ä¸‹ã€‚è¯¥å‡½æ•°æŽ¥å—ä¸€ä¸ªå‚æ•°ï¼š\n\n- `inp`ï¼šè¾“å…¥å›¾åƒçš„ `numpy` æ•°ç»„\n\nç„¶åŽï¼Œå‡½æ•°æ·»åŠ ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦ï¼Œé€šè¿‡æ¨¡åž‹è¿›è¡Œå¤„ç†ï¼Œå¹¶è¿”å›žï¼š\n\n- `confidences`ï¼šé¢„..."
          ],
          [
           "```\n\nè¿™å°†ç”Ÿæˆä»¥ä¸‹ç•Œé¢ï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç«‹å³å°è¯•ï¼ˆå°è¯•ä¸Šä¼ æ‚¨è‡ªå·±çš„ç¤ºä¾‹ï¼ï¼‰ï¼š\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.sp..."
          ],
          [
           "his demo identifies if two speakers are the same person using Gradio's Audio and HTML components...."
          ],
          [
           "Gradio Demo: image_classifier_interface_load\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading f..."
          ],
          [
           "```\n\n\n```\nimport gradio as gr\nimport pathlib\n\ncurrent_dir = pathlib.Path(__file__).parent\n\nimages = ..."
          ],
          [
           "@gradio/image\n\n## 0.5.3\n\n### Fixes\n\n- [#6766](https://github.com/gradio-app/gradio/pull/6766) [`7326..."
          ],
          [
           "## 0.5.0\n\n### Features\n\n- [#6726](https://github.com/gradio-app/gradio/pull/6726) [`21cfb0a`](https:..."
          ],
          [
           "### Fixes\n\n- [#6709](https://github.com/gradio-app/gradio/pull/6709) [`6a9151d`](https://github.com/..."
          ],
          [
           "```\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n## 0.3.6\n\n### Fixes\n\n- [#6441](https://github.com/g..."
          ],
          [
           "## 0.3.4\n\n### Features\n\n- [#6363](https://github.com/gradio-app/gradio/pull/6363) [`4d3aad33a`](http..."
          ],
          [
           "## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gra..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.3.0-beta.9\n\n### Features..."
          ],
          [
           "- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd`](https://github.com/gradio-ap..."
          ],
          [
           "### Fixes\n\n- [#6146](https://github.com/gradio-app/gradio/pull/6146) [`40a171ea6`](https://github.co..."
          ],
          [
           "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.co..."
          ],
          [
           "## 0.3.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5627](https://github.com/gradio-app/gradio/pull/5627) [`b67115e8e`](http..."
          ],
          [
           "## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies [[`8f0fed857`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.3\n\n### Fixes\n\n- [#5528](https://github.com/gradio-app/gradio/pull/5528) [`dc86e4a7`](https://..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/grad..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`667875b2`](https://github.com/gradio-app/grad..."
          ],
          [
           "Gradio Demo: latex\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as ..."
          ],
          [
           "Gradio Demo: video_identity\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the dem..."
          ],
          [
           "@gradio/fallback\n\n## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.co..."
          ],
          [
           "## 0.2.2\n\n### Patch Changes\n\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.2.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.2.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.2.0-beta.3\n\n### Patch Changes\n\n- Updated dependencies [[`14fc612d8`](https://github.com/gradio-..."
          ],
          [
           "## 0.2.0-beta.0\n\n### Features\n\n- [#5507](https://github.com/gradio-app/gradio/pull/5507) [`1385dc688..."
          ],
          [
           "We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highli..."
          ],
          [
           "ðŸš€ Creating Discord Bots from Gradio Apps ðŸš€\n\nTags: NLP, TEXT, CHAT\n\nWe're excited to announce that Gr..."
          ],
          [
           "```\n\n### Step 2: Deploying our App\n\nIn order to create a discord bot for our app, it must be accessi..."
          ],
          [
           "```\n\n### Step 5: Add the bot to your server\n\nVisit the space of your discord bot. You should see \"Ad..."
          ],
          [
           "```\n\n## ðŸ¦¾ Using State of The Art LLMs ðŸ¦¾\n\nWe have created an organization on Hugging Face called [gra..."
          ],
          [
           "```\n\n## ðŸ¦œ Additional LLMs ðŸ¦œ\n\nIn addition to Meta's 70 billion Llama 2 model, we have prepared templa..."
          ],
          [
           "Gradio Demo: blocks_essay\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\n\ncountries_cities_dict = {\n    \"USA\": [\"New York\", \"Los Angeles\", \"Chicago\"]..."
          ],
          [
           "def reset_bounds(minimum, maximum):\n        return gr.Number(minimum=minimum, maximum=maximum)\n\n    ..."
          ],
          [
           "Gradio Demo: generate_tone\n\n\n```\n!pip install -q gradio numpy\n```\n\n\n```\nimport numpy as np\nimport gr..."
          ],
          [
           "`@gradio/form`\n\n```html\n<script>\n\timport { Form } from \"@gradio/form\";\n</script>\n```\n\nForm\n```javasc..."
          ],
          [
           "Gradio Demo: hello_world_3\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef greet(na..."
          ],
          [
           "Gradio Components: The Key Concepts\n\nIn this section, we discuss a few important concepts when it co..."
          ],
          [
           "```\n\nThe interactive version of the component is much more complex -- you can upload images or snap ..."
          ],
          [
           "```python\nimport numpy as np\nimport gradio as gr\n\ndef sepia(input_img):\n    sepia_filter = np.array(..."
          ],
          [
           "```\n\nThis will create a Gradio app which has an `Image` component as the input and the output. \nIn t..."
          ],
          [
           "* As a component author, **YOU** control the format of the data displayed in the frontend as well as..."
          ],
          [
           "### What you need to remember\n\n* If you expect your component to be used as input, it is important t..."
          ],
          [
           "Real Time Speech Recognition\n\nTags: ASR, SPEECH, STREAMING\n\n## Introduction\n\nAutomatic speech recogn..."
          ],
          [
           "Here's how to build a real time speech recognition (ASR) app:\n\n1. [Set up the Transformers ASR Model..."
          ],
          [
           "```\n\nThat's it!\n\n## 2. Create a Full-Context ASR Demo with Transformers\n\nWe will start by creating a..."
          ],
          [
           "Gradio Demo: unified_demo_text_generation\n\n\n```\n!pip install -q gradio torch transformers\n```\n\n\n```\n..."
          ],
          [
           "Gradio Demo: calculator_blocks_cached\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\n..."
          ],
          [
           "Gradio Demo: image-simple\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo ..."
          ],
          [
           "`@gradio/imageeditor`..."
          ],
          [
           "Gradio Demo: chatbot_multimodal\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the..."
          ],
          [
           "```\nimport gradio as gr\nimport os\nimport time\n\n# Chatbot demo with multimodal input (text, markdown,..."
          ],
          [
           "txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\n        bot, chatb..."
          ],
          [
           "Gradio Demo: dataframe_block-ui-test\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwi..."
          ],
          [
           "Gradio Demo: on_listener_test\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.B..."
          ],
          [
           "The Backend ðŸ\n\nThis guide will cover everything you need to know to implement your custom component'..."
          ],
          [
           "```\n\n## The methods you need to implement\n\nWhen you inherit from any of these classes, the following..."
          ],
          [
           "```\n\n### `api_info`\n\nA JSON-schema representation of the value that the `preprocess` expects. \nThis ..."
          ],
          [
           "```\n\n### `read_from_flag`\nConvert from the format stored in the `csv` or `json` file used for flaggi..."
          ],
          [
           "```\n\nBy adding these four lines of code, your component automatically implements the methods needed ..."
          ],
          [
           "```\n\nEven if your component does not expect a \"complex\" JSON data structure it can be beneficial to ..."
          ],
          [
           "Gradio Demo: blocks_flag\n\n\n```\n!pip install -q gradio numpy\n```\n\n\n```\nimport numpy as np\nimport grad..."
          ],
          [
           "Gradio Demo: concurrency_without_queue\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\ni..."
          ],
          [
           "Gradio Demo: dashboard\n### This demo shows how you can build an interactive dashboard with gradio. C..."
          ],
          [
           "```\nimport gradio as gr\nimport pandas as pd\nimport plotly.express as px\nfrom helpers import *\n\n\nLIBR..."
          ],
          [
           "def create_issue_plot(libraries, issue_choices):\n    if \"Issue\" not in issue_choices:\n        return..."
          ],
          [
           "if __name__ == \"__main__\":\n    demo.launch()..."
          ],
          [
           "@gradio/slider\n\n## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.2.0-beta.8\n\n### Features\n\n- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd..."
          ],
          [
           "## 0.2.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "### Fixes\n\n- [#5984](https://github.com/gradio-app/gradio/pull/5984) [`66549d8d2`](https://github.co..."
          ],
          [
           "## 0.2.3\n\n### Patch Changes\n\n- Updated dependencies [[`e70805d54`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5697](https://github.com/gradio-app/gradio/pull/5697) [`f4e4f82b5`](http..."
          ],
          [
           "## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/grad..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "@gradio/model3d\n\n## 0.4.11\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.co..."
          ],
          [
           "## 0.4.8\n\n### Patch Changes\n\n- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.4.7\n\n### Patch Changes\n\n- Updated dependencies [[`206af31`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.4.4\n\n### Patch Changes\n\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.2\n\n### Patch Changes\n\n- Updated dependencies [[`854b482f5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#6240](https://github.com/gradio-app/gradio/pull/6240) [`dd901c1b0`](http..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.3.0-beta.8\n\n### Features..."
          ],
          [
           "- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.3.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.3.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.2.3\n\n### Patch Changes\n\n- Updated dependencies [[`8f0fed857`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5373](https://github.com/gradio-app/gradio/pull/5373) [`79d8f9d8`](https..."
          ],
          [
           "##### Improved markdown support\n\nWe now have better support for markdown in `gr.Markdown` and `gr.Da..."
          ],
          [
           "# @gradio/model3D\n\n## 0.0.2\n\n### Patch Changes\n\n- Updated dependencies [[`667875b2`](https://github...."
          ],
          [
           "mport { Meta } from \"@storybook/blocks\";\n\n<Meta title=\"Introduction\" />\n\n<style>\n\t{`\n    img {\n     ..."
          ],
          [
           "<div class=\"divider\" />\n\n<div class=\"subheading\">Resources</div>\n<ul>\n\n  <li><a href=\"https://gradio..."
          ],
          [
           "simple demo showcasing the upload button used with its `upload` event trigger...."
          ],
          [
           "`@gradio/highlightedtext`\n\n```html\n<script>\n    import { BaseStaticHighlightedText, BaseInteractiveH..."
          ],
          [
           "Gradio Demo: image_component_events\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwit..."
          ],
          [
           "Gradio Demo: reverse_audio\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo..."
          ],
          [
           "Gradio Demo: blocks_page_load\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef prin..."
          ],
          [
           "Gradio & LLM Agents ðŸ¤\n\nLarge Language Models (LLMs) are very impressive but they can be made even mo..."
          ],
          [
           "## gradio_tools - An end-to-end example\n\nTo get started with `gradio_tools`, all you need to do is i..."
          ],
          [
           "agent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbos..."
          ],
          [
           "```\n\nYou'll note that we are using some pre-built tools that come with `gradio_tools`. Please see th..."
          ],
          [
           "```\n\nThe requirements are:\n\n1. The name for your tool\n2. The description for your tool. This is cruc..."
          ],
          [
           "And that's it!\n\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We ..."
          ],
          [
           "```\n\nSome notes on this implementation:\n\n1. All instances of `GradioTool` have an attribute called `..."
          ],
          [
           "his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear..."
          ],
          [
           "`@gradio/video`\n\n```javascript\n<script>\n\timport { BaseInteractiveVideo, BaseStaticVideo, BasePlayer ..."
          ],
          [
           "utomatic speech recognition English. Record from your microphone and the app will transcribe the aud..."
          ],
          [
           "Gradio Demo: longest_word\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef longest_..."
          ],
          [
           "`@gradio/colorpicker`\n\n```html\n<script>\n    import { BaseColorPicker, BaseExample } from \"@gradio/co..."
          ],
          [
           "Gradio Demo: annotatedimage_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nim..."
          ],
          [
           "Running a Gradio App on your Web Server with Nginx\n\nTags: DEPLOYMENT, WEB SERVER, NGINX\n\n## Introduc..."
          ],
          [
           "```\n\n2. Create a new file in the `/etc/nginx/sites-available` directory (create the directory if it ..."
          ],
          [
           "```\n\n2. Start a `tmux` session by typing `tmux` and pressing enter (optional)\n\nIt's recommended that..."
          ],
          [
           "@gradio/upload\n\n## 0.5.6\n\n### Fixes\n\n- [#6766](https://github.com/gradio-app/gradio/pull/6766) [`732..."
          ],
          [
           "## 0.5.3\n\n### Fixes\n\n- [#6709](https://github.com/gradio-app/gradio/pull/6709) [`6a9151d`](https://g..."
          ],
          [
           "## 0.5.0\n\n### Highlights\n\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gr..."
          ],
          [
           "```\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n## 0.4.2\n\n### Fixes\n\n- [#6441](https://github.com/g..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#6356](https://github.com/gradio-app/gradio/pull/6356) [`854b482f5`](http..."
          ],
          [
           "## 0.3.2\n\n### Fixes\n\n- [#6234](https://github.com/gradio-app/gradio/pull/6234) [`aaa55ce85`](https:/..."
          ],
          [
           "## 0.3.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.3.0-beta.6\n\n### Features\n\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40..."
          ],
          [
           "## 0.3.0-beta.5\n\n### Features\n\n- [#6044](https://github.com/gradio-app/gradio/pull/6044) [`9053c95a1..."
          ],
          [
           "## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`e70805d54`](https://github.com/gradio-app/gra..."
          ],
          [
           "### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.1.2\n\n## 0.2.0\n\n### Features\n\n- [#5..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "## 0.0.3\n\n### Fixes\n\n- [#5077](https://github.com/gradio-app/gradio/pull/5077) [`667875b2`](https://..."
          ],
          [
           "```\n\nFrom the backend, streamed outputs are served from the `/stream/` endpoint instead of the `/fil..."
          ],
          [
           "Getting Started with the Gradio Python client\n\nTags: CLIENT, API, SPACES\n\nThe Gradio Python client m..."
          ],
          [
           "```\n\n## Connecting to a running Gradio App\n\nStart by connecting instantiating a `Client` object and ..."
          ],
          [
           "```\n\nIf you have previously duplicated a Space, re-running `duplicate()` will _not_ create a new Spa..."
          ],
          [
           "```\n\nThis shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoi..."
          ],
          [
           "```\n\n## Running jobs asynchronously\n\nOe should note that `.predict()` is a _blocking_ operation as i..."
          ],
          [
           "```\n\n## Status\n\nThe `Job` object also allows you to get the status of the running job by calling the..."
          ],
          [
           "```\n\nIf the first job has started processing, then it will not be canceled. If the second job\nhas no..."
          ],
          [
           "Frequently Asked Questions\n\n## What do I need to install before using Custom Components?\nBefore usin..."
          ],
          [
           "A `data_model` defines the expected data format for your component, simplifying the component develo..."
          ],
          [
           "Create Your Own Friends with a GAN\n\nRelated spaces: https://huggingface.co/spaces/NimaBoscarino/cryp..."
          ],
          [
           "Today we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo ..."
          ],
          [
           "## Step 1 â€” Create the Generator model\n\nTo generate new images with a GAN, you only need the generat..."
          ],
          [
           "```\n\nWe're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/crypto..."
          ],
          [
           "```\n\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor ge..."
          ],
          [
           "```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to th..."
          ],
          [
           "```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in t..."
          ],
          [
           "```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvi..."
          ],
          [
           "def predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n..."
          ],
          [
           "```\n\n---\n\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a ..."
          ],
          [
           "Gradio Demo: blocks_component_shortcut\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n..."
          ],
          [
           "Gradio Demo: progress_component\n\n\n```\n!pip install -q gradio tqdm\n```\n\n\n```\nimport gradio as gr\nimpo..."
          ],
          [
           "Gradio Demo: calculator\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo re..."
          ],
          [
           "Gradio Demo: cancel_events\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport time\nimport gradio as gr\n\n\ndef fake_diffusion(steps):\n    for i in range(steps):\n        ..."
          ],
          [
           "cancel_on_change.change(None, None, None, cancels=[click_event, pred_event])\n    cancel_on_submit.su..."
          ],
          [
           "Gradio Demo: live_with_vars\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndemo = gr.I..."
          ],
          [
           "ååº”å¼ç•Œé¢ (Reactive Interfaces)\n\næœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•ä½¿ Gradio ç•Œé¢è‡ªåŠ¨åˆ·æ–°æˆ–è¿žç»­æµå¼ä¼ è¾“æ•°æ®ã€‚\n\n## å®žæ—¶ç•Œé¢ (Live Interfaces)\n\næ‚¨å¯ä»¥é€šè¿‡åœ¨ç•Œé¢ä¸­..."
          ],
          [
           "Gradio Demo: gpt2_xl\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ntitle = \"gpt2-xl\"\n..."
          ],
          [
           "Gradio Demo: uploadbutton_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef..."
          ],
          [
           "Vision Transformers å›¾åƒåˆ†ç±»\n\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/abidlabs/vision-transformer\næ ‡ç­¾ï¼šVISION, ..."
          ],
          [
           "è®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\n### å…ˆå†³æ¡ä»¶\n\nç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚\n\n## æ­¥éª¤ 1 - é€‰æ‹© Vision å›¾åƒåˆ†ç±»æ¨¡åž‹\n\né¦–å…ˆï¼Œæˆ‘..."
          ],
          [
           "```python\nimport gradio as gr\n\ngr.Interface.load(\n             \"huggingface/google/vit-base-patch16-..."
          ],
          [
           "```\n\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª `examples` å‚æ•°ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨ä¸€äº›é¢„å®šä¹‰çš„ç¤ºä¾‹é¢„å¡«å……æˆ‘ä»¬çš„ç•Œé¢ã€‚\n\nè¿™å°†ç”Ÿæˆä»¥ä¸‹æŽ¥å£ï¼Œæ‚¨å¯ä»¥ç›´æŽ¥åœ¨æµè§ˆå™¨ä¸­å°è¯•ã€‚å½“æ‚¨è¾“å…¥å›¾åƒæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨è¿›è¡Œé¢„å¤„ç†å¹¶å‘é€åˆ° ..."
          ],
          [
           "Gradio Demo: blocks_speech_text_sentiment\n\n\n```\n!pip install -q gradio torch transformers\n```\n\n\n```\n..."
          ],
          [
           "Gradio Demo: spectogram\n\n\n```\n!pip install -q gradio scipy numpy matplotlib\n```\n\n\n```\nimport matplot..."
          ],
          [
           "Gradio Demo: clustering\n### This demo built with Blocks generates 9 plots based on the input.\n      ..."
          ],
          [
           "```\nimport gradio as gr\nimport math\nfrom functools import partial\nimport matplotlib.pyplot as plt\nim..."
          ],
          [
           "def get_moons(n_clusters):\n    X, labels = make_moons(n_samples=N_SAMPLES, noise=0.05, random_state=..."
          ],
          [
           "labels = np.zeros(N_SAMPLES, dtype=int)\n    return normalize(X), labels\n\n\nDATA_MAPPING = {\n    'regu..."
          ],
          [
           "def get_spectral(X, labels, n_clusters, **kwargs):\n    model = SpectralClustering(\n        n_cluster..."
          ],
          [
           "def plot_clusters(ax, X, labels):\n    set_clusters = set(labels)\n    set_clusters.discard(-1)  # -1 ..."
          ],
          [
           "def iter_grid(n_rows, n_cols):\n    # create a grid using gradio Block\n    for _ in range(n_rows):\n  ..."
          ],
          [
           "@gradio/html\n\n## 0.1.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gr..."
          ],
          [
           "## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.1.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.1.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.0.5\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.1.3\n  - @gradio/statustr..."
          ],
          [
           "##### Improved markdown support\n\nWe now have better support for markdown in `gr.Markdown` and `gr.Da..."
          ],
          [
           "`@gradio/gallery`\n\n```html\n<script>\n\timport { BaseGallery } from \"@gradio/gallery\";\n</script>\n```\n\nB..."
          ],
          [
           "å‘½åå®žä½“è¯†åˆ« ï¼ˆNamed-Entity Recognitionï¼‰\n\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/rajistics/biobert_ner_demoï¼Œhtt..."
          ],
          [
           "### æ–¹æ³•ä¸€ï¼šå®žä½“å­—å…¸åˆ—è¡¨\n\nè®¸å¤šå‘½åå®žä½“è¯†åˆ«æ¨¡åž‹è¾“å‡ºçš„æ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ã€‚æ¯ä¸ªå­—å…¸åŒ…å«ä¸€ä¸ª*å®žä½“*ï¼Œä¸€ä¸ª \" èµ·å§‹ \" ç´¢å¼•å’Œä¸€ä¸ª \" ç»“æŸ \" ç´¢å¼•ã€‚è¿™å°±æ˜¯ `transformers` åº“ä¸­çš„ N..."
          ],
          [
           "```\n\nè¾“å‡ºç»“æžœï¼š\n\n```bash\n[{'entity': 'I-LOC',\n  'score': 0.9988978,\n  'index': 2,\n  'word': 'Chicago',\n  ..."
          ],
          [
           "Blocks and Event Listeners\n\nWe briefly descirbed the Blocks class in the [Quickstart](/main/guides/q..."
          ],
          [
           "```\n\n_Note_: What happens if a Gradio component is neither an input nor an output? If a component is..."
          ],
          [
           "1. as a list of arguments, or\n2. as a single dictionary of values, keyed by the component\n\nLet's see..."
          ],
          [
           "```\n\nAbove, each return statement returns two values corresponding to `food_box` and `status_box`, r..."
          ],
          [
           "```\n\nNotice how when there is no food, we only update the `status_box` element. We skipped updating ..."
          ],
          [
           "Here's an example showing how to use `gr.Examples` in a `gr.Blocks` app:\n\n$code_calculator_blocks\n\n*..."
          ],
          [
           "## Gathering Event Data\n\nYou can gather specific data about an event by adding the associated event ..."
          ],
          [
           "Gradio Demo: no_input\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport random\n\nsen..."
          ],
          [
           "Gradio Demo: hello_world_2\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef greet(na..."
          ],
          [
           "Gradio Demo: blocks_plug\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef change_ta..."
          ],
          [
           "PyTorch å›¾åƒåˆ†ç±»\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/pytorch-image-classifier, https..."
          ],
          [
           "è®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\n### å…ˆå†³æ¡ä»¶\n\nç¡®ä¿æ‚¨å·²ç»[å®‰è£…](/getting_started)äº† `gradio` Python åŒ…ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„å›¾åƒåˆ†ç±»æ¨¡åž‹ï¼Œæ‰€ä»¥æ‚¨è¿˜åº”è¯¥å®‰è£…äº† `torch..."
          ],
          [
           "```\n\nç”±äºŽæˆ‘ä»¬å°†ä½¿ç”¨æ¨¡åž‹è¿›è¡ŒæŽ¨æ–­ï¼Œæ‰€ä»¥æˆ‘ä»¬è°ƒç”¨äº† `.eval()` æ–¹æ³•ã€‚\n\n## ç¬¬äºŒæ­¥ - å®šä¹‰ `predict` å‡½æ•°\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æŽ¥å—*ç”¨æˆ·è¾“å…¥*ï¼Œåœ¨æœ¬ç¤ºä¾‹ä¸­..."
          ],
          [
           "```\n\nè®©æˆ‘ä»¬é€æ­¥æ¥çœ‹ä¸€ä¸‹è¿™æ®µä»£ç ã€‚è¯¥å‡½æ•°æŽ¥å—ä¸€ä¸ªå‚æ•°ï¼š\n\n- `inp`ï¼šè¾“å…¥å›¾ç‰‡ï¼Œç±»åž‹ä¸º `PIL` å›¾åƒ\n\nç„¶åŽï¼Œè¯¥å‡½æ•°å°†å›¾åƒè½¬æ¢ä¸º PIL å›¾åƒï¼Œæœ€ç»ˆè½¬æ¢ä¸º PyTorch çš„ `tenso..."
          ],
          [
           "```\n\nè¿™å°†äº§ç”Ÿä»¥ä¸‹ç•Œé¢ï¼Œæ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç›´æŽ¥å°è¯•ï¼ˆè¯•è¯•ä¸Šä¼ è‡ªå·±çš„ç¤ºä¾‹å›¾ç‰‡ï¼ï¼‰ï¼š\n\n<iframe src=\"https://abidlabs-pytorch-image-classifier.hf..."
          ],
          [
           "Gradio Demo: video_subtitle\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the dem..."
          ],
          [
           "```\n\n\n```\nimport gradio as gr\nimport os\n\na = os.path.join(os.path.abspath(''), \"files/a.mp4\")  # Vid..."
          ],
          [
           "@gradio/state\n\n## 0.1.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`2..."
          ],
          [
           "## 0.1.0-beta.1\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "åˆ†äº«æ‚¨çš„åº”ç”¨\n\nå¦‚ä½•åˆ†äº«æ‚¨çš„ Gradio åº”ç”¨ï¼š\n\n1. [ä½¿ç”¨ share å‚æ•°åˆ†äº«æ¼”ç¤º](#sharing-demos)\n2. [åœ¨ HF Spaces ä¸Šæ‰˜ç®¡](#hosting-on-hf-..."
          ],
          [
           "```\n\nè¿™å°†ç”Ÿæˆä¸€ä¸ªå…¬å¼€çš„å¯åˆ†äº«é“¾æŽ¥ï¼Œæ‚¨å¯ä»¥å°†å…¶å‘é€ç»™ä»»ä½•äººï¼å½“æ‚¨å‘é€æ­¤é“¾æŽ¥æ—¶ï¼Œå¯¹æ–¹ç”¨æˆ·å¯ä»¥åœ¨å…¶æµè§ˆå™¨ä¸­å°è¯•æ¨¡åž‹ã€‚å› ä¸ºå¤„ç†è¿‡ç¨‹å‘ç”Ÿåœ¨æ‚¨çš„è®¾å¤‡ä¸Šï¼ˆåªè¦æ‚¨çš„è®¾å¤‡ä¿æŒå¼€å¯ï¼ï¼‰ï¼Œæ‚¨ä¸å¿…æ‹…å¿ƒä»»ä½•æ‰“åŒ…ä¾èµ–é¡¹çš„é—®..."
          ],
          [
           "åœ¨æ‚¨åˆ›å»ºäº†ä¸€ä¸ªå…è´¹çš„ Hugging Face è´¦æˆ·åŽï¼Œæœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥å°†æ‚¨çš„ Gradio åº”ç”¨éƒ¨ç½²åˆ° Hugging Face Spacesï¼š\n\n1. ä»Žç»ˆç«¯ï¼šåœ¨åº”ç”¨ç›®å½•ä¸­è¿è¡Œ `gradio de..."
          ],
          [
           "![åµŒå…¥æ­¤ç©ºé—´ä¸‹æ‹‰é€‰é¡¹](/assets/guides/embed_this_space.png)\n\n### ä½¿ç”¨ Web ç»„ä»¶åµŒå…¥\n\nä¸Ž IFrames ç›¸æ¯”ï¼ŒWeb ç»„ä»¶é€šå¸¸ä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„ä½“éªŒã€‚..."
          ],
          [
           "```\n\n2.  åœ¨æ‚¨æƒ³æ”¾ç½®åº”ç”¨çš„ä½ç½®æ·»åŠ \n    `html\n&lt;gradio-app src=\"https://$your_space_host.hf.space\">&lt;/gradio-a..."
          ],
          [
           "```\n\n<script>\nfetch(\"https://pypi.org/pypi/gradio/json\"\n).then(r => r.json()\n).then(obj => {\n    let..."
          ],
          [
           "æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¼ é€’ç»™ `<gradio-app>` æ ‡ç­¾çš„å±žæ€§æ¥è‡ªå®šä¹‰ Web ç»„ä»¶çš„å¤–è§‚å’Œè¡Œä¸ºï¼š\n\n- `src`ï¼šå¦‚å‰æ‰€è¿°ï¼Œ`src` å±žæ€§é“¾æŽ¥åˆ°æ‚¨æƒ³è¦åµŒå…¥çš„æ‰˜ç®¡ Gradio æ¼”ç¤ºçš„ URL\n- ..."
          ],
          [
           "ä»¥ä¸‹æ˜¯ä½¿ç”¨è¿™äº›å±žæ€§åˆ›å»ºä¸€ä¸ªæ‡’åŠ è½½ä¸”åˆå§‹é«˜åº¦ä¸º 0px çš„ Gradio åº”ç”¨çš„ç¤ºä¾‹ã€‚\n\n```html\n&lt;gradio-app space=\"gradio/Echocardiogram-Segm..."
          ],
          [
           "```\n\n_ æ³¨æ„ï¼šGradio çš„ CSS æ°¸è¿œä¸ä¼šå½±å“åµŒå…¥é¡µé¢ï¼Œä½†åµŒå…¥é¡µé¢å¯ä»¥å½±å“åµŒå…¥çš„ Gradio åº”ç”¨çš„æ ·å¼ã€‚è¯·ç¡®ä¿çˆ¶é¡µé¢ä¸­çš„ä»»ä½• CSS ä¸æ˜¯å¦‚æ­¤é€šç”¨ï¼Œä»¥è‡³äºŽå®ƒä¹Ÿå¯èƒ½é€‚ç”¨äºŽåµŒå…¥çš„ Grad..."
          ],
          [
           "```\n\nåŒæ ·ï¼Œæ‚¨å¯ä»¥åœ¨â€œåµŒå…¥æ­¤ç©ºé—´â€æŒ‰é’®ä¸­æ‰¾åˆ°æ‚¨çš„ Space çš„åµŒå…¥ URL çš„ `src=` å±žæ€§ã€‚\n\næ³¨æ„ï¼šå¦‚æžœæ‚¨ä½¿ç”¨ IFramesï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ·»åŠ ä¸€ä¸ªå›ºå®šçš„ `height` å±žæ€§ï¼Œå¹¶è®¾ç½®..."
          ],
          [
           "```\n\nè¿™å°†è®°å½•è‡ªåŠ¨ç”Ÿæˆçš„ API é¡µé¢çš„ç«¯ç‚¹ `/api/addition/`ã€‚\n\n_æ³¨æ„_ï¼šå¯¹äºŽå¯ç”¨äº†[é˜Ÿåˆ—åŠŸèƒ½](https://gradio.app/key-features#queuing..."
          ],
          [
           "```\n\nä¸ºäº†ä½¿èº«ä»½éªŒè¯æ­£å¸¸å·¥ä½œï¼Œå¿…é¡»åœ¨æµè§ˆå™¨ä¸­å¯ç”¨ç¬¬ä¸‰æ–¹ Cookieã€‚\né»˜è®¤æƒ…å†µä¸‹ï¼ŒSafariã€Chrome éšç§æ¨¡å¼ä¸ä¼šå¯ç”¨æ­¤åŠŸèƒ½ã€‚\n\n## ç›´æŽ¥è®¿é—®ç½‘ç»œè¯·æ±‚\n\nå½“ç”¨æˆ·å‘æ‚¨çš„åº”ç”¨ç¨‹åºè¿›è¡Œé¢„æµ‹æ—¶..."
          ],
          [
           "```\n\næ³¨æ„ï¼šå¦‚æžœç›´æŽ¥è°ƒç”¨å‡½æ•°è€Œä¸æ˜¯é€šè¿‡ UIï¼ˆä¾‹å¦‚åœ¨ç¼“å­˜ç¤ºä¾‹æ—¶ï¼‰ï¼Œåˆ™ `request` å°†ä¸º `None`ã€‚æ‚¨åº”è¯¥æ˜Žç¡®å¤„ç†æ­¤æƒ…å†µï¼Œä»¥ç¡®ä¿æ‚¨çš„åº”ç”¨ç¨‹åºä¸ä¼šæŠ›å‡ºä»»ä½•é”™è¯¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æœ‰æ˜¾å¼æ£€æŸ¥ ..."
          ],
          [
           "ç‰¹åˆ«æ˜¯ï¼ŒGradio åº”ç”¨ç¨‹åºå…è®¸ç”¨æˆ·è®¿é—®ä»¥ä¸‹ä¸‰ç±»æ–‡ä»¶ï¼š\n\n- **ä¸Ž Gradio è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆæˆ–å­ç›®å½•ï¼‰ä¸­çš„æ–‡ä»¶ç›¸åŒã€‚** ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨çš„ Gradio è„šæœ¬çš„è·¯å¾„æ˜¯ `/home/usr/sc..."
          ],
          [
           "- **ç‚¹æ–‡ä»¶**ï¼ˆå…¶åç§°ä»¥ '.' å¼€å¤´çš„ä»»ä½•æ–‡ä»¶ï¼‰æˆ–å…¶åç§°ä»¥ '.' å¼€å¤´çš„ä»»ä½•ç›®å½•ä¸­çš„ä»»ä½•æ–‡ä»¶ã€‚\n\n- **é€šè¿‡ `launch()` ä¸­çš„ `blocked_paths` å‚æ•°å…è®¸çš„æ–‡ä»¶ã€‚**..."
          ],
          [
           "Gradio Demo: english_translator\n\n\n```\n!pip install -q gradio transformers torch\n```\n\n\n```\nimport gra..."
          ],
          [
           "@gradio/row\n\n## 0.1.1\n\n### Features\n\n- [#6399](https://github.com/gradio-app/gradio/pull/6399) [`053..."
          ],
          [
           "## 0.1.0-beta.1\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "Gradio Demo: image_editor\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo ..."
          ],
          [
           "åˆ†å—çŠ¶æ€ (State in Blocks)\n\næˆ‘ä»¬å·²ç»ä»‹ç»äº†[æŽ¥å£çŠ¶æ€](https://gradio.app/interface-state)ï¼Œè¿™ç¯‡æŒ‡å—å°†ä»‹ç»åˆ†å—çŠ¶æ€ï¼Œå®ƒçš„å·¥ä½œåŽŸç†å¤§è‡´ç›¸åŒã€‚\n\n#..."
          ],
          [
           "$code_hangman\n$demo_hangman\n\nè®©æˆ‘ä»¬çœ‹çœ‹åœ¨è¿™ä¸ªæ¸¸æˆä¸­å¦‚ä½•å®Œæˆä¸Šè¿°çš„ 3 ä¸ªæ­¥éª¤ï¼š\n\n1. æˆ‘ä»¬å°†å·²ä½¿ç”¨çš„å­—æ¯å­˜å‚¨åœ¨ `used_letters_var` ä¸­ã€‚åœ¨ `Stat..."
          ],
          [
           "ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full c..."
          ],
          [
           "Gradio Demo: matrix_transpose\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport numpy as np\n\nimport gra..."
          ],
          [
           "Gradio Demo: blocks_textbox_max_lines\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\n..."
          ],
          [
           "ecreate the viral AnimeGAN image transformation demo...."
          ],
          [
           "Gradio Demo: calculator_list_and_dict\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nw..."
          ],
          [
           "åŒºå—å’Œäº‹ä»¶ç›‘å¬å™¨ (Blocks and Event Listeners)\n\næˆ‘ä»¬åœ¨[å¿«é€Ÿå…¥é—¨](https://gradio.app/quickstart/#blocks-more-flexibil..."
          ],
          [
           "## äº‹ä»¶ç›‘å¬å™¨ä¸Žäº¤äº’æ€§ (Event Listeners and Interactivity)\n\nåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°å¯ä»¥ç¼–è¾‘æ–‡æœ¬æ¡† `name`ï¼Œä½†æ— æ³•ç¼–è¾‘æ–‡æœ¬æ¡† `output`ã€‚è¿™æ˜¯å› ä¸º..."
          ],
          [
           "```\n\n## äº‹ä»¶ç›‘å¬å™¨çš„ç±»åž‹ (Types of Event Listeners)\n\nè¯·æŸ¥çœ‹ä¸‹é¢çš„æ¼”ç¤ºï¼š\n\n$code_blocks_hello\n$demo_blocks_hello\n\n`welc..."
          ],
          [
           "1. ä½œä¸ºå‚æ•°åˆ—è¡¨ï¼Œæˆ–\n2. ä½œä¸ºä»¥ç»„ä»¶ä¸ºé”®çš„å•ä¸ªå€¼å­—å…¸\n\nè®©æˆ‘ä»¬åˆ†åˆ«çœ‹ä¸€ä¸ªä¾‹å­ï¼š\n$code_calculator_list_and_dict\n\n`add()` å’Œ `sub()` éƒ½å°† `a` å’Œ..."
          ],
          [
           "é¦–å…ˆè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªï¼ˆ1ï¼‰çš„ç¤ºä¾‹ï¼Œå…¶ä¸­æˆ‘ä»¬é€šè¿‡è¿”å›žä¸¤ä¸ªå€¼æ¥è®¾ç½®ä¸¤ä¸ªè¾“å‡ºç»„ä»¶çš„å€¼ï¼š\n\n```python\nwith gr.Blocks() as demo:\n    food_box = gr.Number..."
          ],
          [
           "```\n\nä¸Šé¢çš„æ¯ä¸ªè¿”å›žè¯­å¥åˆ†åˆ«è¿”å›žä¸Ž `food_box` å’Œ `status_box` ç›¸å¯¹åº”çš„ä¸¤ä¸ªå€¼ã€‚\n\né™¤äº†è¿”å›žä¸Žæ¯ä¸ªè¾“å‡ºç»„ä»¶é¡ºåºç›¸å¯¹åº”çš„å€¼åˆ—è¡¨å¤–ï¼Œæ‚¨è¿˜å¯ä»¥è¿”å›žä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®å¯¹åº”äºŽè¾“å‡ºç»„ä»¶ï¼Œ..."
          ],
          [
           "```\n\næ³¨æ„ï¼Œåœ¨æ²¡æœ‰é£Ÿç‰©çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæ›´æ–° `status_box` å…ƒç´ ã€‚æˆ‘ä»¬è·³è¿‡æ›´æ–° `food_box` ç»„ä»¶ã€‚\n\nå­—å…¸è¿”å›žåœ¨äº‹ä»¶ç›‘å¬å™¨å½±å“å¤šä¸ªç»„ä»¶çš„è¿”å›žå€¼æˆ–æœ‰æ¡ä»¶åœ°å½±å“è¾“å‡ºæ—¶éžå¸¸æœ‰ç”¨ã€‚\n\n..."
          ],
          [
           "$code_chatbot_simple\n$demo_chatbot_simple\n\näº‹ä»¶ç›‘å¬å™¨çš„ `.then()` æ–¹æ³•ä¼šæ‰§è¡ŒåŽç»­äº‹ä»¶ï¼Œæ— è®ºå‰ä¸€ä¸ªäº‹ä»¶æ˜¯å¦å¼•å‘ä»»ä½•é”™è¯¯ã€‚å¦‚æžœåªæƒ³åœ¨å‰ä¸€ä¸ªäº‹ä»¶æˆåŠŸæ‰§è¡Œ..."
          ],
          [
           "åœ¨ä¸‹é¢çš„åŒäººäº•å­—æ¸¸æˆæ¼”ç¤ºä¸­ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹© `DataFrame` ä¸­çš„ä¸€ä¸ªå•å…ƒæ ¼è¿›è¡Œç§»åŠ¨ã€‚äº‹ä»¶æ•°æ®å‚æ•°åŒ…å«æœ‰å…³æ‰€é€‰å•å…ƒæ ¼çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥é¦–å…ˆæ£€æŸ¥å•å…ƒæ ¼æ˜¯å¦ä¸ºç©ºï¼Œç„¶åŽç”¨ç”¨æˆ·çš„ç§»åŠ¨æ›´æ–°å•å…ƒæ ¼ã€‚\n\n$cod..."
          ],
          [
           "Gradio Demo: checkbox_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith g..."
          ],
          [
           "@gradio/icons\n\n## 0.3.2\n\n### Features\n\n- [#6399](https://github.com/gradio-app/gradio/pull/6399) [`0..."
          ],
          [
           "```\n\n Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.2.1\n\n### Fixes\n\n- [#6254](https://github.com/..."
          ],
          [
           "## 0.2.0-beta.3\n\n### Features\n\n- [#6094](https://github.com/gradio-app/gradio/pull/6094) [`c476bd5a5..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5699](https://github.com/gradio-app/gradio/pull/5699) [`8f0fed857`](http..."
          ],
          [
           "Gradio Demo: audio_component_events\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n    ..."
          ],
          [
           "output_video.play(lambda n: n + 1, output_num_play, output_num_play)\n            output_video.pause(..."
          ],
          [
           "Gradio Demo: chatinterface_random_response\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport random\nimp..."
          ],
          [
           "Gradio Demo: colorpicker_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwit..."
          ],
          [
           "Gradio Demo: blocks_update\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.Bloc..."
          ],
          [
           "Getting Started with the Gradio JavaScript client\n\nTags: CLIENT, API, SPACES\n\nThe Gradio JavaScript ..."
          ],
          [
           "```\n\nThe Gradio client works with any hosted Gradio app, whether it be an image generator, a text su..."
          ],
          [
           "```\n\n## Duplicating a Space for private use\n\nWhile you can use any public Space as an API, you may g..."
          ],
          [
           "```\n\n## Connecting a general Gradio app\n\nIf your app is running somewhere else, just provide the ful..."
          ],
          [
           "```\n\nThis shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoi..."
          ],
          [
           "```\n\nFor certain inputs, such as images, you should pass in a `Buffer`, `Blob` or `File` depending o..."
          ],
          [
           "```\n\n## Status\n\nThe event interface also allows you to get the status of the running job by listenin..."
          ],
          [
           "```\n\nIf the first job has started processing, then it will not be canceled but the client will no lo..."
          ],
          [
           "æ›´å¤šç¤ºä¾‹ (More on Examples)\n\næœ¬æŒ‡å—ä»‹ç»äº†æœ‰å…³ç¤ºä¾‹çš„æ›´å¤šå†…å®¹ï¼šä»Žç›®å½•ä¸­åŠ è½½ç¤ºä¾‹ï¼Œæä¾›éƒ¨åˆ†ç¤ºä¾‹å’Œç¼“å­˜ã€‚å¦‚æžœä½ å¯¹ç¤ºä¾‹è¿˜ä¸ç†Ÿæ‚‰ï¼Œè¯·æŸ¥çœ‹ [å…³é”®ç‰¹æ€§](../key-features/#e..."
          ],
          [
           "```csv\nnum,operation,num2\n5,\"add\",3\n4,\"divide\",2\n5,\"multiply\",3..."
          ],
          [
           "```\n\nå½“æµè§ˆæ ‡è®°æ•°æ®æ—¶ï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ã€‚åªéœ€æŒ‡å‘æ ‡è®°ç›®å½•ï¼Œ`Interface` å°†ä»Žæ ‡è®°æ•°æ®åŠ è½½ç¤ºä¾‹ã€‚\n\n### æä¾›éƒ¨åˆ†ç¤ºä¾‹\n\næœ‰æ—¶ä½ çš„åº”ç”¨ç¨‹åºæœ‰è®¸å¤šè¾“å…¥ç»„ä»¶ï¼Œä½†ä½ åªæƒ³ä¸ºå…¶ä¸­çš„ä¸€éƒ¨åˆ†æä¾›ç¤ºä¾‹ã€‚ä¸º..."
          ],
          [
           "ä»Ž Supabase æ•°æ®åˆ›å»ºä»ªè¡¨ç›˜\n\nTags: TABULAR, DASHBOARD, PLOTS\n\n[Supabase](https://supabase.com/) æ˜¯ä¸€ä¸ªåŸºäºŽäº‘çš„å¼€æºåŽç«¯ï¼Œæ..."
          ],
          [
           "1\\. åœ¨ Supabase ä¸­åˆ›å»ºä¸€ä¸ªæ–°é¡¹ç›®ã€‚ä¸€æ—¦æ‚¨ç™»å½•ï¼Œç‚¹å‡» \"New Project\" æŒ‰é’®\n\n2\\. ç»™æ‚¨çš„é¡¹ç›®å‘½åå¹¶è®¾ç½®æ•°æ®åº“å¯†ç ã€‚æ‚¨è¿˜å¯ä»¥é€‰æ‹©å®šä»·è®¡åˆ’ï¼ˆå¯¹äºŽæˆ‘ä»¬æ¥è¯´ï¼Œå…è´¹è®¡åˆ’å·²è¶³å¤Ÿï¼ï¼‰\n\n..."
          ],
          [
           "```\n\n7\\. èŽ·å–é¡¹ç›® URL å’Œ API å¯†é’¥ã€‚ç‚¹å‡»å·¦ä¾§çª—æ ¼ä¸Šçš„è®¾ç½®ï¼ˆé½¿è½®å›¾æ ‡ï¼‰ï¼Œç„¶åŽç‚¹å‡» 'API'ã€‚URL åˆ—åœ¨é¡¹ç›® URL æ¡†ä¸­ï¼ŒAPI å¯†é’¥åˆ—åœ¨é¡¹ç›® API å¯†é’¥ï¼ˆå¸¦æœ‰ `service..."
          ],
          [
           "```\n\nè¿”å›ž Supabase ä»ªè¡¨æ¿å¹¶åˆ·æ–°é¡µé¢ï¼Œæ‚¨å°†çœ‹åˆ° 10 è¡Œæ•°æ®å¡«å……åˆ° `Product` è¡¨ä¸­ï¼\n\n## åœ¨å®žæ—¶ Gradio ä»ªè¡¨ç›˜ä¸­å¯è§†åŒ–æ•°æ®\n\næœ€åŽï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„ `supaba..."
          ],
          [
           "```\n\nè¯·æ³¨æ„ï¼Œé€šè¿‡å°†å‡½æ•°ä¼ é€’ç»™ `gr.BarPlot()`ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç½‘ç»œåº”ç”¨åŠ è½½æ—¶æŸ¥è¯¢æ•°æ®åº“ï¼ˆç„¶åŽæ¯ 60 ç§’æŸ¥è¯¢ä¸€æ¬¡ï¼Œå› ä¸ºæœ‰ `every` å‚æ•°ï¼‰ã€‚æ‚¨çš„æœ€ç»ˆä»ªè¡¨ç›˜åº”å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n<grad..."
          ],
          [
           "Gradio Demo: model3d_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr..."
          ],
          [
           "@gradio/markdown\n\n## 0.6.0\n\n### Features\n\n- [#6842](https://github.com/gradio-app/gradio/pull/6842) ..."
          ],
          [
           "### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.3.1\n  - @gradio/statustracker@0.4...."
          ],
          [
           "## 0.3.2\n\n### Patch Changes\n\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.3.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.3.0-beta.7\n\n### Features\n\n- [#6071](https://github.com/gradio-app/gradio/pull/6071) [`f08da1a6f..."
          ],
          [
           "## 0.3.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.3.2\n\n### Fixes\n\n- [#5897](https://github.com/gradio-app/gradio/pull/5897) [`0592c301d`](https:/..."
          ],
          [
           "## 0.3.0\n\n### Fixes\n\n- [#5755](https://github.com/gradio-app/gradio/pull/5755) [`e842a561a`](https:/..."
          ],
          [
           "### Fixes\n\n- [#5604](https://github.com/gradio-app/gradio/pull/5604) [`faad01f8e`](https://github.co..."
          ],
          [
           "- [#5304](https://github.com/gradio-app/gradio/pull/5304) [`05892302`](https://github.com/gradio-app..."
          ],
          [
           "- [#5368](https://github.com/gradio-app/gradio/pull/5368) [`b27f7583`](https://github.com/gradio-app..."
          ],
          [
           "## 0.1.1\n\n### Fixes\n\n- [#5324](https://github.com/gradio-app/gradio/pull/5324) [`31996c99`](https://..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5268](https://github.com/gradio-app/gr..."
          ],
          [
           "Gradio Demo: video_identity_2\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef video..."
          ],
          [
           "`gradio_client`: Use a Gradio app as an API -- in 3 lines of Python\n\nThis directory contains the sou..."
          ],
          [
           "```\n\nYou can also connect to private Spaces by passing in your HF token with the `hf_token` paramete..."
          ],
          [
           "```\n\n### Inspecting the API endpoints\n\nOnce you have connected to a Gradio app, you can view the API..."
          ],
          [
           "```\n\nFor certain inputs, such as images, you should pass in the filepath or URL to the file. Likewis..."
          ],
          [
           "Gradio Demo: image_classification\n### Simple image classification in Pytorch with Gradio's Image inp..."
          ],
          [
           "Gradio Demo: label_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr.B..."
          ],
          [
           "è‡ªå®šä¹‰çš„ JS å’Œ CSS\n\næœ¬æŒ‡å—ä»‹ç»äº†å¦‚ä½•æ›´çµæ´»åœ°ä¸º Blocks æ·»åŠ æ ·å¼ï¼Œå¹¶æ·»åŠ  JavaScript ä»£ç åˆ°äº‹ä»¶ç›‘å¬å™¨ä¸­ã€‚\n\n**è­¦å‘Š**ï¼šåœ¨è‡ªå®šä¹‰çš„ JS å’Œ CSS ä¸­ä½¿ç”¨æŸ¥è¯¢é€‰æ‹©å™¨ä¸èƒ½..."
          ],
          [
           "```\n\nå¦‚æžœæ‚¨æƒ³åœ¨æ‚¨çš„ CSS ä¸­å¼•ç”¨å¤–éƒ¨æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ `\"file=\"` ä½œä¸ºæ–‡ä»¶è·¯å¾„çš„å‰ç¼€ï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼‰ï¼Œä¾‹å¦‚ï¼š\n\n```python\nwith gr.Blocks(css=\".g..."
          ],
          [
           "```\n\nCSS `#warning` è§„åˆ™é›†ä»…é’ˆå¯¹ç¬¬äºŒä¸ªæ–‡æœ¬æ¡†ï¼Œè€Œ `.feedback` è§„åˆ™é›†å°†åŒæ—¶ä½œç”¨äºŽä¸¤ä¸ªæ–‡æœ¬æ¡†ã€‚è¯·æ³¨æ„ï¼Œåœ¨é’ˆå¯¹ç±»æ—¶ï¼Œæ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨ `!important` é€‰æ‹©å™¨æ¥è¦†ç›–é»˜..."
          ],
          [
           "Gradio Demo: bokeh_plot\n\n\n```\n!pip install -q gradio bokeh>=3.0 xyzservices\n```..."
          ],
          [
           "```\nimport gradio as gr\nimport xyzservices.providers as xyz\nfrom bokeh.models import ColumnDataSourc..."
          ],
          [
           "p.circle(\n            jitter(\"class\", 0.3, range=p.x_range),\n            \"hwy\",\n            source=d..."
          ],
          [
           "# JavaScript Client Library\n\nA javascript (and typescript) client to call Gradio APIs.\n\n## Installat..."
          ],
          [
           "```\n\n##### `status_callback`\n\nThis should be a function which will notify your of the status of a sp..."
          ],
          [
           "```\n\nThe gradio client returns an object with a number of methods and properties:\n\n#### `predict`\n\nT..."
          ],
          [
           "```\n\nThe `submit` method accepts the same [`endpoint`](#endpoint) and [`payload`](#payload) argument..."
          ],
          [
           "```\n\n##### `off`\n\nThe `off` method unsubscribes from a specific event of the submitted job and works..."
          ],
          [
           "```\n\n#### `view_api`\n\nThe `view_api` method provides details about the API you are connected to. It ..."
          ],
          [
           "```\n\nThis function accepts two arguments: `source` and `options`:\n\n#### `source`\n\nThe space to dupli..."
          ],
          [
           "```\n\n##### `hardware`\n\nThis is an optional property specific to `duplicate`'s options object and wil..."
          ],
          [
           "iles in this directory are used in:\n\n- tests for the gradio library\n- example inputs in the view API..."
          ],
          [
           "How to Use the 3D Model Component\n\nRelated spaces: https://huggingface.co/spaces/gradio/Model3D, htt..."
          ],
          [
           "```python\nimport gradio as gr\nimport os\n\n\ndef load_mesh(mesh_file_name):\n    return mesh_file_name\n\n..."
          ],
          [
           "```\n\nLet's break down the code above:\n\n`load_mesh`: This is our 'prediction' function and for simpli..."
          ],
          [
           "@gradio/dataset\n\n## 0.1.13\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.co..."
          ],
          [
           "## 0.1.10\n\n### Patch Changes\n\n- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.1.7\n\n### Patch Changes\n\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.4\n\n### Patch Changes\n\n- Updated dependencies [[`854b482f5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`aaa55ce85`](https://github.com/gradio-app/gra..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.1.0-beta.2\n\n### Features\n\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40..."
          ],
          [
           "## 0.1.0-beta.0\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.0.5-beta.3\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.2.0-beta.3\n\n## 0...."
          ],
          [
           "### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.1.1\n\n## 0.0.2\n\n### Features\n\n- [#5..."
          ],
          [
           "Gradio Demo: blocks_kinematics\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport pandas as pd\nimport nu..."
          ],
          [
           "Gradio Demo: blocks_neural_instrument_coding\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading f..."
          ],
          [
           "```\n# A Blocks implementation of https://erlj.notion.site/Neural-Instrument-Cloning-from-very-few-sa..."
          ],
          [
           "io4 = gr.Interface(\n    lambda x, y, z: os.path.join(os.path.abspath(''),\"sax2.wav\"),\n    [\n        ..."
          ],
          [
           "m(\n        \"\"\"\\n\n        Here is a **generated** saxophone recordings:\"\"\"\n    )\n    a = gr.Audio(os...."
          ],
          [
           "Gradio Demo: image_classifier\n\n\n```\n!pip install -q gradio numpy tensorflow\n```\n\n\n```\n# Downloading ..."
          ],
          [
           "```\n\n\n```\nimport os\nimport requests\nimport tensorflow as tf\n\nimport gradio as gr\n\ninception_net = tf..."
          ],
          [
           "Using Gradio Blocks Like Functions\n\nTags: TRANSLATION, HUB, SPACES\n\n**Prerequisite**: This Guide bui..."
          ],
          [
           "Option 1 technically always works, but it often introduces unwanted complexity.\n\nOption 2 lets you b..."
          ],
          [
           "```\n\nThe `api_name` gives this function a unique name in our app. You can use this name to tell grad..."
          ],
          [
           "Gradio Demo: animeganv2\n### Recreate the viral AnimeGAN image transformation demo.\n        \n\n\n```\n!p..."
          ],
          [
           "```\nimport gradio as gr\nimport torch\n\nmodel2 = torch.hub.load(\n    \"AK391/animegan2-pytorch:main\",\n ..."
          ],
          [
           "demo = gr.Interface(\n    fn=inference, \n    inputs=[gr.Image(type=\"pil\"),gr.Radio(['version 1 (ðŸ”º sty..."
          ],
          [
           "Gradio Demo: image_segmentation\n### Simple image segmentation using gradio's AnnotatedImage componen..."
          ],
          [
           "```\nimport gradio as gr\nimport numpy as np\nimport random\n\nwith gr.Blocks() as demo:\n    section_labe..."
          ],
          [
           "section_btn = gr.Button(\"Identify Sections\")\n    selected_section = gr.Textbox(label=\"Selected Secti..."
          ],
          [
           "Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create th..."
          ],
          [
           "```\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import I..."
          ],
          [
           "advanced_button = gr.Button(\"Advanced options\", elem_id=\"advanced-btn\")\n\n        with gr.Row(elem_id..."
          ],
          [
           "`@gradio/statustracker`\n\n```html\n<script>\n    import {StatusTracker, Toast, Loader} from `@gradio/st..."
          ],
          [
           "@gradio/simpletextbox\n\n## 0.1.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://gith..."
          ],
          [
           "## 0.1.4\n\n### Patch Changes\n\n- Updated dependencies [[`206af31`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.0-beta.2\n\n### Features\n\n- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd..."
          ],
          [
           "Gradio Demo: chatbot_consecutive\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport ..."
          ],
          [
           "Gradio Demo: file_explorer\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\nfrom pathlib import Path\n\ncurrent_file_path = Path(__file__).resolve()\nrelat..."
          ],
          [
           "code = gr.Code(lines=30, scale=2, language=\"python\")\n\n    file_3.change(get_file_content, file_3, co..."
          ],
          [
           "è¿è¡ŒåŽå°ä»»åŠ¡\n\nRelated spaces: https://huggingface.co/spaces/freddyaboulton/gradio-google-forms\nTags: TASKS..."
          ],
          [
           "æˆ‘ä»¬å°†ä½¿ç”¨ `sqlite3` åº“æ¥è¿žæŽ¥æˆ‘ä»¬çš„ sqlite æ•°æ®åº“ï¼Œä½† gradio å¯ä»¥ä¸Žä»»ä½•åº“ä¸€èµ·ä½¿ç”¨ã€‚\n\nä»£ç å¦‚ä¸‹ :\n\n```python\nDB_FILE = \"./reviews.db\"\n..."
          ],
          [
           "```\n\nè®©æˆ‘ä»¬è¿˜å†™ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨ gradio åº”ç”¨ç¨‹åºåŠ è½½æ—¶åŠ è½½æœ€æ–°çš„è¯„è®º :\n\n```python\ndef load_data():\n    db = sqlite3.connect(DB_FIL..."
          ],
          [
           "```\n\n## ç¬¬ä¸‰æ­¥ - ä¸Ž HuggingFace æ•°æ®é›†åŒæ­¥ ðŸ¤—\n\nåœ¨ç¬¬ 2 æ­¥åŽæˆ‘ä»¬å¯ä»¥è°ƒç”¨ `demo.launch()` æ¥è¿è¡Œä¸€ä¸ªå®Œæ•´åŠŸèƒ½çš„åº”ç”¨ç¨‹åºã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ•°æ®å°†å­˜å‚¨åœ¨æœ¬åœ°æœºå™¨ä¸Šã€‚..."
          ],
          [
           "```\n\nè¯·æ³¨æ„ï¼Œæ‚¨éœ€è¦ä»Ž HuggingFace çš„â€œè®¾ç½®â€é€‰é¡¹å¡ä¸­èŽ·å–è®¿é—®ä»¤ç‰Œï¼Œä»¥ä¸Šä»£ç æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚åœ¨è„šæœ¬ä¸­ï¼Œé€šè¿‡çŽ¯å¢ƒå˜é‡å®‰å…¨è®¿é—®ä»¤ç‰Œã€‚\n\n![access_token](/assets/gui..."
          ],
          [
           "```\n\n## ç¬¬å››æ­¥ï¼ˆé™„åŠ ï¼‰- éƒ¨ç½²åˆ° HuggingFace Spaces\n\næ‚¨å¯ä»¥ä½¿ç”¨ HuggingFace [Spaces](https://huggingface.co/spaces) å¹³..."
          ],
          [
           "Gradio Demo: file_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr.Bl..."
          ],
          [
           "Gradio Demo: blocks_group\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\nwith gr.Blocks() as demo..."
          ],
          [
           "gr.Markdown(\"### Several columns grouped together. If columns are uneven, there is a gray group back..."
          ],
          [
           "with gr.Column():\n        gr.Radio([1,2,3], container=False)\n        gr.Slider(0, 20, container=Fals..."
          ],
          [
           "é€šè¿‡è‡ªåŠ¨é‡è½½å®žçŽ°æ›´å¿«çš„å¼€å‘\n\n**å…ˆå†³æ¡ä»¶**ï¼šæœ¬æŒ‡å—è¦æ±‚æ‚¨äº†è§£å—çš„çŸ¥è¯†ã€‚è¯·ç¡®ä¿[å…ˆé˜…è¯»å—æŒ‡å—](https://gradio.app/quickstart/#blocks-more-flexibil..."
          ],
          [
           "inp.change(fn=lambda x: f\"æ¬¢è¿Žï¼Œ{x}ï¼\",\n               inputs=inp,\n               outputs=out)\n\nif __nam..."
          ],
          [
           "```\n\né—®é¢˜åœ¨äºŽï¼Œæ¯å½“æ‚¨æƒ³è¦æ›´æ”¹å¸ƒå±€ã€äº‹ä»¶æˆ–ç»„ä»¶æ—¶ï¼Œéƒ½å¿…é¡»é€šè¿‡ç¼–å†™ `python run.py` æ¥å…³é—­å’Œé‡æ–°è¿è¡Œåº”ç”¨ç¨‹åºã€‚\n\nè€Œä¸æ˜¯è¿™æ ·åšï¼Œæ‚¨å¯ä»¥é€šè¿‡æ›´æ”¹ 1 ä¸ªå•è¯æ¥ä»¥**é‡æ–°åŠ è½½æ¨¡å¼**è¿è¡Œ..."
          ],
          [
           "```\n\nè¿™é‡Œæœ€é‡è¦çš„ä¸€è¡Œæ˜¯ `æ­£åœ¨è§‚å¯Ÿ ...`ã€‚è¿™é‡Œå‘ç”Ÿçš„æƒ…å†µæ˜¯ Gradio å°†è§‚å¯Ÿ `run.py` æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼Œå¦‚æžœæ–‡ä»¶å‘ç”Ÿæ›´æ”¹ï¼Œå®ƒå°†è‡ªåŠ¨ä¸ºæ‚¨é‡æ–°è¿è¡Œæ–‡ä»¶ã€‚å› æ­¤ï¼Œæ‚¨åªéœ€ä¸“æ³¨äºŽç¼–å†™ä»£ç ï¼ŒG..."
          ],
          [
           "```\n\né‚£ä¹ˆæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨å®ƒï¼š`gradio run.py my_demo.app`ã€‚\n\nGradioé»˜è®¤ä½¿ç”¨UTF-8ç¼–ç æ ¼å¼ã€‚å¯¹äºŽ**é‡æ–°åŠ è½½æ¨¡å¼**ï¼Œå¦‚æžœä½ çš„è„šæœ¬ä½¿ç”¨çš„æ˜¯é™¤UTF-8ä»¥å¤–çš„ç¼–ç ..."
          ],
          [
           "```\n\næ‚¨å¯ä»¥åƒè¿™æ ·è¿è¡Œå®ƒï¼š`gradio run.py --name Gretel`\n\nä½œä¸ºä¸€ä¸ªå°æç¤ºï¼Œåªè¦æ›´æ”¹äº† `run.py` æºä»£ç æˆ– Gradio æºä»£ç ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½å°±ä¼šå‘ç”Ÿã€‚è¿™æ„å‘³ç€..."
          ],
          [
           "```\n\nè¯·æ³¨æ„ï¼š\n\n- æ‚¨ä¸éœ€è¦æ”¾ç½®æ ·æ¿ä»£ç  `with gr.Blocks() as demo:` å’Œ `demo.launch()` â€” Gradio ä¼šè‡ªåŠ¨ä¸ºæ‚¨å®Œæˆï¼\n\n- æ¯æ¬¡é‡æ–°è¿è¡Œå•å…ƒæ ¼..."
          ],
          [
           "Gradio Demo: sentiment_analysis\n### This sentiment analaysis demo takes in input text and returns it..."
          ],
          [
           "Gradio Demo: image_mod\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo rep..."
          ],
          [
           "Gradio Demo: hello_blocks\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef greet(nam..."
          ],
          [
           "Installing Gradio in a Virtual Environment\n\nTags: INSTALLATION\n\nIn this guide, we will describe step..."
          ],
          [
           "```\n\n5. **Verification**:\n   To verify the installation, run `python` and then type:\n\n   ```python\n ..."
          ],
          [
           "Gradio Demo: score_tracker\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nscores = []\n..."
          ],
          [
           "`@gradio/file`\n\n```html\n<script>\n\timport { BaseFile, BaseFileUpload, FilePreview, BaseExample } from..."
          ],
          [
           "Gradio Demo: scatterplot_component\n\n\n```\n!pip install -q gradio vega_datasets\n```\n\n\n```\nimport gradi..."
          ],
          [
           "Gradio Demo: xgboost-income-prediction-with-explainability\n### This demo takes in 12 inputs from the..."
          ],
          [
           "```\nimport gradio as gr\nimport random\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sha..."
          ],
          [
           "def interpret(*args):\n    df = pd.DataFrame([args], columns=X_train.columns)\n    df = df.astype({col..."
          ],
          [
           "with gr.Blocks() as demo:\n    gr.Markdown(\"\"\"\n    **Income Classification with XGBoost ðŸ’°**:  This de..."
          ],
          [
           ")\n            relationship = gr.Dropdown(\n                label=\"Relationship Status\",\n             ..."
          ],
          [
           "relationship,\n                    sex,\n                    capital_gain,\n                    capital..."
          ],
          [
           "demo.launch()..."
          ],
          [
           "his demo built with Blocks generates 9 plots based on the input...."
          ],
          [
           "Gradio & LLM Agents ðŸ¤\n\néžå¸¸å¼ºå¤§çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ï¼Œå¦‚æžœæˆ‘ä»¬èƒ½èµ‹äºˆå®ƒä»¬å®Œæˆä¸“é—¨ä»»åŠ¡çš„æŠ€èƒ½ï¼Œå®ƒä»¬å°†å˜å¾—æ›´åŠ å¼ºå¤§ã€‚\n\n[gradio_tools](https://github..."
          ],
          [
           "### Gradioæ˜¯ä»€ä¹ˆï¼Ÿ\n\n[Gradio](https://github.com/gradio-app/gradio)æ˜¯ç”¨äºŽæž„å»ºæœºå™¨å­¦ä¹ Webåº”ç”¨ç¨‹åºå¹¶ä¸Žå…¨çƒå…±äº«çš„äº‹å®žä¸Šçš„æ ‡å‡†æ¡†æž¶-å®Œå…¨ç”±Pyt..."
          ],
          [
           "from langchain.memory import ConversationBufferMemory\n\nllm = OpenAI(temperature=0)\nmemory = Conversa..."
          ],
          [
           "```\n\næ‚¨ä¼šæ³¨æ„åˆ°æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ä¸€äº›ä¸Ž`gradio_tools`ä¸€èµ·æä¾›çš„é¢„æž„å»ºå·¥å…·ã€‚è¯·å‚é˜…æ­¤[æ–‡æ¡£](https://github.com/freddyaboulton/gradio-tools#..."
          ],
          [
           "```\n\néœ€è¦æ»¡è¶³çš„è¦æ±‚æ˜¯ï¼š..."
          ],
          [
           "1. å·¥å…·çš„åç§°\n2. å·¥å…·çš„æè¿°ã€‚è¿™éžå¸¸å…³é”®ï¼ä»£ç†æ ¹æ®å…¶æè¿°å†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚è¯·ç¡®åˆ‡æè¿°è¾“å…¥å’Œè¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œæœ€å¥½åŒ…æ‹¬ç¤ºä¾‹ã€‚\n3. Gradioåº”ç”¨ç¨‹åºçš„urlæˆ–space idï¼Œä¾‹å¦‚`fred..."
          ],
          [
           "5. postprocess - ç»™å®šä½œä¸šçš„ç»“æžœï¼Œå°†å…¶è½¬æ¢ä¸ºLLMå¯ä»¥å‘ç”¨æˆ·æ˜¾ç¤ºçš„å­—ç¬¦ä¸²ã€‚\n6. _Optionalå¯é€‰_ - æŸäº›åº“ï¼Œä¾‹å¦‚[MiniChain](https://github.com..."
          ],
          [
           "å°±æ˜¯è¿™æ ·ï¼\n\nä¸€æ—¦æ‚¨åˆ›å»ºäº†è‡ªå·±çš„å·¥å…·ï¼Œè¯·åœ¨`gradio_tools`å­˜å‚¨åº“ä¸Šå‘èµ·æ‹‰å–è¯·æ±‚ï¼æˆ‘ä»¬æ¬¢è¿Žæ‰€æœ‰è´¡çŒ®ã€‚\n\n## ç¤ºä¾‹å·¥å…· - ç¨³å®šæ‰©æ•£\n\nä»¥ä¸‹æ˜¯ä½œä¸ºç¤ºä¾‹çš„ç¨³å®šæ‰©æ•£å·¥å…·ä»£ç ï¼š\n\nfrom gra..."
          ],
          [
           "```\nå…³äºŽæ­¤å®žçŽ°çš„ä¸€äº›æ³¨æ„äº‹é¡¹ï¼š\n1. æ‰€æœ‰çš„ `GradioTool` å®žä¾‹éƒ½æœ‰ä¸€ä¸ªåä¸º `client` çš„å±žæ€§ï¼Œå®ƒæŒ‡å‘åº•å±‚çš„ [gradio å®¢æˆ·ç«¯](https://github.com/g..."
          ],
          [
           "Gradio Demo: barplot_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport pa..."
          ],
          [
           "Gradio Demo: blocks_hello\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef welcome(n..."
          ],
          [
           "his is a fake GAN that shows how to create a text-to-image interface for image generation. Check out..."
          ],
          [
           "Gradio Demo: loginbutton_component\n\n\n```\n!pip install -q gradio gradio[oauth]\n```\n\n\n```\nimport gradi..."
          ],
          [
           "@gradio/gallery\n\n## 0.4.14\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.co..."
          ],
          [
           "## 0.4.12\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.4.10\n\n### Patch Changes\n\n- Updated dependencies [[`b639e04`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.4.8\n\n### Patch Changes\n\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.6\n\n### Patch Changes\n\n- Updated dependencies [[`324867f63`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.4\n\n### Patch Changes\n\n- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.2\n\n### Fixes\n\n- [#6277](https://github.com/gradio-app/gradio/pull/6277) [`5fe091367`](https:/..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.4.0-beta.9\n\n### Features\n\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40..."
          ],
          [
           "## 0.4.0-beta.8\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "- Updated dependencies []:\n  - @gradio/image@0.3.0-beta.7\n\n## 0.4.0-beta.6\n\n### Features\n\n- [#5960](..."
          ],
          [
           "## 0.5.1\n\n### Patch Changes\n\n- Updated dependencies [[`b67115e8e`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.5.0\n\n### Features\n\n- [#5780](https://github.com/gradio-app/gradio/pull/5780) [`ed0f9a21b`](http..."
          ],
          [
           "## 0.4.1\n\n### Fixes\n\n- [#5735](https://github.com/gradio-app/gradio/pull/5735) [`abb5e9df4`](https:/..."
          ],
          [
           "## 0.3.4\n\n### Patch Changes\n\n- Updated dependencies [[`e0d61b8ba`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/grad..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5025](https://github.com/gradio-app/gradio/pull/5025) [`6693660a`](https..."
          ],
          [
           "åœ¨ Web æœåŠ¡å™¨ä¸Šä½¿ç”¨ Nginx è¿è¡Œ Gradio åº”ç”¨\n\næ ‡ç­¾ï¼šéƒ¨ç½²ï¼ŒWeb æœåŠ¡å™¨ï¼ŒNginx\n\n## ä»‹ç»\n\nGradio æ˜¯ä¸€ä¸ª Python åº“ï¼Œå…è®¸æ‚¨å¿«é€Ÿåˆ›å»ºå¯å®šåˆ¶çš„ Web åº”ç”¨ç¨‹..."
          ],
          [
           "```\n\n2. åœ¨ `/etc/nginx/sites-available` ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ï¼ˆå¦‚æžœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰ï¼Œæ–‡ä»¶åè¡¨ç¤ºæ‚¨çš„åº”ç”¨ï¼Œä¾‹å¦‚ï¼š`sudo nano /etc/nginx/sit..."
          ],
          [
           "```\n\n2. é€šè¿‡é”®å…¥ `tmux` å¹¶æŒ‰å›žè½¦é”®ï¼ˆå¯é€‰ï¼‰å¯åŠ¨ `tmux` ä¼šè¯\n\næŽ¨èåœ¨ `tmux` ä¼šè¯ä¸­è¿è¡Œ Gradio åº”ç”¨ï¼Œä»¥ä¾¿å¯ä»¥è½»æ¾åœ°åœ¨åŽå°è¿è¡Œå®ƒ\n\n3. ç„¶åŽï¼Œå¯åŠ¨æ‚¨çš„ Grad..."
          ],
          [
           "@gradio/highlightedtext\n\n## 0.4.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://gi..."
          ],
          [
           "## 0.4.4\n\n### Patch Changes\n\n- Updated dependencies [[`206af31`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.4.1\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.4.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.4.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.3.4\n\n### Patch Changes\n\n- Updated dependencies [[`e70805d54`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.1\n\n### Fixes\n\n- [#5602](https://github.com/gradio-app/gradio/pull/5602) [`54d21d3f1`](https:/..."
          ],
          [
           "## 0.2.2\n\n### Patch Changes\n\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5046](https://github.com/gradio-app/gradio/pull/5046) [`5244c587`](https..."
          ],
          [
           "Gradio Demo: stream_frames\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport numpy ..."
          ],
          [
           "@gradio/utils\n\n## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`2..."
          ],
          [
           "## 0.2.0-beta.5\n\n### Features\n\n- [#5966](https://github.com/gradio-app/gradio/pull/5966) [`9cad2127b..."
          ],
          [
           "## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`e4a307ed6`](https://github.com/gradio-app/gra..."
          ],
          [
           "##### Improved markdown support\n\nWe now have better support for markdown in `gr.Markdown` and `gr.Da..."
          ],
          [
           "ä»Ž Google Sheets åˆ›å»ºå®žæ—¶ä»ªè¡¨ç›˜\n\nTags: TABULAR, DASHBOARD, PLOTS\n[Google Sheets](https://www.google.com/shee..."
          ],
          [
           "```\n\n2. çŽ°åœ¨ï¼Œä¿®æ”¹æ­¤ç½‘å€å¹¶ä½¿ç”¨å®ƒä»Ž Google Sheets è¯»å–æ•°æ®åˆ° Pandas DataFrame ä¸­ã€‚ (åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œç”¨æ‚¨çš„å…¬å¼€ Google Sheet çš„ç½‘å€æ›¿æ¢ `URL..."
          ],
          [
           "```\n\nåˆ°æ­¤ä¸ºæ­¢ï¼æ‚¨çŽ°åœ¨æ‹¥æœ‰ä¸€ä¸ªä»ªè¡¨ç›˜ï¼Œæ¯ 5 ç§’åˆ·æ–°ä¸€æ¬¡ï¼Œä»Ž Google Sheets ä¸­èŽ·å–æ•°æ®ã€‚\n\n## ç§æœ‰ Google Sheets\n\nå¯¹äºŽç§æœ‰ Google Sheetsï¼Œæµç¨‹éœ€è¦æ›´..."
          ],
          [
           "```json\n{\n\t\"type\": \"service_account\",\n\t\"project_id\": \"your project\",\n\t\"private_key_id\": \"your privat..."
          ],
          [
           "```\n\n### æŸ¥è¯¢\n\nåœ¨èŽ·å¾—å‡­æ®çš„ `.json` æ–‡ä»¶åŽï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æŸ¥è¯¢æ‚¨çš„ Google Sheetï¼š\n\n1. å•å‡» Google Sheet å³ä¸Šè§’çš„â€œå…±äº«â€æŒ‰é’®ã€‚ä½¿ç”¨èº«ä»½éªŒè¯å­éƒ¨åˆ†ç¬¬..."
          ],
          [
           "```\n\n4\\. æ•°æ®æŸ¥è¯¢æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè¿™æ„å‘³ç€å¯ä»¥ä½¿ç”¨ `gr.DataFrame` ç»„ä»¶å®žæ—¶æ˜¾ç¤ºæ•°æ®ï¼Œæˆ–ä½¿ç”¨ `gr.LinePlot` ç»„ä»¶å®žæ—¶ç»˜åˆ¶æ•°æ®ï¼ˆå½“ç„¶ï¼Œæ ¹æ®æ•°æ®çš„ä¸åŒï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ä¸åŒçš„å›¾..."
          ],
          [
           "@gradio/radio\n\n## 0.3.7\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/g..."
          ],
          [
           "## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.1\n\n### Fixes\n\n- [#6262](https://github.com/gradio-app/gradio/pull/6262) [`afb72bd19`](https:/..."
          ],
          [
           "## 0.3.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.3.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.3.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.1.3\n  - @gradio/statustr..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gr..."
          ],
          [
           "Gradio Demo: markdown_example\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\n\ncss = (\n    \"footer {display: none !important;} .gradio-container {min-heig..."
          ],
          [
           "## Tech\n\nDillinger uses a number of open source projects to work properly:\n\n- [AngularJS] - HTML enh..."
          ],
          [
           "```\n\nFor production environments...\n\n```bash\nnpm install --production\nNODE_ENV=production node app\n`..."
          ],
          [
           "```\n\nThis will create the dillinger image and pull in the necessary dependencies.\nBe sure to swap ou..."
          ],
          [
           "```\n\n## License\n\nMIT\n\n**Free Software, Hell Yeah!**\n\n[//]: # (These are reference links used in the ..."
          ],
          [
           "[PlDb]: <https://github.com/joemccann/dillinger/tree/master/plugins/dropbox/README.md>\n   [PlGh]: <h..."
          ],
          [
           "gradio_client\n\n## 0.7.3\n\n### Fixes\n\n- [#6693](https://github.com/gradio-app/gradio/pull/6693) [`34f9..."
          ],
          [
           "## 0.7.2\n\n### Features\n\n- [#6598](https://github.com/gradio-app/gradio/pull/6598) [`7cbf96e`](https:..."
          ],
          [
           "### Fixes\n\n- [#6556](https://github.com/gradio-app/gradio/pull/6556) [`d76bcaa`](https://github.com/..."
          ],
          [
           "### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github..."
          ],
          [
           "## 0.7.0-beta.2\n\n### Features\n\n- [#6094](https://github.com/gradio-app/gradio/pull/6094) [`c476bd5a5..."
          ],
          [
           "## 0.7.0-beta.1\n\n### Features\n\n- [#6082](https://github.com/gradio-app/gradio/pull/6082) [`037e5af33..."
          ],
          [
           "## 0.7.0-beta.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13..."
          ],
          [
           "## 0.6.0\n\n### Highlights\n\n#### new `FileExplorer` component ([#5672](https://github.com/gradio-app/g..."
          ],
          [
           "Thanks [@aliabid94](https://github.com/aliabid94)!\n\n## 0.5.3\n\n### Features\n\n- [#5721](https://github..."
          ],
          [
           "## 0.5.0\n\n### Highlights\n\n#### Enable streaming audio in python client ([#5248](https://github.com/g..."
          ],
          [
           "```\n\n Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n### Fixes\n\n- [#5295](https://git..."
          ],
          [
           "Thanks [@freddyaboulton](https://github.com/freddyaboulton)!\n\n### Features\n\n- [#5076](https://github..."
          ],
          [
           "We're excited to announce that Gradio can now automatically create a discord bot from any `gr.ChatIn..."
          ],
          [
           "But once again, you can deploy ANY `gr.ChatInterface` app exposed on the internet! So don't hesitate..."
          ],
          [
           "### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\n- Pinned dependencies to major..."
          ],
          [
           "No changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n# 0.2.7\n\n### New Features:\n..."
          ],
          [
           "### Breaking Changes:\n\nNo changes to highlight.\n\n### Full Changelog:\n\nNo changes to highlight.\n\n# 0...."
          ],
          [
           "### Contributors Shoutout:\n\nNo changes to highlight.\n\n# 0.2.3\n\n### New Features:\n\nNo changes to high..."
          ],
          [
           "No changes to highlight.\n\n# 0.2.1\n\n### New Features:\n\nNo changes to highlight.\n\n### Bug Fixes:\n\nRemo..."
          ],
          [
           "### Documentation Changes:\n\nNo changes to highlight.\n\n### Testing and Infrastructure Changes:\n\nNo ch..."
          ],
          [
           "Using the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files ..."
          ],
          [
           "```\n\nRead more about how to use the `gradio_client` library here: https://gradio.app/getting-started..."
          ],
          [
           "Gradio Demo: blocks_form\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.Blocks..."
          ],
          [
           "Gradio Demo: tictactoe\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.Blocks()..."
          ],
          [
           "Gradio Demo: fake_gan_2\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo re..."
          ],
          [
           "```\n\n\n```\n# This demo needs to be run from the repo folder.\n# python demo/fake_gan/run.py\nimport ran..."
          ],
          [
           "simple dashboard ranking spaces by number of likes...."
          ],
          [
           "@gradio/json\n\n## 0.1.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gr..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.1.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.1.3\n\n### Patch Changes\n\n- Updated dependencies [[`e70805d54`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.0.5\n\n### Patch Changes\n\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/grad..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "`gradio/model3d`\n\n```html\n<script>\n    import {BaseModel3D, BaseModel3DUpload, BaseExample } from `@..."
          ],
          [
           "@gradio/theme\n\n## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`2..."
          ],
          [
           "Thanks to a new capability that allows components to communicate directly with the server _without_ ..."
          ],
          [
           "## 0.0.3\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://git..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.0.2\n\n### Fixes\n\n- [#4997](https://github.com/gradio..."
          ],
          [
           "Gradio Demo: automatic-speech-recognition\n### Automatic speech recognition English. Record from your..."
          ],
          [
           "Gradio Demo: event_trigger\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo..."
          ],
          [
           "```\n# %%\nimport gradio as gr\n\n\nTEST_VIDEO_A = \"mp4/a.mp4\"\nTEST_VIDEO_B = \"mp4/b.mp4\"\n\nTEST_IMAGE_A =..."
          ],
          [
           "def change_video(index):\n            if index == 0:\n                return TEST_VIDEO_A\n            ..."
          ],
          [
           "file_btn = gr.Button(\"Change interactive\")\n\n            with gr.Column():\n                file1 = gr..."
          ],
          [
           "`@gradio/button`\n\n```html\n<script>\n\timport { BaseChatBot } from \"@gradio/chatbot\";\n</script>\n```\n\n\nB..."
          ],
          [
           "div align=\"center\">\n\n[<img src=\"readme_files/gradio.svg\" alt=\"gradio\" width=400>](https://gradio.app..."
          ],
          [
           "</div>\n\n# Gradio: Build Machine Learning Web Apps â€” in Python\n\n$getting_started\n\n## Questions?\n\nIf y..."
          ],
          [
           "If you like Gradio, please leave us a â­ on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of ..."
          ],
          [
           "## License\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in ..."
          ],
          [
           "```\n@article{abid2019gradio,\n  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the ..."
          ],
          [
           "Gradio Demo: theme_new_step_3\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nfrom __future__ import annotations\nfrom typing import Iterable\nimport gradio as gr\nfrom gradio.t..."
          ],
          [
           "class Seafoam(Base):\n    def __init__(\n        self,\n        *,\n        primary_hue: colors.Color | ..."
          ],
          [
           "text_size=text_size,\n            font=font,\n            font_mono=font_mono,\n        )\n        super..."
          ],
          [
           "seafoam = Seafoam()\n\nwith gr.Blocks(theme=seafoam) as demo:\n    textbox = gr.Textbox(label=\"Name\")\n ..."
          ],
          [
           "Gradio Demo: dataframe_datatype\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport p..."
          ],
          [
           "Gradio Demo: Echocardiogram-Segmentation\n\n\n```\n!pip install -q gradio -f https://download.pytorch.or..."
          ],
          [
           "```\nimport os\nimport numpy as np\nimport torch\nimport torchvision\nimport wget \n\n\ndestination_folder =..."
          ],
          [
           "print(\"loading weights from \", os.path.join(destination_for_weights, \"deeplabv3_resnet50_random\"))\n\n..."
          ],
          [
           "import gradio as gr\n\ni = gr.Image(label=\"Echocardiogram\")\no = gr.Image(label=\"Segmentation Mask\")\n\ne..."
          ],
          [
           "`@gradio/button`\n\n```javascript\n<script>\n\timport { BaseButton } from \"@gradio/button\";\n\timport { cre..."
          ],
          [
           "Gradio Demo: reverse_audio_2\n\n\n```\n!pip install -q gradio numpy\n```\n\n\n```\nimport gradio as gr\nimport..."
          ],
          [
           "Gradio Demo: model3d_component_events\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nw..."
          ],
          [
           "Gradio Demo: sentence_builder\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\n\n\ndef sentence_builder(quantity, animal, countries, place, activity_list, mo..."
          ],
          [
           "demo = gr.Interface(\n    sentence_builder,\n    [\n        gr.Slider(2, 20, value=4, label=\"Count\", in..."
          ],
          [
           "@gradio/audio\n\n## 0.6.3\n\n### Fixes\n\n- [#6766](https://github.com/gradio-app/gradio/pull/6766) [`7326..."
          ],
          [
           "## 0.6.1\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.5.5\n\n### Fixes\n\n- [#6551](https://github.com/gradio-app/gradio/pull/6551) [`8fc562a`](https://g..."
          ],
          [
           "## 0.5.2\n\n### Features\n\n- [#6419](https://github.com/gradio-app/gradio/pull/6419) [`1959471a8`](http..."
          ],
          [
           "## 0.5.1\n\n### Fixes\n\n- [#6382](https://github.com/gradio-app/gradio/pull/6382) [`2090aad73`](https:/..."
          ],
          [
           "## 0.4.3\n\n### Fixes\n\n- [#6317](https://github.com/gradio-app/gradio/pull/6317) [`19af2806a`](https:/..."
          ],
          [
           "## 0.4.1\n\n### Patch Changes\n\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gra..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.4.0-beta.9\n\n### Features..."
          ],
          [
           "- [#6153](https://github.com/gradio-app/gradio/pull/6153) [`1162ed621`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.4.0-beta.8\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.4.0-beta.7\n\n### Patch Changes\n\n- Updated dependencies [[`174b73619`](https://github.com/gradio-..."
          ],
          [
           "## 0.4.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5627](https://github.com/gradio-app/gradio/pull/5627) [`b67115e8e`](http..."
          ],
          [
           "## 0.3.6\n\n### Patch Changes\n\n- Updated dependencies [[`8f0fed857`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.3\n\n### Fixes\n\n- [#5459](https://github.com/gradio-app/gradio/pull/5459) [`bd2fda77`](https://..."
          ],
          [
           "## 0.3.0\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://git..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gr..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5149](https://github.com/gradio-app/gradio/pull/5149) [`144df459`](https..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#4993](https://github.com/gradio-app/gradio/pull/4993) [`dc07a9f9`](https..."
          ],
          [
           "Gradio Demo: waveform\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport random\n\n\nCO..."
          ],
          [
           "Gradio Demo: hello_login\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport argparse..."
          ],
          [
           "@gradio/fileexplorer\n\n## 0.3.13\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://gith..."
          ],
          [
           "## 0.3.11\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.3.9\n\n### Fixes\n\n- [#6550](https://github.com/gradio-app/gradio/pull/6550) [`3156598`](https://g..."
          ],
          [
           "## 0.3.7\n\n### Patch Changes\n\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.5\n\n### Patch Changes\n\n- Updated dependencies [[`324867f63`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.0-beta.2\n\n### Features\n\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40..."
          ],
          [
           "## 0.3.0-beta.0\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`796145e2c`](https://github.com/gradio-app/gra..."
          ],
          [
           "Using Gradio and Comet\n\nTags: COMET, SPACES\nContributed by the Comet team\n\n## Introduction\n\nIn this ..."
          ],
          [
           "```\n\nNext, you will need to [sign up for a Comet Account](https://www.comet.com/signup?utm_source=gr..."
          ],
          [
           "```\n\n## 1. Logging Gradio UI's to your Comet Experiments\n\n[![Open In Colab](https://colab.research.g..."
          ],
          [
           "inputs = gr.Image()\noutputs = gr.Label(num_top_classes=3)\n\nio = gr.Interface(\n    fn=predict, inputs..."
          ],
          [
           "```\n\nThe last line in this snippet will log the URL of the Gradio Application to your Comet Experime..."
          ],
          [
           "Go to your Comet Project page, and head over to the Panels tab. Click the `+ Add` button to bring up..."
          ],
          [
           "## 3. Embedding Hugging Face Spaces directly into your Comet Projects\n\n<iframe width=\"560\" height=\"3..."
          ],
          [
           "## 4. Logging Model Inferences to Comet\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.c..."
          ],
          [
           "MODEL_NAME = \"gpt2\"\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\n# set model decoder t..."
          ],
          [
           "return plot\n\n\nwith gr.Blocks() as demo:\n    start_experiment_btn = gr.Button(\"Start New Experiment\")..."
          ],
          [
           "```\n\nInferences from this snippet will be saved in the HTML tab of your experiment.\n\n<video width=\"5..."
          ],
          [
           "`@gradio/markdown`\n\n```html\n<script>\n    import { BaseMarkdown, MarkdownCode, BaseExample } from `@g..."
          ],
          [
           "Gradio Demo: calculator_live\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef calcul..."
          ],
          [
           "Named-Entity Recognition\n\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, ..."
          ],
          [
           "```\n\nOutput:\n\n```bash\n[{'entity': 'I-LOC',\n  'score': 0.9988978,\n  'index': 2,\n  'word': 'Chicago',\n..."
          ],
          [
           "Gradio Demo: fraud_detector\n\n\n```\n!pip install -q gradio pandas\n```\n\n\n```\n# Downloading files from t..."
          ],
          [
           "!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/1)getting_start..."
          ],
          [
           "[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio...."
          ],
          [
           "```\npip install gradio\n```\n\n\n> [!TIP]\n > it is best to install Gradio in a virtual environment. Deta..."
          ],
          [
           "```\n\n\n\n> [!TIP]\n > We shorten the imported name from <code>gradio</code> to <code>gr</code> for bett..."
          ],
          [
           "The `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) ..."
          ],
          [
           "### Sharing Your Demo\n\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily s..."
          ],
          [
           "```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, so..."
          ],
          [
           "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular ima..."
          ],
          [
           "Or, if you already know the basics and are looking for something specific, you can search the more [..."
          ],
          [
           "Building a FastAPI App with the Gradio Python Client\n\nTags: CLIENT, API, WEB APP\n\nIn this blog post,..."
          ],
          [
           "```\n\nYou will also need to have ffmpeg installed. You can check to see if you already have ffmpeg by..."
          ],
          [
           "```\n\nThat's all the code that's needed -- notice that the API endpoints returns two audio files (one..."
          ],
          [
           "```\n\nEverything else remains the same!\n\n---\n\nNow, of course, we are working with video files, so we ..."
          ],
          [
           "```\n\nYou can read up on [ffmpeg documentation](https://ffmpeg.org/ffmpeg.html) if you'd like to unde..."
          ],
          [
           "```\n\nIn this example, the FastAPI app has two routes: `/` and `/uploadvideo/`.\n\nThe `/` route return..."
          ],
          [
           "```\n\nWrite the following as the contents of `home.html`:..."
          ],
          [
           "```html\n&lt;!DOCTYPE html> &lt;html> &lt;head> &lt;title>Video Gallery&lt;/title>\n&lt;style> body { ..."
          ],
          [
           "&lt;div class=\"gallery\"> {% for video in videos %} &lt;div class=\"video\">\n&lt;video controls> &lt;so..."
          ],
          [
           "```\n\n## Step 4: Run your FastAPI app\n\nFinally, we are ready to run our FastAPI app, powered by the G..."
          ],
          [
           "Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Block..."
          ],
          [
           "```\n\nThe problem is that anytime that you want to make a change to your layout, events, or component..."
          ],
          [
           "```\n\nThe important part here is the line that says `Watching...` What's happening here is that Gradi..."
          ],
          [
           "```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gr..."
          ],
          [
           "```\n\nWhich you could run like this: `gradio run.py --name Gretel`\n\nAs a small aside, this auto-reloa..."
          ],
          [
           "```\n\nNotice that:\n\n- You do not need to launch your demo â€” Gradio does that for you automatically!\n\n..."
          ],
          [
           "Running Background Tasks\n\nRelated spaces: https://huggingface.co/spaces/freddyaboulton/gradio-google..."
          ],
          [
           "The code will look like this:\n\n```python\nDB_FILE = \"./reviews.db\"\ndb = sqlite3.connect(DB_FILE)\n\n# C..."
          ],
          [
           "```\n\nLet's also write a function to load the latest reviews when the gradio application loads:\n\n```p..."
          ],
          [
           "```\n\n## Step 3 - Synchronize with HuggingFace Datasets ðŸ¤—\n\nWe could call `demo.launch()` after step 2..."
          ],
          [
           "```\n\nNote that you'll have to get an access token from the \"Settings\" tab of your HuggingFace for th..."
          ],
          [
           "```\n\n## Step 4 (Bonus) - Deployment to HuggingFace Spaces\n\nYou can use the HuggingFace [Spaces](http..."
          ],
          [
           "How to Use the Plot Component for Maps\n\nTags: PLOTS, MAPS\n\n## Introduction\n\nThis guide explains how ..."
          ],
          [
           "```\n\nIn the code above, we first load the csv data into a pandas dataframe. Let's begin by defining ..."
          ],
          [
           "```\n\nAbove, we create a scatter plot on mapbox by passing it our list of latitudes and longitudes to..."
          ],
          [
           "```\n\nWe layout these components using the `gr.Column` and `gr.Row` and we'll also add event triggers..."
          ],
          [
           "Gradio Demo: blocks_style\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\n\n\n```\nimport gradio as gr\n\nwith gr.Blocks(title=\"Styling Examples\") as demo:\n    with gr.Column(..."
          ],
          [
           "Using Flagging\n\nRelated spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsource..."
          ],
          [
           "There are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that co..."
          ],
          [
           "## What happens to flagged data?\n\nWithin the directory provided by the `flagging_dir` argument, a CS..."
          ],
          [
           "```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nWhen you click the flag button..."
          ],
          [
           "```\n\nIf you wish for the user to provide a reason for flagging, you can pass a list of strings to th..."
          ],
          [
           "```\n\n## The HuggingFaceDatasetSaver Callback\n\nSometimes, saving the data to a local CSV file doesn't..."
          ],
          [
           "```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Fa..."
          ],
          [
           "At the same time, you might want to use an existing `FlaggingCallback` to avoid writing extra code.\n..."
          ],
          [
           "his demo shows how you can build a live interactive dashboard with gradio.\nThe current time is refre..."
          ],
          [
           "Gradio Demo: plot_component\n\n\n```\n!pip install -q gradio matplotlib numpy\n```\n\n\n```\nimport gradio as..."
          ],
          [
           "Gradio Demo: request_ip_headers\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef pr..."
          ],
          [
           "è¿žæŽ¥åˆ°æ•°æ®åº“\n\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/gradio/chicago-bike-share-dashboard\næ ‡ç­¾ï¼šTABULAR, PLOTS\n\n##..."
          ],
          [
           "**é‡è¦æç¤º**ï¼šå¦‚æžœæ‚¨è®¡åˆ’åœ¨ HuggingFace Spaces ä¸Šæ‰˜ç®¡æ­¤æ¼”ç¤ºï¼Œè¯·ç¡®ä¿æ•°æ®åº“åœ¨ **8080** ç«¯å£ä¸Šã€‚Spaces\nå°†é˜»æ­¢é™¤ç«¯å£ 80ã€443 æˆ– 8080 ä¹‹å¤–çš„æ‰€æœ‰å¤–éƒ¨è¿žæŽ¥..."
          ],
          [
           "```python\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDB_USER = os.getenv(\"DB_USE..."
          ],
          [
           "def get_most_popular_stations():\n\n    df = pd.read_sql(\n        \"\"\"\n    SELECT COUNT(ride_id) as n, ..."
          ],
          [
           "```\n\nå¦‚æžœæ‚¨åœ¨æœ¬åœ°è¿è¡Œæˆ‘ä»¬çš„è„šæœ¬ï¼Œå¯ä»¥åƒä¸‹é¢è¿™æ ·å°†å‡­æ®ä½œä¸ºçŽ¯å¢ƒå˜é‡ä¼ é€’ï¼š\n\n```bash\nDB_USER='username' DB_PASSWORD='password' DB_HOST='h..."
          ],
          [
           "```\n\n## æ­¥éª¤ 3 - éƒ¨ç½²\n\nå¦‚æžœæ‚¨è¿è¡Œä¸Šè¿°ä»£ç ï¼Œæ‚¨çš„åº”ç”¨ç¨‹åºå°†åœ¨æœ¬åœ°è¿è¡Œã€‚\næ‚¨ç”šè‡³å¯ä»¥é€šè¿‡å°† `share=True` å‚æ•°ä¼ é€’ç»™ `launch` æ¥èŽ·å¾—ä¸€ä¸ªä¸´æ—¶å…±äº«é“¾æŽ¥ã€‚\n\nä½†æ˜¯å¦‚æžœæ‚¨æƒ³..."
          ],
          [
           "Gradio Demo: chatinterface_streaming_echo\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport time\nimport..."
          ],
          [
           "Gradio Demo: hangman\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nsecret_word = \"gra..."
          ],
          [
           "The 4 Kinds of Gradio Interfaces\n\nSo far, we've always assumed that in order to build an Gradio demo..."
          ],
          [
           "$code_fake_gan_no_input\n$demo_fake_gan_no_input\n\n## Input-only demos\n\nSimilarly, to create a demo th..."
          ],
          [
           "`@gradio/dataframe`\n\n```html\n<script>\n    import { BaseDataFrame, BaseExample } from \"@gradio/datafr..."
          ],
          [
           "Gradio Demo: line_plot\n\n\n```\n!pip install -q gradio vega_datasets pandas\n```..."
          ],
          [
           "```\nimport gradio as gr\nfrom vega_datasets import data\n\nstocks = data.stocks()\ngapminder = data.gapm..."
          ],
          [
           "def line_plot_fn(dataset):\n    if dataset == \"stocks\":\n        return gr.LinePlot(\n            stock..."
          ],
          [
           "if __name__ == \"__main__\":\n    line_plot.launch()..."
          ],
          [
           "Contributing to Gradio\n\n![GitHub issues by-label](https://img.shields.io/github/issues/gradio-app/gr..."
          ],
          [
           "```\n\n### ðŸ“¦ Using dev containers\n\nYou can alternatively use dev containers. This is supported on all ..."
          ],
          [
           "If you're a newcomer to Gradio, we recommend getting familiar with the overall structure of the repo..."
          ],
          [
           "```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Button()\n    \nif __name__ == \"__main__\":\n  ..."
          ],
          [
           "```\npnpm test:browser\n```\n\nTo build the frontend code before running browser tests:\n\n```\npnpm test:b..."
          ],
          [
           "```\npnpm storybook\n```\n\n\n## ðŸ“® Submitting PRs\n\nAll PRs should be against `main`, and ideally should a..."
          ],
          [
           "```\nbash scripts/format_backend.sh\n```\n\n```\nbash scripts/format_frontend.sh\n```\n\nThank you for takin..."
          ],
          [
           "Gradio Demo: leaderboard\n### A simple dashboard ranking spaces by number of likes.\n        \n\n\n```\n!p..."
          ],
          [
           "```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport requests\nimport pandas as pd\nfrom h..."
          ],
          [
           "Gradio ç•Œé¢çš„ 4 ç§ç±»åž‹\n\nåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´å‡è®¾æž„å»º Gradio æ¼”ç¤ºéœ€è¦åŒæ—¶å…·å¤‡è¾“å…¥å’Œè¾“å‡ºã€‚ä½†å¯¹äºŽæœºå™¨å­¦ä¹ æ¼”ç¤ºæ¥è¯´ï¼Œå¹¶ä¸æ€»æ˜¯å¦‚æ­¤ï¼šä¾‹å¦‚ï¼Œ*æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡åž‹*ä¸éœ€è¦ä»»ä½•è¾“å…¥ï¼Œä½†ä¼šç”Ÿæˆä¸€..."
          ],
          [
           "æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ `Interface` ç±»æž„å»ºæ¯ç§ç±»åž‹çš„æ¼”ç¤ºï¼Œä»¥åŠç¤ºä¾‹ï¼š\n\n## æ ‡å‡†æ¼”ç¤º (Standard demos)\n\nè¦åˆ›å»ºå…·æœ‰è¾“å…¥å’Œè¾“å‡ºç»„ä»¶çš„æ¼”ç¤ºï¼Œåªéœ€åœ¨ `Interface()`..."
          ],
          [
           "Gradio Demo: calculator_blocks\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef cal..."
          ],
          [
           "@gradio/dataframe\n\n## 0.4.3\n\n### Patch Changes\n\n- Updated dependencies [[`846d52d`](https://github.c..."
          ],
          [
           "## 0.4.1\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.3.9\n\n### Patch Changes\n\n- Updated dependencies [[`46f13f496`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.7\n\n### Patch Changes\n\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.4\n\n### Features\n\n- [#6318](https://github.com/gradio-app/gradio/pull/6318) [`d3b53a457`](http..."
          ],
          [
           "## 0.3.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.3.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.co..."
          ],
          [
           "## 0.3.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5877](https://github.com/gradio-app/gradio/pull/5877) [`a55b80942`](http..."
          ],
          [
           "## 0.3.0\n\n### Features\n\n- [#5569](https://github.com/gradio-app/gradio/pull/5569) [`2a5b9e03b`](http..."
          ],
          [
           "## 0.2.4\n\n### Patch Changes\n\n- Updated dependencies [[`ee8eec1e5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.1\n\n### Fixes\n\n- [#5445](https://github.com/gradio-app/gradio/pull/5445) [`67bb7bcb`](https://..."
          ],
          [
           "## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`05892302`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`31996c99`](https://github.com/gradio-app/grad..."
          ],
          [
           "- [#5268](https://github.com/gradio-app/gradio/pull/5268) [`f49028cf`](https://github.com/gradio-app..."
          ],
          [
           "- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app..."
          ],
          [
           "### Fixes\n\n- [#5256](https://github.com/gradio-app/gradio/pull/5256) [`933db53e`](https://github.com..."
          ],
          [
           "Gradio Demo: rows_and_columns\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the d..."
          ],
          [
           "Gradio Demo: lineplot_component\n\n\n```\n!pip install -q gradio vega_datasets\n```\n\n\n```\nimport gradio a..."
          ],
          [
           "@gradio/box\n\n## 0.1.6\n\n### Patch Changes\n\n- Updated dependencies [[`73268ee`](https://github.com/gra..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.0-beta.6\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.1.0-beta.5\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.0.7\n\n### Patch Changes\n\n- Updated dependencies [[`e70805d54`](https://github.com/gradio-app/gra..."
          ],
          [
           "Gradio Demo: webcam\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n\nimport gradio as gr\n\n\ndef snap(image, v..."
          ],
          [
           "Gradio Demo: theme_new_step_2\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nfrom __future__ import annotations\nfrom typing import Iterable\nimport gradio as gr\nfrom gradio.t..."
          ],
          [
           "def repeat(name, count):\n        time.sleep(3)\n        return name * count\n\n    button.click(repeat,..."
          ],
          [
           "Gradio å’Œ ONNX åœ¨ Hugging Face ä¸Š\n\nRelated spaces: https://huggingface.co/spaces/onnx/EfficientNet-Lite..."
          ],
          [
           "## ONNX æ¨¡åž‹ä»“åº“æ˜¯ä»€ä¹ˆï¼Ÿ\n\nOpen Neural Network Exchangeï¼ˆ[ONNX](https://onnx.ai/)ï¼‰æ˜¯ä¸€ç§è¡¨ç¤ºæœºå™¨å­¦ä¹ æ¨¡åž‹çš„å¼€æ”¾æ ‡å‡†æ ¼å¼ã€‚ONNX ç”±ä¸€ä¸ªå®ž..."
          ],
          [
           "åœ¨æ­¤å¤„å¼€å§‹[https://gradio.app/getting_started](https://gradio.app/getting_started)\n\n### Hugging Face Spac..."
          ],
          [
           "## ONNX Runtime çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\n\nONNX Runtime æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„æŽ¨ç†å’Œè®­ç»ƒæœºå™¨å­¦ä¹ åŠ é€Ÿå™¨ã€‚å®ƒä½¿å¾—åœ¨ Hugging Face ä¸Šä½¿ç”¨ ONNX æ¨¡åž‹ä»“åº“ä¸­çš„æ¨¡åž‹è¿›è¡Œå®žæ—¶ Gr..."
          ],
          [
           "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Gradio ä¸º EfficientNet-Lite4 è®¾ç½®ç¤ºä¾‹æ¼”ç¤º\n\né¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„ä¾èµ–é¡¹å¹¶ä¸‹è½½å’Œè½½å…¥æ¥è‡ª ONNX æ¨¡åž‹ä»“åº“çš„ efficientnet-lite..."
          ],
          [
           "# ä½¿ç”¨ç­‰æ¯”ä¾‹ç¼©æ”¾è°ƒæ•´å›¾åƒå°ºå¯¸\ndef resize_with_aspectratio(img, out_height, out_width, scale=87.5, inter_pol=cv2.IN..."
          ],
          [
           "title = \"EfficientNet-Lite4\"\ndescription = \"EfficientNet-Lite 4æ˜¯æœ€å¤§çš„å˜ä½“ï¼Œä¹Ÿæ˜¯EfficientNet-Liteæ¨¡åž‹é›†åˆä¸­æœ€å‡†ç¡®çš„ã€‚å®ƒ..."
          ],
          [
           "```\n\n## å¦‚ä½•ä½¿ç”¨ ONNX æ¨¡åž‹åœ¨ HF Spaces ä¸Šè´¡çŒ® Gradio æ¼”ç¤º\n\n- å°†æ¨¡åž‹æ·»åŠ åˆ°[onnx model zoo](https://github.com/onnx/mode..."
          ],
          [
           "Gradio Demo: slider_release\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef identi..."
          ],
          [
           "How to add support for more languages\n\nWe would love to support more languages for Gradio ðŸŒŽ\n\nTo add ..."
          ],
          [
           "Setting Up a Demo for Maximum Performance\n\nTags: CONCURRENCY, LATENCY, PERFORMANCE\n\nLet's say that y..."
          ],
          [
           "(2) They allow the server to send multiple updates to the frontend. This means, for example, that th..."
          ],
          [
           "```\n\n**How Requests are Processed from the Queue**\n\nWhen a Gradio server is launched, a pool of thre..."
          ],
          [
           "```\n\nInitially, 3 workers will get dispatched to handle requests 1, 2, and 5 (corresponding to funct..."
          ],
          [
           "### The `concurrency_limit` parameter in events\n\nYou can also set the number of requests that can be..."
          ],
          [
           "**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your exp..."
          ],
          [
           "```\n\nHere's the same function rewritten to take in a batch of samples:\n\n```py\nimport time\n\ndef trim_..."
          ],
          [
           "Gradio Demo: blocks_outputs\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\n\n\ndef make_markdown():\n    return [\n        [\n            \"# hello again\",\n ..."
          ],
          [
           "with gr.Blocks() as demo:\n    with gr.Column():\n        txt = gr.Textbox(label=\"Small Textbox\", line..."
          ],
          [
           "highlight = gr.HighlightedText(show_label=False)\n        gr.Dataframe(interactive=True, col_count=(3..."
          ],
          [
           "[\n                    \"## hello\",\n                    \"Hello my name is frank, I am liking the small..."
          ],
          [
           "Gradio Demo: slider_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr...."
          ],
          [
           "`@gradio/radio`\n\n```html\n<script>\n    import { BaseRadio, BaseExample } from \"@gradio/radio\"; \n</scr..."
          ],
          [
           "Gradio Demo: clearbutton_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\nwit..."
          ],
          [
           "`@gradio/html`\n\n```javascript\nimport { BaseHTML } from \"@gradio/html\";\n```\n\nBaseHTML\n```javascript\n\t..."
          ],
          [
           "Gradio Demo: hello_world\n### The simplest possible Gradio demo. It wraps a 'Hello {name}!' function ..."
          ],
          [
           "Gradio Demo: ner_pipeline\n\n\n```\n!pip install -q gradio torch transformers\n```\n\n\n```\nfrom transformer..."
          ],
          [
           "æŽ¥å£çŠ¶æ€ (Interface State)\n\næœ¬æŒ‡å—ä»‹ç»äº† Gradio ä¸­å¦‚ä½•å¤„ç†çŠ¶æ€ã€‚äº†è§£å…¨å±€çŠ¶æ€å’Œä¼šè¯çŠ¶æ€çš„åŒºåˆ«ï¼Œä»¥åŠå¦‚ä½•åŒæ—¶ä½¿ç”¨å®ƒä»¬ã€‚\n\n## å…¨å±€çŠ¶æ€ (Global State)\n\næ‚¨çš„..."
          ],
          [
           "èŠå¤©æœºå™¨äººå°±æ˜¯éœ€è¦ä¼šè¯çŠ¶æ€çš„ä¸€ä¸ªä¾‹å­ - æ‚¨å¸Œæœ›è®¿é—®ç”¨æˆ·ä¹‹å‰çš„æäº¤ï¼Œä½†ä¸èƒ½å°†èŠå¤©è®°å½•å­˜å‚¨åœ¨å…¨å±€å˜é‡ä¸­ï¼Œå› ä¸ºè¿™æ ·èŠå¤©è®°å½•ä¼šåœ¨ä¸åŒç”¨æˆ·ä¹‹é—´æ··ä¹±ã€‚\n\n$code_chatbot_dialogpt\n$demo..."
          ],
          [
           "ä½¿ç”¨ Hugging Face é›†æˆ\n\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/gradio/helsinki_translation_en_es\næ ‡ç­¾ï¼šHUBï¼ŒSPAC..."
          ],
          [
           "ðŸ¤— transformers åº“æœ‰ä¸€ä¸ªéžå¸¸æ˜“äºŽä½¿ç”¨çš„æŠ½è±¡å±‚ï¼Œ[`pipeline()`](https://huggingface.co/docs/transformers/v4.16.2/en/mai..."
          ],
          [
           "```\n\nä½†æ˜¯ï¼Œ`gradio` å®žé™…ä¸Šä½¿å°† `pipeline` è½¬æ¢ä¸ºæ¼”ç¤ºæ›´åŠ å®¹æ˜“ï¼Œåªéœ€ä½¿ç”¨ `gradio.Interface.from_pipeline` æ–¹æ³•ï¼Œæ— éœ€æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºç»„ä»¶ï¼š\n\n`..."
          ],
          [
           "```\n\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬åªéœ€æŒ‡å®šæ¨¡åž‹åç§°å¹¶è¯´æ˜Ž `src` åº”ä¸º `models`ï¼ˆHugging Face çš„ Model Hubï¼‰ã€‚ç”±äºŽæ‚¨ä¸ä¼šåœ¨è®¡ç®—æœºä¸ŠåŠ è½½æ¨¡åž‹ï¼Œå› æ­¤æ— éœ€å®‰è£…ä»»ä½•ä¾èµ–é¡¹ï¼ˆé™¤äº† `gr..."
          ],
          [
           "```python\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file,\n)\n..."
          ],
          [
           "```\n\nåœ¨è¿™é‡Œï¼Œ`create_repo` ä½¿ç”¨ç‰¹å®šå¸æˆ·çš„ Write Token åœ¨ç‰¹å®šå¸æˆ·ä¸‹åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç›®æ ‡åç§°çš„ gradio repoã€‚`repo_name` èŽ·å–ç›¸å…³å­˜å‚¨åº“çš„å®Œæ•´å­˜å‚¨åº“åç§°..."
          ],
          [
           "```\n\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `gr.load()`ï¼Œè¿™ä¸Žä½¿ç”¨æŽ¨ç† API åŠ è½½æ¨¡åž‹æ‰€ä½¿ç”¨çš„æ–¹æ³•ç›¸åŒã€‚ä½†æ˜¯ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‡å®š `src` ä¸º `spaces`ï¼ˆHugging Face Spacesï¼‰..."
          ],
          [
           "Gradio Demo: chatbot_dialogpt\n\n\n```\n!pip install -q gradio torch transformers\n```\n\n\n```\nimport gradi..."
          ],
          [
           "Gradio Demo: blocks_joined\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo..."
          ],
          [
           "Gradio Demo: zip_to_json\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nfrom zipfile import ZipFile\n\nimport..."
          ],
          [
           "@gradio/group\n\n## 0.1.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`2..."
          ],
          [
           "## 0.0.2-beta.0\n\n### Features\n\n- [#5648](https://github.com/gradio-app/gradio/pull/5648) [`c573e2339..."
          ],
          [
           "Gradio Demo: titanic_survival\n\n\n```\n!pip install -q gradio scikit-learn numpy pandas\n```\n\n\n```\n# Dow..."
          ],
          [
           "```\nimport os\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn...."
          ],
          [
           "def encode_df(df):\n    df = encode_age(df)\n    df = encode_fare(df)\n    sex_mapping = {\"male\": 0, \"f..."
          ],
          [
           "clf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\n\ndef pre..."
          ],
          [
           "`@gradio/uploadbutton`\n\n```html\n<script>\n    import { BaseUploadButton } from \"@gradio/uploadbutton\"..."
          ],
          [
           "Gradio Demo: translation\n### This translation demo takes in the text, source and target languages, a..."
          ],
          [
           "```\n!pip install -q gradio git+https://github.com/huggingface/transformers gradio torch\n```\n\n\n```\nim..."
          ],
          [
           "Using Hugging Face Integrations\n\nRelated spaces: https://huggingface.co/spaces/gradio/en2es\nTags: HU..."
          ],
          [
           "```\n\nFor any Hugging Face model supported in the Inference API, Gradio automatically infers the expe..."
          ],
          [
           "```\n\nHere, `create_repo` creates a gradio repo with the target name under a specific account using t..."
          ],
          [
           "```\n\nNotice that we use `gr.load()`, the same method we used to load models using the Inference API...."
          ],
          [
           "```\n\nThe previous code produces the following interface, which you can try right here in your browse..."
          ],
          [
           "ä½¿ç”¨ Gradio å—åƒå‡½æ•°ä¸€æ ·\n\nTags: TRANSLATION, HUB, SPACES\n\n**å…ˆå†³æ¡ä»¶**: æœ¬æŒ‡å—æ˜¯åœ¨å—ä»‹ç»çš„åŸºç¡€ä¸Šæž„å»ºçš„ã€‚è¯·ç¡®ä¿[å…ˆé˜…è¯»è¯¥æŒ‡å—](https://grad..."
          ],
          [
           "çŽ°åœ¨ï¼Œå‡è®¾ä½ æœ‰ä¸€ä¸ªç”Ÿæˆè‹±æ–‡æ–‡æœ¬çš„åº”ç”¨ç¨‹åºï¼Œä½†ä½ è¿˜æƒ³é¢å¤–ç”Ÿæˆå¾·æ–‡æ–‡æœ¬ã€‚\n\nä½ å¯ä»¥é€‰æ‹©ï¼š\n\n1. å°†æˆ‘çš„è‹±å¾·ç¿»è¯‘çš„æºä»£ç å¤åˆ¶ç²˜è´´åˆ°ä½ çš„åº”ç”¨ç¨‹åºä¸­ã€‚\n\n2. åœ¨ä½ çš„åº”ç”¨ç¨‹åºä¸­åŠ è½½æˆ‘çš„è‹±å¾·ç¿»è¯‘ï¼Œå¹¶å°†å…¶å½“ä½œæ™®é€š..."
          ],
          [
           "translate_btn.click(translate, inputs=english, outputs=german, api_name=\"translate-to-german\")\n\nè¿™ä¸ª `..."
          ],
          [
           "### å¼€å§‹æž„å»ºï¼âš’ï¸\n\n## Parting Remarks\n\næˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°† Blocks åº”ç”¨ç¨‹åºè§†ä¸ºå¸¸è§„ Python å‡½æ•°ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„åº”ç”¨ç¨‹åºä¹‹é—´ç»„åˆåŠŸèƒ½ã€‚\nä»»ä½• Blocks åº”ç”¨ç¨‹åº..."
          ],
          [
           "Gradio Demo: unispeech-speaker-verification\n\n\n```\n!pip install -q gradio git+https://github.com/hugg..."
          ],
          [
           "```\nimport gradio as gr\nimport torch\nfrom torchaudio.sox_effects import apply_effects_file\nfrom tran..."
          ],
          [
           "STYLE = \"\"\"\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/boots..."
          ],
          [
           "<div class=\"container\">\n        <div class=\"row\"><h1 style=\"text-align: center\">The speakers are</h1..."
          ],
          [
           "EFFECTS = [\n    [\"remix\", \"-\"],\n    [\"channels\", \"1\"],\n    [\"rate\", \"16000\"],\n    [\"gain\", \"-1.0\"],\n..."
          ],
          [
           "with torch.no_grad():\n        emb1 = model(input1).embeddings\n        emb2 = model(input2).embedding..."
          ],
          [
           "description = (\n    \"This demo will compare two speech samples and determine if they are from the sa..."
          ],
          [
           "Gradio Demo: white_noise_vid_not_playable\n\n\n```\n!pip install -q gradio opencv-python\n```\n\n\n```\nimpor..."
          ],
          [
           "Gradio Demo: logoutbutton_component\n\n\n```\n!pip install -q gradio gradio[oauth]\n```\n\n\n```\nimport grad..."
          ],
          [
           "Gradio Demo: chicago-bikeshare-dashboard\n\n\n```\n!pip install -q gradio psycopg2 matplotlib SQLAlchemy..."
          ],
          [
           "```\nimport os\nimport gradio as gr\nimport pandas as pd\n\nDB_USER = os.getenv(\"DB_USER\")\nDB_PASSWORD = ..."
          ],
          [
           "If data were added to the database, the plots in this demo would update\n    whenever the webpage is ..."
          ],
          [
           "Gradio Demo: textbox_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr..."
          ],
          [
           "ä½¿ç”¨æ ‡è®°\n\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingfac..."
          ],
          [
           "## åœ¨ `gradio.Interface` ä¸­ä½¿ç”¨**æ ‡è®°**æŒ‰é’®\n\nä½¿ç”¨ Gradio çš„ `Interface` è¿›è¡Œæ ‡è®°ç‰¹åˆ«ç®€å•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨è¾“å‡ºç»„ä»¶ä¸‹æ–¹æœ‰ä¸€ä¸ªæ ‡è®°ä¸º**æ ‡è®°**çš„æŒ‰é’®ã€‚å½“..."
          ],
          [
           "- `allow_flagging`ï¼šæ­¤å‚æ•°å¯ä»¥è®¾ç½®ä¸º `\"manual\"`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼Œ`\"auto\"` æˆ– `\"never\"`ã€‚\n  - `manual`ï¼šç”¨æˆ·å°†çœ‹åˆ°ä¸€ä¸ªæ ‡è®°æŒ‰é’®ï¼Œåªæœ‰åœ¨ç‚¹å‡»æŒ‰é’®æ—¶æ ·..."
          ],
          [
           "## æ ‡è®°çš„æ•°æ®ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\n\nåœ¨ `flagging_dir` å‚æ•°æä¾›çš„ç›®å½•ä¸­ï¼Œå°†è®°å½•æ ‡è®°çš„æ•°æ®çš„ CSV æ–‡ä»¶ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼šä¸‹é¢çš„ä»£ç åˆ›å»ºäº†åµŒå…¥å…¶ä¸­çš„è®¡ç®—å™¨ç•Œé¢ï¼š\n\n```python\n..."
          ],
          [
           "```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nå½“æ‚¨ç‚¹å‡»ä¸Šé¢çš„æ ‡è®°æŒ‰é’®æ—¶ï¼Œå¯åŠ¨ç•Œé¢çš„ç›®å½•å°†åŒ…æ‹¬ä¸€ä¸ªæ–°çš„æ ‡è®°å­..."
          ],
          [
           "```\n\nå¦‚æžœæ‚¨å¸Œæœ›ç”¨æˆ·ä¸ºæ ‡è®°æä¾›ä¸€ä¸ªåŽŸå› ï¼Œæ‚¨å¯ä»¥å°†å­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ç»™ Interface çš„ `flagging_options` å‚æ•°ã€‚ç”¨æˆ·åœ¨æ ‡è®°æ—¶å¿…é¡»é€‰æ‹©å…¶ä¸­ä¸€é¡¹ï¼Œé€‰é¡¹å°†ä½œä¸ºé™„åŠ åˆ—ä¿å­˜åœ¨ CSV ..."
          ],
          [
           "```\n\n## HuggingFaceDatasetSaver å›žè°ƒ\n\næœ‰æ—¶ï¼Œå°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ° CSV æ–‡ä»¶æ˜¯ä¸åˆç†çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ Hugging Face Spaces ä¸Š\nï¼Œå¼€å‘è€…é€šå¸¸æ— æ³•è®¿é—®æ‰˜ç®¡ ..."
          ],
          [
           "```\n\næ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„ Hugging Face ä»¤ç‰Œå’Œ\nè¦ä¿å­˜æ ·æœ¬çš„æ•°æ®é›†çš„åç§°ï¼Œå®šä¹‰äº†æˆ‘ä»¬è‡ªå·±çš„\n`gradio.HuggingFaceDatasetSaver` çš„å®žä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°† ..."
          ],
          [
           "åŒæ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä½¿ç”¨çŽ°æœ‰çš„ `FlaggingCallback` æ¥é¿å…ç¼–å†™é¢å¤–çš„ä»£ç ã€‚\nè¿™éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š\n\n1. æ‚¨å¿…é¡»åœ¨ä»£ç ä¸­çš„æŸä¸ªä½ç½®è¿è¡Œæ‚¨çš„å›žè°ƒçš„ `.setup()` æ–¹æ³•\n   åœ¨ç¬¬ä¸€æ¬¡æ ‡..."
          ],
          [
           "Image Classification in PyTorch\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/pytorch-imag..."
          ],
          [
           "```\n\nBecause we will be using the model for inference, we have called the `.eval()` method.\n\n## Step..."
          ],
          [
           "```\n\nLet's break this down. The function takes one parameter:\n\n- `inp`: the input image as a `PIL` i..."
          ],
          [
           "Theming\n\nTags: THEMES\n\n## Introduction\n\nGradio features a built-in theming engine that lets you cust..."
          ],
          [
           "```\n\n$demo_theme_builder\n\nYou can use the Theme Builder running on Spaces above, though it runs much..."
          ],
          [
           "```\n\n<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-extended-step-1.hf.space?__theme=light..."
          ],
          [
           "```\n\n<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-extended-step-2.hf.space?__theme=light..."
          ],
          [
           "```\n\n<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-extended-step-3.hf.space?__theme=light..."
          ],
          [
           "```\n\nIn the example above, we've set the `loader_color` and `slider_color` variables to `#FF0000`, d..."
          ],
          [
           "#### Referencing Core Variables\n\nTo reference one of the core constructor variables, precede the var..."
          ],
          [
           "```\n\nIn the example above, we've set the `button_primary_background_fill` and `button_primary_backgr..."
          ],
          [
           "```\n\nNow, if we change the `button_primary_background_fill` variable, the `button_primary_background..."
          ],
          [
           "```\n\n`button_primary_border_dark` will draw its value from `button_primary_background_fill_dark`, be..."
          ],
          [
           "$code_theme_new_step_2\n\n<div class=\"wrapper\">\n<iframe\n\tsrc=\"https://gradio-theme-new-step-2.hf.space..."
          ],
          [
           "- Via the class instance\n\nEach theme instance has a method called `push_to_hub` we can use to upload..."
          ],
          [
           "```\n\n- Via the command line\n\nFirst save the theme to disk\n\n```python\nseafoam.dump(filename=\"seafoam...."
          ],
          [
           "```\n\nIn order to upload a theme, you must have a HuggingFace account and pass your [Access Token](ht..."
          ],
          [
           "You can sort the themes by the number of likes on the space and from most to least recently created ..."
          ],
          [
           "```\n\nYou can also pass the theme string directly to `Blocks` or `Interface` (`gr.Blocks(theme=\"gradi..."
          ],
          [
           "@gradio/tabs\n\n## 0.1.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`28..."
          ],
          [
           "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.co..."
          ],
          [
           "### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/utils@0.1.2\n\n## 0.0.6\n\n### Features\n\n- [#5..."
          ],
          [
           "##### Improved markdown support\n\nWe now have better support for markdown in `gr.Markdown` and `gr.Da..."
          ],
          [
           "å¦‚ä½•åˆ›å»ºä¸€ä¸ªæ–°ç»„ä»¶\n\n## ç®€ä»‹\n\næœ¬æŒ‡å—æ—¨åœ¨è¯´æ˜Žå¦‚ä½•æ·»åŠ ä¸€ä¸ªæ–°ç»„ä»¶ï¼Œä½ å¯ä»¥åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨è¯¥ç»„ä»¶ã€‚è¯¥æŒ‡å—å°†é€šè¿‡ä»£ç ç‰‡æ®µé€æ­¥å±•ç¤ºå¦‚ä½•æ·»åŠ [ColorPicker](https://gr..."
          ],
          [
           "## 1. åˆ›å»ºä¸€ä¸ªæ–°çš„ Python ç±»å¹¶å¯¼å…¥å®ƒ\n\né¦–å…ˆè¦åšçš„æ˜¯åœ¨[components.py](https://github.com/gradio-app/gradio/blob/main/gra..."
          ],
          [
           "def __init__(\n        self,\n        value: str = None,\n        *,\n        label: Optional[str] = Non..."
          ],
          [
           "# è¾“å…¥åŠŸèƒ½\n    def preprocess(self, x: str | None) -> Any:\n        \"\"\"\n        Any preprocessing needed ..."
          ],
          [
           "```\n\nä¸€æ—¦å®šä¹‰å®Œï¼Œå°±éœ€è¦åœ¨[\\_\\_init\\_\\_](https://github.com/gradio-app/gradio/blob/main/gradio/__init__.py)æ¨¡å—ç±»ä¸­..."
          ],
          [
           "```\n\n### 1.1 ä¸º Python ç±»ç¼–å†™å•å…ƒæµ‹è¯•\n\nåœ¨å¼€å‘æ–°ç»„ä»¶æ—¶ï¼Œè¿˜åº”ä¸ºå…¶ç¼–å†™ä¸€å¥—å•å…ƒæµ‹è¯•ã€‚è¿™äº›æµ‹è¯•åº”è¯¥æ”¾åœ¨[gradio/test/test_components.py](https:/..."
          ],
          [
           "color_picker_input.interpretation_replacement = \"unknown\"\n\n        self.assertEqual(\n            col..."
          ],
          [
           "```\n\n## 2. åˆ›å»ºä¸€ä¸ªæ–°çš„ Svelte ç»„ä»¶\n\nè®©æˆ‘ä»¬æ¥çœ‹çœ‹åˆ›å»ºæ–°ç»„ä»¶çš„å‰ç«¯å¹¶å°†å…¶ä¸Žå…¶ Python ä»£ç æ˜ å°„èµ·æ¥çš„æ­¥éª¤ï¼š\n\n- åœ¨ [js æ–‡ä»¶å¤¹](https://github.com/..."
          ],
          [
           "$: value;\n\t$: handle_change(value);\n\n\tconst dispatch = createEventDispatcher<{\n\t\tchange: string;\n\t\ts..."
          ],
          [
           "```\n\n- é€šè¿‡æ‰§è¡Œ `export { default as FileName } from \"./FileName.svelte\"`ï¼Œåœ¨æ‚¨å°† Svelte ç»„ä»¶æ”¾ç½®çš„åŒ…çš„ index.ts æ–‡ä»¶..."
          ],
          [
           "export let label: string = \"ColorPicker\";\n\texport let elem_id: string = \"\";\n\texport let visible: boo..."
          ],
          [
           "```\n\nç¬¬äºŒä¸ªæ–‡ä»¶åŒ…å«äº†å‰ç«¯çš„æµ‹è¯•ï¼Œä¾‹å¦‚ ColorPicker ç»„ä»¶çš„æµ‹è¯•ï¼š\n\n```typescript\nimport { test, describe, assert, afterEach }..."
          ],
          [
           "```\n\n- `directory.ts` æ–‡ä»¶ä¸­æ·»åŠ ç»„ä»¶çš„æ˜ å°„ã€‚å¤åˆ¶å¹¶ç²˜è´´ä»»ä½•ç»„ä»¶çš„æ˜ å°„è¡Œï¼Œå¹¶ç¼–è¾‘å…¶æ–‡æœ¬ã€‚é”®åå¿…é¡»æ˜¯ Python åº“ä¸­å®žé™…ç»„ä»¶åç§°çš„å°å†™ç‰ˆæœ¬ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽ ColorPicker ç»„..."
          ],
          [
           "```\n\n### 2.1 ä¸º Svelte ç»„ä»¶ç¼–å†™å•å…ƒæµ‹è¯•\n\nåœ¨å¼€å‘æ–°ç»„ä»¶æ—¶ï¼Œæ‚¨è¿˜åº”è¯¥ä¸ºå…¶ç¼–å†™ä¸€å¥—å•å…ƒæµ‹è¯•ã€‚æµ‹è¯•åº”è¯¥æ”¾ç½®åœ¨æ–°ç»„ä»¶çš„æ–‡ä»¶å¤¹ä¸­ï¼Œæ–‡ä»¶åä¸º MyAwesomeComponent.test...."
          ],
          [
           "è¦æµ‹è¯•åº”ç”¨ç¨‹åºï¼š\n\n- åœ¨ç»ˆç«¯ä¸Šè¿è¡Œ `python path/demo/run.py`ï¼Œå®ƒä¼šåœ¨åœ°å€ [http://localhost:7860](http://localhost:7860) å¯åŠ¨..."
          ],
          [
           "Gradio Demo: concurrency_with_queue\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimpo..."
          ],
          [
           "Gradio Demo: blocks_essay_simple\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef c..."
          ],
          [
           "@gradio/form\n\n## 0.1.6\n\n### Patch Changes\n\n- Updated dependencies [[`73268ee`](https://github.com/gr..."
          ],
          [
           "## 0.1.3\n\n### Patch Changes\n\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.co..."
          ],
          [
           "## 0.0.8\n\n### Patch Changes\n\n- Updated dependencies [[`e70805d54`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.0.3\n\n### Patch Changes\n\n- Updated dependencies [[`fe057300`](https://github.com/gradio-app/grad..."
          ],
          [
           "Gradio Demo: blocks_chained_events\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport time\n\n\ndef failure():\n    raise gr...."
          ],
          [
           "Gradio Demo: dataframe_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith g..."
          ],
          [
           "Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\n\nTags: SERVERLESS, BROWSER, PYODIDE\n..."
          ],
          [
           "```\n\nAnd that's it! You should now be able to open your HTML page in the browser and see the Gradio ..."
          ],
          [
           "```\n\n**Try it out**: You can see this example running in [this Hugging Face Static Space](https://hu..."
          ],
          [
           "æŽ§åˆ¶å¸ƒå±€ (Controlling Layout)\n\né»˜è®¤æƒ…å†µä¸‹ï¼Œå—ä¸­çš„ç»„ä»¶æ˜¯åž‚ç›´æŽ’åˆ—çš„ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é‡æ–°æŽ’åˆ—ç»„ä»¶ã€‚åœ¨å¹•åŽï¼Œè¿™ç§å¸ƒå±€ç»“æž„ä½¿ç”¨äº†[Web å¼€å‘çš„ flexbox æ¨¡åž‹](https:/..."
          ],
          [
           "```\n\nå¯ä»¥é€šè¿‡æ¯ä¸ªç»„ä»¶ä¸­å­˜åœ¨çš„ `scale` å’Œ `min_width` å‚æ•°æ¥æŽ§åˆ¶è¡Œä¸­å…ƒç´ çš„å®½åº¦ã€‚\n\n- `scale` æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå®šä¹‰äº†å…ƒç´ åœ¨è¡Œä¸­çš„å ç”¨ç©ºé—´ã€‚å¦‚æžœå°† scale è®¾ç½®ä¸º `..."
          ],
          [
           "```\n\n- `min_width` å°†è®¾ç½®å…ƒç´ çš„æœ€å°å®½åº¦ã€‚å¦‚æžœæ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´æ»¡è¶³æ‰€æœ‰çš„ `min_width` å€¼ï¼Œè¡Œå°†æ¢è¡Œã€‚\n\nåœ¨[æ–‡æ¡£](https://gradio.app/docs/#row..."
          ],
          [
           "ä¾‹å¦‚ï¼š\n\n$code_blocks_flipper\n$demo_blocks_flipper\n\nè¿˜è¯·æ³¨æ„æœ¬ç¤ºä¾‹ä¸­çš„ `gr.Accordion('label')`ã€‚æ‰‹é£Žç´æ˜¯ä¸€ç§å¯ä»¥åˆ‡æ¢æ‰“å¼€æˆ–å…³é—­çš„å¸ƒå±€..."
          ],
          [
           "ä¾‹å¦‚ï¼š\n\n$code_variable_outputs\n$demo_variable_outputs\n\n## åˆ†å¼€å®šä¹‰å’Œæ¸²æŸ“ç»„ä»¶ (Defining and Rendering Components ..."
          ],
          [
           "Gradio Demo: blocks_kitchen_sink\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\nimport time\nfrom os.path import abspath, join, pardir\n\nKS_FILES = abspath(jo..."
          ],
          [
           "var link_elem = document.createElement('link');\n                link_elem.classList.add('link-css');..."
          ],
          [
           "with gr.Row():\n        slider1 = gr.Slider(label=\"Slider 1\")\n        slider2 = gr.Slider(label=\"Slid..."
          ],
          [
           "def go(*args):\n                    time.sleep(3)\n                    return \"https://i.ibb.co/6BgKdS..."
          ],
          [
           "gr.Markdown(\"## Media Files\")\n\n    with gr.Tabs() as tabs:\n        with gr.Tab(\"Audio\"):\n           ..."
          ],
          [
           "with gr.Row():\n        with gr.Column(scale=2):\n            highlight = gr.HighlightedText(\n        ..."
          ],
          [
           "gr.Markdown(\"## Dataset Examples\")\n\n    component_example_set = [\n        (gr.Audio(render=False), j..."
          ],
          [
           "isplay an interactive map of AirBnB locations with Plotly. Data is hosted on HuggingFace Datasets...."
          ],
          [
           "ä½¿ç”¨ Gradio å’Œ Comet\n\nTags: COMET, SPACES\nç”± Comet å›¢é˜Ÿè´¡çŒ®\n\n## ä»‹ç»\n\nåœ¨è¿™ä¸ªæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºæ‚¨å¯ä»¥å¦‚ä½•ä½¿ç”¨ Gradio å’Œ Cometã€‚æˆ‘ä»¬å°†ä»‹ç»..."
          ],
          [
           "```\n\næŽ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦[æ³¨å†Œä¸€ä¸ª Comet è´¦æˆ·](https://www.comet.com/signup?utm_source=gradio&utm_medium=referral&utm_c..."
          ],
          [
           "```\n\n## 1. å°† Gradio UI è®°å½•åˆ°æ‚¨çš„ Comet å®žéªŒä¸­\n\n[![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/assets/col..."
          ],
          [
           "# ä¸º ImageNet ä¸‹è½½å¯è¯»çš„æ ‡ç­¾ã€‚\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"..."
          ],
          [
           "```\n\næ­¤ç‰‡æ®µä¸­çš„æœ€åŽä¸€è¡Œå°†å°† Gradio åº”ç”¨ç¨‹åºçš„ URL è®°å½•åˆ°æ‚¨çš„ Comet å®žéªŒä¸­ã€‚æ‚¨å¯ä»¥åœ¨å®žéªŒçš„æ–‡æœ¬é€‰é¡¹å¡ä¸­æ‰¾åˆ°è¯¥ URLã€‚\n\n<video width=\"560\" height=\"..."
          ],
          [
           "è½¬åˆ°æ‚¨çš„ Comet é¡¹ç›®é¡µé¢ï¼Œè½¬åˆ°é¢æ¿é€‰é¡¹å¡ã€‚å•å‡»â€œ+ æ·»åŠ â€æŒ‰é’®ä»¥æ‰“å¼€é¢æ¿æœç´¢é¡µé¢ã€‚\n\n<img width=\"560\" alt=\"adding-panels\" src=\"https://user..."
          ],
          [
           "## 3. ç›´æŽ¥å°† Hugging Face Spaces åµŒå…¥åˆ°æ‚¨çš„ Comet é¡¹ç›®ä¸­\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.you..."
          ],
          [
           "## 4. è®°å½•æ¨¡åž‹æŽ¨æ–­ç»“æžœåˆ° Comet\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KZnpH7msP..."
          ],
          [
           "if torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\nMODEL_NAME = \"gpt2\"\n\nmod..."
          ],
          [
           "if experiment is not None:\n        experiment.log_other(\"message\", message)\n        experiment.log_h..."
          ],
          [
           "```\n\nè¯¥ä»£ç æ®µä¸­çš„æŽ¨æ–­ç»“æžœå°†ä¿å­˜åœ¨å®žéªŒçš„ HTML é€‰é¡¹å¡ä¸­ã€‚\n\n<video width=\"560\" height=\"315\" controls>\n    <source src=\"https:..."
          ],
          [
           "@gradio/column\n\n## 0.1.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`..."
          ],
          [
           "## 0.1.0-beta.2\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "Gradio Demo: asr\n\n\n```\n!pip install -q gradio torch torchaudio transformers\n```\n\n\n```\nimport gradio ..."
          ],
          [
           "he simplest possible Gradio demo. It wraps a 'Hello {name}!' function in an Interface that accepts a..."
          ],
          [
           "his  demo converts text to speech in 14 languages...."
          ],
          [
           "Contributing a Guide\n\nWant to help teach Gradio? Consider contributing a Guide! ðŸ¤—\n\nBroadly speaking,..."
          ],
          [
           "## How to Contribute a Guide\n\n1. Clone or fork this `gradio` repo\n2. Add a new markdown document wit..."
          ],
          [
           "Gradio Demo: blocks_xray\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport time\n\ndi..."
          ],
          [
           "Gradio Demo: fake_gan\n### This is a fake GAN that shows how to create a text-to-image interface for ..."
          ],
          [
           "```\n\n\n```\n# This demo needs to be run from the repo folder.\n# python demo/fake_gan/run.py\nimport ran..."
          ],
          [
           "å¦‚ä½•ä½¿ç”¨ 3D æ¨¡åž‹ç»„ä»¶\n\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/dawood/Model3D, https://huggingface.co/spaces/radam..."
          ],
          [
           "```python\nimport gradio as gr\n\ndef load_mesh(mesh_file_name):\n    return mesh_file_name\n\ndemo = gr.I..."
          ],
          [
           "```\n\nè®©æˆ‘ä»¬æ¥è§£æžä¸Šé¢çš„ä»£ç ï¼š\n\n`load_mesh`ï¼šè¿™æ˜¯æˆ‘ä»¬çš„â€œé¢„æµ‹â€å‡½æ•°ï¼Œä¸ºç®€å•èµ·è§ï¼Œè¯¥å‡½æ•°å°†æŽ¥æ”¶ 3D æ¨¡åž‹ç½‘æ ¼å¹¶è¿”å›žå®ƒã€‚\n\nåˆ›å»ºç•Œé¢ï¼š\n\n- `fn`ï¼šå½“ç”¨æˆ·ç‚¹å‡»æäº¤æ—¶ä½¿ç”¨çš„é¢„æµ‹å‡½æ•°ã€‚..."
          ],
          [
           "ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ PIFu æ¨¡åž‹å°†ç©¿ç€è¡£ç‰©çš„äººçš„å›¾åƒè½¬æ¢ä¸º 3D æ•°å­—åŒ–æ¨¡åž‹çš„æ¼”ç¤ºã€‚æŸ¥çœ‹[spaces.py](https://huggingface.co/spaces/radames/PIFu-Cl..."
          ],
          [
           "@gradio/accordion\n\n## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.c..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.2.0-beta.1\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "- Updated dependencies []:\n  - @gradio/atoms@0.1.4\n  - @gradio/statustracker@0.2.2\n\n## 0.1.1\n\n### Pa..."
          ],
          [
           "## 0.0.2\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https..."
          ],
          [
           "Gradio Demo: highlightedtext_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n..."
          ],
          [
           "Gradio Demo: on_listener_decorator\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith..."
          ],
          [
           "Gradio Demo: zip_files\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo rep..."
          ],
          [
           "his demo uses a fake model to showcase iterative output. The Image output will update every time a g..."
          ],
          [
           "Gradio Demo: color_picker\n\n\n```\n!pip install -q gradio Pillow\n```\n\n\n```\n# Downloading files from the..."
          ],
          [
           "Gradio Demo: tax_calculator\n### Calculate taxes using Textbox, Radio, and Dataframe components\n     ..."
          ],
          [
           "Customizing your demo with CSS and Javascript\n\nGradio allows you to customize your demo in several w..."
          ],
          [
           "```\n\nNote: By default, files in the host machine are not accessible to users running the Gradio app...."
          ],
          [
           "```\n\nThe CSS `#warning` ruleset will only target the second Textbox, while the `.feedback` ruleset w..."
          ],
          [
           "```python\nhead = f\"\"\"\n<script async src=\"https://www.googletagmanager.com/gtag/js?id={google_analyti..."
          ],
          [
           "```\n\nNote: The `head` parameter accepts any HTML tags you would normally insert into the `<head>` of..."
          ],
          [
           "The `Interface` class\n\nAs mentioned in the [Quickstart](/main/guides/quickstart), the `gr.Interface`..."
          ],
          [
           "Just as each component in the `inputs` list corresponds to one of the parameters of the function, in..."
          ],
          [
           "```\n\nAlso note that our input `Image` component comes with an edit button ðŸ–‰, which allows for croppi..."
          ],
          [
           "![annotated](https://github.com/gradio-app/gradio/blob/main/guides/assets/annotated.png?raw=true)\n\nI..."
          ],
          [
           "```\n\n## Flagging\n\nBy default, an `Interface` will have \"Flag\" button. When a user testing your `Inte..."
          ],
          [
           "Gradio Demo: variable_outputs\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nmax_textb..."
          ],
          [
           "Security Policy\n\n## Reporting a Vulnerability\n\nIf you discover a security vulnerability, we would be..."
          ],
          [
           "Configuring Your Custom Component\n\nThe custom components workflow focuses on [convention over config..."
          ],
          [
           "```\n\n\nTip: Remember to change the import statement in `demo/app.py`!\n\n## Top Level Python Exports\n\nB..."
          ],
          [
           "```\n\n## Directory Structure\n\nBy default, the CLI will place the Python code in `backend` and the Jav..."
          ],
          [
           "Gradio Demo: blocks_gpt\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\napi = gr.load(\"..."
          ],
          [
           "imple image segmentation using gradio's AnnotatedImage component...."
          ],
          [
           "Gradio Demo: save_file_no_output\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport random\nimport string..."
          ],
          [
           "@gradio/colorpicker\n\n## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.2.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.2.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.1.3\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.1.3\n  - @gradio/statustr..."
          ],
          [
           "## 0.0.2\n\n### Fixes\n\n- [#5118](https://github.com/gradio-app/gradio/pull/5118) [`1b017e68`](https://..."
          ],
          [
           "Gradio Demo: code_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.Blo..."
          ],
          [
           "Gradio Demo: fake_diffusion\n### This demo uses a fake model to showcase iterative output. The Image ..."
          ],
          [
           "Gradio Demo: altair_plot\n\n\n```\n!pip install -q gradio altair vega_datasets\n```..."
          ],
          [
           "```\nimport altair as alt\nimport gradio as gr\nimport numpy as np\nimport pandas as pd\nfrom vega_datase..."
          ],
          [
           "pts = alt.selection(type=\"single\", encodings=['x'])\n\n        rect = alt.Chart(data.movies.url).mark_..."
          ],
          [
           "c2 = base.mark_text(radiusOffset=10).encode(text=\"values:Q\")\n\n        return c1 + c2\n    elif plot_t..."
          ],
          [
           "Gradio Demo: native_plots\n\n\n```\n!pip install -q gradio vega_datasets\n```\n\n\n```\n# Downloading files f..."
          ],
          [
           "Gradio Demo: same-person-or-different\n### This demo identifies if two speakers are the same person u..."
          ],
          [
           "```\nimport gradio as gr\nimport torch\nfrom torchaudio.sox_effects import apply_effects_file\nfrom tran..."
          ],
          [
           "OUTPUT_OK = (\n    \"\"\"\n    <div class=\"container\">\n        <div class=\"row\"><h1 style=\"text-align: ce..."
          ],
          [
           "EFFECTS = [\n    [\"remix\", \"-\"],\n    [\"channels\", \"1\"],\n    [\"rate\", \"16000\"],\n    [\"gain\", \"-1.0\"],\n..."
          ],
          [
           "with torch.no_grad():\n        emb1 = model(input1).embeddings\n        emb2 = model(input2).embedding..."
          ],
          [
           "interface = gr.Interface(\n    fn=similarity_fn,\n    inputs=inputs,\n    outputs=output,\n    layout=\"h..."
          ],
          [
           "How to Create a Custom Chatbot with Gradio Blocks\n\nTags: NLP, TEXT, CHAT\nRelated spaces: https://hug..."
          ],
          [
           "$code_chatbot_simple\n\nThere are three Gradio components here:\n\n- A `Chatbot`, whose value stores the..."
          ],
          [
           "2. The second method, `bot()` updates the chatbot history with the bot's response. Instead of creati..."
          ],
          [
           "```py\nimport gradio as gr\n\ndef greet(history, input):\n    return history + [(input, \"Hello, \" + inpu..."
          ],
          [
           "```\n\n## Adding Markdown, Images, Audio, or Videos\n\nThe `gr.Chatbot` component supports a subset of m..."
          ],
          [
           "Gradio Demo: video_component_events\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwit..."
          ],
          [
           "Gradio Demo: bar_plot\n\n\n```\n!pip install -q gradio pandas\n```..."
          ],
          [
           "```\nimport gradio as gr\nimport pandas as pd\nimport random\n\nsimple = pd.DataFrame(\n    {\n        \"a\":..."
          ],
          [
           "def bar_plot_fn(display):\n    if display == \"simple\":\n        return gr.BarPlot(\n            simple,..."
          ],
          [
           "with gr.Blocks() as bar_plot:\n    with gr.Row():\n        with gr.Column():\n            display = gr...."
          ],
          [
           "Gradio Demo: progress_simple\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport time..."
          ],
          [
           "@gradio/uploadbutton\n\n## 0.3.4\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://githu..."
          ],
          [
           "## 0.3.2\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.3.0\n\n### Features\n\n- [#6584](https://github.com/gradio-app/gradio/pull/6584) [`9bcb1da`](https:..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#6461](https://github.com/gradio-app/gradio/pull/6461) [`6b53330a5`](http..."
          ],
          [
           "## 0.1.3\n\n### Patch Changes\n\n- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gra..."
          ],
          [
           "### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github..."
          ],
          [
           "## 0.1.0-beta.7\n\n### Features..."
          ],
          [
           "## 0.1.0-beta.6\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.1.0-beta.5\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.0.11\n\n### Patch Changes\n\n- Updated dependencies [[`796145e2c`](https://github.com/gradio-app/gr..."
          ],
          [
           "## 0.0.8\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/upload@0.3.1\n  - @gradio/button@..."
          ],
          [
           "## 0.0.5\n\n### Patch Changes\n\n- Updated dependencies [[`26fef8c7`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.0.3\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://git..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.0.2\n\n### Patch Changes\n\n- Updated dependencies [[`6..."
          ],
          [
           "The Frontend ðŸŒâ­ï¸\n\nThis guide will cover everything you need to know to implement your custom compone..."
          ],
          [
           "```\n\n* `elem_id` and `elem_classes` allow Gradio app developers to target your component with custom..."
          ],
          [
           "```\n\n## The Example.svelte file\n\nThe `Example.svelte` file should expose the following props:\n\n```ty..."
          ],
          [
           "```\n\n## Handling Files\n\nIf your component deals with files, these files **should** be uploaded to th..."
          ],
          [
           "```\n\nThe component exposes a prop named `root`. \nThis is passed down by the parent gradio app and it..."
          ],
          [
           "```\n\n## Leveraging Existing Gradio Components\n\nMost of Gradio's frontend components are published on..."
          ],
          [
           "```\n\nYou can also combine existing Gradio components to create entirely unique experiences.\nLike ren..."
          ],
          [
           "@gradio/simpledropdown\n\n## 0.1.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://git..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.0-beta.3\n\n### Features\n\n- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd..."
          ],
          [
           "Test Coverage\n\nJust a little reference docs to understand what is tested/ needs testing. Perhaps tem..."
          ],
          [
           "| Component       | `value` | `visible` | `elem_id` | `elem_classes` | `container` | `label` | `show..."
          ],
          [
           "| ColorPicker     | `âŒ`    | `âœ…`      | `âœ…`      | `âœ…`           | `âŒ`        | `âœ…`    | `âŒ`        ..."
          ],
          [
           "| Number          | `âŒ`    | `âœ…`      | `âœ…`      | `âœ…`           | `âŒ`        | `âœ…`    | `âŒ`        ..."
          ],
          [
           "### Events..."
          ],
          [
           "| Component       | `value` | `visible` | `elem_id` | `elem_classes` | `container` | `label` | `show..."
          ],
          [
           "| ColorPicker     | `âŒ`    | `âŒ`      | `âŒ`      | `âŒ`           | `âŒ`        | `âŒ`    | `âŒ`        ..."
          ],
          [
           "| Markdown        | `âŒ`    | `âŒ`      | `âŒ`      | `âŒ`           | `âŒ`        | `âŒ`    | `âŒ`        ..."
          ],
          [
           "### `AnnotatedImage`\n\n### `Audio`\n\n### `BarPlot`\n\n### `Button`\n\n### `Chatbot`\n\n### `Checkbox`\n\n### `..."
          ],
          [
           "### `Request`\n\n### `mount_gradio_app`\n\n## Clients\n\n### Python (`gradio_client`)\n\n### JavaScript (`@g..."
          ],
          [
           "@gradio/code\n\n## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gr..."
          ],
          [
           "## 0.2.9\n\n### Patch Changes\n\n- Updated dependencies [[`206af31`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.3\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gra..."
          ],
          [
           "### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/upload@0.3.1\n\n## 0.2.0\n\n### Features\n\n- [#..."
          ],
          [
           "## 0.2.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`8f0fed857`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/grad..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "## 0.0.3\n\n### Patch Changes\n\n- Updated dependencies [[`667875b2`](https://github.com/gradio-app/grad..."
          ],
          [
           "Gradio Demo: text_analysis\n### This simple demo takes advantage of Gradio's HighlightedText, JSON an..."
          ],
          [
           "Gradio Demo: diff_texts\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nfrom difflib import Differ\n\nimport g..."
          ],
          [
           "Gradio Demo: code\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo repo\nimp..."
          ],
          [
           "Reactive Interfaces\n\nFinally, we cover how to get Gradio demos to refresh automatically or continuou..."
          ],
          [
           "`@gradio/theme`\n\ncss for gradio..."
          ],
          [
           "Gradio Demo: chatbot_simple\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport rando..."
          ],
          [
           "his sentiment analaysis demo takes in input text and returns its classification for either positive,..."
          ],
          [
           "Gradio Demo: count_generator\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport time..."
          ],
          [
           "Creating a Real-Time Dashboard from Google Sheets\n\nTags: TABULAR, DASHBOARD, PLOTS\n\n[Google Sheets](..."
          ],
          [
           "```\n\n2\\. Now, let's modify this URL and then use it to read the data from the Google Sheets into a P..."
          ],
          [
           "```\n\nAnd that's it! You have a dashboard that refreshes every 5 seconds, pulling the data from your ..."
          ],
          [
           "6\\. After selecting the service account, select the \"JSON\" key type and then click on the \"Create\" b..."
          ],
          [
           "```\n\n### Querying\n\nOnce you have the credentials `.json` file, you can use the following steps to qu..."
          ],
          [
           "```\n\n4\\. The data query is a function, which means that it's easy to display it real-time using the ..."
          ],
          [
           "@gradio/number\n\n## 0.3.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/..."
          ],
          [
           "## 0.3.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.1.3\n  - @gradio/statustr..."
          ],
          [
           "## 0.2.0\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://git..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gr..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5047](https://github.com/gradio-app/gradio/pull/5047) [`883ac364`](https..."
          ],
          [
           "@gradio/plot\n\n## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gr..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`3cdeabc68`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.0-beta.8\n\n### Patch Changes\n\n- Updated dependencies [[`667802a6c`](https://github.com/gradio-..."
          ],
          [
           "## 0.2.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.2.2\n\n### Fixes\n\n- [#5795](https://github.com/gradio-app/gradio/pull/5795) [`957ba5cfd`](https:/..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5642](https://github.com/gradio-app/gradio/pull/5642) [`21c7225bd`](http..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gr..."
          ],
          [
           "Gradio Demo: chatbot_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr...."
          ],
          [
           "Gradio Demo: interface_random_slider\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\nd..."
          ],
          [
           "his demo identifies musical instruments from an audio file. It uses Gradio's Audio and Label compone..."
          ],
          [
           "Gradio Demo: blocks_js_load\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef welcome..."
          ],
          [
           "Gradio Demo: image_mod_default_image\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files fro..."
          ],
          [
           "Gradio Demo: on_listener_basic\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr...."
          ],
          [
           "Gradio Demo: reversible_flow\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef increa..."
          ],
          [
           "@gradio/checkboxgroup\n\n## 0.3.7\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://gith..."
          ],
          [
           "## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`f816136a0`](https://github.com/gradio-app/gra..."
          ],
          [
           "Gradio Demo: digit_classifier\n\n\n```\n!pip install -q gradio tensorflow\n```\n\n\n```\nfrom urllib.request ..."
          ],
          [
           "@gradio/wasm\n\n## 0.4.0\n\n### Features\n\n- [#6398](https://github.com/gradio-app/gradio/pull/6398) [`67..."
          ],
          [
           "## 0.3.0\n\n### Features\n\n- [#6099](https://github.com/gradio-app/gradio/pull/6099) [`d84209703`](http..."
          ],
          [
           "## 0.2.0-beta.1\n\n### Features\n\n- [#5963](https://github.com/gradio-app/gradio/pull/5963) [`174b73619..."
          ],
          [
           "## 0.2.0-beta.0\n\n### Features\n\n- [#5956](https://github.com/gradio-app/gradio/pull/5956) [`f769876e0..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5868](https://github.com/gradio-app/gradio/pull/5868) [`4e0d87e9c`](http..."
          ],
          [
           "### Fixes\n\n- [#5919](https://github.com/gradio-app/gradio/pull/5919) [`1724918f0`](https://github.co..."
          ],
          [
           "## 0.0.2\n\n### Fixes\n\n- [#5538](https://github.com/gradio-app/gradio/pull/5538) [`b5c6f7b08`](https:/..."
          ],
          [
           "@gradio/checkbox\n\n## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.co..."
          ],
          [
           "## 0.2.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.2.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/atoms@0.1.3\n  - @gradio/statustr..."
          ],
          [
           "## 0.1.1\n\n### Fixes\n\n- [#5340](https://github.com/gradio-app/gradio/pull/5340) [`df090e89`](https://..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gr..."
          ],
          [
           "Gradio Demo: input_output\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef image_mo..."
          ],
          [
           "Key Features\n\nLet's go through some of the key features of Gradio. This guide is intended to be a hi..."
          ],
          [
           "**Preprocessing and Postprocessing**\n\nWhen a component is used as an input, Gradio automatically han..."
          ],
          [
           "```\n\nPostprocessing is even simpler! Gradio automatically recognizes the format of the returned data..."
          ],
          [
           "```\n\nThis limits the number of requests processed for this event listener at a single time to 5. By ..."
          ],
          [
           "```\n\nYou supply a generator into Gradio the same way as you would a regular function. For example, h..."
          ],
          [
           "```\n\nGradio comes with a set of prebuilt themes which you can load from `gr.themes.*`. You can exten..."
          ],
          [
           "```\n\nThe advantage of using batched functions is that if you enable queuing, the Gradio server can a..."
          ],
          [
           "Gradio Demo: file_explorer_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nw..."
          ],
          [
           "Gradio Demo: timeseries-forecasting-with-prophet\n### A simple dashboard showing pypi stats for pytho..."
          ],
          [
           "```\nimport gradio as gr\nimport pypistats\nfrom datetime import date\nfrom dateutil.relativedelta impor..."
          ],
          [
           "plt = gr.Plot()\n\n    lib.change(get_forecast, [lib, time], plt, queue=False)\n    time.change(get_for..."
          ],
          [
           "Custom Components in 5 minutes\n\nGradio 4.0 introduces Custom Components -- the ability for developer..."
          ],
          [
           "Each of these steps is done via the Custom Component CLI. You can invoke it with `gradio cc` or `gra..."
          ],
          [
           "```\n\nInstead of `MyComponent`, give your component any name.\n\nInstead of `SimpleTextbox`, you can us..."
          ],
          [
           "```\n\nThis will create a `tar.gz` and `.whl` file in a `dist/` subdirectory.\nIf you or anyone install..."
          ],
          [
           "Test Strategy\n\nVery brief, mildly aspirational test strategy document. This isn't where we are but i..."
          ],
          [
           "## Types of testing\n\nOur tests will broadly fall into one of three categories:\n\n- Static Quality che..."
          ],
          [
           "## Testing tools\n\nWe currently use the following tools:\n\n### Static quality checks\n\n- Python type-ch..."
          ],
          [
           "## Test execution\n\nTests need to be executed in a number of environments and at different stages of ..."
          ],
          [
           "Gradio Demo: audio_debugger\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the dem..."
          ],
          [
           "`@gradio/upload`\n\n```html\n<script>\n    import { Upload, ModifyUpload, normalise_file, get_fetchable_..."
          ],
          [
           "```\n\nModifyUpload\n```javascript\n    export let editable = false;\n\texport let undoable = false;\n\texpo..."
          ],
          [
           "Gradio Demo: hello_blocks_decorator\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\nwi..."
          ],
          [
           "Connecting to a Database\n\nRelated spaces: https://huggingface.co/spaces/gradio/chicago-bikeshare-das..."
          ],
          [
           "Once your database is created, download the dataset from Kaggle and upload it to your database.\nFor ..."
          ],
          [
           "connection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}?port={PORT}&dbname={DB_NAME}\"\n\n..."
          ],
          [
           "```\n\nIf you were to run our script locally, you could pass in your credentials as environment variab..."
          ],
          [
           "```\n\n## Step 3 - Deployment\n\nIf you run the code above, your app will start running locally.\nYou can..."
          ],
          [
           "Gradio Demo: image_classifier_2\n\n\n```\n!pip install -q gradio pillow torch torchvision\n```\n\n\n```\n# Do..."
          ],
          [
           "Gradio Demo: scatter_plot\n\n\n```\n!pip install -q gradio vega_datasets pandas\n```..."
          ],
          [
           "```\nimport gradio as gr\nfrom vega_datasets import data\n\ncars = data.cars()\niris = data.iris()\n\n# # O..."
          ],
          [
           "# cars = pd.DataFrame(cars_data)\n# iris = pd.DataFrame(iris_data)\n\n\ndef scatter_plot_fn(dataset):\n  ..."
          ],
          [
           "ä½¿ç”¨Gradio Pythonå®¢æˆ·ç«¯æž„å»ºFastAPIåº”ç”¨\n\nTags: CLIENT, API, WEB APP\n\nåœ¨æœ¬åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ `gradio_client` [Python..."
          ],
          [
           "```\n\nå¦åˆ™ï¼Œé€šè¿‡æŒ‰ç…§è¿™äº›è¯´æ˜Žå®‰è£…ffmpeg [é“¾æŽ¥](https://www.hostinger.com/tutorials/how-to-install-ffmpeg)ã€‚\n\n## æ­¥éª¤1ï¼šç¼–å†™..."
          ],
          [
           "```\n\næ‰€éœ€çš„ä»£ç ä»…å¦‚ä¸Šæ‰€ç¤º--è¯·æ³¨æ„ï¼ŒAPIç«¯ç‚¹è¿”å›žä¸€ä¸ªåŒ…å«ä¸¤ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆä¸€ä¸ªæ²¡æœ‰éŸ³ä¹ï¼Œä¸€ä¸ªåªæœ‰éŸ³ä¹ï¼‰çš„åˆ—è¡¨ï¼Œå› æ­¤æˆ‘ä»¬åªè¿”å›žåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚\n\n---\n\n**æ³¨æ„**ï¼šç”±äºŽè¿™æ˜¯ä¸€ä¸ªå…¬å…±Space..."
          ],
          [
           "```\n\nå…¶ä»–çš„ä»£ç ä¿æŒä¸å˜ï¼\n\n---\n\nçŽ°åœ¨ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬æ­£åœ¨å¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆéœ€è¦ä»Žè§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`ffmpeg`åº“ï¼Œå®ƒåœ¨å¤„ç†éŸ³é¢‘å’Œè§†é¢‘æ–‡ä»¶æ—¶åšäº†å¾ˆå¤šè‰°å·¨çš„å·¥ä½œã€‚ä½¿ç”¨..."
          ],
          [
           "```\n\nå¦‚æžœæ‚¨æƒ³äº†è§£æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·é˜…è¯»[ffmpegæ–‡æ¡£](https://ffmpeg.org/ffmpeg.html)ï¼Œå› ä¸ºå®ƒä»¬è¶…å‡ºäº†æœ¬æ•™ç¨‹çš„èŒƒå›´ã€‚\n\n## æ­¥éª¤2: åˆ›å»ºä¸€ä¸ªFa..."
          ],
          [
           "```\n\nåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼ŒFastAPIåº”ç”¨ç¨‹åºæœ‰ä¸¤ä¸ªè·¯ç”±ï¼š`/` å’Œ `/uploadvideo/`ã€‚\n\n`/` è·¯ç”±è¿”å›žä¸€ä¸ªæ˜¾ç¤ºæ‰€æœ‰ä¸Šä¼ è§†é¢‘çš„ç”»å»Šçš„HTMLæ¨¡æ¿ã€‚\n\n`/uploadvideo/` ..."
          ],
          [
           "```\n\nå°†ä»¥ä¸‹å†…å®¹å†™å…¥`home.html`æ–‡ä»¶ä¸­ï¼š..."
          ],
          [
           "```html\n&lt;!DOCTYPE html> &lt;html> &lt;head> &lt;title> è§†é¢‘åº“ &lt;/title> &lt;style>\nbody { font-fam..."
          ],
          [
           "&lt;div class=\"gallery\"> {% for video in videos %} &lt;div class=\"video\">\n&lt;video controls> &lt;so..."
          ],
          [
           "```\n\n## ç¬¬4æ­¥ï¼šè¿è¡Œ FastAPI åº”ç”¨\n\næœ€åŽï¼Œæˆ‘ä»¬å‡†å¤‡å¥½è¿è¡Œç”± Gradio Python å®¢æˆ·ç«¯æä¾›æ”¯æŒçš„ FastAPI åº”ç”¨ç¨‹åºã€‚\n\næ‰“å¼€ç»ˆç«¯å¹¶å¯¼èˆªåˆ°åŒ…å« `main.py` æ–‡ä»¶..."
          ],
          [
           "Gradio Demo: theme_extended_step_3\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimpor..."
          ],
          [
           "@gradio/tabitem\n\n## 0.1.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [..."
          ],
          [
           "## 0.1.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.0.5\n\n### Features\n\n- [#5590](https://github.com/gradio-app/gradio/pull/5590) [`d1ad1f671`](http..."
          ],
          [
           "##### Improved markdown support\n\nWe now have better support for markdown in `gr.Markdown` and `gr.Da..."
          ],
          [
           "ä»Ž BigQuery æ•°æ®åˆ›å»ºå®žæ—¶ä»ªè¡¨ç›˜\n\nTags: è¡¨æ ¼ , ä»ªè¡¨ç›˜ , ç»˜å›¾\n\n[Google BigQuery](https://cloud.google.com/bigquery) æ˜¯ä¸€ä¸ªåŸº..."
          ],
          [
           "## è®¾ç½® BigQuery å‡­æ®\n\nè¦ä½¿ç”¨ Gradio å’Œ BigQueryï¼Œæ‚¨éœ€è¦èŽ·å–æ‚¨çš„ BigQuery å‡­æ®ï¼Œå¹¶å°†å…¶ä¸Ž [BigQuery Python å®¢æˆ·ç«¯](https://pypi..."
          ],
          [
           "6. åœ¨é€‰æ‹©æœåŠ¡å¸å·åŽï¼Œé€‰æ‹©â€œJSONâ€å¯†é’¥ç±»åž‹ï¼Œç„¶åŽå•å‡»â€œåˆ›å»ºâ€æŒ‰é’®ã€‚è¿™å°†ä¸‹è½½åŒ…å«æ‚¨å‡­æ®çš„ JSON å¯†é’¥æ–‡ä»¶åˆ°æ‚¨çš„è®¡ç®—æœºã€‚å®ƒçš„å¤–è§‚ç±»ä¼¼äºŽä»¥ä¸‹å†…å®¹ï¼š\n\n```json\n{\n\t\"type\": \"ser..."
          ],
          [
           "```\n\n## ä½¿ç”¨ BigQuery å®¢æˆ·ç«¯\n\nèŽ·å¾—å‡­æ®åŽï¼Œæ‚¨éœ€è¦ä½¿ç”¨ BigQuery Python å®¢æˆ·ç«¯ä½¿ç”¨æ‚¨çš„å‡­æ®è¿›è¡Œèº«ä»½éªŒè¯ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£… BigQuery Pyt..."
          ],
          [
           "```\n\n## æž„å»ºå®žæ—¶ä»ªè¡¨ç›˜\n\nä¸€æ—¦æ‚¨æœ‰äº†æŸ¥è¯¢æ•°æ®çš„å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Gradio åº“çš„ `gr.DataFrame` ç»„ä»¶ä»¥è¡¨æ ¼å½¢å¼æ˜¾ç¤ºç»“æžœã€‚è¿™æ˜¯ä¸€ç§æ£€æŸ¥æ•°æ®å¹¶ç¡®ä¿æŸ¥è¯¢æ­£ç¡®çš„æœ‰ç”¨æ–¹å¼ã€‚\n\nä»¥ä¸‹æ˜¯å¦‚..."
          ],
          [
           "```\n\nä¹Ÿè®¸æ‚¨æƒ³åœ¨æˆ‘ä»¬çš„ä»ªè¡¨ç›˜ä¸­æ·»åŠ ä¸€ä¸ªå¯è§†åŒ–æ•ˆæžœã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `gr.ScatterPlot()` ç»„ä»¶å°†æ•°æ®å¯è§†åŒ–ä¸ºæ•£ç‚¹å›¾ã€‚è¿™å¯ä»¥è®©æ‚¨æŸ¥çœ‹æ•°æ®ä¸­ä¸åŒå˜é‡ï¼ˆä¾‹å¦‚ç—…ä¾‹æ•°å’Œæ­»äº¡æ•°ï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¯ç”¨äºŽ..."
          ],
          [
           "Gradio Demo: progress\n\n\n```\n!pip install -q gradio tqdm datasets\n```..."
          ],
          [
           "```\nimport gradio as gr\nimport random\nimport time\nimport tqdm\nfrom datasets import load_dataset\nimpo..."
          ],
          [
           "# track iterable of unknown length\n    def load_random(data, progress=gr.Progress()):\n        def yi..."
          ],
          [
           "def bind_internal_tqdm(data, progress=gr.Progress(track_tqdm=True)):\n        outdir = \"__tmp/\" + str..."
          ],
          [
           "Gradio Demo: theme_new_step_1\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nfrom gradi..."
          ],
          [
           "Interface State\n\nSo far, we've assumed that your demos are *stateless*: that they do not persist inf..."
          ],
          [
           "Notice how the state persists across submits within each page, but if you load this demo in another ..."
          ],
          [
           "Gradio Demo: tabbed_interface_lite\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nhell..."
          ],
          [
           "Gradio Demo: theme_extended_step_1\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimpor..."
          ],
          [
           "ä½¿ç”¨ GAN åˆ›å»ºæ‚¨è‡ªå·±çš„æœ‹å‹\n\nspaces/NimaBoscarino/cryptopunks, https://huggingface.co/spaces/nateraw/cryptopunks..."
          ],
          [
           "ä»Šå¤©æˆ‘ä»¬å°†ç®€è¦ä»‹ç» GAN çš„é«˜çº§ç›´è§‰ï¼Œç„¶åŽæˆ‘ä»¬å°†å›´ç»•ä¸€ä¸ªé¢„è®­ç»ƒçš„ GAN æž„å»ºä¸€ä¸ªå°åž‹æ¼”ç¤ºï¼Œçœ‹çœ‹è¿™ä¸€åˆ‡éƒ½æ˜¯æ€Žä¹ˆå›žäº‹ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬å°†è¦ç»„åˆçš„ä¸œè¥¿çš„ä¸€çž¥ï¼š\n\n<iframe src=\"https://ni..."
          ],
          [
           "ç”Ÿæˆå™¨ä¸æ–­è®­ç»ƒä»¥åˆ›å»ºå¯¹é‰´åˆ«å™¨æ›´éš¾ä»¥è¯†åˆ«çš„å›¾åƒï¼Œè€Œé‰´åˆ«å™¨æ¯æ¬¡æ­£ç¡®æ£€æµ‹åˆ°ä¼ªé€ å›¾åƒæ—¶ï¼Œéƒ½ä¼šä¸ºç”Ÿæˆå™¨è®¾ç½®æ›´é«˜çš„é—¨æ§›ã€‚éšç€ç½‘ç»œä¹‹é—´çš„è¿™ç§ç«žäº‰ï¼ˆ**adversarial å¯¹æŠ—æ€§ï¼**ï¼‰ï¼Œç”Ÿæˆçš„å›¾åƒæ”¹å–„åˆ°äº†å¯¹äººçœ¼..."
          ],
          [
           "```python\nfrom torch import nn\n\nclass Generator(nn.Module):\n    # æœ‰å…³ncï¼Œnzå’Œngfçš„è§£é‡Šï¼Œè¯·å‚è§ä¸‹é¢çš„é“¾æŽ¥\n    # http..."
          ],
          [
           "```\n\næˆ‘ä»¬æ­£åœ¨ä½¿ç”¨æ¥è‡ª[æ­¤ repo çš„ @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py..."
          ],
          [
           "```\n\n## æ­¥éª¤ 2 - å®šä¹‰â€œpredictâ€å‡½æ•°\n\n`predict` å‡½æ•°æ˜¯ä½¿ Gradio å·¥ä½œçš„å…³é”®ï¼æˆ‘ä»¬é€šè¿‡ Gradio ç•Œé¢é€‰æ‹©çš„ä»»ä½•è¾“å…¥éƒ½å°†é€šè¿‡æˆ‘ä»¬çš„ `predict` å‡½æ•°ä¼ ..."
          ],
          [
           "```\n\næˆ‘ä»¬ç»™ `predict` å‡½æ•°ä¸€ä¸ª `seed` å‚æ•°ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ä¸€ä¸ªç§å­å›ºå®šéšæœºå¼ é‡ç”Ÿæˆã€‚ç„¶åŽï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼ å…¥ç›¸åŒçš„ç§å­å†æ¬¡æŸ¥çœ‹ç”Ÿæˆçš„ punksã€‚\n\n_æ³¨æ„ï¼_ æˆ‘ä»¬çš„æ¨¡åž‹éœ€..."
          ],
          [
           "```\n\nå¯åŠ¨ç•Œé¢åŽï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ°åƒè¿™æ ·çš„ä¸œè¥¿ :\n\n<iframe src=\"https://nimaboscarino-cryptopunks-1.hf.space\" frameBorder=\"0..."
          ],
          [
           "```\n\næ–°çš„è¾“å…¥å°†ä¼ é€’ç»™æˆ‘ä»¬çš„ `predict()` å‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å¯¹è¯¥å‡½æ•°è¿›è¡Œä¸€äº›æ›´æ”¹ï¼Œä»¥æŽ¥å—ä¸€ä¸ªæ–°çš„å‚æ•° :\n\n```python\ndef predict(seed, num_punks)..."
          ],
          [
           "```\n\n`examples` å‚æ•°æŽ¥å—ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œå…¶ä¸­å­åˆ—è¡¨ä¸­çš„æ¯ä¸ªé¡¹ç›®çš„é¡ºåºä¸Žæˆ‘ä»¬åˆ—å‡ºçš„ `inputs` çš„é¡ºåºç›¸åŒã€‚æ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œ`[seed, num_punks]`ã€‚è¯•ä¸€è¯•å§ï¼\n..."
          ],
          [
           "```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvi..."
          ],
          [
           "```\n\n---\n\næ­å–œï¼ä½ å·²ç»æˆåŠŸæž„å»ºäº†è‡ªå·±çš„åŸºäºŽ GAN çš„ CryptoPunks ç”Ÿæˆå™¨ï¼Œé…å¤‡äº†ä¸€ä¸ªæ—¶å°šçš„ Gradio ç•Œé¢ï¼Œä½¿ä»»ä½•äººéƒ½èƒ½è½»æ¾ä½¿ç”¨ã€‚çŽ°åœ¨ä½ å¯ä»¥åœ¨ Hub ä¸Š[å¯»æ‰¾æ›´å¤šçš„ GA..."
          ],
          [
           "Gradio Demo: blocks_flipper\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport numpy as np\nimport gradio..."
          ],
          [
           "@gradio/annotatedimage\n\n## 0.3.13\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://gi..."
          ],
          [
           "## 0.3.11\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.3.10\n\n### Patch Changes\n\n- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.3.9\n\n### Patch Changes\n\n- Updated dependencies [[`206af31`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.3.6\n\n### Patch Changes\n\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.4\n\n### Patch Changes\n\n- Updated dependencies [[`854b482f5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.3.2\n\n### Patch Changes\n\n- Updated dependencies [[`aaa55ce85`](https://github.com/gradio-app/gra..."
          ],
          [
           "### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github..."
          ],
          [
           "## 0.3.0-beta.2\n\n### Features\n\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40..."
          ],
          [
           "## 0.3.0-beta.0\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "## 0.0.2\n\n### Patch Changes\n\n- Updated dependencies [[`667875b2`](https://github.com/gradio-app/grad..."
          ],
          [
           "Gradio Demo: stt_or_tts\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ntts_examples = ..."
          ],
          [
           "Gradio and W&B Integration\n\nç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/akhaliq/JoJoGAN\næ ‡ç­¾ï¼šWANDB, SPACES\nç”± Gr..."
          ],
          [
           "<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195..."
          ],
          [
           "è®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\n1. åˆ›å»º W&B è´¦å·\n\n   å¦‚æžœæ‚¨è¿˜æ²¡æœ‰ W&B è´¦å·ï¼Œè¯·æŒ‰ç…§[è¿™äº›å¿«é€Ÿè¯´æ˜Ž](https://app.wandb.ai/login)åˆ›å»ºå…è´¹è´¦å·ã€‚è¿™ä¸åº”è¯¥è¶…è¿‡å‡ åˆ†é’Ÿçš„æ—¶é—´ã€‚ä¸€..."
          ],
          [
           "```\n\n3. å¾®è°ƒ StyleGAN å’Œ W&B å®žéªŒè·Ÿè¸ª\n\n   ä¸‹ä¸€æ­¥å°†æ‰“å¼€ä¸€ä¸ª W&B ä»ªè¡¨æ¿ï¼Œä»¥è·Ÿè¸ªå®žéªŒï¼Œå¹¶æ˜¾ç¤ºä¸€ä¸ª Gradio æ¼”ç¤ºæä¾›çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œæ‚¨å¯ä»¥ä»Žä¸‹æ‹‰èœå•ä¸­é€‰æ‹©ã€‚è¿™æ˜¯æ‚¨éœ€è¦çš„..."
          ],
          [
           "for idx in tqdm(range(num_iter)):\n       mean_w = generator.get_latent(torch.randn([latents.size(0),..."
          ],
          [
           "```\n\n4. ä¿å­˜ã€ä¸‹è½½å’ŒåŠ è½½æ¨¡åž‹\n\n   ä»¥ä¸‹æ˜¯å¦‚ä½•ä¿å­˜å’Œä¸‹è½½æ‚¨çš„æ¨¡åž‹ã€‚\n\n   ```python\n\n   from PIL import Image\n   import torch\n   to..."
          ],
          [
           "generator = deepcopy(original_generator)\n\n   ckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\"..."
          ],
          [
           "```\n\n5. æž„å»º Gradio æ¼”ç¤º\n\n   ```python\n\n   import gradio as gr\n\n   title = \"JoJoGAN\"\n   description = \"J..."
          ],
          [
           "```\n\n7.ï¼ˆå¯é€‰ï¼‰åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­åµŒå…¥ W&B å›¾\n\n    ä¹Ÿå¯ä»¥åœ¨ Gradio åº”ç”¨ç¨‹åºä¸­åµŒå…¥ W&B å›¾ã€‚ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ª W&B æŠ¥å‘Šï¼Œå¹¶åœ¨ä¸€ä¸ª `gr.HTML` å—ä¸­..."
          ],
          [
           "```\n\n## ç»“è®º\n\nå¸Œæœ›æ‚¨å–œæ¬¢æ­¤åµŒå…¥ Gradio æ¼”ç¤ºåˆ° W&B æŠ¥å‘Šçš„ç®€çŸ­æ¼”ç¤ºï¼æ„Ÿè°¢æ‚¨ä¸€ç›´é˜…è¯»åˆ°æœ€åŽã€‚å›žé¡¾ä¸€ä¸‹ :\n\n- ä»…éœ€è¦ä¸€ä¸ªå•ä¸€å‚è€ƒå›¾åƒå³å¯å¯¹ JoJoGAN è¿›è¡Œå¾®è°ƒï¼Œé€šå¸¸åœ¨ GPU..."
          ],
          [
           "Gradio Demo: dataset\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo repo\n..."
          ],
          [
           "```\nimport gradio as gr\nimport os\nimport numpy as np\n\n\ntxt = \"the quick brown fox\"\nnum = 10\n\nimg = o..."
          ],
          [
           "c_2 = gr.CheckboxGroup(visible=False, choices=['a', 'b', 'c'])\n    gr.Dataset(\n        label=\"Checkb..."
          ],
          [
           "[np.random.randint(0, 10, (10, 10)).tolist()],\n        ],\n    )\n    d_2 = gr.Dropdown(visible=False,..."
          ],
          [
           ")\n    i = gr.Image(visible=False)\n    gr.Dataset(\n        components=[i],\n        label=\"Image\",\n   ..."
          ],
          [
           ")\n    s = gr.Slider(visible=False)\n    gr.Dataset(\n        label=\"Slider\",\n        components=[s],\n ..."
          ],
          [
           "`@gradio/dropdown`\n\n```html\n<script>\n    import {BaseDropdown, BaseMultiselect, BaseExample } from \"..."
          ],
          [
           "Gradio Demo: fake_diffusion_with_gif\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files fro..."
          ],
          [
           "```\n\n\n```\nimport gradio as gr\nimport numpy as np\nimport time\nimport os\nfrom PIL import Image\nimport ..."
          ],
          [
           "Gradio Demo: theme_soft\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport time\n\nwit..."
          ],
          [
           "Gradio Demo: radio_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr.B..."
          ],
          [
           "Gradio Demo: image_selections\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport gradio as gr\nimport numpy as np\n\nwith gr.Blocks() as demo:\n    tolerance = gr.Slider(labe..."
          ],
          [
           "out = img.copy() * 0.2\n        out = out.astype(np.uint8)\n        for pixel in pixels_in_segment:\n  ..."
          ],
          [
           "ä½¿ç”¨ Gradio Python å®¢æˆ·ç«¯å…¥é—¨\n\nTags: CLIENT, API, SPACES\n\nGradio Python å®¢æˆ·ç«¯ä½¿å¾—å°†ä»»ä½• Gradio åº”ç”¨ç¨‹åºä½œä¸º API ä½¿ç”¨å˜å¾—éžå¸¸å®¹æ˜“..."
          ],
          [
           "```\n\nGradio å®¢æˆ·ç«¯é€‚ç”¨äºŽä»»ä½•æ‰˜ç®¡åœ¨ Hugging Face Spaces ä¸Šçš„ Gradio åº”ç”¨ç¨‹åºï¼Œæ— è®ºæ˜¯å›¾åƒç”Ÿæˆå™¨ã€æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ã€æœ‰çŠ¶æ€èŠå¤©æœºå™¨äººã€ç¨Žé‡‘è®¡ç®—å™¨è¿˜æ˜¯å…¶ä»–ä»»ä½•åº”ç”¨ç¨‹åº..."
          ],
          [
           "```\n\nä½ è¿˜å¯ä»¥é€šè¿‡åœ¨ `hf_token` å‚æ•°ä¸­ä¼ é€’ä½ çš„ HF ä»¤ç‰Œæ¥è¿žæŽ¥åˆ°ç§æœ‰ç©ºé—´ã€‚ä½ å¯ä»¥åœ¨è¿™é‡ŒèŽ·å–ä½ çš„ HF ä»¤ç‰Œï¼šhttps://huggingface.co/settings/token..."
          ],
          [
           "```\n\n> > \" è¿™æ˜¯ Whisper è¯­éŸ³è¯†åˆ«æ¨¡åž‹çš„æµ‹è¯•ã€‚\"\n\nå¦‚æžœä¹‹å‰å·²å¤åˆ¶äº†ä¸€ä¸ªç©ºé—´ï¼Œé‡æ–°è¿è¡Œ `duplicate()` å°†*ä¸ä¼š*åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºé—´ã€‚ç›¸åï¼Œå®¢æˆ·ç«¯å°†è¿žæŽ¥åˆ°ä¹‹å‰åˆ›å»ºçš„ç©ºé—´ã€‚å› ..."
          ],
          [
           "```\n\nè¿™æ˜¾ç¤ºäº†åœ¨æ­¤ç©ºé—´ä¸­æœ‰ 1 ä¸ª API ç«¯ç‚¹ï¼Œå¹¶æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨ API ç«¯ç‚¹è¿›è¡Œé¢„æµ‹ï¼šæˆ‘ä»¬åº”è¯¥è°ƒç”¨ `.predict()` æ–¹æ³•ï¼ˆæˆ‘ä»¬å°†åœ¨ä¸‹é¢æŽ¢è®¨ï¼‰ï¼Œæä¾›ç±»åž‹ä¸º `str` çš„å‚æ•° `inp..."
          ],
          [
           "```\n\nå¦‚æžœæœ‰å¤šä¸ªå‚æ•°ï¼Œé‚£ä¹ˆä½ åº”è¯¥å°†å®ƒä»¬ä½œä¸ºå•ç‹¬çš„å‚æ•°ä¼ é€’ç»™ `.predict()`ï¼Œå°±åƒè¿™æ ·ï¼š\n\n````python\nfrom gradio_client import Client\n\ncli..."
          ],
          [
           "```\n\n## æ·»åŠ å›žè°ƒ ï¼ˆAdding callbacksï¼‰\n\næˆ–è€…ï¼Œå¯ä»¥æ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªå›žè°ƒæ¥åœ¨ä½œä¸šå®ŒæˆåŽæ‰§è¡Œæ“ä½œï¼Œåƒè¿™æ ·ï¼š\n\n```python\nfrom gradio_client import..."
          ],
          [
           "```\n\n_æ³¨æ„_ï¼š`Job`ç±»è¿˜æœ‰ä¸€ä¸ª`.done()`å®žä¾‹æ–¹æ³•ï¼Œè¿”å›žä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºä½œä¸šæ˜¯å¦å·²å®Œæˆã€‚\n\n## å–æ¶ˆä½œä¸š ï¼ˆCancelling Jobsï¼‰\n\n`Job`ç±»è¿˜æœ‰ä¸€ä¸ª`.cancel(..."
          ],
          [
           "```\n\nè¯·æ³¨æ„ï¼Œåœ¨ç”Ÿæˆå™¨ç«¯ç‚¹ä¸Šè¿è¡Œ`job.result()`åªä¼šèŽ·å¾—ç«¯ç‚¹è¿”å›žçš„*ç¬¬ä¸€ä¸ª*å€¼ã€‚\n\n`Job`å¯¹è±¡è¿˜æ˜¯å¯è¿­ä»£çš„ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨å®ƒæŒ‰ç…§ä»Žç«¯ç‚¹è¿”å›žçš„ç»“æžœé€ä¸ªæ˜¾ç¤ºç”Ÿæˆå™¨å‡½æ•°çš„ç»“æžœã€‚ä»¥ä¸‹æ˜¯..."
          ],
          [
           "Gradio Demo: markdown_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr..."
          ],
          [
           "Gradio Demo: gpt2_xl_unified\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ncomponent ..."
          ],
          [
           "How to Create a Chatbot with Gradio\n\nTags: NLP, TEXT, CHAT\n\n## Introduction\n\nChatbots are a popular ..."
          ],
          [
           "```\n\nNow, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the w..."
          ],
          [
           "```\n\nNotice that we've [enabled queuing](/guides/key-features#queuing), which is required to use gen..."
          ],
          [
           "```\n\n## Additional Inputs\n\nYou may want to add additional parameters to your chatbot and expose them..."
          ],
          [
           "```\n\nIf you need to create something even more custom, then its best to construct the chatbot UI usi..."
          ],
          [
           "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your key\n\nllm = ChatOpenAI(temperature=1.0, ..."
          ],
          [
           "```\n\n## A streaming example using `openai`\n\nOf course, we could also use the `openai` library direct..."
          ],
          [
           "```\n\n## Example using a local, open-source LLM with Hugging Face\n\nOf course, in many cases you want ..."
          ],
          [
           "model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n    streamer = TextIteratorStre..."
          ],
          [
           "```\n\nWith those examples, you should be all set to create your own Gradio Chatbot demos soon! For bu..."
          ],
          [
           "Create a Dashboard from Supabase Data\n\nTags: TABULAR, DASHBOARD, PLOTS\n\n[Supabase](https://supabase...."
          ],
          [
           "3\\. You'll be presented with your API keys while the database spins up (can take up to 2 minutes).\n\n..."
          ],
          [
           "```\n\n7\\. Get your project URL and API key. Click the Settings (gear icon) on the left pane and click..."
          ],
          [
           "```\n\nReturn to your Supabase dashboard and refresh the page, you should now see 10 rows populated in..."
          ],
          [
           "```\n\nNotice that by passing in a function to `gr.BarPlot()`, we have the BarPlot query the database ..."
          ],
          [
           "Gradio Demo: hello_world_4\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndef greet(na..."
          ],
          [
           "Gradio Demo: fake_gan_no_input\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport time\n\nimport gradio as..."
          ],
          [
           "gradio_test\n\n## 0.3.3\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gra..."
          ],
          [
           "## 0.2.3\n\n### Patch Changes\n\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.2.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "@gradio/preview\n\n## 0.6.0\n\n### Features\n\n- [#6738](https://github.com/gradio-app/gradio/pull/6738) [..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#6532](https://github.com/gradio-app/gradio/pull/6532) [`96290d304`](http..."
          ],
          [
           "```\n\n Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.2.2\n\n### Features\n\n- [#6467](https://github.c..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#6261](https://github.com/gradio-app/gradio/pull/6261) [`8bbeca0e7`](http..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.1.0-beta.8\n\n### Features\n\n- [#6094](https://github.com/gradio-app/gradio/pull/6094) [`c476bd5a5..."
          ],
          [
           "## 0.1.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.co..."
          ],
          [
           "- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5938](https://github.com/gradio-app/gradio/pull/5938) [`13ed8a485`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5962](https://github.com/gradio-app/gradio/pull/5962) [`d298e7695`](https://github.com/gradio-ap..."
          ],
          [
           "### Fixes\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.co..."
          ],
          [
           "## 0.1.0-beta.3\n\n### Features\n\n- [#5648](https://github.com/gradio-app/gradio/pull/5648) [`c573e2339..."
          ],
          [
           "## 0.1.0-beta.0\n\n### Features\n\n- [#5507](https://github.com/gradio-app/gradio/pull/5507) [`1385dc688..."
          ],
          [
           "Gradio Demo: blocks_js_methods\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nblocks = gr.Blocks()\n\nwith blocks as demo..."
          ],
          [
           "Gradio Demo: theme_extended_step_4\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimpor..."
          ],
          [
           "@gradio/textbox\n\n## 0.4.7\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com..."
          ],
          [
           "## 0.4.5\n\n### Fixes\n\n- [#6635](https://github.com/gradio-app/gradio/pull/6635) [`b639e04`](https://g..."
          ],
          [
           "## 0.4.2\n\n### Fixes\n\n- [#6323](https://github.com/gradio-app/gradio/pull/6323) [`55fda81fa`](https:/..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.4.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.co..."
          ],
          [
           "## 0.4.3\n\n### Patch Changes\n\n- Updated dependencies [[`e70805d54`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5652](https://github.com/gradio-app/gradio/pull/5652) [`2e25d4305`](http..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#5417](https://github.com/gradio-app/gradio/pull/5417) [`d14d63e3`](https..."
          ],
          [
           "## 0.1.1\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://git..."
          ],
          [
           "### Fixes\n\n- [#5114](https://github.com/gradio-app/gradio/pull/5114) [`56d2609d`](https://github.com..."
          ],
          [
           "Gradio Demo: dataset_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr...."
          ],
          [
           "Gradio Demo: blocks_layout\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndemo = gr.B..."
          ],
          [
           "@gradio/app\n\n## 1.17.0\n\n### Features\n\n- [#6831](https://github.com/gradio-app/gradio/pull/6831) [`f3..."
          ],
          [
           "## 1.16.2\n\n### Patch Changes\n\n- Updated dependencies [[`245d58e`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 1.16.1\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 1.16.0\n\n### Features\n\n- [#6398](https://github.com/gradio-app/gradio/pull/6398) [`67ddd40`](https..."
          ],
          [
           "## 1.15.0\n\n### Features\n\n- [#6512](https://github.com/gradio-app/gradio/pull/6512) [`4f040c7`](https..."
          ],
          [
           "## 1.13.1\n\n### Fixes\n\n- [#6536](https://github.com/gradio-app/gradio/pull/6536) [`1bbd6cab3`](https:..."
          ],
          [
           "<video src=\"https://user-images.githubusercontent.com/12937446/284027169-31188926-fd16-4a1c-8718-998..."
          ],
          [
           "```\n\nThanks [@pngwn](https://github.com/pngwn)!\n\n## 1.12.0\n\n### Features\n\n- [#6427](https://github.c..."
          ],
          [
           "## 1.11.0\n\n### Features\n\n- [#6099](https://github.com/gradio-app/gradio/pull/6099) [`d84209703`](htt..."
          ],
          [
           "- Updated dependencies [[`6204ccac5`](https://github.com/gradio-app/gradio/commit/6204ccac5967763e0e..."
          ],
          [
           "- @gradio/textbox@0.4.2\n  - @gradio/audio@0.5.0\n  - @gradio/client@0.8.0\n  - @gradio/gallery@0.4.5\n ..."
          ],
          [
           "## 1.10.2\n\n### Patch Changes..."
          ],
          [
           "- Updated dependencies [[`4b1011bab`](https://github.com/gradio-app/gradio/commit/4b1011bab03c0b6a09..."
          ],
          [
           "- @gradio/dataframe@0.3.4\n  - @gradio/atoms@0.2.1\n  - @gradio/upload@0.3.3\n  - @gradio/video@0.1.3\n ..."
          ],
          [
           "- @gradio/label@0.2.1\n  - @gradio/markdown@0.3.1\n  - @gradio/number@0.3.1\n  - @gradio/plot@0.2.1\n  -..."
          ],
          [
           "## 1.10.1\n\n### Patch Changes\n\n- Updated dependencies [[`92278729e`](https://github.com/gradio-app/gr..."
          ],
          [
           "- [#6266](https://github.com/gradio-app/gradio/pull/6266) [`e32bac894`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6236](https://github.com/gradio-app/gradio/pull/6236) [`6bce259c5`](https://github.com/gradio-ap..."
          ],
          [
           "## 1.9.2\n\n### Fixes\n\n- [#6191](https://github.com/gradio-app/gradio/pull/6191) [`b555bc09f`](https:/..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "### Fixes\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.co..."
          ],
          [
           "- [#6124](https://github.com/gradio-app/gradio/pull/6124) [`a7435ba9e`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6118](https://github.com/gradio-app/gradio/pull/6118) [`88bccfdba`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6069](https://github.com/gradio-app/gradio/pull/6069) [`bf127e124`](https://github.com/gradio-ap..."
          ],
          [
           "## 1.9.0-beta.2\n\n### Features..."
          ],
          [
           "- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6107](https://github.com/gradio-app/gradio/pull/6107) [`9a40de7bf`](https://github.com/gradio-ap..."
          ],
          [
           "- [#5990](https://github.com/gradio-app/gradio/pull/5990) [`85056de5c`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6065](https://github.com/gradio-app/gradio/pull/6065) [`7d07001e8`](https://github.com/gradio-ap..."
          ],
          [
           "## 1.9.0-beta.1\n\n### Patch Changes\n\n- Updated dependencies [[`174b73619`](https://github.com/gradio-..."
          ],
          [
           "## 1.9.0-beta.0\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "### Fixes\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.co..."
          ],
          [
           "## 1.7.1\n\n### Patch Changes\n\n- Updated dependencies [[`796145e2c`](https://github.com/gradio-app/gra..."
          ],
          [
           "For more information check the [`FileExplorer` documentation](https://gradio.app/docs/fileexplorer)...."
          ],
          [
           "- Updated dependencies [[`abb5e9df4`](https://github.com/gradio-app/gradio/commit/abb5e9df47989b2c56..."
          ],
          [
           "- @gradio/audio@0.3.6\n  - @gradio/code@0.2.1\n  - @gradio/dropdown@0.3.1\n  - @gradio/file@0.1.5\n  - @..."
          ],
          [
           "## 1.6.2\n\n### Features\n\n- [#5721](https://github.com/gradio-app/gradio/pull/5721) [`84e03fe50`](http..."
          ],
          [
           "## 1.6.1\n\n### Patch Changes\n\n- Updated dependencies [[`ee8eec1e5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 1.5.4\n\n### Features\n\n- [#5514](https://github.com/gradio-app/gradio/pull/5514) [`52f783175`](http..."
          ],
          [
           "## 1.5.2\n\n### Patch Changes\n\n- Updated dependencies [[`a0cc9ac9`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 1.5.0\n\n### Features\n\n- [#5505](https://github.com/gradio-app/gradio/pull/5505) [`9ee20f49`](https..."
          ],
          [
           "## 1.4.3\n\n### Patch Changes\n\n- Updated dependencies [[`6e381c4f`](https://github.com/gradio-app/grad..."
          ],
          [
           "- Updated dependencies [[`afac0006`](https://github.com/gradio-app/gradio/commit/afac0006337ce2840cf..."
          ],
          [
           "- @gradio/dropdown@0.1.3\n  - @gradio/file@0.1.2\n  - @gradio/gallery@0.3.2\n  - @gradio/highlightedtex..."
          ],
          [
           "## 1.4.0\n\n### Features\n\n- [#5267](https://github.com/gradio-app/gradio/pull/5267) [`119c8343`](https..."
          ],
          [
           "## 1.3.2\n\n### Patch Changes\n\n- Updated dependencies [[`5f25eb68`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 1.3.0\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://git..."
          ],
          [
           "We now have an event `render` on the <gradio-app> web component, which is triggered once the embedde..."
          ],
          [
           "```\n\nThanks [@hannahblair](https://github.com/hannahblair)!\n\n### Features..."
          ],
          [
           "- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app..."
          ],
          [
           "- [#5264](https://github.com/gradio-app/gradio/pull/5264) [`46a2b600`](https://github.com/gradio-app..."
          ],
          [
           "### Fixes\n\n- [#5285](https://github.com/gradio-app/gradio/pull/5285) [`cdfd4217`](https://github.com..."
          ],
          [
           "## 1.2.0\n\n### Highlights\n\n#### Client.predict will now return the final output for streaming endpoin..."
          ],
          [
           "- [#5025](https://github.com/gradio-app/gradio/pull/5025) [`6693660a`](https://github.com/gradio-app..."
          ],
          [
           "- [#5005](https://github.com/gradio-app/gradio/pull/5005) [`f5539c76`](https://github.com/gradio-app..."
          ],
          [
           "## 1.1.0\n\n### Features\n\n- [#4995](https://github.com/gradio-app/gradio/pull/4995) [`3f8c210b`](https..."
          ],
          [
           "Gradio Demo: kitchen_sink\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo ..."
          ],
          [
           "```\nimport os\nimport json\n\nimport numpy as np\n\nimport gradio as gr\n\nCHOICES = [\"foo\", \"bar\", \"baz\"]\n..."
          ],
          [
           "def fn(\n    text1,\n    text2,\n    num,\n    slider1,\n    slider2,\n    single_checkbox,\n    checkboxes..."
          ],
          [
           "[\n            (\"The\", \"art\"),\n            (\"quick brown\", \"adj\"),\n            (\"fox\", \"nn\"),\n       ..."
          ],
          [
           "json.loads(JSONOBJ),  # JSON\n        \"<button style='background-color: red'>Click Me: \"\n        + ra..."
          ],
          [
           "demo = gr.Interface(\n    fn,\n    inputs=[\n        gr.Textbox(value=\"Lorem ipsum\", label=\"Textbox\"),\n..."
          ],
          [
           "gr.Audio(label=\"Audio\"),\n        gr.Audio(label=\"Microphone\", sources=[\"microphone\"]),\n        gr.Fi..."
          ],
          [
           "# os.path.join(os.path.abspath(''), \"files/cheetah1.jpg\"),\n            # os.path.join(os.path.abspat..."
          ],
          [
           "`@gradio/utils`\n\nGeneral functions for handling events in Gradio Svelte components\n\n\n```javascript\ne..."
          ],
          [
           "Gradio Demo: audio_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.Bl..."
          ],
          [
           "Gradio Demo: musical_instrument_identification\n### This demo identifies musical instruments from an ..."
          ],
          [
           "```\nimport gradio as gr\nimport torch\nimport torchaudio\nfrom timeit import default_timer as timer\nfro..."
          ],
          [
           "def predict(audio_path):\n    start_time = timer()\n    wavform, sample_rate = torchaudio.load(audio_p..."
          ],
          [
           "Gradio Demo: change_vs_input\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the de..."
          ],
          [
           "```\nimport os\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    set_button = gr.Button(\"Set Values\"..."
          ],
          [
           "with gr.Column(min_width=200):\n            gr.Markdown(\"# ON:CHANGE\")\n            text_ch = gr.Textb..."
          ],
          [
           "counter = gr.Number(label=\"Change counter\")\n\n    lion = os.path.join(os.path.abspath(''), \"files/lio..."
          ],
          [
           "text.change(lambda x,y:(x,y+1), [text, counter], [text_ch, counter])\n    num.change(lambda x,y:(x, y..."
          ],
          [
           "text_ch.change(lambda x:x, text_ch, text_ch2)\n    num_ch.change(lambda x:x, num_ch, num_ch2)\n    sli..."
          ],
          [
           "@gradio/chatbot\n\n## 0.5.5\n\n### Patch Changes\n\n- Updated dependencies [[`846d52d`](https://github.com..."
          ],
          [
           "## 0.5.3\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.5.1\n\n### Fixes\n\n- [#6574](https://github.com/gradio-app/gradio/pull/6574) [`2b625ad`](https://g..."
          ],
          [
           "## 0.4.8\n\n### Features\n\n- [#6296](https://github.com/gradio-app/gradio/pull/6296) [`46f13f496`](http..."
          ],
          [
           "## 0.4.6\n\n### Patch Changes\n\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.4\n\n### Patch Changes\n\n- Updated dependencies [[`854b482f5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.4.1\n\n### Patch Changes\n\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gra..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "- [#6135](https://github.com/gradio-app/gradio/pull/6135) [`bce37ac74`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.4.0-beta.8\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.4.0-beta.7\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.5.2\n\n### Patch Changes\n\n- Updated dependencies [[`e4a307ed6`](https://github.com/gradio-app/gra..."
          ],
          [
           "### Fixes\n\n- [#5755](https://github.com/gradio-app/gradio/pull/5755) [`e842a561a`](https://github.co..."
          ],
          [
           "## 0.4.0\n\n### Features\n\n- [#5671](https://github.com/gradio-app/gradio/pull/5671) [`6a36c3b78`](http..."
          ],
          [
           "### Fixes\n\n- [#5604](https://github.com/gradio-app/gradio/pull/5604) [`faad01f8e`](https://github.co..."
          ],
          [
           "## 0.3.1\n\n### Patch Changes\n\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/grad..."
          ],
          [
           "### Fixes\n\n- [#5304](https://github.com/gradio-app/gradio/pull/5304) [`05892302`](https://github.com..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`31996c99`](https://github.com/gradio-app/grad..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gr..."
          ],
          [
           "### Fixes\n\n- [#5242](https://github.com/gradio-app/gradio/pull/5242) [`2b397791`](https://github.com..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5125](https://github.com/gradio-app/gradio/pull/5125) [`80be7a1c`](https..."
          ],
          [
           "@gradio/button\n\n## 0.2.13\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com..."
          ],
          [
           "## 0.2.10\n\n### Patch Changes\n\n- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.2.7\n\n### Patch Changes\n\n- Updated dependencies [[`9caddc17b`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.4\n\n### Patch Changes\n\n- Updated dependencies [[`854b482f5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`2ba14b284`](https://github.com/gradio-app/gra..."
          ],
          [
           "### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github..."
          ],
          [
           "## 0.2.0-beta.7\n\n### Features\n\n- [#6143](https://github.com/gradio-app/gradio/pull/6143) [`e4f7b4b40..."
          ],
          [
           "## 0.2.0-beta.6\n\n### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](..."
          ],
          [
           "- Updated dependencies []:\n  - @gradio/upload@0.3.3\n\n## 0.2.2\n\n### Patch Changes\n\n- Updated dependen..."
          ],
          [
           "## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/grad..."
          ],
          [
           "##### Various performance improvements\n\nThese improvements will be particularly beneficial to large ..."
          ],
          [
           "Gradio Demo: dataframe_colorful\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport pandas as pd \nimport ..."
          ],
          [
           "How to Style the Gradio Dataframe\n\nTags: DATAFRAME, STYLE, COLOR\n\n## Introduction\n\nData visualizatio..."
          ],
          [
           "# Applying style to highlight the maximum value in each row\nstyler = df.style.highlight_max(color = ..."
          ],
          [
           "```\n\nThe Styler class can be used to apply conditional formatting and styling to dataframes, making ..."
          ],
          [
           "```\n\nHere's how it looks:\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/reso..."
          ],
          [
           "```\n\nIn this script, we define a custom function highlight_cols that changes the text color to purpl..."
          ],
          [
           "```\n\nIn this script, the format method of the Styler object is used to set the precision of numbers ..."
          ],
          [
           "Image Classification with Vision Transformers\n\nRelated spaces: https://huggingface.co/spaces/abidlab..."
          ],
          [
           "## Step 2 â€” Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Fa..."
          ],
          [
           "```\n\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate ou..."
          ],
          [
           "Gradio Demo: text_generation\n### This text generation demo takes in input text and returns generated..."
          ],
          [
           "Gradio Demo: diffusers_with_batching\n\n\n```\n!pip install -q gradio torch transformers diffusers\n```\n\n..."
          ],
          [
           "Gradio Demo: depth_estimation\n### A demo for predicting the depth of an image and generating a 3D mo..."
          ],
          [
           "```\nimport gradio as gr\nfrom transformers import DPTFeatureExtractor, DPTForDepthEstimation\nimport t..."
          ],
          [
           "def create_3d_obj(rgb_image, depth_image, image_path, depth=10):\n    depth_o3d = o3d.geometry.Image(..."
          ],
          [
           "print('run Poisson surface reconstruction')\n    with o3d.utility.VerbosityContextManager(o3d.utility..."
          ],
          [
           "iface = gr.Interface(fn=process_image,\n                     inputs=[gr.Image(\n                      ..."
          ],
          [
           "Using Gradio for Tabular Data Science Workflows\n\nRelated spaces: https://huggingface.co/spaces/sciki..."
          ],
          [
           "model = joblib.load(\"model.pkl\")\n\n# we will give our dataframe as example\ndf = datasets.load_dataset..."
          ],
          [
           "```\n\nLet's break down above code.\n\n- `fn`: the inference function that takes input dataframe and ret..."
          ],
          [
           "def plot(df):\n  plt.scatter(df.measurement_13, df.measurement_15, c = df.loading,alpha=0.5)\n  plt.sa..."
          ],
          [
           "```\n\n<gradio-app space=\"gradio/gradio-analysis-dashboard-minimal\"></gradio-app>\n\nWe will use the sam..."
          ],
          [
           "Controlling Layout\n\nBy default, Components in Blocks are arranged vertically. Let's take a look at h..."
          ],
          [
           "```\n\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn'..."
          ],
          [
           "```\n\nIn this example, the Column layout component is given a height of 100% of the viewport height (..."
          ],
          [
           "$code_blocks_form\n$demo_blocks_form\n\n## Variable Number of Outputs\n\nBy adjusting the visibility of c..."
          ],
          [
           "Build a Custom Multimodal Chatbot - Part 1\n\nThis is the first in a two part series where we build a ..."
          ],
          [
           "```\n\nAnd we're ready to go!\n\nTip: Make sure to modify the `Author` key in the `pyproject.toml` file...."
          ],
          [
           "```\n\n\nTip: The `data_model`s are implemented using `Pydantic V2`. Read the documentation [here](http..."
          ],
          [
           "```\n\nBefore we wrap up with the backend code, let's modify the `example_inputs` method to return a v..."
          ],
          [
           "```\n\nWe need to normalize each message to make sure each file has a proper URL to fetch its contents..."
          ],
          [
           "```\n\nNow for the fun part, actually rendering the text and files in the same message!\n\nYou should se..."
          ],
          [
           "```\n\nWe will modify this code to always display the text message and then loop through the files and..."
          ],
          [
           "```\n\nWe did it! ðŸŽ‰\n\n## Part 4 - The demo\n\nFor this tutorial, let's keep the demo simple and just disp..."
          ],
          [
           "```\n\n\nTip: Change the filepaths so that they correspond to files on your machine. Also, if you are r..."
          ],
          [
           "Creating a Real-Time Dashboard from BigQuery Data\n\nTags: TABULAR, DASHBOARD, PLOTS\n\n[Google BigQuery..."
          ],
          [
           "## Setting up your BigQuery Credentials\n\nTo use Gradio with BigQuery, you will need to obtain your B..."
          ],
          [
           "6. After selecting the service account, select the \"JSON\" key type and then click on the \"Create\" bu..."
          ],
          [
           "```\n\n## Using the BigQuery Client\n\nOnce you have the credentials, you will need to use the BigQuery ..."
          ],
          [
           "```\n\n## Building the Real-Time Dashboard\n\nOnce you have a function to query the data, you can use th..."
          ],
          [
           "Gradio Demo: sepia_filter\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport numpy as np\nimport gradio a..."
          ],
          [
           "`@gradio/image`\n\n```html\n<script>\n\timport { BaseImageUploader, BaseStaticImage, Webcam, BaseExample ..."
          ],
          [
           "`@gradio/audio`\n\n```html\n<script>\n\timport { BaseStaticAudio, BaseInteractiveAudio, BasePlayer, BaseE..."
          ],
          [
           "```\n\nBasePlayer:\n```javascript\n\texport let value: null | { name: string; data: string } = null;\n\texp..."
          ],
          [
           "Gradio Demo: blocks_multiple_event_triggers\n\n\n```\n!pip install -q gradio plotly pypistats\n```..."
          ],
          [
           "```\n!pip install -q gradio plotly pypistats\n```\n\n\n```\nimport gradio as gr\nimport pypistats\nfrom date..."
          ],
          [
           "!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/1)getting_start..."
          ],
          [
           "[å®˜ç½‘](https://gradio.app)\n| [æ–‡æ¡£](https://gradio.app/docs/)\n| [æŒ‡å—](https://gradio.app/guides/)\n| [å¼€å§‹](..."
          ],
          [
           "**ä¾èµ–**: Gradioåªéœ€è¦Python 3.7åŠä»¥ä¸Šç‰ˆæœ¬ï¼\n\n#### Gradioèƒ½åšä»€ä¹ˆï¼Ÿ\n\nä¸Žä»–äººå…±äº«æœºå™¨å­¦ä¹ æ¨¡åž‹ã€APIæˆ–æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹çš„æœ€ä½³æ–¹æ³•ä¹‹ä¸€å°±æ˜¯åˆ›å»ºä¸€ä¸ª**äº¤äº’å¼åº”ç”¨**ï¼Œ..."
          ],
          [
           "```\n\n2\\. ç”¨Pythonè„šæœ¬æˆ–åœ¨Jupyter Notebookä¸­è¿è¡Œä¸‹é¢çš„ä»£ç  ï¼ˆæˆ–è€…ä½¿ç”¨ [Google Colab](https://colab.research.google.com/..."
          ],
          [
           "```\n\n3\\. ä¸‹é¢çš„æ¼”ç¤ºä¼šè‡ªåŠ¨å‡ºçŽ°åœ¨Jupyter Notebookä¸­ï¼Œå¦‚æžœä½¿ç”¨è„šæœ¬è¿è¡Œåˆ™ä¼šåœ¨æµè§ˆå™¨[http://localhost:7860](http://localhost:7860)å¼¹å‡º..."
          ],
          [
           "```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interfac..."
          ],
          [
           "```\n\n![`hello_world_2` demo](../../demo/hello_world_2/screenshot.gif)\n\n#### å¤šè¾“å…¥å’Œè¾“å‡ºç»„ä»¶\n\nå‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„å‡½æ•°ï¼Œæœ‰..."
          ],
          [
           "```\n\n![`hello_world_3` demo](../../demo/hello_world_3/screenshot.gif)\n\næ‚¨åªéœ€å°†ç»„ä»¶åŒ…è£…åœ¨åˆ—è¡¨ä¸­ã€‚è¾“å…¥åˆ—è¡¨`inputs`ä¸­çš„æ¯ä¸ª..."
          ],
          [
           "```\n\n![`sepia_filter` demo](../../demo/sepia_filter/screenshot.gif)\n\nå½“ä½¿ç”¨`Image`ç»„ä»¶ä½œä¸ºè¾“å…¥æ—¶ï¼Œæ‚¨çš„å‡½æ•°å°†æŽ¥æ”¶ä¸€ä¸ªå½¢çŠ¶ä¸º ..."
          ],
          [
           "```\n\nè¿˜è¦æ³¨æ„ï¼Œæˆ‘ä»¬çš„è¾“å…¥ `Image` ç»„ä»¶å¸¦æœ‰ä¸€ä¸ªç¼–è¾‘æŒ‰é’® ðŸ–‰ï¼Œå®ƒå…è®¸è£å‰ªå’Œæ”¾å¤§å›¾åƒã€‚ä»¥è¿™ç§æ–¹å¼æ“ä½œå›¾åƒå¯ä»¥å¸®åŠ©æ­ç¤ºæœºå™¨å­¦ä¹ æ¨¡åž‹ä¸­çš„åè§æˆ–éšè—çš„ç¼ºé™·ï¼\n\næ‚¨å¯ä»¥åœ¨[Gradioæ–‡æ¡£](htt..."
          ],
          [
           "```\n\n![`hello_blocks` demo](../../demo/hello_blocks/screenshot.gif)\n\næ³¨æ„äº‹é¡¹ï¼š\n\n- `Blocks` ç”± `with` å­å¥ç»„æˆ..."
          ],
          [
           "```\n\n![`blocks_flipper` demo](../../demo/blocks_flipper/screenshot.gif)\n\nè¿˜æœ‰å¾ˆå¤šäº‹æƒ…å¯ä»¥åšï¼æˆ‘ä»¬å°†åœ¨[ä½¿ç”¨blocksæž„å»º](..."
          ],
          [
           "## å¼€æºæ ˆ\n\nGradioæ˜¯ç”±è®¸å¤šå¾ˆæ£’çš„å¼€æºåº“æž„å»ºçš„ï¼Œè¯·ä¸€å¹¶æ”¯æŒå®ƒä»¬!\n\n[<img src=\"../huggingface_mini.svg\" alt=\"huggingface\" height=4..."
          ],
          [
           "More on Examples\n\nIn the [previous Guide](/main/guides/the-interface-class), we discussed how to pro..."
          ],
          [
           "```\n\nThis can be helpful when browsing flagged data. Simply point to the flagged directory and the `..."
          ],
          [
           "Backend Testing Guidelines\n\n- All the tests should test Backend functionalities. Frontend functional..."
          ],
          [
           "Gradio and ONNX on Hugging Face\n\nRelated spaces: https://huggingface.co/spaces/onnx/EfficientNet-Lit..."
          ],
          [
           "Get started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces\n\nHugging Face Spaces..."
          ],
          [
           "ONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live ..."
          ],
          [
           "# loads ONNX model from ONNX Model Zoo\nmodel = hub.load(\"efficientnet-lite4\")\n# loads the labels tex..."
          ],
          [
           "# crops the image around the center based on given height and width\ndef center_crop(img, out_height,..."
          ],
          [
           "```\n\n## How to contribute Gradio demos on HF spaces using ONNX models\n\n- Add model to the [onnx mode..."
          ],
          [
           "# ä½¿ç”¨ Gradio è¿›è¡Œè¡¨æ ¼æ•°æ®ç§‘å­¦å·¥ä½œæµ\n\nRelated spaces: https://huggingface.co/spaces/scikit-learn/gradio-skops-int..."
          ],
          [
           "inputs = [gr.Dataframe(row_count = (2, \"dynamic\"), col_count=(4,\"dynamic\"), label=\"Input Data\", inte..."
          ],
          [
           "```\n\nè®©æˆ‘ä»¬æ¥è§£æžä¸Šè¿°ä»£ç ã€‚\n\n- `fn`ï¼šæŽ¨ç†å‡½æ•°ï¼ŒæŽ¥å—è¾“å…¥æ•°æ®å¸§å¹¶è¿”å›žé¢„æµ‹ç»“æžœã€‚\n- `inputs`ï¼šæˆ‘ä»¬ä½¿ç”¨ `Dataframe` ç»„ä»¶ä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬å°†è¾“å…¥å®šä¹‰ä¸ºå…·æœ‰ 2 è¡Œ 4 åˆ—çš„..."
          ],
          [
           "```\n\n<gradio-app space=\"gradio/gradio-analysis-dashboard-minimal\"></gradio-app>\n\næˆ‘ä»¬å°†ä½¿ç”¨ä¸Žè®­ç»ƒæ¨¡åž‹ç›¸åŒçš„æ•°æ®é›†ï¼Œä½†è¿™..."
          ],
          [
           "```\n\n<gradio-app space=\"gradio/gradio-skops-integration\"></gradio-app>\n\nä½¿ç”¨ `skops` å°† `sklearn` æ¨¡åž‹æŽ¨é€åˆ°..."
          ],
          [
           "his translation demo takes in the text, source and target languages, and returns the translation. It..."
          ],
          [
           "Gradio Demo: model3D\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo repo\n..."
          ],
          [
           "```\n\n\n```\nimport gradio as gr\nimport os\n\n\ndef load_mesh(mesh_file_name):\n    return mesh_file_name\n\n..."
          ],
          [
           "@gradio/lite\n\n## 0.4.4\n\n## 0.4.4-beta.0\n\n### Features\n\n- [#6147](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.4.1\n\n### Fixes\n\n- [#5988](https://github.com/gradio-app/gradio/pull/5988) [`bea931c31`](https:/..."
          ],
          [
           "## 0.3.1\n\n### Features\n\n- [#5226](https://github.com/gradio-app/gradio/pull/5226) [`64039707`](https..."
          ],
          [
           "- [#4826](https://github.com/gradio-app/gradio/pull/4826) [`f0150c62`](https://github.com/gradio-app..."
          ],
          [
           "- [#4785](https://github.com/gradio-app/gradio/pull/4785) [`da0e9447`](https://github.com/gradio-app..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- [#4731](https://github.com/gradio-app/gradio/pull/4731) [`f9171288`](..."
          ],
          [
           "@gradio/video\n\n## 0.2.3\n\n### Fixes\n\n- [#6766](https://github.com/gradio-app/gradio/pull/6766) [`7326..."
          ],
          [
           "## 0.2.0\n\n### Features\n\n- [#6726](https://github.com/gradio-app/gradio/pull/6726) [`21cfb0a`](https:..."
          ],
          [
           "## 0.1.9\n\n### Fixes\n\n- [#6566](https://github.com/gradio-app/gradio/pull/6566) [`d548202`](https://g..."
          ],
          [
           "## 0.1.6\n\n### Patch Changes\n\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.4\n\n### Patch Changes\n\n- Updated dependencies [[`6204ccac5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.2\n\n### Fixes\n\n- [#6234](https://github.com/gradio-app/gradio/pull/6234) [`aaa55ce85`](https:/..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.1.0-beta.9\n\n### Features..."
          ],
          [
           "- [#6149](https://github.com/gradio-app/gradio/pull/6149) [`90318b1dd`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.1.0-beta.8\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.1.0-beta.7\n\n### Patch Changes\n\n- Updated dependencies [[`174b73619`](https://github.com/gradio-..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5627](https://github.com/gradio-app/gradio/pull/5627) [`b67115e8e`](http..."
          ],
          [
           "## 0.0.10\n\n### Patch Changes\n\n- Updated dependencies [[`8f0fed857`](https://github.com/gradio-app/gr..."
          ],
          [
           "## 0.0.6\n\n### Patch Changes\n\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.0.4\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://git..."
          ],
          [
           "## 0.0.3\n\n### Fixes\n\n- [#5140](https://github.com/gradio-app/gradio/pull/5140) [`cd1353fa`](https://..."
          ],
          [
           "Gradio Demo: outbreak_forecast\n### Generate a plot based on 5 inputs.\n        \n\n\n```\n!pip install -q..."
          ],
          [
           "```\nimport altair\n\nimport gradio as gr\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nimport ..."
          ],
          [
           "inputs = [\n    gr.Dropdown([\"Matplotlib\", \"Plotly\", \"Altair\"], label=\"Plot Type\"),\n    gr.Slider(1, ..."
          ],
          [
           "@gradio/file\n\n## 0.4.3\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com/gr..."
          ],
          [
           "## 0.4.1\n\n### Patch Changes\n\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.3.0\n\n### Features\n\n- [#6511](https://github.com/gradio-app/gradio/pull/6511) [`71f1a1f99`](http..."
          ],
          [
           "## 0.2.6\n\n### Patch Changes\n\n- Updated dependencies [[`2f805a7dd`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.4\n\n### Patch Changes\n\n- Updated dependencies [[`854b482f5`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.2.2\n\n### Patch Changes\n\n- Updated dependencies [[`aaa55ce85`](https://github.com/gradio-app/gra..."
          ],
          [
           "- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-ap..."
          ],
          [
           "## 0.2.0-beta.8\n\n### Features..."
          ],
          [
           "## 0.2.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.2.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.2.1\n\n### Patch Changes\n\n- Updated dependencies [[`796145e2c`](https://github.com/gradio-app/gra..."
          ],
          [
           "Thanks [@aliabid94](https://github.com/aliabid94)!\n\n## 0.1.6\n\n### Patch Changes\n\n- Updated dependenc..."
          ],
          [
           "## 0.1.3\n\n### Patch Changes\n\n- Updated dependencies [[`c57f1b75e`](https://github.com/gradio-app/gra..."
          ],
          [
           "## 0.1.2\n\n### Patch Changes\n\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.1.1\n\n### Patch Changes\n\n- Updated dependencies [[`119c8343`](https://github.com/gradio-app/grad..."
          ],
          [
           "- [#5215](https://github.com/gradio-app/gradio/pull/5215) [`fbdad78a`](https://github.com/gradio-app..."
          ],
          [
           "- [#5265](https://github.com/gradio-app/gradio/pull/5265) [`06982212`](https://github.com/gradio-app..."
          ],
          [
           "### Fixes\n\n- [#5253](https://github.com/gradio-app/gradio/pull/5253) [`ddac7e4d`](https://github.com..."
          ],
          [
           "## 0.0.3\n\n### Patch Changes\n\n- Updated dependencies [[`61129052`](https://github.com/gradio-app/grad..."
          ],
          [
           "Gradio Demo: blocks_flashcards\n\n\n```\n!pip install -q gradio \n```..."
          ],
          [
           "```\nimport random\n\nimport gradio as gr\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        \"Loa..."
          ],
          [
           "flip_btn.click(flip_card, [selected_card], [back, answer_col])\n\n    def mark_correct(card, results):..."
          ],
          [
           "`@gradio/json`\n\n```html\n<script>\n\timport { BaseJSON } from \"@gradio/json\";\n</script>\n```\n\nBaseJSON\n`..."
          ],
          [
           "ä½¿ç”¨Gradio JavaScriptå®¢æˆ·ç«¯å¿«é€Ÿå…¥é—¨\n\nTags: CLIENT, API, SPACES\n\nGradio JavaScriptå®¢æˆ·ç«¯ä½¿å¾—ä½¿ç”¨ä»»ä½•Gradioåº”ç”¨ä½œä¸ºAPIéžå¸¸ç®€å•ã€‚ä¾‹..."
          ],
          [
           "```\n\nGradioå®¢æˆ·ç«¯é€‚ç”¨äºŽä»»ä½•æ‰˜ç®¡çš„Gradioåº”ç”¨ï¼Œæ— è®ºæ˜¯å›¾åƒç”Ÿæˆå™¨ã€æ–‡æœ¬æ‘˜è¦ç”Ÿæˆå™¨ã€æœ‰çŠ¶æ€çš„èŠå¤©æœºå™¨äººã€ç¨Žæ”¶è®¡ç®—å™¨è¿˜æ˜¯å…¶ä»–ä»»ä½•åº”ç”¨ï¼Gradioå®¢æˆ·ç«¯é€šå¸¸ä¸Žæ‰˜ç®¡åœ¨[Hugging Face..."
          ],
          [
           "```\n\n## ä¸ºç§äººä½¿ç”¨å¤åˆ¶ä¸€ä¸ªSpace\n\nè™½ç„¶æ‚¨å¯ä»¥å°†ä»»ä½•å…¬å…±Spaceç”¨ä½œAPIï¼Œä½†æ˜¯å¦‚æžœæ‚¨å‘å‡ºçš„è¯·æ±‚è¿‡å¤šï¼ŒHugging Faceå¯èƒ½ä¼šå¯¹æ‚¨è¿›è¡Œé€ŸçŽ‡é™åˆ¶ã€‚ä¸ºäº†æ— é™åˆ¶ä½¿ç”¨Spaceï¼Œåªéœ€å¤åˆ¶S..."
          ],
          [
           "```\n\nå¦‚æžœæ‚¨ä¹‹å‰å¤åˆ¶è¿‡ä¸€ä¸ªSpaceï¼Œåˆ™é‡æ–°è¿è¡Œ`duplicate`ä¸ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„Spaceã€‚è€Œæ˜¯å®¢æˆ·ç«¯å°†è¿žæŽ¥åˆ°å…ˆå‰åˆ›å»ºçš„Spaceã€‚å› æ­¤ï¼Œå¯ä»¥å®‰å…¨åœ°å¤šæ¬¡ä½¿ç”¨ç›¸åŒçš„Spaceé‡æ–°è¿è¡Œ`dupl..."
          ],
          [
           "```\n\n## æ£€æŸ¥APIç«¯ç‚¹\n\nä¸€æ—¦è¿žæŽ¥åˆ°Gradioåº”ç”¨ç¨‹åºï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨`client`çš„`view_api`æ–¹æ³•æ¥æŸ¥çœ‹å¯ç”¨çš„APIç«¯ç‚¹ã€‚\n\nå¯¹äºŽWhisper Spaceï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š\n\n..."
          ],
          [
           "```\n\nè¿™å‘Šè¯‰æˆ‘ä»¬è¯¥Spaceä¸­æœ‰1ä¸ªAPIç«¯ç‚¹ï¼Œå¹¶æ˜¾ç¤ºäº†å¦‚ä½•ä½¿ç”¨APIç«¯ç‚¹è¿›è¡Œé¢„æµ‹ï¼šæˆ‘ä»¬åº”è¯¥è°ƒç”¨`.predict()`æ–¹æ³•ï¼ˆä¸‹é¢å°†è¿›è¡Œæ›´å¤šæŽ¢ç´¢ï¼‰ï¼Œå¹¶æä¾›ç±»åž‹ä¸º`string`çš„å‚æ•°`input_..."
          ],
          [
           "```\n\nå¯¹äºŽæŸäº›è¾“å…¥ï¼Œä¾‹å¦‚å›¾åƒï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‰€éœ€è¦çš„æ–¹ä¾¿ç¨‹åº¦ä¼ å…¥`Buffer`ã€`Blob`æˆ–`File`ã€‚åœ¨Node.jsä¸­ï¼Œå¯ä»¥ä½¿ç”¨`Buffer`æˆ–`Blob`ï¼›åœ¨æµè§ˆå™¨çŽ¯å¢ƒä¸­ï¼Œå¯ä»¥ä½¿ç”¨`Bl..."
          ],
          [
           "```\n\n## çŠ¶æ€\n\näº‹ä»¶æŽ¥å£è¿˜å¯ä»¥é€šè¿‡ç›‘å¬`\"status\"`äº‹ä»¶æ¥èŽ·å–è¿è¡Œä½œä¸šçš„çŠ¶æ€ã€‚è¿™å°†è¿”å›žä¸€ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹å±žæ€§ï¼š`status`ï¼ˆå½“å‰ä½œä¸šçš„äººç±»å¯è¯»çŠ¶æ€ï¼Œ`\"pending\" | \"g..."
          ],
          [
           "```\n\nå¦‚æžœç¬¬ä¸€ä¸ªä½œä¸šå·²ç»å¼€å§‹å¤„ç†ï¼Œé‚£ä¹ˆå®ƒå°†ä¸ä¼šè¢«å–æ¶ˆï¼Œä½†å®¢æˆ·ç«¯å°†ä¸å†ç›‘å¬æ›´æ–°ï¼ˆä¸¢å¼ƒè¯¥ä½œä¸šï¼‰ã€‚å¦‚æžœç¬¬äºŒä¸ªä½œä¸šå°šæœªå¯åŠ¨ï¼Œå®ƒå°†è¢«æˆåŠŸå–æ¶ˆå¹¶ä»Žé˜Ÿåˆ—ä¸­ç§»é™¤ã€‚\n\n## ç”Ÿæˆå™¨ç«¯ç‚¹\n\næŸäº›Gradio APIç«¯..."
          ],
          [
           "@gradio/tootils\n\n## 0.1.7\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.com..."
          ],
          [
           "- Updated dependencies []:\n  - @gradio/statustracker@0.3.1\n\n## 0.1.1\n\n### Fixes\n\n- [#6234](https://g..."
          ],
          [
           "## 0.1.0\n\n### Features\n\n- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](http..."
          ],
          [
           "## 0.1.0-beta.6\n\n### Features\n\n- [#6044](https://github.com/gradio-app/gradio/pull/6044) [`9053c95a1..."
          ],
          [
           "## 0.1.0-beta.4\n\n### Features\n\n- [#5648](https://github.com/gradio-app/gradio/pull/5648) [`c573e2339..."
          ],
          [
           "- Updated dependencies []:\n  - @gradio/utils@0.2.0-beta.1\n\n## 0.1.0-beta.0\n\n### Features\n\n- [#5507](..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!..."
          ],
          [
           "Quickstart\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or w..."
          ],
          [
           "```\n\n\nTip: it is best to install Gradio in a virtual environment. Detailed installation instructions..."
          ],
          [
           "$demo_hello_world_4\n\nType your name in the textbox on the left, drag the slider, and then press the ..."
          ],
          [
           "The `input` and `output` arguments take one or more Gradio components. As we'll see, Gradio includes..."
          ],
          [
           "```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, so..."
          ],
          [
           "You can build very custom and complex applications using `gr.Blocks()`. For example, the popular ima..."
          ],
          [
           "Or, if you already know the basics and are looking for something specific, you can search the more [..."
          ],
          [
           "å¦‚ä½•åˆ›å»ºä¸€ä¸ªèŠå¤©æœºå™¨äºº\n\nTags: NLP, TEXT, CHAT\nRelated spaces: https://huggingface.co/spaces/gradio/chatbot_stre..."
          ],
          [
           "## ç®€å•èŠå¤©æœºå™¨äººæ¼”ç¤º\n\nè®©æˆ‘ä»¬ä»Žé‡æ–°åˆ›å»ºä¸Šé¢çš„ç®€å•æ¼”ç¤ºå¼€å§‹ã€‚æ­£å¦‚æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°çš„ï¼Œæˆ‘ä»¬çš„æœºå™¨äººåªæ˜¯éšæœºå¯¹ä»»ä½•è¾“å…¥å›žå¤ \" ä½ å¥½å—ï¼Ÿ\"ã€\" æˆ‘çˆ±ä½  \" æˆ– \" æˆ‘éžå¸¸é¥¿ \"ã€‚è¿™æ˜¯ä½¿ç”¨ Gradio..."
          ],
          [
           "$ æ¼”ç¤º \\_ ç®€å•èŠå¤©æœºå™¨äºº\n\n## ä¸ºèŠå¤©æœºå™¨äººæ·»åŠ æµå¼å“åº”\n\næˆ‘ä»¬å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼æ¥æ”¹è¿›ä¸Šè¿°èŠå¤©æœºå™¨äººçš„ç”¨æˆ·ä½“éªŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥æµå¼ä¼ è¾“å“åº”ï¼Œä»¥ä¾¿ç”¨æˆ·ä¸å¿…ç­‰å¾…å¤ªé•¿æ—¶é—´æ‰èƒ½ç”Ÿæˆæ¶ˆæ¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯ä»¥..."
          ],
          [
           "3. ç¬¬ä¸‰ä¸ªæ–¹æ³•ä½¿è¾“å…¥å­—æ®µå†æ¬¡å¯ä»¥äº¤äº’ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥å‘æœºå™¨äººå‘é€å¦ä¸€æ¡æ¶ˆæ¯ã€‚\n\nå½“ç„¶ï¼Œå®žé™…ä¸Šï¼Œæ‚¨ä¼šç”¨è‡ªå·±æ›´å¤æ‚çš„å‡½æ•°æ›¿æ¢ `bot()`ï¼Œè¯¥å‡½æ•°å¯èƒ½è°ƒç”¨é¢„è®­ç»ƒæ¨¡åž‹æˆ– API æ¥ç”Ÿæˆå“åº”ã€‚\n\næœ€åŽï¼Œæˆ‘ä»¬..."
          ],
          [
           "```\n\næ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥å¤„ç†å›¾ç‰‡ã€éŸ³é¢‘å’Œè§†é¢‘ç­‰åª’ä½“æ–‡ä»¶ã€‚è¦ä¼ é€’åª’ä½“æ–‡ä»¶ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ–‡ä»¶ä½œä¸ºä¸¤ä¸ªå­—ç¬¦ä¸²çš„å…ƒç»„ä¼ é€’ï¼Œå¦‚`(filepath, alt_text)` æ‰€ç¤ºã€‚`alt_text` æ˜¯å¯é€‰çš„ï¼Œå› ..."
          ],
          [
           "Gradio Demo: chatbot_streaming\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport ra..."
          ],
          [
           "Gradio Demo: upload_button\n### A simple demo showcasing the upload button used with its `upload` eve..."
          ],
          [
           "Gradio Demo: button_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\nwith gr.B..."
          ],
          [
           "Gradio Demo: image_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr.B..."
          ],
          [
           "`@gradio/checkbox`\n\n```html\n<script>\n    import { BaseCheckbox } from \"@gradio/checkbox\";\n</script>\n..."
          ],
          [
           "Gradio Demo: dropdown_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith g..."
          ],
          [
           "Gradio Demo: filter_records\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\n\ndef filter..."
          ],
          [
           "@gradio/dropdown\n\n## 0.4.3\n\n### Patch Changes\n\n- Updated dependencies [[`828fb9e`](https://github.co..."
          ],
          [
           "## 0.4.1\n\n### Patch Changes\n\n- Updated dependencies [[`206af31`](https://github.com/gradio-app/gradi..."
          ],
          [
           "## 0.3.2\n\n### Fixes\n\n- [#6425](https://github.com/gradio-app/gradio/pull/6425) [`b3ba17dd1`](https:/..."
          ],
          [
           "## 0.3.0-beta.8\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gradio/pull/6136) [`667802a6c..."
          ],
          [
           "## 0.3.0-beta.7\n\n### Features\n\n- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676..."
          ],
          [
           "## 0.3.0-beta.6\n\n### Features\n\n- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f..."
          ],
          [
           "## 0.3.2\n\n### Patch Changes\n\n- Updated dependencies []:\n  - @gradio/utils@0.1.2\n  - @gradio/atoms@0...."
          ],
          [
           "## 0.2.1\n\n### Fixes\n\n- [#5525](https://github.com/gradio-app/gradio/pull/5525) [`21f1db40`](https://..."
          ],
          [
           "## 0.1.3\n\n### Patch Changes\n\n- Updated dependencies [[`afac0006`](https://github.com/gradio-app/grad..."
          ],
          [
           "## 0.1.0\n\n### Highlights\n\n#### Improve startup performance and markdown support ([#5279](https://git..."
          ],
          [
           "Thanks [@pngwn](https://github.com/pngwn)!\n\n### Features\n\n- [#5215](https://github.com/gradio-app/gr..."
          ],
          [
           "## 0.0.2\n\n### Fixes\n\n- [#5062](https://github.com/gradio-app/gradio/pull/5062) [`7d897165`](https://..."
          ],
          [
           "Gradio Demo: autocomplete\n### This text generation demo works like autocomplete. There's only one te..."
          ],
          [
           "Gradio Demo: generate_english_german\n\n\n```\n!pip install -q gradio transformers torch\n```\n\n\n```\nimpor..."
          ],
          [
           "Gradio Demo: gallery_component_events\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\n..."
          ],
          [
           "Gradio Demo: theme_builder\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\n\ndemo = gr.th..."
          ],
          [
           "Gradio Demo: blocks_inputs\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the demo..."
          ],
          [
           "Gradio Demo: html_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr.Bl..."
          ],
          [
           "Gradio Demo: checkboxgroup_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nw..."
          ],
          [
           "Gradio Demo: state_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr.B..."
          ],
          [
           "`@gradio/textbox`\n\n```html\n<script>\n    import { BaseTextbox, BaseExample } from \"@gradio/textbox\";\n..."
          ],
          [
           "Gradio Demo: clear_components\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading files from the d..."
          ],
          [
           "```\nimport gradio as gr\nfrom datetime import datetime\nimport os\nimport random\nimport string\nimport p..."
          ],
          [
           "images = [\n    \"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?ixlib=rb-1.2.1&ixid=Mnw..."
          ],
          [
           "\"index\": 2,\n        \"word\": \"Chicago\",\n        \"start\": 5,\n        \"end\": 12,\n    },\n    {\n        \"..."
          ],
          [
           "highlighted_text = \"Does Chicago have any Pakistani restaurants\"\n\n\ndef random_model3d():\n    model_3..."
          ],
          [
           "components = [\n    gr.Textbox(value=lambda: datetime.now(), label=\"Current Time\"),\n    gr.Number(val..."
          ],
          [
           "gr.Audio(value=lambda: os.path.join(file_dir, \"cantina.wav\")),\n    gr.File(\n        value=lambda: ra..."
          ],
          [
           "value=lambda: images\n    ),\n    gr.Model3D(value=random_model3d),\n    gr.Plot(value=random_plot),\n  ..."
          ],
          [
           "def evaluate_values(*args):\n    are_false = []\n    for a in args:\n        if isinstance(a, (pd.DataF..."
          ],
          [
           "å¿«é€Ÿå¼€å§‹\n\n**å…ˆå†³æ¡ä»¶**ï¼šGradio éœ€è¦ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå°±æ˜¯è¿™æ ·ï¼\n\n## Gradio æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ\n\nä¸Žä»–äººåˆ†äº«æ‚¨çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ã€API æˆ–æ•°æ®ç§‘å­¦æµç¨‹çš„*æœ€ä½³æ–¹å¼ä¹‹ä¸€..."
          ],
          [
           "```\n\n2. å°†ä¸‹é¢çš„ä»£ç ä½œä¸º Python è„šæœ¬è¿è¡Œæˆ–åœ¨ Jupyter Notebook ä¸­è¿è¡Œï¼ˆæˆ–è€… [Google Colab](https://colab.research.google...."
          ],
          [
           "```\n\næ³¨æ„ï¼šæ‚¨ä¹Ÿå¯ä»¥è¿è¡Œ `python app.py`ï¼Œä½†å®ƒä¸ä¼šæä¾›è‡ªåŠ¨é‡æ–°åŠ è½½æœºåˆ¶ã€‚\n\n## `Interface` ç±»\n\næ‚¨ä¼šæ³¨æ„åˆ°ä¸ºäº†åˆ›å»ºæ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª `gr.Interface`..."
          ],
          [
           "$code_hello_world_2\n$demo_hello_world_2\n\n## å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºç»„ä»¶\n\nå‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ›´å¤æ‚çš„å‡½æ•°ï¼Œå…·æœ‰å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæŽ¥å—å­—ç¬¦ä¸²ã€å¸ƒ..."
          ],
          [
           "```\n\nè¿˜è¦æ³¨æ„ï¼Œæˆ‘ä»¬çš„è¾“å…¥ `Image` ç»„ä»¶é™„å¸¦æœ‰ä¸€ä¸ªç¼–è¾‘æŒ‰é’®ðŸ–‰ï¼Œå…è®¸è£å‰ªå’Œç¼©æ”¾å›¾åƒã€‚é€šè¿‡è¿™ç§æ–¹å¼æ“ä½œå›¾åƒå¯ä»¥å¸®åŠ©æ­ç¤ºæœºå™¨å­¦ä¹ æ¨¡åž‹ä¸­çš„åè§æˆ–éšè—çš„ç¼ºé™·ï¼\n\næ‚¨å¯ä»¥åœ¨[Gradio æ–‡æ¡£](ht..."
          ],
          [
           "$code_hello_blocks\n$demo_hello_blocks\n\néœ€è¦æ³¨æ„çš„äº‹é¡¹ï¼š\n\n- `Blocks` å¯ä»¥ä½¿ç”¨ `with` å­å¥åˆ›å»ºï¼Œæ­¤å­å¥ä¸­åˆ›å»ºçš„ä»»ä½•ç»„ä»¶éƒ½ä¼šè‡ªåŠ¨æ·»åŠ åˆ°åº”ç”¨ç¨‹åºä¸­..."
          ],
          [
           "@gradio/storybook\n\n## 0.2.0\n\n### Features\n\n- [#6451](https://github.com/gradio-app/gradio/pull/6451)..."
          ],
          [
           "## 0.1.0-beta.0\n\n### Features\n\n- [#5966](https://github.com/gradio-app/gradio/pull/5966) [`9cad2127b..."
          ],
          [
           "### Fixes\n\n- [#6065](https://github.com/gradio-app/gradio/pull/6065) [`7d07001e8`](https://github.co..."
          ],
          [
           "Sharing Your App\n\nHow to share your Gradio app:\n\n1. [Sharing demos with the share parameter](#sharin..."
          ],
          [
           "```\n\nThis generates a public, shareable link that you can send to anybody! When you send this link, ..."
          ],
          [
           "After you have [created a free Hugging Face account](https://huggingface.co/join), you have two meth..."
          ],
          [
           "### Embedding with Web Components\n\nWeb components typically offer a better experience to users than ..."
          ],
          [
           "```\n\n2. Add\n\n```html\n<gradio-app src=\"https://$your_space_host.hf.space\"></gradio-app>\n```\n\nelement ..."
          ],
          [
           "```\n\n<script>\nfetch(\"https://pypi.org/pypi/gradio/json\"\n).then(r => r.json()\n).then(obj => {\n    let..."
          ],
          [
           "You can also customize the appearance and behavior of your web component with attributes that you pa..."
          ],
          [
           "```\n\nHere's another example of how to use the `render` event. An event listener is used to capture t..."
          ],
          [
           "```\n\nAgain, you can find the `src=` attribute to your Space's embed URL, which you can find in the \"..."
          ],
          [
           "```\n\nThis will add and document the endpoint `/api/addition/` to the automatically generated API pag..."
          ],
          [
           "```\n\nFor authentication to work properly, third party cookies must be enabled in your browser.\nThis ..."
          ],
          [
           "def hello(profile: gr.OAuthProfile | None) -> str:\n    if profile is None:\n        return \"I don't k..."
          ],
          [
           "```\n\nWhen the user clicks on the login button, they get redirected in a new page to authorize your S..."
          ],
          [
           "```\n\nNote: if your function is called directly instead of through the UI (this happens, for\nexample,..."
          ],
          [
           "- **Cached examples created by Gradio.** These are files that are created by Gradio as part of cachi..."
          ],
          [
           "`@gradio/tooltip`\n\n```javascript\nimport { Tooltip } from \"@gradio/tooltip\";\n```\n\n```javascript\n\texpo..."
          ],
          [
           "enerate a plot based on 5 inputs...."
          ],
          [
           "Gradio Demo: stock_forecast\n\n\n```\n!pip install -q gradio numpy matplotlib\n```\n\n\n```\nimport matplotli..."
          ],
          [
           "simple dashboard showing pypi stats for python libraries. Updates on load, and has no buttons!..."
          ],
          [
           "Gradio Demo: neon-tts-plugin-coqui\n### This  demo converts text to speech in 14 languages.\n        \n..."
          ],
          [
           "@gradio/tooltip\n\n## 0.1.0\n\n## 0.1.0-beta.2\n\n### Features\n\n- [#6136](https://github.com/gradio-app/gr..."
          ],
          [
           "`@gradio/code`\n\n```html\n<script>\n    import { BaseCode, BaseCopy, BaseDownload, BaseWidget, BaseExam..."
          ],
          [
           "Gradio Demo: gif_maker\n\n\n```\n!pip install -q gradio opencv-python\n```\n\n\n```\nimport cv2\nimport gradio..."
          ],
          [
           "Gradio Demo: file_explorer_component_events\n\n\n```\n!pip install -q gradio \n```\n\n\n```\n# Downloading fi..."
          ],
          [
           "```\nimport gradio as gr\nfrom pathlib import Path\n\nbase_root = Path(__file__).parent.resolve()\n\nwith ..."
          ],
          [
           "with gr.Row():\n        a = gr.Textbox(elem_id=\"input-box\")\n        a.change(lambda x: x, inputs=[a])..."
          ],
          [
           "Gradio Demo: stream_audio\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport numpy a..."
          ],
          [
           "Gradio Demo: gallery_selections\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr\nimport n..."
          ],
          [
           "Gradio Demo: stream_asr\n\n\n```\n!pip install -q gradio torch torchaudio transformers\n```\n\n\n```\nimport ..."
          ],
          [
           "his demo takes in 12 inputs from the user in dropdowns and sliders and predicts income. It also has ..."
          ],
          [
           "`@gradio/label`\n\n```html\n<script>\n\timport { BaseLabel } from \"@gradio/label\";\n</script>\n```\n\nBaseLab..."
          ],
          [
           "alculate taxes using Textbox, Radio, and Dataframe components..."
          ],
          [
           "Gradio Demo: json_component\n\n\n```\n!pip install -q gradio \n```\n\n\n```\nimport gradio as gr \n\nwith gr.Bl..."
          ]
         ],
         "hovertemplate": "source=gradio<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "gradio, circle",
         "marker": {
          "color": "#ab63fa",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "gradio, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          10.52566,
          10.681319,
          10.106037,
          10.245061,
          10.539794,
          10.671648,
          10.239666,
          10.861549,
          10.536572,
          10.706751,
          10.71876,
          10.580017,
          10.977386,
          11.886463,
          11.812497,
          10.023915,
          10.370341,
          10.182922,
          10.278009,
          10.340192,
          11.813964,
          11.963341,
          11.27077,
          10.488464,
          10.011295,
          11.339914,
          12.411297,
          11.136359,
          11.199254,
          10.474375,
          11.24823,
          -0.11771866,
          2.1160667,
          0.83571076,
          -0.22863653,
          0.6198603,
          2.0702865,
          11.620594,
          11.664258,
          11.0550375,
          11.972184,
          11.960548,
          11.569089,
          15.159998,
          18.079405,
          17.141054,
          14.7023325,
          15.117463,
          18.055605,
          8.936766,
          9.684177,
          14.877052,
          12.481068,
          10.978971,
          8.350713,
          8.168133,
          8.344047,
          7.6621404,
          8.195122,
          8.175189,
          9.204275,
          11.722636,
          11.913063,
          10.707672,
          10.612369,
          10.5454445,
          10.28343,
          10.690525,
          10.245005,
          10.880317,
          10.660023,
          10.179599,
          10.125333,
          9.980908,
          10.048455,
          10.6328,
          10.548145,
          7.2690883,
          9.077716,
          9.301815,
          10.6570425,
          10.49307,
          11.038363,
          16.08517,
          14.608687,
          14.8060875,
          14.503134,
          14.8669815,
          15.129865,
          15.0559435,
          16.01131,
          14.786091,
          14.92643,
          14.785783,
          14.7467985,
          14.589202,
          13.154151,
          14.646561,
          16.071419,
          14.882472,
          14.697868,
          14.414375,
          14.585777,
          14.183371,
          16.217787,
          14.738335,
          14.823734,
          14.448105,
          14.446342,
          14.142014,
          16.426548,
          14.309977,
          14.823895,
          14.01546,
          15.03394,
          13.718146,
          15.386858,
          15.085672,
          14.515215,
          14.210194,
          13.463031,
          14.661239,
          15.121539,
          14.809244,
          16.255234,
          14.608001,
          14.529295,
          14.688873,
          14.803921,
          14.757241,
          16.266464,
          14.662364,
          15.100923,
          15.231301,
          15.625155,
          14.498601,
          14.904653,
          14.676106,
          14.958341,
          16.187346,
          14.762411,
          14.799442,
          14.669575,
          15.200352,
          14.969635,
          15.129377,
          14.675531,
          16.404041,
          15.357561,
          14.442565,
          15.31416,
          15.22101,
          12.854995,
          11.617793,
          11.891155,
          11.648429,
          10.487452,
          11.432959,
          15.003978,
          14.550849,
          14.715477,
          15.195053,
          14.5488825,
          14.979214,
          14.700492,
          14.944268,
          14.762267,
          14.420167,
          14.865643,
          15.0816965,
          15.857013,
          14.526743,
          14.736848,
          15.097269,
          14.794636,
          14.862465,
          14.96046,
          15.434005,
          14.624824,
          13.71163,
          14.377544,
          14.757382,
          15.1319,
          14.427856,
          14.882953,
          14.946977,
          14.82629,
          15.074973,
          16.441326,
          14.573951,
          13.912932,
          14.461674,
          14.812966,
          14.67579,
          14.727266,
          13.159003,
          14.824401,
          14.802026,
          16.206785,
          14.35948,
          15.1103325,
          14.827321,
          14.580146,
          14.855306,
          14.477189,
          15.002524,
          14.648315,
          14.216985,
          14.619666,
          14.550173,
          14.7016945,
          15.294845,
          15.753268,
          14.545276,
          14.610267,
          14.920039,
          14.739326,
          14.182891,
          14.83651,
          15.5067,
          14.951628,
          14.224125,
          14.603223,
          15.060231,
          14.590878,
          14.285817,
          14.775058,
          14.942983,
          14.778913,
          15.612817,
          15.310741,
          14.779886,
          14.473035,
          14.937697,
          14.586386,
          14.876728,
          16.14528,
          14.484106,
          14.708521,
          13.88545,
          14.855619,
          13.758032,
          15.892602,
          14.641406,
          14.422273,
          14.631737,
          11.978383,
          6.4808736,
          14.275718,
          14.647509,
          14.632436,
          14.601322,
          14.816544,
          15.053686,
          16.60162,
          14.90154,
          11.556048,
          6.576568,
          15.723116,
          14.296844,
          14.753714,
          14.694624,
          14.869732,
          13.689586,
          15.052939,
          14.765372,
          15.168934,
          14.525867,
          15.101235,
          14.857043,
          12.991231,
          11.327166,
          8.185919,
          14.385395,
          14.371606,
          14.5561495,
          14.438089,
          14.737305,
          14.888842,
          14.289635,
          14.917739,
          14.722895,
          14.7276745,
          14.709929,
          13.135987,
          -5.067455,
          13.106934,
          14.47807,
          13.503253,
          14.862723,
          13.737211,
          11.698331,
          15.063461,
          12.874766,
          13.14452,
          13.229562,
          10.423833,
          15.193359,
          15.161447,
          15.440289,
          15.644305,
          15.629766,
          16.04511,
          15.020511,
          14.937237,
          15.014155,
          14.949491,
          15.011828,
          16.047678,
          14.150141,
          14.839766,
          15.453867,
          15.109218,
          14.727455,
          13.896664,
          1.7998791,
          15.8762,
          15.021836,
          14.873073,
          14.81646,
          15.386009,
          15.536274,
          14.810667,
          14.863382,
          15.337379,
          15.609107,
          15.640717,
          14.959238,
          14.058497,
          15.696326,
          15.185645,
          15.555467,
          15.639684,
          14.303652,
          14.197024,
          15.549502,
          15.622938,
          15.310429,
          15.289004,
          15.054455,
          15.6223,
          15.664978,
          14.887904,
          15.2541065,
          15.771966,
          15.585419,
          15.003244,
          15.040425,
          15.107677,
          15.229956,
          15.424434,
          11.109929,
          12.661116,
          13.067519,
          11.157953,
          15.962812,
          15.143706,
          14.58126,
          15.581452,
          15.6549225,
          15.002267,
          15.083278,
          15.172254,
          14.5841465,
          14.8685055,
          15.460797,
          15.407464,
          14.912377,
          15.671616,
          15.670675,
          14.464944,
          15.584318,
          4.844884,
          5.925603,
          12.907691,
          11.854085,
          11.428853,
          14.757275,
          15.132346,
          15.209066,
          15.43644,
          15.593449,
          15.401783,
          13.278414,
          13.495024,
          15.063146,
          12.988296,
          15.101354,
          14.828079,
          14.988629,
          14.277533,
          15.314783,
          15.432625,
          15.511634,
          11.095327,
          14.218036,
          14.450905,
          15.116645,
          14.869699,
          15.576194,
          14.447357,
          14.617206,
          15.582017,
          15.012129,
          14.8959055,
          14.856442,
          15.289924,
          15.67667,
          15.095548,
          15.3808365,
          11.798867,
          14.367708,
          15.156035,
          15.173079,
          15.273289,
          14.862927,
          15.041925,
          15.465827,
          15.800997,
          15.131234,
          14.988003,
          15.643936,
          12.43088,
          7.0923877,
          15.15847,
          15.305974,
          15.001235,
          10.2165785,
          14.474031,
          15.423215,
          15.399296,
          14.753591,
          15.542895,
          13.630437,
          15.336059,
          10.848186,
          11.13487,
          12.026534,
          14.465268,
          15.548642,
          15.401875,
          11.367305,
          11.788458,
          12.384781,
          15.49646,
          15.549636,
          11.902299,
          12.498941,
          14.629036,
          15.575486,
          15.587383,
          15.539369,
          15.591541,
          15.538419,
          11.686245,
          14.531121,
          15.618657,
          10.649298,
          15.430023,
          15.368223,
          12.934726,
          11.646238,
          15.0165205,
          15.504066,
          14.949981,
          14.234988,
          15.383168,
          10.131908,
          11.319562,
          15.106782,
          15.381765,
          15.61721,
          15.384808,
          15.119487,
          14.295047,
          14.945271,
          15.647791,
          15.092468,
          14.935824,
          15.536025,
          14.701625,
          15.234893,
          15.008619,
          15.4560585,
          11.295449,
          11.356918,
          7.4105268,
          14.716185,
          15.100249,
          14.790749,
          14.594832,
          14.739538,
          12.646601,
          11.915997,
          16.032595,
          14.554208,
          14.574212,
          14.03934,
          13.958338,
          11.045191,
          11.2237835,
          14.608317,
          14.817715,
          14.875193,
          14.838672,
          14.222057,
          13.266703,
          11.725278,
          11.558467,
          11.210593,
          16.150883,
          13.350115,
          12.965472,
          15.340666,
          14.51874,
          15.291489,
          14.456377,
          13.222905,
          14.72262,
          14.7113,
          14.733853,
          15.098955,
          14.58409,
          14.984357,
          14.6233225,
          14.290799,
          15.0634,
          14.554184,
          14.420316,
          14.691994,
          14.235758,
          14.40536,
          14.263677,
          13.85081,
          14.52412,
          15.057035,
          14.923718,
          13.941301,
          15.115406,
          14.482318,
          14.032822,
          14.67656,
          14.750675,
          14.218082,
          14.17596,
          14.380979,
          13.229996,
          17.59157,
          15.367487,
          17.77719,
          17.668766,
          17.89331,
          16.403131,
          16.11775,
          4.6505013,
          10.97058,
          12.312062,
          8.684426,
          8.365979,
          11.365667,
          11.925537,
          10.257614,
          10.565597,
          8.013873,
          10.5906105,
          9.938426,
          10.091855,
          10.441705,
          10.758235,
          10.353887,
          8.063388,
          10.301172,
          9.682315,
          11.077791,
          -4.1471505,
          11.573256,
          10.638972,
          14.164523,
          15.0948925,
          15.449883,
          16.101719,
          16.333149,
          16.455809,
          16.23804,
          14.699136,
          13.952808,
          14.9838,
          15.531978,
          15.306534,
          16.402796,
          17.671835,
          14.7338505,
          14.627365,
          14.639272,
          18.142097,
          18.778248,
          18.457891,
          16.577295,
          16.249144,
          17.543951,
          18.47968,
          17.332798,
          15.993931,
          12.441698,
          12.732386,
          12.883306,
          12.909161,
          12.565565,
          11.101936,
          12.072323,
          12.504658,
          12.021906,
          10.572898,
          10.875081,
          11.672574,
          12.074839,
          12.120561,
          8.060575,
          12.177981,
          11.609826,
          11.660741,
          12.790456,
          11.8421755,
          11.385899,
          11.228351,
          12.821,
          4.2459135,
          11.853187,
          11.954999,
          17.300928,
          15.436005,
          16.442225,
          14.906457,
          16.226194,
          17.438766,
          17.428349,
          16.056812,
          16.298103,
          14.856427,
          14.264842,
          15.667391,
          16.282692,
          16.218952,
          15.683293,
          16.665445,
          14.977055,
          17.108416,
          14.84805,
          16.246185,
          15.4057045,
          10.978074,
          11.061151,
          10.844946,
          -0.69508207,
          11.040712,
          11.999157,
          11.801232,
          11.923928,
          11.525612,
          11.242063,
          17.139637,
          15.369172,
          15.26145,
          16.404278,
          16.374517,
          16.536537,
          17.357113,
          16.934462,
          2.3585126,
          11.845882,
          10.260837,
          10.547074,
          9.971633,
          10.6127615,
          10.396262,
          10.94093,
          11.743965,
          11.196772,
          17.585535,
          15.797439,
          16.940245,
          15.1377735,
          15.513085,
          18.218086,
          17.874552,
          15.61834,
          14.739389,
          16.302183,
          15.765899,
          15.209811,
          15.503585,
          17.451057,
          16.974495,
          16.003721,
          17.62598,
          18.078781,
          17.350647,
          16.23633,
          15.970694,
          11.583799,
          11.776642,
          18.17974,
          18.50799,
          17.214693,
          16.284422,
          17.448797,
          17.99026,
          16.798111,
          16.921124,
          13.088706,
          12.965188,
          13.062419,
          12.797045,
          13.078796,
          12.048207,
          10.762884,
          0.42435956,
          11.220541,
          12.397022,
          12.182886,
          11.986171,
          11.579183,
          11.074691,
          11.339279,
          11.969172,
          12.289561,
          10.798944,
          -7.151554,
          10.156306,
          11.654554,
          11.699727,
          11.832037,
          14.683362,
          12.360293,
          12.31697,
          10.891131,
          11.528254,
          12.114894,
          12.57917,
          10.11813,
          10.978081,
          10.850283,
          11.316474,
          11.66592,
          11.58011,
          12.119964,
          11.028419,
          10.791697,
          9.746541,
          9.738069,
          18.145649,
          16.81888,
          16.334114,
          16.10885,
          15.667992,
          18.381142,
          16.166162,
          17.481276,
          16.071035,
          17.628613,
          18.459913,
          18.640894,
          17.975515,
          18.511906,
          15.983479,
          15.515107,
          16.25155,
          14.633893,
          16.159735,
          17.41875,
          18.60248,
          15.872725,
          17.056738,
          17.76442,
          12.532004,
          11.960548,
          9.432595,
          12.404147,
          11.825937,
          11.371289,
          12.062737,
          11.701362,
          11.808916,
          1.1884782,
          11.5975065,
          11.926982,
          11.6102915,
          11.670701,
          11.862576,
          12.250345,
          9.484705,
          11.657487,
          12.39689,
          11.665502,
          12.112672,
          12.176917,
          12.116692,
          17.420708,
          17.554129,
          15.485368,
          15.266737,
          16.372723,
          16.567812,
          16.30656,
          16.124033,
          16.910986,
          18.517483,
          16.66989,
          16.036005,
          14.311804,
          8.686666,
          11.630922,
          10.686084,
          7.859121,
          12.352408,
          11.509883,
          11.274796,
          11.468216,
          12.392566,
          11.8786125,
          -6.4086123,
          -8.881743,
          2.0793805,
          10.862161,
          10.903007,
          11.139446,
          7.910092,
          1.0927937,
          10.607151,
          11.537456,
          12.015776,
          11.972084,
          11.624012,
          12.010091,
          11.2957735,
          9.60513,
          12.01114,
          10.418726,
          11.717881,
          11.9102545,
          10.382831,
          10.490681,
          11.652577,
          10.434194,
          11.2744665,
          11.2614975,
          11.289957,
          10.390873,
          0.5951965,
          0.484039,
          0.58608496,
          0.58059055,
          1.4651284,
          18.184729,
          18.587933,
          16.898643,
          16.190659,
          17.601027,
          17.747847,
          17.027851,
          12.310442,
          9.071015,
          7.5159445,
          9.808624,
          11.59916,
          11.765158,
          11.241064,
          10.59256,
          11.512919,
          11.829001,
          11.676142,
          12.016904,
          12.129877,
          12.006185,
          10.177099,
          10.365504,
          10.155918,
          10.460725,
          10.502939,
          11.863848,
          11.455188,
          16.0001,
          16.860828,
          10.631229,
          10.585713,
          10.419445,
          10.767892,
          10.749777,
          11.028974,
          10.644578,
          10.702305,
          10.744992,
          9.666879,
          10.755754,
          11.18626,
          11.128317,
          11.585834,
          11.326595,
          11.354284,
          14.885492,
          17.021412,
          11.695825,
          10.407831,
          10.165325,
          0.13239686,
          11.4913645,
          11.967351,
          0.44308522,
          11.781304,
          10.233793,
          10.221965,
          10.297562,
          8.6788645,
          10.300227,
          9.934715,
          10.147895,
          10.171406,
          9.64483,
          12.044623,
          14.845329,
          15.509833,
          16.083424,
          15.281413,
          11.621713,
          11.187428,
          9.64729,
          12.569528,
          11.955161,
          11.929092,
          11.874627,
          11.981861,
          10.040415,
          12.253433,
          12.253714,
          11.363416,
          11.550541,
          11.856388,
          9.323293,
          0.4055191,
          10.46639,
          10.269465,
          9.317134,
          9.592412,
          10.268874,
          10.605314,
          11.794907,
          16.660778,
          18.042088,
          18.539284,
          16.85412,
          16.222864,
          15.889859,
          16.917608,
          15.932448,
          15.354784,
          16.04608,
          14.775138,
          14.780768,
          17.116316,
          15.899679,
          11.811091,
          11.396769,
          10.176216,
          12.223584,
          11.628027,
          10.884182,
          11.918218,
          10.843054,
          10.72496,
          10.889051,
          11.392933,
          10.454695,
          9.811976,
          12.232764,
          10.924914,
          12.221344,
          11.798884,
          11.735259,
          11.502873,
          9.781029,
          7.6722803,
          13.820012,
          11.330491,
          11.409185,
          11.205473,
          17.770903,
          18.262598,
          18.486944,
          18.547798,
          18.000778,
          15.600348,
          14.332827,
          16.326727,
          16.864779,
          18.411194,
          17.91047,
          11.20431,
          11.55021,
          10.94855,
          -7.167276,
          -7.246376,
          11.060103,
          10.692011,
          11.684066,
          9.748134,
          11.496433,
          11.46947,
          10.929033,
          9.937708,
          11.517991,
          10.831363,
          9.781847,
          11.732516,
          2.0926557,
          10.075693,
          12.232072,
          17.894854,
          18.662844,
          18.524788,
          16.393322,
          12.68066,
          11.991222,
          11.854622,
          9.830581,
          10.34599,
          10.182764,
          10.530123,
          6.8960595,
          8.84189,
          9.933359,
          11.958414,
          12.024219,
          11.871615,
          10.076261,
          9.942244,
          10.915954,
          9.652299,
          11.122854,
          11.125618,
          10.986913,
          10.902252,
          10.62486,
          11.585333,
          11.776177,
          12.132177,
          12.183393,
          12.337497,
          11.735526,
          12.361157,
          11.2126045,
          11.279698,
          10.679239,
          0.9050189,
          11.293251,
          9.933829,
          0.867193,
          9.383748,
          1.0536299,
          10.716161,
          10.823686,
          10.697014,
          11.121118,
          6.1388826,
          10.506353,
          10.586037,
          10.686147,
          10.866513,
          11.313962,
          12.156006,
          -0.59566504,
          12.0134325,
          18.195787,
          17.87531,
          18.43214,
          18.576057,
          18.118773,
          18.211348,
          16.399769,
          14.89503,
          16.114325,
          16.277678,
          17.616333,
          18.45911,
          15.34681,
          15.885315,
          18.257504,
          17.566484,
          16.089218,
          15.196027,
          10.888735,
          11.321375,
          11.337206,
          18.081915,
          18.749247,
          18.555714,
          16.41661,
          16.392097,
          16.35436,
          18.590746,
          17.611,
          17.47621,
          15.168997,
          11.644524,
          15.575715,
          16.238207,
          17.072222,
          17.060047,
          10.393239,
          10.3686,
          9.400139,
          4.844011,
          9.974362,
          10.535741,
          18.115921,
          18.510244,
          15.103275,
          16.193563,
          16.15977,
          17.544443,
          17.840446,
          16.038727,
          15.91261,
          12.042485,
          6.4920225,
          5.8094053,
          5.0094633,
          4.9281125,
          6.36144,
          6.208806,
          14.00793,
          15.707044,
          14.830148,
          15.298954,
          16.303259,
          16.19381,
          16.329258,
          14.392193,
          14.363389,
          12.692115,
          13.064365,
          13.481626,
          13.059794,
          13.34478,
          15.593439,
          15.546245,
          15.612544,
          15.630268,
          15.701444,
          15.664258,
          11.039012,
          12.045627,
          11.927996,
          11.8231945,
          11.918136,
          11.338055,
          -4.879406,
          18.105875,
          17.798908,
          16.233019,
          17.037573,
          18.473331,
          17.520096,
          16.031662,
          12.351778,
          14.537703,
          12.7597475,
          17.15558,
          14.89064,
          11.022088,
          11.904154,
          11.121426,
          9.892556,
          9.982168,
          12.326045,
          13.109123,
          12.207353,
          12.421706,
          12.257107,
          -6.099996,
          12.014992,
          11.516533,
          9.071122,
          8.254979,
          10.18122,
          11.188112,
          11.302916,
          0.5493509,
          -0.25881892,
          11.839546,
          12.040189,
          11.318114,
          11.715966,
          11.96197,
          10.106373,
          9.526772,
          15.206327,
          17.00267,
          16.44665,
          15.760905,
          15.614602,
          16.351059,
          17.864683,
          15.568284,
          14.915583,
          16.251558,
          15.101047,
          15.951068,
          16.053673,
          17.704145,
          16.9419,
          15.545145,
          18.364939,
          16.681786,
          17.204416,
          15.8577,
          16.394943,
          15.52913,
          11.383115,
          12.145317,
          18.194563,
          17.700071,
          15.793927,
          18.695614,
          18.202156,
          18.143803,
          17.672377,
          16.356455,
          17.587128,
          14.319931,
          11.386576,
          11.67492,
          11.19829,
          10.122507,
          12.160455,
          10.108558,
          9.331723,
          11.637091,
          0.11395992,
          10.167278,
          10.544386,
          12.366333,
          11.713125,
          -4.7134185,
          -0.5200525,
          11.372264,
          13.032019,
          12.026958,
          12.149739,
          11.832691,
          11.496889,
          11.705009,
          12.879964,
          12.141018,
          12.744555,
          11.655247,
          9.936776,
          10.859825,
          9.461515,
          9.401023,
          9.224815,
          6.8620625,
          8.268678,
          9.027283,
          11.011734,
          11.930282,
          11.969939,
          11.606237,
          11.584047,
          11.619333,
          11.911004,
          11.204706,
          2.9809213,
          10.92912,
          3.5611627,
          11.084045,
          10.533928,
          10.958854,
          10.77831,
          10.3881645,
          10.332482,
          12.037301,
          10.885382,
          11.447608,
          11.326622,
          11.152847,
          11.795536,
          10.783016,
          11.00713,
          11.236812,
          9.702632,
          11.035877,
          11.33288,
          12.113348,
          10.512244,
          8.355994,
          9.962244,
          9.391123,
          10.743468,
          10.3363695,
          12.760333,
          12.030335,
          11.568082,
          8.785769,
          12.489265,
          11.272442,
          10.81767,
          9.871896,
          9.807007,
          12.508051,
          13.204416,
          12.485577,
          12.404776,
          12.644356,
          4.512774,
          13.426468,
          11.593458,
          11.351156,
          10.562374,
          9.838486,
          11.773801,
          16.973446,
          18.45828,
          18.272942,
          18.48318,
          16.521402,
          17.054579,
          16.348038,
          16.162128,
          16.37544,
          16.189161,
          16.295141,
          16.748154,
          16.498014,
          18.486897,
          17.202417,
          15.838218,
          13.202041,
          16.189754,
          11.755971,
          11.3676605,
          18.229687,
          18.371822,
          16.18623,
          17.116804,
          18.032179,
          11.793853,
          11.964228,
          11.086179,
          9.778343,
          10.415253,
          10.409168,
          10.515354,
          -2.5807757,
          10.402771,
          9.84514,
          9.351608,
          10.193343,
          11.906398,
          13.941704,
          11.900889,
          11.523349,
          12.115535,
          11.790091,
          11.384865,
          -1.1063899,
          -0.579585,
          12.04617,
          11.814931,
          10.138605,
          10.155616,
          7.1447144,
          12.037632,
          12.388846,
          12.030719,
          12.348929,
          12.117841,
          11.521155,
          10.567884,
          10.01157,
          8.2320175,
          7.917519,
          11.14376,
          10.258333,
          5.132143,
          10.847039,
          10.43022,
          12.858896,
          11.842926,
          11.912499,
          16.100454,
          16.865301,
          11.92892,
          10.749541,
          -0.02049659,
          0.019014655,
          12.261811,
          11.641015,
          11.4148445,
          10.4417515,
          10.889646,
          11.638712,
          11.070269,
          11.172606,
          10.51805,
          8.938822,
          7.954237,
          7.7453003,
          11.119326,
          10.880123,
          7.130357,
          7.608807,
          -7.157534,
          -7.113218,
          -7.2899294,
          11.684835,
          12.036121,
          11.239621,
          10.738612,
          10.15327,
          11.994833,
          10.440875,
          10.601242,
          10.009957,
          10.538932,
          10.736901,
          10.49606,
          10.285469,
          10.306554,
          10.061433,
          10.75884,
          -0.13908216,
          11.046562,
          11.408563,
          11.015305,
          10.912027,
          10.635593,
          11.007882,
          9.98332,
          10.23738,
          9.729072,
          10.156724,
          11.193325,
          11.25891,
          4.9383774,
          5.867371,
          6.582564,
          11.113788,
          11.318326,
          16.912352,
          16.416891,
          15.469589,
          17.031738,
          10.721825,
          10.560535,
          9.123036,
          9.717055,
          10.735846,
          10.639364,
          8.932988,
          10.80665,
          9.610436,
          10.667758,
          8.915832,
          9.848009,
          7.9668126,
          10.752096,
          10.744362,
          12.122583,
          11.927937,
          18.246298,
          18.561033,
          16.499289,
          16.578167,
          18.566391,
          18.516325,
          12.041123,
          12.060073,
          11.50189,
          12.245993,
          12.413409,
          12.177159,
          10.026599,
          10.106118,
          10.580343,
          10.401089,
          10.179773,
          12.005646,
          12.051743,
          8.102131,
          10.002972,
          10.003172,
          10.048225,
          10.292024,
          10.00111,
          6.6553445,
          10.549483,
          10.697576,
          10.640302,
          9.747974,
          10.509882,
          10.446596,
          10.400568,
          10.429521,
          -0.033076216,
          10.144313,
          10.591187,
          16.292192,
          16.93477,
          11.054036,
          12.108057,
          -6.753723,
          12.088235,
          12.9143915,
          11.662086,
          11.570667,
          11.400624,
          10.563351,
          11.352474,
          10.58371,
          10.274875,
          18.125172,
          17.306992,
          16.490887,
          17.929314,
          16.12513,
          12.102136,
          12.059673,
          11.987114,
          1.4857646,
          11.5819845,
          11.461566,
          11.859512,
          12.466826,
          12.093654,
          11.332348,
          6.409013,
          11.607412,
          11.472988,
          12.028668,
          11.791484,
          10.23117,
          11.906419,
          13.759927,
          12.670079,
          12.829398,
          7.4909463,
          11.883057,
          12.235839,
          11.763034,
          18.008598,
          17.03718,
          16.274067,
          17.527403,
          17.736273,
          15.256878,
          12.072067,
          11.488142,
          11.376637,
          10.287541,
          9.631811,
          9.92047,
          11.303746,
          10.999101,
          11.0674095,
          6.9358845,
          -7.1304617,
          -7.0043745,
          11.2323265,
          12.839034,
          12.739008,
          12.722247,
          12.06841,
          12.180356,
          11.781863,
          11.292138,
          10.959995,
          9.702381,
          10.100756,
          11.943304,
          17.84596,
          18.07575,
          17.218092,
          17.715855,
          17.847115,
          15.83385,
          16.273079,
          16.178307,
          17.23057,
          18.145182,
          18.185406,
          18.137945,
          17.16706,
          17.651356,
          12.396844,
          11.678253,
          8.786416,
          11.913928,
          11.742089,
          12.162883,
          12.183824,
          17.94918,
          18.485773,
          16.386517,
          5.4588947,
          8.566559,
          8.184539,
          7.7372155,
          7.7430177,
          8.684103,
          8.083562,
          7.7816176,
          9.752553,
          12.337805,
          18.268803,
          18.663248,
          18.114521,
          18.501297,
          17.447685,
          17.44821,
          18.361551,
          17.444662,
          16.010223,
          18.539694,
          11.503327,
          11.843662,
          12.038314,
          11.496857,
          12.948437,
          12.893398,
          -4.449214,
          11.822563,
          10.769921,
          10.601034,
          5.0189114,
          4.746184,
          5.032802,
          10.75655,
          18.248705,
          17.010155,
          18.103365,
          17.198792,
          16.11507,
          15.100196,
          17.741478,
          17.977688,
          17.810883,
          17.57237,
          17.876827,
          16.597767,
          15.960684,
          12.8625,
          11.853961,
          11.2661085,
          12.079503,
          11.7862,
          12.126552,
          11.854422,
          18.057678,
          18.351805,
          11.0662,
          14.76102,
          15.2047615,
          16.47087,
          15.1692505,
          15.5806,
          15.239787,
          16.43982,
          18.141636,
          16.327902,
          16.191063,
          18.212524,
          17.156677,
          16.12951,
          11.867338,
          11.917247,
          11.424271,
          11.590978,
          11.456632,
          11.569097,
          11.55451,
          11.786284,
          11.954085,
          11.231089,
          11.209461,
          10.262152,
          13.000155,
          12.627515,
          12.631753,
          12.848195,
          12.545685,
          4.7865143,
          3.311992,
          12.979707,
          11.494539,
          12.165224,
          9.6407995,
          12.097079,
          10.590138,
          9.727526,
          9.6666155,
          11.113221,
          10.73039,
          10.929315,
          11.189401,
          10.787269,
          10.130843,
          11.072165,
          10.413696,
          10.340349,
          9.852182,
          9.62074,
          9.4070215,
          9.018993,
          9.137697,
          9.165697,
          11.148598,
          12.02212,
          16.678951,
          17.1276,
          15.762873,
          16.880056,
          10.374658,
          10.353565,
          9.325849,
          9.981197,
          10.633741,
          10.496928,
          11.769329,
          10.941195,
          10.616836,
          1.6282437,
          11.9395,
          11.736956,
          9.36686,
          12.065501,
          11.830304,
          9.351426,
          10.360061,
          9.970916,
          0.20887436,
          6.8948703,
          10.64393,
          10.572149,
          10.478218,
          10.621284,
          10.4475,
          10.430473,
          10.292954,
          11.680691,
          18.031805,
          18.054451,
          18.476107,
          18.707859,
          18.544449,
          18.553534,
          18.003323,
          15.890168,
          16.316835,
          17.587084,
          16.164562,
          17.953226,
          11.67153,
          10.540515,
          10.414016,
          10.117409,
          10.175637,
          -0.2592227,
          1.2565464,
          2.0314455,
          10.70581,
          10.639761,
          10.333937,
          11.854131,
          10.6575365,
          9.833585,
          9.866352,
          9.845046,
          9.888693,
          12.362247,
          11.60429,
          11.357382,
          11.98404,
          11.927439,
          11.763885,
          11.060961,
          2.4118786,
          10.815002,
          10.752115,
          10.18704,
          10.591152,
          10.7717,
          10.923102,
          10.687671,
          10.686955,
          10.761387,
          12.146988,
          11.792679,
          12.898319,
          12.659235,
          12.779883,
          12.546925,
          12.911777,
          -2.7826235,
          12.050599,
          -2.4583213,
          -1.8563191,
          12.982195,
          10.607963,
          3.6631536,
          4.564891,
          10.471787,
          10.703627,
          12.176418,
          11.582218,
          18.17827,
          18.539066,
          16.103107,
          16.257328,
          15.86254,
          16.28696,
          15.616608,
          16.515457,
          14.240529,
          14.555715,
          16.45428,
          16.361586,
          16.169628,
          14.463674,
          13.76201,
          14.103749,
          15.925615,
          16.988882,
          16.531963,
          12.098899,
          11.992854,
          11.916732,
          18.009594,
          18.09244,
          17.377867,
          17.041477,
          16.29662,
          16.600697,
          18.526289,
          15.16272,
          17.225298,
          17.155306,
          14.915312,
          11.640357,
          11.910547,
          16.096561,
          17.602516,
          17.52015,
          15.526076,
          16.71618,
          15.544819,
          8.302574,
          15.168605,
          15.280224,
          15.932203,
          14.398538,
          16.576815,
          17.154753,
          14.776624,
          9.756177,
          16.984152,
          15.227184,
          14.800729,
          16.311155,
          15.185801,
          14.774805,
          14.993117,
          15.370085,
          14.89731,
          14.744444,
          16.265366,
          15.525037,
          14.759206,
          14.357523,
          15.0177965,
          17.249323,
          16.49862,
          15.377165,
          14.275138,
          14.751114,
          17.190516,
          14.780122,
          15.554041,
          17.18313,
          16.897804,
          17.65864,
          16.28432,
          16.560532,
          16.225077,
          14.616878,
          15.95259,
          17.429033,
          17.030905,
          11.429748,
          6.6589866,
          16.083088,
          14.962499,
          15.401,
          12.596581,
          14.172726,
          14.558993,
          15.367539,
          11.953993,
          10.648661,
          9.177493,
          7.314303,
          7.491243,
          10.024602,
          10.094029,
          11.90237,
          11.972011,
          11.591754,
          11.261746,
          10.949491,
          -7.199551,
          11.977203,
          10.752518,
          9.893502,
          10.758871,
          9.596757,
          9.21421,
          17.364244,
          17.290413,
          15.352213,
          18.376827,
          18.486889,
          17.287567,
          17.942612,
          15.562435,
          15.234318,
          16.326326,
          17.068766,
          17.163021,
          16.175835,
          16.363743,
          15.075217,
          16.898695,
          14.827555,
          17.2862,
          15.698004,
          15.096548,
          16.610271,
          17.998072,
          18.195833,
          18.271929,
          18.295738,
          17.86804,
          15.865877,
          16.104988,
          16.929045,
          17.73273,
          17.384325,
          16.132465,
          11.15583,
          11.175365,
          10.878985,
          10.934099,
          10.163589,
          10.604075,
          11.070383,
          -8.741293,
          11.317213,
          11.261581,
          11.613071,
          11.522242,
          11.556504,
          0.47540948,
          2.560631,
          2.8317325,
          10.425933,
          11.142294,
          0.47043392,
          11.100617,
          9.610855,
          11.149433,
          9.455171,
          10.933326,
          9.717248,
          11.424012,
          12.732957,
          12.094783,
          11.617906,
          11.218108,
          9.261927,
          8.39019,
          8.982133,
          12.271197,
          12.8274765,
          10.762324,
          4.737755,
          4.748967,
          4.023947,
          10.861633,
          11.517619,
          12.307403,
          12.283708,
          8.983385,
          11.809378,
          11.192565,
          11.6081705,
          10.757065,
          11.021511,
          10.706651,
          10.32701,
          12.050513,
          10.490311,
          10.58435,
          10.766431,
          10.577802,
          10.462154,
          10.38923,
          10.757047,
          8.9630165,
          11.388726,
          12.442063,
          10.77864,
          10.022689,
          -2.4960413,
          2.443357,
          2.749664,
          10.198879,
          10.63392,
          0.4190093,
          10.521025,
          10.636321,
          10.45678,
          -5.495415,
          11.784399,
          11.504826,
          16.214201,
          16.589264,
          15.88366,
          13.810399,
          14.734207,
          15.083947,
          17.105358,
          15.577978,
          17.364035,
          17.539375,
          17.713276,
          16.541468,
          15.357861,
          16.244923,
          14.885508,
          16.30155,
          17.450258,
          15.800417,
          18.456896,
          18.477428,
          17.159636,
          17.00104,
          11.115129,
          10.335791,
          9.910805,
          18.351639,
          18.427294,
          16.4035,
          18.591747,
          18.504269,
          17.897621,
          15.072378,
          16.256151,
          16.238716,
          17.6561,
          14.735798,
          18.053345,
          18.283646,
          18.488161,
          17.40811,
          16.021835,
          14.976519,
          15.012686,
          18.4465,
          12.032541,
          10.79189,
          -0.051097766,
          12.502739,
          10.835253,
          11.079271,
          10.586497,
          10.405088,
          10.881022,
          10.958829,
          10.725683,
          10.931722,
          10.710587,
          18.130795,
          16.128473,
          17.046219,
          16.800701,
          17.432922,
          16.888971,
          7.2422843,
          11.9626045,
          12.011751,
          11.66055,
          11.568656,
          12.890593,
          12.222956,
          12.738947,
          10.247396,
          10.144056,
          10.047211,
          9.980229,
          9.392564,
          12.804763,
          11.916246,
          12.018575,
          11.859446,
          12.380018,
          12.049375,
          11.468269,
          16.982504,
          18.472382,
          17.700542,
          16.275774,
          16.240374,
          16.433708,
          17.80678,
          15.210667,
          18.055965,
          17.186996,
          15.95517,
          14.886782,
          11.716005,
          11.438712,
          11.796178,
          11.976717,
          11.866976,
          12.171957,
          11.984109,
          11.990211,
          12.472415,
          12.022478,
          11.002063,
          6.894063,
          -0.12015531,
          0.45436856,
          9.956084,
          9.838004,
          9.843578,
          0.8942825,
          11.089955,
          10.938935,
          10.238804,
          10.479178,
          10.643087,
          10.464846,
          15.268664,
          15.946544,
          15.401573,
          11.571099,
          11.775967,
          10.179795,
          11.300078,
          9.511548,
          12.481764,
          10.841566,
          10.8205805,
          10.189096,
          5.516316,
          6.281984,
          5.6507726,
          6.916057,
          12.270344,
          12.30274,
          12.468185,
          9.756316,
          11.182997,
          4.2814226,
          11.29055,
          16.141012,
          12.397042,
          11.674344,
          11.925328,
          11.2908535,
          10.076212,
          11.331164,
          11.75417,
          10.991117,
          -7.2104897,
          12.2868805,
          0.8750687,
          12.013545
         ],
         "xaxis": "x",
         "y": [
          -2.6480298,
          -2.7235198,
          -3.4715052,
          -3.3053153,
          -3.2170205,
          -2.7758472,
          -2.8745856,
          -3.5248194,
          -2.5343983,
          -2.9391124,
          -2.9655907,
          -2.693993,
          -3.130553,
          -6.3723855,
          -4.389911,
          -2.5945718,
          -5.4079285,
          -3.0371358,
          -2.988293,
          -2.6863978,
          -6.1294293,
          -6.3505893,
          -4.4301767,
          -5.414534,
          -5.128355,
          -5.1148577,
          -3.4509552,
          -5.5076284,
          -4.054314,
          -3.4958873,
          -4.333271,
          -1.2242652,
          5.7139053,
          -2.6532297,
          -1.3250874,
          -0.38598612,
          5.700097,
          -4.4054646,
          -4.431556,
          -3.8583975,
          -6.335414,
          -6.136567,
          -4.9142666,
          -4.2515864,
          -4.2430277,
          -4.3298054,
          -4.4463563,
          -4.2369514,
          -4.227115,
          -3.8688822,
          -4.04274,
          -4.2841377,
          -5.95401,
          -5.2765284,
          -3.7439344,
          -3.6397815,
          -3.7237415,
          -3.7918856,
          -3.7643442,
          -3.8347352,
          -4.7322373,
          -5.7846055,
          -6.2566366,
          -2.6612563,
          -2.7052333,
          -2.7901587,
          -2.9991257,
          -2.7517312,
          -3.0657966,
          -2.6078832,
          -2.7616408,
          -2.8405209,
          -2.6664515,
          -2.9319565,
          -2.8161824,
          -2.6965137,
          -2.6421776,
          -0.8832226,
          -2.6436024,
          -2.2443633,
          -2.6454844,
          -2.7374096,
          -4.825908,
          -4.3932734,
          -4.2766485,
          -4.376907,
          -4.4821777,
          -4.525143,
          -4.314029,
          -4.269128,
          -4.394324,
          -4.26104,
          -4.2299814,
          -4.436347,
          -4.3734765,
          -4.383633,
          -4.256643,
          -4.402976,
          -1.9440118,
          -4.731568,
          -4.4012094,
          -4.4474363,
          -4.5825105,
          -4.2490396,
          -1.6863638,
          -4.3812637,
          -4.5514736,
          -4.357639,
          -4.4320846,
          -4.7353153,
          -4.57291,
          -4.3702087,
          -4.4284053,
          -4.5577474,
          -4.440626,
          -4.3755245,
          -4.0919003,
          -4.34454,
          -4.489283,
          -4.4766135,
          -4.238846,
          -4.51882,
          -4.5392656,
          -4.4036903,
          -1.6705656,
          -4.573491,
          -4.367001,
          -4.6596923,
          -4.3691726,
          -4.303726,
          -1.694142,
          -4.369762,
          -4.7223697,
          -4.348323,
          -4.0282235,
          -4.3187356,
          -4.6319976,
          -4.405171,
          -4.387132,
          -4.3031535,
          -4.375545,
          -4.3584876,
          -4.247804,
          -4.460282,
          -4.32442,
          -4.4706683,
          -4.441171,
          -4.5326576,
          -3.8169713,
          -4.135341,
          -3.9412694,
          -3.8558645,
          -4.372988,
          -4.5438457,
          -4.043249,
          -4.3707457,
          -4.3353205,
          -4.4427137,
          -4.6031,
          -4.474012,
          -4.452972,
          -4.7527575,
          -4.5911922,
          -4.5294065,
          -4.3154197,
          -4.7400765,
          -4.502594,
          -4.4117417,
          -4.493931,
          -4.59036,
          -5.174201,
          -4.50465,
          -4.453685,
          -4.5016174,
          -4.4565854,
          -4.49939,
          -4.3123007,
          -4.870996,
          -4.483328,
          -4.3875084,
          -4.419415,
          -4.4076376,
          -4.4989204,
          -4.4708943,
          -4.329189,
          -4.343437,
          -4.3599563,
          -4.303272,
          -4.8027544,
          -4.5483184,
          -4.393859,
          -4.602682,
          -4.4559093,
          -4.4894924,
          -4.46702,
          -4.385569,
          -4.3695664,
          -4.4420333,
          -1.6775488,
          -4.41542,
          -4.674719,
          -4.4428024,
          -4.293676,
          -4.4330893,
          -4.414748,
          -4.1831045,
          -4.2626405,
          -4.2997293,
          -4.357692,
          -4.3594074,
          -4.4482913,
          -3.9165459,
          -3.7150407,
          -4.3500004,
          -4.389741,
          -4.1800094,
          -4.228853,
          -4.276465,
          -4.351645,
          -4.125686,
          -4.3017917,
          -4.296006,
          -4.1732554,
          -4.2293954,
          -4.5133333,
          -3.9269624,
          -4.4308534,
          -4.2027807,
          -4.338022,
          -4.3517118,
          -4.747369,
          -4.2942924,
          -4.468652,
          -4.4745803,
          -4.337521,
          -4.4615984,
          -1.7641274,
          -4.4263124,
          -4.42824,
          -4.1842885,
          -4.5516806,
          -4.410722,
          -3.9648604,
          -4.199769,
          -4.304579,
          -4.350345,
          -5.0446568,
          -3.3509893,
          -4.4039135,
          -4.3748665,
          -4.421259,
          -4.2625585,
          -4.225508,
          -4.2913775,
          -3.5828633,
          -3.426762,
          -3.7944808,
          -3.376083,
          -3.9045444,
          -4.251764,
          -4.351505,
          -4.3514857,
          -4.3774943,
          -4.307998,
          -4.653823,
          -4.3156095,
          -4.7620864,
          -4.325012,
          -4.653204,
          -4.5923986,
          -3.992591,
          -4.92451,
          -3.731591,
          -4.4997644,
          -4.469364,
          -4.3346286,
          -4.44729,
          -4.5645614,
          -4.4592395,
          -4.3984923,
          -4.4234304,
          -4.3392887,
          -4.399107,
          -4.6118584,
          -5.266591,
          0.93923193,
          -5.3078933,
          -4.397673,
          -4.9602327,
          -3.9816408,
          -4.5220156,
          -4.308428,
          -3.7729146,
          -5.4229507,
          -5.1465764,
          -4.968234,
          -4.616019,
          -3.9414887,
          -3.9348466,
          -3.8906581,
          -3.2098112,
          -3.0545464,
          -2.060521,
          -4.14877,
          -4.3265257,
          -4.030076,
          -4.4827332,
          -4.088881,
          -2.0620718,
          -4.312873,
          -4.2912903,
          -3.5053043,
          -3.5984185,
          -4.4442415,
          -4.57751,
          -1.7039433,
          -2.4464116,
          -4.203057,
          -4.0289598,
          -4.3321056,
          -3.6564977,
          -3.3743732,
          -4.158808,
          -4.430998,
          -3.3933527,
          -3.0476928,
          -2.9691534,
          -4.114369,
          -4.438615,
          -3.6549919,
          -4.0896544,
          -3.3960862,
          -3.1020093,
          -4.3892326,
          -4.254267,
          -3.4849892,
          -3.0899212,
          -3.9696195,
          -4.513408,
          -4.162203,
          -2.9717684,
          -2.9307237,
          -3.8022382,
          -3.498467,
          -3.4819725,
          -3.1075618,
          -3.8706858,
          -4.1595016,
          -4.3108735,
          -3.6897345,
          -3.6380315,
          -4.679332,
          -4.688512,
          -4.4968696,
          -4.9511814,
          -2.0287862,
          -3.874557,
          -4.279285,
          -3.2498105,
          -3.0682387,
          -4.272496,
          -3.918881,
          -3.7399569,
          -4.3031845,
          -4.3105717,
          -3.9762626,
          -3.627876,
          -4.1233664,
          -3.0493367,
          -2.9703872,
          -4.250448,
          -3.062428,
          -1.2203137,
          -1.7650759,
          -4.9983406,
          -4.7942686,
          -5.1313014,
          -4.391741,
          -3.97316,
          -3.9039826,
          -3.4473078,
          -3.1631577,
          -3.4448304,
          -4.5664415,
          -4.410334,
          -4.139746,
          -4.242164,
          -3.993591,
          -4.127201,
          -4.1072774,
          -4.2278204,
          -3.6390455,
          -3.4323854,
          -3.3158362,
          -5.878031,
          -4.2217407,
          -4.389213,
          -3.9139194,
          -4.1575637,
          -3.224902,
          -4.1868005,
          -4.262409,
          -3.1436687,
          -4.6253514,
          -4.1937017,
          -3.9864156,
          -3.771113,
          -2.9722137,
          -3.900162,
          -3.4026127,
          -5.2615747,
          -4.310245,
          -4.1320357,
          -3.8759317,
          -3.7591426,
          -4.130507,
          -4.098774,
          -3.5201252,
          -2.6573775,
          -4.1635365,
          -4.185972,
          -3.0630314,
          -4.418399,
          -3.4227092,
          -3.840782,
          -3.5584607,
          -3.5537038,
          -5.3957744,
          -4.25285,
          -3.4045722,
          -3.397745,
          -4.1453032,
          -2.9884627,
          -4.2030473,
          -3.6830592,
          -5.6008816,
          -5.0226693,
          -4.4166675,
          -4.394688,
          -3.415792,
          -3.300727,
          -4.6599116,
          -5.007253,
          -4.535777,
          -4.1508694,
          -3.1411538,
          -4.4161015,
          -4.15657,
          -4.1680655,
          -3.0317843,
          -3.1360338,
          -3.4976854,
          -3.1747646,
          -3.215035,
          -4.243538,
          -4.06416,
          -3.1402717,
          -5.180017,
          -3.3838644,
          -3.3747463,
          -4.184752,
          -4.159401,
          -4.1605315,
          -3.2084534,
          -4.208631,
          -4.4770236,
          -3.1521094,
          -4.329569,
          -4.3242292,
          -4.3355803,
          -3.3534672,
          -3.1092875,
          -3.6930008,
          -4.116186,
          -4.0797157,
          -4.3251023,
          -3.0527039,
          -4.539689,
          -4.3240905,
          -3.345638,
          -4.1807427,
          -4.4690566,
          -4.0495934,
          -3.1052098,
          -4.4936714,
          -4.6579304,
          -3.5161483,
          -4.096907,
          -3.9175777,
          -4.148166,
          -4.1471,
          -4.130444,
          -4.3317757,
          -4.4761677,
          -2.1643822,
          -4.1516685,
          -4.1668506,
          -4.3543124,
          -4.023991,
          -4.508114,
          -4.3346334,
          -4.071564,
          -4.171301,
          -3.9790676,
          -4.1691113,
          -4.4475255,
          -4.1632686,
          -4.2848644,
          -4.7202663,
          -4.6232667,
          -1.8946816,
          -4.3836765,
          -4.3676295,
          -3.7821805,
          -3.9397614,
          -3.6468346,
          -4.379095,
          -4.2903633,
          -4.2961636,
          -4.211774,
          -4.343777,
          -4.5373816,
          -4.2520266,
          -4.1877265,
          -4.184139,
          -4.298095,
          -4.2224593,
          -4.2008243,
          -4.2602773,
          -4.24867,
          -4.3237486,
          -4.1915884,
          -4.2449317,
          -4.1327,
          -4.0858846,
          -4.143972,
          -4.187646,
          -4.397628,
          -4.4270034,
          -4.1092663,
          -4.302531,
          -4.224772,
          -4.172888,
          -4.2580614,
          -4.2528176,
          -4.298332,
          -4.156908,
          -4.1744876,
          -4.1356196,
          -4.2976413,
          -4.607963,
          -4.5217304,
          -3.7313914,
          -3.6451032,
          -1.6385089,
          -4.5369363,
          -3.1423159,
          -3.5216832,
          -3.315496,
          -5.738048,
          -6.2144523,
          -2.6722755,
          -2.7547708,
          -0.77686656,
          -3.7575278,
          -2.8226447,
          -3.184416,
          -2.7990036,
          -5.1276,
          -2.6629336,
          -2.884945,
          -2.9837992,
          -3.092745,
          -5.6988235,
          -2.4891458,
          -5.5915074,
          -4.518932,
          -4.2384377,
          -4.2822304,
          -4.4228907,
          -5.065326,
          -4.9826164,
          -5.0983863,
          -5.412434,
          -4.429009,
          -4.2710805,
          -4.255835,
          -4.34989,
          -4.502419,
          -4.5995626,
          -4.237919,
          -4.501263,
          -4.3537707,
          -4.379092,
          -4.221045,
          -4.2492566,
          -4.328939,
          -4.992687,
          -5.444495,
          -5.1304016,
          -4.265739,
          -3.4880965,
          -3.7037442,
          -4.2338314,
          -4.096128,
          -4.126482,
          -4.0027814,
          -4.2085323,
          -3.2906759,
          -3.9809659,
          -4.0328774,
          -3.6997263,
          -3.6615734,
          -3.6701226,
          -4.009006,
          -3.9317262,
          -3.840198,
          -3.5939686,
          -4.1432395,
          -4.380114,
          -4.9081893,
          -4.5167723,
          -4.0859094,
          -5.7232485,
          -4.956733,
          -4.398974,
          -2.0340436,
          -6.0308204,
          -6.230369,
          -4.229107,
          -4.0838127,
          -4.5787854,
          -3.923262,
          -4.5801835,
          -4.3891945,
          -4.5291014,
          -5.2154937,
          -1.5854508,
          -4.486983,
          -4.281421,
          -4.472708,
          -4.0182643,
          -4.436081,
          -4.1745043,
          -4.705749,
          -4.2194333,
          -3.278182,
          -4.4045305,
          -4.5938683,
          -4.0870695,
          -5.1811895,
          -5.787813,
          -4.296195,
          -2.2239223,
          -4.4246826,
          -6.30074,
          -6.0941205,
          -6.22749,
          -5.8304515,
          -5.088624,
          -4.1319633,
          -4.056435,
          -4.683001,
          -4.786204,
          -5.246169,
          -4.7935185,
          -3.614749,
          -3.233125,
          5.546114,
          -5.8008823,
          -3.0301278,
          -2.9947777,
          -3.469971,
          -3.0158834,
          -2.6375437,
          -4.9504757,
          -5.7976637,
          -4.7750263,
          -4.21829,
          -4.2368236,
          -4.1486306,
          -4.399179,
          -4.2330933,
          -4.2429137,
          -4.273328,
          -4.955603,
          -4.6233115,
          -1.5571781,
          -5.0305896,
          -4.5691724,
          -4.446055,
          -4.559529,
          -4.9772997,
          -4.3493166,
          -4.305782,
          -4.256029,
          -3.547395,
          -3.251578,
          -4.0203824,
          -5.892819,
          -5.72271,
          -4.206181,
          -4.275483,
          -5.008502,
          -5.226402,
          -4.887514,
          -4.4542284,
          -3.612852,
          -3.2185447,
          -5.4079523,
          -5.32624,
          -5.4333243,
          -5.3809147,
          -5.4163084,
          -6.2782493,
          -5.055454,
          -3.6614811,
          -5.4854684,
          -3.083221,
          -6.0871296,
          -4.3411517,
          -4.290832,
          -4.606468,
          -4.29829,
          -4.262428,
          -4.185314,
          -5.1759934,
          -6.02919,
          -4.937105,
          -5.460499,
          -6.202101,
          -6.1715536,
          -4.3545895,
          -5.37083,
          -5.2922544,
          -5.230247,
          -5.9913187,
          -6.2134337,
          -4.247589,
          -4.0469236,
          -4.1005344,
          -4.1810665,
          -4.2096233,
          -3.9889622,
          -5.6563497,
          -6.2104654,
          -4.9361477,
          -5.716922,
          -5.3966146,
          -4.469664,
          -4.213091,
          -5.2317085,
          -4.812917,
          -5.3918757,
          -4.327557,
          -4.2288685,
          -4.346301,
          -3.62352,
          -3.3175097,
          -4.247703,
          -4.303075,
          -4.2617435,
          -4.2470655,
          -4.2763176,
          -4.0489364,
          -4.957001,
          -1.650397,
          -4.456727,
          -5.4088416,
          -5.0913076,
          -4.281763,
          -3.7517858,
          -3.1721766,
          -4.222111,
          -4.155267,
          -3.8522189,
          -3.8520896,
          -3.040411,
          -5.969824,
          -5.693497,
          -6.1763277,
          -4.481198,
          -4.675215,
          -2.2372267,
          -4.431507,
          -4.3391967,
          -4.7948704,
          -4.311739,
          -4.677159,
          -3.4379456,
          -4.9070706,
          -5.7185416,
          -2.9950309,
          -5.627066,
          -4.2524385,
          -4.175606,
          -4.105631,
          -4.2233205,
          -4.2219577,
          -4.1359916,
          -4.31949,
          -4.483677,
          -4.476756,
          -5.148587,
          -5.221644,
          -4.7932177,
          -4.2446322,
          -3.8005843,
          -3.3706405,
          -4.405094,
          -3.8653252,
          -4.2586904,
          -3.593875,
          -2.289227,
          -3.968434,
          -4.073295,
          -3.9209602,
          -3.7965982,
          -4.230926,
          -4.0465436,
          1.4509498,
          -0.4459926,
          6.030143,
          -3.9661405,
          -3.8693945,
          -4.2698364,
          -3.4117308,
          -0.6735878,
          -3.8075876,
          -4.33777,
          -6.019749,
          -6.261126,
          -6.2351465,
          -6.399435,
          -4.543441,
          -4.25584,
          -6.1259446,
          -2.708941,
          -5.8302927,
          -5.6901507,
          -2.7223113,
          -2.9182887,
          -5.1131034,
          -2.6719973,
          -5.345943,
          -5.7164745,
          -5.444095,
          -5.073661,
          -5.055526,
          -5.34433,
          -4.7526417,
          -5.1025715,
          -4.394533,
          -4.2187586,
          -4.281713,
          -5.1320796,
          -5.311432,
          -5.0416446,
          -3.8879025,
          -3.217861,
          -3.0774152,
          -2.835024,
          -1.4322288,
          -2.8045204,
          -4.5472417,
          -4.408283,
          -4.4833813,
          -4.1570783,
          -4.4390917,
          -4.416011,
          -4.3830266,
          -6.0204945,
          -6.323469,
          -6.283848,
          -2.8525634,
          -3.0132303,
          -3.2648056,
          -3.3227038,
          -2.616846,
          -5.8952146,
          -5.339339,
          -4.937374,
          -5.140105,
          -2.7931864,
          -2.6813014,
          -2.5180488,
          -2.67203,
          -2.772668,
          -2.9986973,
          -2.6197996,
          -2.7000673,
          -2.6284585,
          -2.9246626,
          -2.9571238,
          -3.166861,
          -3.0721354,
          -3.910727,
          -3.4397395,
          -5.102749,
          -4.1221876,
          -5.2795,
          -5.9122396,
          -2.5508876,
          -2.504858,
          7.697235,
          -5.8822327,
          -6.058994,
          6.40325,
          -6.2020154,
          -2.492425,
          -3.0973668,
          -2.509912,
          -2.262788,
          -3.058976,
          -3.0737927,
          -2.5950105,
          -2.6879666,
          -3.0466504,
          -6.322815,
          -4.163732,
          -4.668151,
          -4.662752,
          -3.7481234,
          -6.0456343,
          -5.0993466,
          -4.4397388,
          -5.670289,
          -6.164399,
          -6.228556,
          -3.6826289,
          -3.76628,
          -2.9315271,
          -3.7058737,
          -3.795182,
          -3.8682137,
          -3.6010754,
          -3.7060738,
          -3.0003293,
          -4.510899,
          -2.869751,
          -3.346424,
          -2.7368171,
          -2.8335235,
          -3.0868442,
          -3.0514376,
          -6.0363784,
          -3.9785073,
          -4.174998,
          -4.2625227,
          -5.113215,
          -5.1479034,
          -4.9329376,
          -4.960239,
          -4.052985,
          -4.2699237,
          -3.7404022,
          -4.168171,
          -4.2780914,
          -3.3186305,
          -3.816518,
          -5.7928257,
          -4.075683,
          -3.300011,
          -3.8901796,
          -4.291937,
          -4.4696865,
          -6.331649,
          -2.7791057,
          -2.7259529,
          -2.8139606,
          -5.905462,
          -5.681564,
          -5.450222,
          -3.6379576,
          -3.5248613,
          -3.7579696,
          -3.7808735,
          -3.8429105,
          -3.4571855,
          -2.954802,
          -2.2425728,
          -4.4000683,
          -4.473575,
          -4.9932404,
          -4.5026746,
          -4.2079673,
          -4.227058,
          -4.2507315,
          -4.2508454,
          -4.209274,
          -4.9513206,
          -4.567976,
          -5.3712177,
          -5.1036754,
          -4.3192463,
          -4.4055448,
          -5.6581807,
          -5.7817626,
          -4.902658,
          -6.8753514,
          -6.794064,
          -4.7825418,
          -4.560911,
          -4.611599,
          -3.9430223,
          -4.448189,
          -5.257448,
          -4.1573467,
          -4.618426,
          -4.9768806,
          -4.9579206,
          -5.1423154,
          -4.9161587,
          6.9235573,
          -4.9851213,
          -3.149088,
          -4.2216463,
          -4.273938,
          -4.305465,
          -4.8339744,
          -5.7150407,
          -6.0584655,
          -5.027574,
          -4.452088,
          -2.801734,
          -2.958874,
          -3.2409801,
          -0.7573298,
          -1.889319,
          -2.4927516,
          -6.289089,
          -6.4565744,
          -5.238438,
          -4.9546366,
          -5.0080996,
          -3.0988429,
          -4.2386637,
          -3.3543837,
          -3.3376908,
          -3.1914935,
          -3.184271,
          -2.783554,
          -5.3333263,
          -6.2309613,
          -6.2394934,
          -5.1387086,
          -4.615742,
          -5.8252373,
          -3.104725,
          -6.0096993,
          -5.4621744,
          -5.491688,
          -4.6634946,
          -4.9256597,
          -5.1258683,
          -4.5499988,
          -4.035609,
          5.8292522,
          -2.9701548,
          -3.0526764,
          -4.736286,
          -3.5299056,
          -3.6628282,
          -2.8488348,
          -2.9456913,
          -2.8884873,
          -3.0914216,
          -6.079284,
          -6.343063,
          7.2365127,
          -6.295337,
          -4.237549,
          -4.279665,
          -4.2510395,
          -4.2678537,
          -4.2328525,
          -4.2517595,
          -4.248688,
          -4.74889,
          -5.0992355,
          -5.3052497,
          -4.348687,
          -4.2886004,
          -4.3159256,
          -4.122512,
          -4.2480965,
          -3.5827487,
          -3.3484516,
          -4.1885753,
          -2.895394,
          -3.4078898,
          -3.3310764,
          -4.2259917,
          -4.280181,
          -4.334654,
          -4.954479,
          -5.258847,
          -5.3972917,
          -4.2571754,
          -4.1575594,
          -4.184324,
          -4.027973,
          -5.5939517,
          -4.77819,
          -4.6809683,
          -4.043339,
          -3.1231673,
          -2.9482038,
          -3.268232,
          -2.8464735,
          -4.1265574,
          -2.960018,
          -3.082166,
          -4.2016063,
          -4.242577,
          -4.439356,
          -5.2701735,
          -5.4258523,
          -5.043622,
          -4.08979,
          -3.3774898,
          -3.7621279,
          -6.102361,
          -2.8728116,
          -2.456175,
          -1.9035505,
          -1.7142696,
          -2.8167496,
          -2.6605585,
          -4.171047,
          -4.2091913,
          -4.2956266,
          -4.43228,
          -4.862689,
          -4.6517997,
          -4.6210527,
          -4.3662424,
          -4.270894,
          -4.1392,
          -4.0232186,
          -5.1716385,
          -5.3600264,
          -5.1188383,
          -3.3935134,
          -3.1164703,
          -3.108332,
          -3.0344899,
          -2.880079,
          -3.6489098,
          -4.958067,
          -4.3619156,
          -6.237493,
          -6.1253753,
          -5.9628954,
          -4.924734,
          -2.9727223,
          -4.2783694,
          -4.58761,
          -5.3837686,
          -4.95713,
          -4.244907,
          -3.5112832,
          -3.342284,
          -3.0774417,
          -4.4327626,
          -4.0728254,
          -3.2201855,
          -4.0370593,
          -4.8690047,
          -5.998751,
          -4.711274,
          -4.3823957,
          -4.421857,
          -3.11344,
          -4.1263967,
          -4.275284,
          -4.0879583,
          -4.0809755,
          1.348786,
          -6.141665,
          -4.41146,
          -3.8799274,
          -3.944529,
          -4.688651,
          -5.729342,
          -4.995741,
          -0.9613079,
          -1.4068336,
          -4.6563725,
          -3.7057726,
          -5.5822787,
          -5.7622504,
          -5.949125,
          -4.7283893,
          -4.773995,
          -4.4674687,
          -4.3918185,
          -4.5730047,
          -4.691893,
          -4.8312697,
          -4.573826,
          -4.248102,
          -4.9352946,
          -4.6288753,
          -1.6444515,
          -4.6580358,
          -5.1526995,
          -5.1295238,
          -4.4127145,
          -4.895706,
          -4.4494867,
          -4.260256,
          -4.4289427,
          -3.2007647,
          -3.879488,
          -4.63862,
          -4.70472,
          -5.507023,
          -6.227822,
          -4.2344847,
          -4.30808,
          -4.3625736,
          -4.260248,
          -4.226116,
          -4.225417,
          -4.4336824,
          -5.2720265,
          -4.873354,
          -4.3002834,
          -4.210553,
          -4.1051955,
          -4.3934665,
          -4.440141,
          -4.0471544,
          -3.321444,
          -3.126321,
          -4.233534,
          -1.3186274,
          -4.9691176,
          -3.846614,
          -3.050028,
          -6.1813955,
          -2.6969244,
          -4.5315647,
          -5.837748,
          -4.3270707,
          -4.259036,
          -5.921164,
          -4.451907,
          -4.5035696,
          -4.2068458,
          -5.2450266,
          -4.503372,
          -4.1900253,
          -4.059696,
          -4.1174736,
          -4.2941575,
          -3.9766996,
          -3.8531072,
          -3.7645743,
          -2.8816845,
          -3.45886,
          -3.5646303,
          -4.305889,
          -4.2424207,
          -4.215298,
          -4.103955,
          -4.35199,
          -4.3547683,
          -4.2714734,
          -4.227328,
          -3.9723861,
          -4.4731708,
          -2.7244852,
          -4.3883443,
          -3.5400293,
          -5.14906,
          -5.46192,
          -5.2516665,
          -3.4727383,
          -6.4200664,
          -4.9351397,
          -4.310855,
          -4.0817924,
          -4.105802,
          -4.1343584,
          -3.7982497,
          -3.9903872,
          -4.2543144,
          -3.7017853,
          -4.9663324,
          -6.0975733,
          -4.780702,
          -2.947647,
          -1.6674016,
          -5.308804,
          -5.1742163,
          -3.485257,
          -2.5441215,
          -5.7490287,
          -6.222023,
          -4.4511495,
          -3.6194494,
          -2.9318848,
          -6.0618925,
          -5.5633907,
          -5.458341,
          -4.5902963,
          -4.479644,
          -4.284572,
          -4.502917,
          -4.209168,
          -4.2086678,
          -1.3010577,
          -4.364881,
          -4.850607,
          -5.160851,
          -2.715946,
          -2.8388255,
          -6.1991267,
          -4.126261,
          -4.2344513,
          -4.1064525,
          -4.2727423,
          -4.078696,
          -5.268029,
          -5.326849,
          -4.663749,
          -4.0379415,
          -3.9359539,
          -3.9502165,
          -4.0393906,
          -4.01091,
          -4.2226214,
          -3.3282292,
          -3.906758,
          -4.352142,
          -4.1451516,
          -6.024062,
          -6.072713,
          -4.202542,
          -4.3473034,
          -5.320593,
          -4.9733024,
          -4.249592,
          -5.6283484,
          -6.0756407,
          -4.577698,
          -4.4312572,
          -2.656927,
          -2.7946982,
          -2.4707427,
          1.5606089,
          -2.9545588,
          -3.1021018,
          -3.0091512,
          -2.4914362,
          -6.3927035,
          -4.357523,
          -4.125041,
          -4.2101164,
          -4.0567083,
          -4.1605506,
          -3.9944816,
          2.3959694,
          1.8497123,
          -6.4551754,
          -4.6935463,
          -5.03275,
          -5.1195335,
          -3.711123,
          -6.376644,
          -3.100133,
          -6.2979693,
          -3.09595,
          -6.159576,
          -5.3045745,
          -2.578176,
          -2.747614,
          -1.2443887,
          -0.843762,
          -4.1174536,
          -2.5051193,
          -0.72232467,
          -3.1339123,
          -2.6228092,
          -5.535104,
          -6.120357,
          -5.8633432,
          -4.972303,
          -5.2212605,
          -5.9575696,
          -5.406929,
          -4.6567225,
          -4.909457,
          -3.2221065,
          -5.176368,
          -5.113449,
          -3.8996813,
          -3.8917942,
          -4.0813475,
          -4.2715726,
          -4.237689,
          -2.7690737,
          -2.5308805,
          -2.5803027,
          -1.0930877,
          -5.397756,
          -5.2537518,
          -3.8555455,
          -3.7774405,
          -7.227011,
          -7.0069966,
          -5.8014803,
          -5.672188,
          -6.2889977,
          -5.33274,
          -4.923746,
          -4.9287567,
          -6.2536793,
          -2.894224,
          -2.7440884,
          -3.1799169,
          -3.2565722,
          -3.123363,
          -3.0870667,
          -3.055701,
          -2.6274197,
          -3.0749733,
          -4.2452955,
          -2.7057958,
          -4.4067063,
          -3.7060003,
          -3.8200512,
          -3.5548484,
          -3.3381276,
          -3.6992934,
          -3.6345718,
          -3.7492394,
          -3.64035,
          -3.5663126,
          -3.8078346,
          -3.7758815,
          -1.1458437,
          -1.8192183,
          -1.7403779,
          -3.7150507,
          -4.059702,
          -4.851253,
          -4.6633525,
          -3.8773365,
          -3.225304,
          -2.8484063,
          -2.7356172,
          -3.8002253,
          -3.0444453,
          -2.7355068,
          -2.8094015,
          -3.6942487,
          -2.8180711,
          -3.6532404,
          -2.7993731,
          -3.502401,
          -2.983973,
          -3.1340508,
          -2.7962189,
          -2.9844513,
          -6.1987476,
          -5.984914,
          -4.225076,
          -4.2651052,
          -5.2192564,
          -4.784105,
          -4.2720366,
          -4.2162848,
          -6.3775105,
          -6.2828755,
          -6.0806575,
          -4.37927,
          -4.524465,
          -4.362232,
          -3.0530403,
          -3.2632496,
          -3.2000253,
          -2.5363798,
          -2.9115071,
          -6.36365,
          -4.5386963,
          -3.5870752,
          -5.048989,
          -4.433115,
          -4.926597,
          -5.121071,
          -5.0673857,
          -2.8646317,
          -2.6318495,
          -3.0947459,
          -3.028511,
          -3.49662,
          -2.63969,
          -2.658296,
          -2.601254,
          -2.7633076,
          -1.050667,
          -4.9098306,
          -2.556938,
          -4.9756346,
          -5.141533,
          -5.335688,
          -5.3875766,
          -5.47191,
          -4.325067,
          -4.1949453,
          -5.96612,
          -5.00116,
          -4.7644415,
          -2.8749852,
          -4.938767,
          -2.941839,
          -2.5199575,
          -4.2150965,
          -4.5754347,
          -5.2277646,
          -4.2013826,
          -4.095751,
          -6.0140934,
          -6.264104,
          -5.8228693,
          5.8915334,
          -5.094803,
          -5.99861,
          -4.046848,
          -4.099829,
          -4.050538,
          -4.3222065,
          -2.5208008,
          -4.55144,
          -4.466717,
          -4.3848786,
          -4.7700763,
          -3.8800147,
          -6.249814,
          -4.28007,
          -4.1826253,
          -4.4259567,
          -3.2196968,
          -6.090828,
          -4.842288,
          -5.7622585,
          -4.2138786,
          -5.200341,
          -5.301671,
          -5.015374,
          -3.9102783,
          -4.0700603,
          -6.263448,
          -4.957395,
          -6.0229964,
          -5.463925,
          -5.3455424,
          -5.138619,
          -5.9378457,
          -5.168937,
          -5.3636765,
          -3.9648125,
          -7.2365656,
          -7.029456,
          -4.421625,
          -5.398088,
          -5.375426,
          -5.4129424,
          -5.3006396,
          -5.4749284,
          -5.7712464,
          -6.0268245,
          -5.447064,
          -5.433068,
          -5.15493,
          -6.2351203,
          -4.2439895,
          -4.2238092,
          -4.26399,
          -4.2669396,
          -4.2596116,
          -5.0243664,
          -1.6170634,
          -5.463723,
          -4.9353204,
          -4.22075,
          -4.2057915,
          -4.200391,
          -3.2578516,
          -4.33026,
          -4.146288,
          -3.9178524,
          -3.5632753,
          -3.7481148,
          -3.7122908,
          -3.9596834,
          -4.1032386,
          -4.1731696,
          -4.2900057,
          -4.8530474,
          -2.1361964,
          -3.8335078,
          -3.7198064,
          -3.4436436,
          -2.7089553,
          -3.902466,
          -3.7796278,
          -3.6866019,
          -4.444195,
          -3.9053333,
          -4.2517843,
          -4.2591786,
          -4.217901,
          -4.2769055,
          -4.664846,
          -5.05923,
          -4.275551,
          -3.5317025,
          -3.3509767,
          -4.219704,
          -5.088737,
          -5.6144366,
          -5.6169596,
          -4.2504807,
          -3.880048,
          -5.549378,
          -2.8506248,
          -6.2026844,
          -4.7963505,
          -4.597019,
          -2.0997286,
          -2.1734843,
          -2.9024613,
          -4.8284254,
          -4.2206097,
          -5.2191896,
          -4.1778264,
          -3.212468,
          -3.733243,
          -4.431888,
          -4.1436925,
          -4.554105,
          -4.5635266,
          -5.071394,
          -4.3100924,
          -4.131701,
          -3.7435913,
          -5.493503,
          -6.2792225,
          -5.4186225,
          -5.7977247,
          -6.0246925,
          -6.240044,
          -6.410407,
          -4.2040434,
          -4.2430396,
          -4.5783825,
          -4.386387,
          -4.3876033,
          -4.892361,
          -4.266234,
          -4.5293074,
          -4.3341746,
          -4.6129746,
          -4.2161074,
          -5.2852435,
          -5.3855567,
          -4.1872563,
          -3.2950962,
          -3.8177018,
          -6.200319,
          -4.253628,
          -4.4928145,
          -4.426928,
          -4.2099404,
          -4.439771,
          -4.131783,
          -4.4245,
          -6.210512,
          -5.611224,
          -5.5163407,
          -4.8835535,
          -4.3186994,
          -4.280765,
          -4.205604,
          -4.3561926,
          -4.249102,
          -2.4808004,
          -0.87138844,
          -4.272472,
          -5.7624583,
          -3.3116713,
          -3.5694292,
          -6.2019835,
          -4.4423347,
          -4.8724294,
          -5.1855807,
          -4.612758,
          -3.5649521,
          -4.586671,
          -6.0601797,
          -5.637857,
          -5.4886727,
          -3.2112668,
          -2.9726093,
          -2.947066,
          -3.2221239,
          -3.108139,
          -3.335274,
          -2.9575732,
          -3.157514,
          -3.1860385,
          -3.4376667,
          -6.1650968,
          -4.907956,
          -4.932183,
          -4.1115084,
          -3.252179,
          -2.8684645,
          -2.8797412,
          -2.7748034,
          -3.4041896,
          -3.3031256,
          -3.1421037,
          -6.0556226,
          -4.7721524,
          -4.610561,
          -0.81823796,
          -6.2370048,
          -4.466481,
          -3.893137,
          -5.9292045,
          -6.123247,
          -2.8412604,
          -2.7147703,
          -2.7967255,
          -1.3485508,
          -0.6437562,
          -3.6171446,
          -3.2369113,
          -2.6901832,
          -3.345824,
          -2.7556014,
          -3.3404427,
          -2.8353286,
          -5.973909,
          -4.244171,
          -4.2349596,
          -4.236914,
          -4.233506,
          -4.253506,
          -4.2713056,
          -4.232007,
          -5.034214,
          -5.255554,
          -4.9060516,
          -3.3454287,
          -4.252187,
          -5.74482,
          -2.7920904,
          -2.5654612,
          -2.75156,
          -2.9960625,
          -0.7135503,
          -0.5674993,
          5.6013074,
          -3.0019681,
          -2.6836846,
          -2.5764093,
          -5.988584,
          -4.9699078,
          -5.1495104,
          -5.16306,
          -5.0619035,
          -5.1008153,
          -3.0741944,
          -5.302462,
          -4.9905577,
          -6.216604,
          -6.339201,
          -5.884433,
          -5.054431,
          6.077485,
          -3.012578,
          -2.9466608,
          -2.7974946,
          -2.9076138,
          -2.931535,
          -3.1252818,
          -2.9876158,
          -3.0944045,
          -3.1092272,
          -6.2795587,
          -5.895564,
          -5.4638696,
          -5.333491,
          -5.368241,
          -5.3052754,
          -5.3510838,
          -2.4529638,
          -4.489712,
          -0.29533067,
          -2.6862295,
          -5.173467,
          -4.6197824,
          -3.141422,
          -2.780541,
          -4.6776056,
          -4.773475,
          -6.312432,
          -5.2293134,
          -4.2087784,
          -4.264958,
          -4.648745,
          -5.2367067,
          -3.9669962,
          -4.6839232,
          -4.6224623,
          -4.8164062,
          -4.6531563,
          -4.484137,
          -4.905435,
          -5.4428234,
          -4.658409,
          -4.474876,
          -4.481459,
          -4.5367446,
          -4.5484,
          -5.195998,
          -4.916494,
          -6.5225306,
          -6.0006757,
          -6.069535,
          -4.252261,
          -4.249788,
          -4.1930895,
          -5.241654,
          -5.2880173,
          -4.727604,
          -4.2587485,
          -4.460945,
          -4.1909995,
          -3.2237363,
          -4.386342,
          -6.100604,
          -6.252271,
          -4.02055,
          -4.2269425,
          -4.252625,
          -4.017545,
          -4.6304903,
          -4.0822144,
          -3.733725,
          -4.44389,
          -4.3261046,
          -4.5319495,
          -4.373669,
          -2.2898066,
          -4.208757,
          -4.2126913,
          -4.2351284,
          -4.1588945,
          -4.4005036,
          -4.41304,
          -4.7767134,
          -4.7448378,
          -4.548673,
          -4.3698044,
          -4.6481004,
          -4.5937543,
          -4.4869328,
          -1.6569382,
          -4.9454937,
          -4.363115,
          -4.3594284,
          -4.383761,
          -4.476526,
          -4.7981696,
          -4.456328,
          -4.340363,
          -4.460779,
          -4.22851,
          -4.3514447,
          -4.236549,
          -4.0788774,
          -4.54004,
          -4.2303,
          -4.316806,
          -4.0008717,
          -4.1472015,
          -4.224764,
          -3.7440374,
          -4.1600485,
          -3.2472708,
          -3.7525227,
          -3.3704658,
          -3.8236833,
          -4.6227674,
          -4.108448,
          -3.8857741,
          -4.3844123,
          -4.3677554,
          -4.3222127,
          -6.001768,
          -5.050143,
          -4.249004,
          -3.9413207,
          -4.1545963,
          -5.0335174,
          -4.9776664,
          -4.6119804,
          -3.7554994,
          -6.0427704,
          -5.5444713,
          -5.2828474,
          -7.1321616,
          -6.033029,
          -5.018705,
          -5.0389743,
          -4.9209094,
          -4.718776,
          -4.270783,
          -4.0562286,
          -4.192823,
          -4.09586,
          -4.3300476,
          -4.2148933,
          -4.1534333,
          -4.2627945,
          -4.898231,
          -4.525046,
          -5.356158,
          -4.873966,
          -3.9275959,
          -4.0389476,
          -4.7943425,
          -4.173416,
          -4.1051054,
          -4.153479,
          -3.366995,
          -4.0525055,
          -4.0954657,
          -4.182065,
          -4.226826,
          -4.2158146,
          -4.2242436,
          -4.218372,
          -4.262031,
          -5.066664,
          -5.114933,
          -4.89159,
          -4.2373457,
          -3.5235348,
          -3.3143122,
          -5.7101126,
          -5.1429806,
          -5.0349894,
          -5.1189904,
          -5.0667915,
          -5.230242,
          -5.018836,
          -0.29412335,
          -4.5309525,
          -4.3537216,
          -5.29192,
          -4.911595,
          -4.8596826,
          -1.9542685,
          4.7787423,
          4.7702947,
          -4.6816525,
          -4.6356716,
          -4.158921,
          -5.2444005,
          -5.5073924,
          -4.588031,
          -3.766879,
          -3.9406037,
          -3.959078,
          -4.6427484,
          -5.3065147,
          -5.3494987,
          -5.4672894,
          -5.0312877,
          -3.9000146,
          -3.4135172,
          -3.7858558,
          -5.1153293,
          -4.6485505,
          -4.7935743,
          -1.7349038,
          -2.2739067,
          -3.925796,
          -4.672338,
          -5.16229,
          -3.1897748,
          -3.2680361,
          -3.7016497,
          -6.125651,
          -5.661219,
          -3.3773108,
          -2.987585,
          -3.189443,
          -3.1810186,
          -2.9247274,
          -5.580868,
          -3.0088968,
          -3.0549595,
          -3.7111428,
          -2.8172812,
          -2.851594,
          -2.50386,
          -2.9255412,
          -3.5781834,
          -4.274907,
          -4.160595,
          -3.854544,
          -3.218892,
          1.6293027,
          5.823868,
          5.4430037,
          -3.435559,
          -3.243193,
          -4.635601,
          -3.1549153,
          -3.236037,
          -3.0361202,
          -1.7168435,
          -5.5654798,
          -4.83649,
          -4.6850595,
          -4.7282505,
          -3.9197302,
          -4.4044495,
          -4.2153335,
          -4.254334,
          -4.167492,
          -4.329952,
          -4.1803946,
          -4.221265,
          -4.26628,
          -4.5167036,
          -4.91146,
          -1.6567247,
          -4.633167,
          -5.3710647,
          -4.748951,
          -4.299087,
          -4.2529635,
          -4.2509713,
          -3.2520165,
          -4.2337875,
          -5.6542706,
          -5.622309,
          -5.329008,
          -4.221041,
          -4.264094,
          -4.013109,
          -4.2501845,
          -4.2554865,
          -4.2465353,
          -4.379149,
          -1.5958383,
          -5.403156,
          -4.957799,
          -4.366196,
          -4.2397733,
          -4.2072115,
          -4.2604885,
          -3.566819,
          -3.778298,
          -4.5889587,
          -4.381685,
          -4.2484756,
          -6.318567,
          -5.04894,
          -4.3028235,
          -3.337254,
          -2.8381467,
          -3.077053,
          -2.840869,
          -2.785378,
          -2.8448083,
          -2.8538907,
          -2.8694263,
          -2.8904727,
          -2.7858934,
          -4.2148757,
          -4.458207,
          -5.1232104,
          -4.7299275,
          -4.780865,
          -3.2993126,
          -3.5278962,
          -4.153909,
          -4.589133,
          -4.505145,
          -4.6033425,
          -5.250537,
          -4.367123,
          -4.1688223,
          -2.663018,
          -2.5735686,
          -2.6248176,
          -2.746834,
          -2.8838122,
          -5.6268773,
          -5.647806,
          -6.3273473,
          -6.101955,
          -3.1069481,
          -6.318916,
          -5.877214,
          -4.077657,
          -4.2541656,
          -4.177366,
          -5.098013,
          -5.363117,
          -4.539707,
          -4.1971397,
          -4.3430796,
          -4.17987,
          -3.2643118,
          -3.7406385,
          -4.4684834,
          -5.2110777,
          -5.356353,
          -5.901881,
          -6.082035,
          -6.25354,
          -5.9906244,
          -6.322286,
          -6.310317,
          -2.9833596,
          -6.2393856,
          -5.7060995,
          -3.629888,
          -5.5195365,
          -2.9223063,
          -5.035284,
          -5.0211816,
          -4.9901075,
          -4.5755167,
          -3.2383912,
          -3.1499147,
          -3.006085,
          -2.9802024,
          -2.8018892,
          -2.6459014,
          -4.6874857,
          -4.601775,
          -4.3883367,
          -4.066362,
          -4.0287766,
          -3.4236066,
          -3.622719,
          -3.0710516,
          -3.752381,
          -3.534882,
          -3.6784575,
          -3.3634236,
          -2.039626,
          -1.1687896,
          -1.8818623,
          -1.7330703,
          -4.110061,
          -4.1174493,
          -2.9850965,
          -5.546127,
          -5.703227,
          -2.741933,
          -5.3101954,
          -4.7564554,
          -3.057378,
          -5.4725494,
          -5.9389286,
          -4.778229,
          -4.7759624,
          -5.5274158,
          -5.6214137,
          -5.2123876,
          3.514006,
          -3.018128,
          -4.239101,
          -6.284053
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "### What kinds of TPU are available?\n\nNew users are often very confused by the range of TPUs, and th..."
          ],
          [
           "</Tip>\n\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directl..."
          ],
          [
           "### I keep hearing about this XLA thing. Whatâ€™s XLA, and how does it relate to TPUs?\n\nXLA is an opti..."
          ],
          [
           "<Tip>\n\n**ðŸ¤—Specific HuggingFace TipðŸ¤—:** Weâ€™ve put a lot of effort into rewriting our TensorFlow model..."
          ],
          [
           "```\n\nThis might seem very restrictive at first, but most neural net code doesnâ€™t need to do this. Yo..."
          ],
          [
           "```\n\nThis code is totally fine in NumPy or PyTorch, but it breaks in XLA! Why? Because the shape of ..."
          ],
          [
           "```\n\nHere, we avoid data-dependent shapes by computing the loss for every position, but zeroing out ..."
          ],
          [
           "<Tip>\n\n**ðŸ¤—Specific HuggingFace TipðŸ¤—:** Our tokenizers and data collators have methods that can help ..."
          ],
          [
           "### Summary\n\nThere was a lot in here, so letâ€™s summarize with a quick checklist you can follow when ..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nðŸ‘€ The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-fun..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## VisionTextDualEncoderConfig\n\n[[autodoc]] VisionTextDualEncoderConfig\n\n## VisionTextDualEncoderPro..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "With this approach, the model can detect objects based on textual descriptions without prior trainin..."
          ],
          [
           "```\n\n## Zero-shot object detection pipeline\n\nThe simplest way to try out inference with OWL-ViT is t..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "```py\n>>> predictions = detector(\n...     image,\n...     candidate_labels=[\"human face\", \"rocket\", \"..."
          ],
          [
           "```\n\nLet's visualize the predictions:\n\n```py\n>>> from PIL import ImageDraw\n\n>>> draw = ImageDraw.Dra..."
          ],
          [
           "```\n\nLet's take a different image to switch things up.\n\n```py\n>>> import requests\n\n>>> url = \"https:..."
          ],
          [
           "```\n\nPass the inputs through the model, post-process, and visualize the results. Since the image pro..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "```\n\nPreviously for post-processing you passed the single image's size as a tensor, but you can also..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "<p align=\"center\">\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\n        <img alt=..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/blob/main/R..."
          ],
          [
           "<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/hug..."
          ],
          [
           "ÐœÐ¾Ð´ÐµÐ»Ð¸ transformers Ñ‚Ð°ÐºÐ¶Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÑ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð·Ð°Ð´Ð°Ñ‡, Ñ‚Ð°ÐºÐ¸Ðµ ÐºÐ°Ðº Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° Ñ‚Ð°Ð±Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ñ€Ð°..."
          ],
          [
           "ðŸ¤— Transformers Ð¾Ð¿Ð¸Ñ€Ð°ÐµÑ‚ÑÑ Ð½Ð° Ñ‚Ñ€Ð¸ ÑÐ°Ð¼Ñ‹Ðµ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ - [Jax](https://jax.r..."
          ],
          [
           "Ð’ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ NLP ( ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð¾Ð² Ð½Ð° ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¼ ÑÐ·Ñ‹ÐºÐµ ):\n- [ÐœÐ°ÑÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð² Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ..."
          ],
          [
           "- [ÐžÐ±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+me..."
          ],
          [
           "- [ÐžÑ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ..."
          ],
          [
           "DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+u..."
          ],
          [
           "s+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portugu..."
          ],
          [
           "8Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon..."
          ],
          [
           "C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud..."
          ],
          [
           "egenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest..."
          ],
          [
           "f+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000..."
          ],
          [
           "00%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%28..."
          ],
          [
           "tres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belongi..."
          ],
          [
           "+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+t..."
          ],
          [
           "%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amount..."
          ],
          [
           "r+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+depa..."
          ],
          [
           "+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+ha..."
          ],
          [
           "+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+..."
          ],
          [
           "diverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+..."
          ],
          [
           "l+trees+divided+into+16%2C000+species)..."
          ],
          [
           "- [ÐŸÐµÑ€ÐµÐ²Ð¾Ð´ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berli..."
          ],
          [
           "Ð’ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ ÐºÐ¾Ð¼Ð¿ÑŒÑŽÑ‚ÐµÑ€Ð½Ð¾Ð³Ð¾ Ð·Ñ€ÐµÐ½Ð¸Ñ:\n- [ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ViT](https://huggingface.co/g..."
          ],
          [
           "Ð’ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ð·Ð²ÑƒÐºÐ°:\n- [ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Wav2Vec2](https://huggingface.co/fac..."
          ],
          [
           "Ð’ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ…:\n- [ÐžÑ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¿Ð¾ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ TAPAS](https://huggingface.co/g..."
          ],
          [
           "## 100 Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ð², Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ñ… Transformers\n\nTransformers - ÑÑ‚Ð¾ Ð½Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð½Ð°Ð±Ð¾Ñ€ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾..."
          ],
          [
           "Ð•ÑÐ»Ð¸ Ð²Ñ‹ ÑÐ²Ð»ÑÐµÑ‚ÐµÑÑŒ Ð²Ð»Ð°Ð´ÐµÐ»ÑŒÑ†ÐµÐ¼ Ð¸Ð»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹, Ð¿Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ Ð¼Ð½ÐµÐ½Ð¸ÑŽ, Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð²ÐºÐ»ÑŽÑ‡..."
          ],
          [
           "## Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ Ð³Ð°Ð¹Ð´\n\nÐ”Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð·Ð°Ð´Ð°Ð½Ð½Ð¾Ð¼ Ð²Ñ…Ð¾Ð´Ðµ (Ñ‚ÐµÐºÑÑ‚, Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ, Ð·Ð²ÑƒÐº, ...) Ð¼Ñ‹ Ð¿Ñ€ÐµÐ´Ð¾..."
          ],
          [
           "```\n\nÐ’Ñ‚Ð¾Ñ€Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° ÐºÐ¾Ð´Ð° Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð¸ ÐºÑÑˆÐ¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ÑƒÑŽ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€Ð¾..."
          ],
          [
           "# Ð’Ñ‹Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ€Ð° Ð´Ð»Ñ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²\n>>> object_detector = pipeline('object-detection')\n>>..."
          ],
          [
           "```\n\nÐ—Ð´ÐµÑÑŒ Ð¼Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð², Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð½Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¸, Ñ Ñ€Ð°Ð¼ÐºÐ¾Ð¹ Ð²Ð¾ÐºÑ€ÑƒÐ³ Ð¾Ð±ÑŠÐµÐºÑ‚Ð° Ð¸ Ð¾Ñ†ÐµÐ½Ðº..."
          ],
          [
           ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> model = AutoModel.from_pretra..."
          ],
          [
           "```\n\nÐ Ð²Ð¾Ñ‚ ÑÐºÐ²Ð¸Ð²Ð°Ð»ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÐºÐ¾Ð´ Ð´Ð»Ñ TensorFlow:\n```python\n>>> from transformers import AutoTokenizer, T..."
          ],
          [
           "```\n\nÐ¢Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð·Ð° Ð²ÑÑŽ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ, ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð°Ñ..."
          ],
          [
           "Ð¡Ð°Ð¼Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¾Ð±Ð¾Ð¹ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html..."
          ],
          [
           "## ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ transformers?\n\n1. ÐŸÑ€Ð¾ÑÑ‚Ñ‹Ðµ Ð² Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸:\n    ..."
          ],
          [
           "1. Ð‘Ð¾Ð»ÐµÐµ Ð½Ð¸Ð·ÐºÐ¸Ðµ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚Ñ‹, Ð¼ÐµÐ½ÑŒÑˆÐ¸Ð¹ \"ÑƒÐ³Ð»ÐµÑ€Ð¾Ð´Ð½Ñ‹Ð¹ ÑÐ»ÐµÐ´\":\n    - Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¾Ð±Ð¼ÐµÐ½Ð¸Ð²..."
          ],
          [
           "1. Ð’Ñ‹Ð±Ð¾Ñ€ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰ÐµÐ³Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€ÐºÐ° Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÑ‚Ð°Ð¿Ð° Ð¶Ð¸Ð·Ð½Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸:\n    - ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ°Ð¼Ñ‹Ñ… ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´..."
          ],
          [
           "1. Ð›ÐµÐ³ÐºÐ¾ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¿Ð¾Ð´ ÑÐ²Ð¾Ð¸ Ð½ÑƒÐ¶Ð´Ñ‹:\n    - ÐœÑ‹ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð°Ñ€Ñ…Ð¸Ñ‚..."
          ],
          [
           "- Ð”Ð°Ð½Ð½Ð°Ñ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ° Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð¼Ð¾Ð´ÑƒÐ»ÑŒÐ½Ñ‹Ð¼ Ð½Ð°Ð±Ð¾Ñ€Ð¾Ð¼ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð±Ð»Ð¾ÐºÐ¾Ð² Ð´Ð»Ñ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹. ÐšÐ¾Ð´ Ð² Ñ„Ð°Ð¹..."
          ],
          [
           "- ÐÐµÑÐ¼Ð¾Ñ‚Ñ€Ñ Ð½Ð° Ñ‚Ð¾, Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ ÑÑ‚Ñ€ÐµÐ¼Ð¸Ð¼ÑÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÐ°Ðº Ð¼Ð¾Ð¶Ð½Ð¾ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð² Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ, ÑÐºÑ€Ð¸Ð¿Ñ‚Ñ‹ Ð² Ð½Ð°..."
          ],
          [
           "## Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°\n\n### Ð¡ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ pip\n\nÐ”Ð°Ð½Ð½Ñ‹Ð¹ Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð½Ð° Python 3.8+, Flax 0.4.1+, PyTor..."
          ],
          [
           "Ð—Ð°Ñ‚ÐµÐ¼ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ Ð¾Ð´Ð¸Ð½ Ð±ÐµÐºÐµÐ½Ð´ Ð¸Ð· Flax, PyTorch Ð¸Ð»Ð¸ TensorFlow.\nÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ‚..."
          ],
          [
           "```\n\nÐ•ÑÐ»Ð¸ Ð²Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¿Ð¾Ð¸Ð³Ñ€Ð°Ñ‚ÑŒ Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ð¼Ð¸ Ð¸Ð»Ð¸ Ð²Ð°Ð¼ Ð½ÑƒÐ¶ÐµÐ½ ÑÐ°Ð¼Ñ‹Ð¹ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ ÐºÐ¾Ð´ Ð¸ Ð²Ñ‹ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð¶Ð´Ð°Ñ‚ÑŒ Ð½Ð¾..."
          ],
          [
           "```\n\nÐž Ñ‚Ð¾Ð¼, ÐºÐ°Ðº ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Flax, PyTorch Ð¸Ð»Ð¸ TensorFlow Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ conda, Ñ‡Ð¸Ñ‚Ð°Ð¹Ñ‚Ðµ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°Ñ…, Ð¿Ð¾ÑÐ²Ñ..."
          ],
          [
           "## ÐœÐ¾Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñ‹\n\n**[Ð’ÑÐµ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹](https://huggingface.co/models)**, Ð¿Ñ€ÐµÐ´Ð¾Ñ..."
          ],
          [
           "1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and..."
          ],
          [
           "1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua Un..."
          ],
          [
           "1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (from VinAI Research) r..."
          ],
          [
           "1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (from G..."
          ],
          [
           "1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (from Google AI) released with ..."
          ],
          [
           "1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (from Salesforce) release..."
          ],
          [
           "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) releas..."
          ],
          [
           "1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with..."
          ],
          [
           "1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (from MetaAI) rele..."
          ],
          [
           "1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (from Facebook AI) re..."
          ],
          [
           "1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)** (from Salesforce) released wi..."
          ],
          [
           "1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) ..."
          ],
          [
           "1. **[DePlot](https://huggingface.co/docs/transformers/model_doc/deplot)** (from Google AI) released..."
          ],
          [
           "1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)** (from SHI Labs) released wi..."
          ],
          [
           "1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace..."
          ],
          [
           "1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (from Facebook) released with t..."
          ],
          [
           "1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/S..."
          ],
          [
           "1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (from Baidu) released wi..."
          ],
          [
           "1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovati..."
          ],
          [
           "1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) rele..."
          ],
          [
           "1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (from Google Research) releas..."
          ],
          [
           "1. **[GIT](https://huggingface.co/docs/transformers/model_doc/git)** (from Microsoft Research) relea..."
          ],
          [
           "1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from EleutherAI) rel..."
          ],
          [
           "1. **[GPT-Sw3](https://huggingface.co/docs/transformers/model_doc/gpt-sw3)** (from AI-Sweden) releas..."
          ],
          [
           "1. **[GPTSAN-japanese](https://huggingface.co/docs/transformers/model_doc/gptsan-japanese)** release..."
          ],
          [
           "1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released ..."
          ],
          [
           "1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang Univers..."
          ],
          [
           "1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft R..."
          ],
          [
           "1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (from Meta AI) released wit..."
          ],
          [
           "1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Me..."
          ],
          [
           "1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** (from AllenAI) re..."
          ],
          [
           "1. **[M-CTC-T](https://huggingface.co/docs/transformers/model_doc/mctct)** (from Facebook) released ..."
          ],
          [
           "1. **[MarkupLM](https://huggingface.co/docs/transformers/model_doc/markuplm)** (from Microsoft Resea..."
          ],
          [
           "1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released wi..."
          ],
          [
           "1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDI..."
          ],
          [
           "1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (from CMU/Google ..."
          ],
          [
           "1. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)** (from Apple) re..."
          ],
          [
           "1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released ..."
          ],
          [
           "1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (from Meta) released ..."
          ],
          [
           "1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) release..."
          ],
          [
           "1. **[Persimmon](https://huggingface.co/docs/transformers/main/model_doc/persimmon)** (from ADEPT) r..."
          ],
          [
           "1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (from Google) rel..."
          ],
          [
           "1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (from Microsoft R..."
          ],
          [
           "1. **[REALM](https://huggingface.co/docs/transformers/model_doc/realm.html)** (from Google Research)..."
          ],
          [
           "1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), releas..."
          ],
          [
           "1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (from Bo Peng), released on [..."
          ],
          [
           "1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** (from ASAPP) released with ..."
          ],
          [
           "1. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** (from Tel Aviv Univer..."
          ],
          [
           "1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Micros..."
          ],
          [
           "1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released..."
          ],
          [
           "1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transf..."
          ],
          [
           "1. **[TVLT](https://huggingface.co/docs/transformers/model_doc/tvlt)** (from UNC Chapel Hill) releas..."
          ],
          [
           "1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (from Micros..."
          ],
          [
           "1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Ente..."
          ],
          [
           "1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) ..."
          ],
          [
           "1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (from Meta AI) released ..."
          ],
          [
           "1. **[Wav2Vec2-Conformer](https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer)** (..."
          ],
          [
           "1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (from Microsoft Research) ..."
          ],
          [
           "1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released togeth..."
          ],
          [
           "1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (from Meta AI) released wit..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Faceb..."
          ],
          [
           "Ð§Ñ‚Ð¾Ð±Ñ‹ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñƒ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½Ð° Flax, PyTorch Ð¸Ð»Ð¸ TensorFlow, Ð¸Ð»Ð¸ ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ð¹ Ñ..."
          ],
          [
           "| Ð¡ÐµÐºÑ†Ð¸Ñ | ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ |\n|-|-|\n| [Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ](https://huggingface.co/docs/transformers/) | ÐŸÐ¾Ð»Ð½Ð°Ñ Ð´Ð¾Ðº..."
          ],
          [
           "| [Ð¡Ð¾Ð²Ð¼ÐµÑÑ‚Ð½Ð¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹](https://huggingface.co/docs/transformers/model_shari..."
          ],
          [
           "## Ð¦Ð¸Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ\n\nÐ¢ÐµÐ¿ÐµÑ€ÑŒ Ñƒ Ð½Ð°Ñ ÐµÑÑ‚ÑŒ [ÑÑ‚Ð°Ñ‚ÑŒÑ](https://www.aclweb.org/anthology/2020.emnlp-demos.6/), ÐºÐ¾..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\n\nTo share a model with the community, you need an account on [huggingface.co](https://huggingf..."
          ],
          [
           "```\n\nFiles are also easily edited in a repository, and you can view the commit history as well as th..."
          ],
          [
           "```\n\n## Convert a model for all frameworks\n\nTo ensure your model can be used by someone working with..."
          ],
          [
           "```\n</jax>\n</frameworkcontent>\n\n## Push a model during training\n\n<frameworkcontent>\n<pt>\n<Youtube id..."
          ],
          [
           "```\n</pt>\n<tf>\nShare a model to the Hub with [`PushToHubCallback`]. In the [`PushToHubCallback`] fun..."
          ],
          [
           "```\n\nThe `push_to_hub` function can also be used to add other files to a model repository. For examp..."
          ],
          [
           "```\n\nNow when you navigate to your Hugging Face profile, you should see your newly created model rep..."
          ],
          [
           "* Manually creating and uploading a `README.md` file.\n* Clicking on the **Edit model card** button i..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We present a convolution-free approach to video clas..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Large-scale NLP models have been shown to significan..."
          ],
          [
           "A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started wit..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We re-evaluate the standard practice of sharing weig..."
          ],
          [
           "## RemBertConfig\n\n[[autodoc]] RemBertConfig\n\n## RemBertTokenizer\n\n[[autodoc]] RemBertTokenizer\n    -..."
          ],
          [
           "## TFRemBertForMaskedLM\n\n[[autodoc]] TFRemBertForMaskedLM\n    - call\n\n## TFRemBertForCausalLM\n\n[[aut..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model augments the Transformer as a deep decomposition architecture, which can progressively de..."
          ],
          [
           "- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Vision-and-language reasoning requires an understand..."
          ],
          [
           "## Usage tips\n\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any k..."
          ],
          [
           "[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n..."
          ],
          [
           "Author: [@vasudevgupta7](https://github.com/thevasudevgupta/)\n\n## Intro\n\nIn this project, we fine-tu..."
          ],
          [
           "```\n\n## Evaluation\n\nOur evaluation script is different from the original script and we are evaluatin..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "| Notebook     |      Description      |   |   |\n|:----------|:-------------|:-------------|------:|..."
          ],
          [
           "| [Preprocessing data](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/prepro..."
          ],
          [
           "| [Summary of the tokenizers](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en..."
          ],
          [
           "### PyTorch Examples\n\n#### Natural Language Processing[[pytorch-nlp]]..."
          ],
          [
           "| Notebook     |      Description      |   |   |\n|:----------|:-------------|:-------------|------:|..."
          ],
          [
           "| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/ma..."
          ],
          [
           "| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/m..."
          ],
          [
           "| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/e..."
          ],
          [
           "| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/exa..."
          ],
          [
           "| [How to generate text](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate...."
          ],
          [
           "| [Reformer](https://github.com/huggingface/blog/blob/main/notebooks/03_reformer.ipynb)| How Reforme..."
          ],
          [
           "#### Computer Vision[[pytorch-cv]]..."
          ],
          [
           "| Notebook                                                                                          ..."
          ],
          [
           "|:--------------------------------------------------------------------------------------------------..."
          ],
          [
           "| [How to fine-tune a model on image classification (Torchvision)](https://github.com/huggingface/no..."
          ],
          [
           "| [How to fine-tune a model on image classification (Kornia)](https://github.com/huggingface/noteboo..."
          ],
          [
           "| [How to fine-tune an image captioning model](https://github.com/huggingface/notebooks/blob/main/ex..."
          ],
          [
           "| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/noteb..."
          ],
          [
           "#### Audio[[pytorch-audio]]..."
          ],
          [
           "| Notebook     |      Description      |   |   |\n|:----------|:-------------|:-------------|------:|..."
          ],
          [
           "| [How to fine-tune a speech recognition model in any language](https://github.com/huggingface/noteb..."
          ],
          [
           "#### Biological Sequences[[pytorch-bio]]..."
          ],
          [
           "| Notebook     | Description                                                                        ..."
          ],
          [
           "| [How to generate protein folds](https://github.com/huggingface/notebooks/blob/main/examples/protei..."
          ],
          [
           "| [Fine-tune a Nucleotide Transformer model with LoRA](https://github.com/huggingface/notebooks/blob..."
          ],
          [
           "#### Other modalities[[pytorch-other]]\n\n| Notebook     | Description                                ..."
          ],
          [
           "#### Utility notebooks[[pytorch-utility]]\n\n| Notebook     |      Description      |   |   |\n|:------..."
          ],
          [
           "| Notebook     |      Description      |   |   |\n|:----------|:-------------|:-------------|------:|..."
          ],
          [
           "| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/ma..."
          ],
          [
           "| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/m..."
          ],
          [
           "| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/e..."
          ],
          [
           "| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/exa..."
          ],
          [
           "#### Computer Vision[[tensorflow-cv]]..."
          ],
          [
           "| Notebook                                                                                          ..."
          ],
          [
           "| [How to fine-tune a model on image classification](https://github.com/huggingface/notebooks/blob/m..."
          ],
          [
           "#### Biological Sequences[[tensorflow-bio]]\n\n| Notebook     |      Description      |   |   |\n|:----..."
          ],
          [
           "#### Utility notebooks[[tensorflow-utility]]\n\n| Notebook     |      Description      |   |          ..."
          ],
          [
           "| Notebook     |      Description      |   |   |\n|:----------|:-------------|:-------------|------:|..."
          ],
          [
           "| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/..."
          ],
          [
           "| [How to fine-tune a model on text classification with ONNX Runtime](https://github.com/huggingface..."
          ],
          [
           "| [How to fine-tune a model on summarization with ONNX Runtime](https://github.com/huggingface/noteb..."
          ],
          [
           "## Community notebooks:\n\nMore notebooks developed by the community are available [here](https://hf.c..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip warning={true}>\n\nThe [`Trainer`] class is optimized for ðŸ¤— Transformers models and can have surp..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformers-based models, such as BERT, have been o..."
          ],
          [
           "## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classifi..."
          ],
          [
           "## BigBirdForMultipleChoice\n\n[[autodoc]] BigBirdForMultipleChoice\n    - forward\n\n## BigBirdForTokenC..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "In terms of model details, the work outlines the architecture and training methodology of Persimmon-..."
          ],
          [
           "</Tip>\n\n\nTips:\n\n- To convert the model, you need to clone the original repository using `git clone h..."
          ],
          [
           "```\n\nFor the chat model:\n```bash\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-o..."
          ],
          [
           "*TEMPLATE**\n=====================================\n\n*search & replace the following keywords, e.g.:*\n..."
          ],
          [
           "To start, let's try to get a general overview of the Transformers\nlibrary.\n\nGeneral overview of ðŸ¤— Tr..."
          ],
          [
           "Let's take a look:\n\n![image](https://huggingface.co/datasets/huggingface/documentation-images/resolv..."
          ],
          [
           "```\n\nSimilar to the model, the configuration inherits basic serialization and\ndeserialization functi..."
          ],
          [
           "From experience, we can tell you that the most important things to keep\nin mind when adding a model ..."
          ],
          [
           "7.  [ ] Successfully ran forward pass in Transformers that gives\n    identical output to original ch..."
          ],
          [
           "-   What type of model is *[camelcase name of model]*? BERT-like encoder-only\n    model? GPT2-like d..."
          ],
          [
           "If any of the mentioned aspects above are **not** clear to you, now is a great time to talk to [name..."
          ],
          [
           "```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    `..."
          ],
          [
           "```\n\nNow you have set up a development environment to port *[camelcase name of model]*\nto ðŸ¤— Transfor..."
          ],
          [
           "-   Where to find the pretrained weights?\n-   How to load the pretrained weights into the correspond..."
          ],
          [
           "Jupyter notebooks have the advantage that they allow for cell-by-cell\nexecution which can be helpful..."
          ],
          [
           "```\n\nNext, regarding the debugging strategy, there are generally a few from\nwhich to choose from:\n\n-..."
          ],
          [
           "However, if the original code-base is very complex or only allows\nintermediate components to be run ..."
          ],
          [
           "The outputs of the following layers often consist of multi-dimensional\nfloat arrays and can look lik..."
          ],
          [
           "```\n\nWe expect that every model added to ðŸ¤— Transformers passes a couple of\nintegration tests, meanin..."
          ],
          [
           "-   Find the best way of debugging intermediate results. Is the original\n    repository written in P..."
          ],
          [
           "`autoregressive_sample`, `generate`.\n-   Try to separate the tokenization from the model's\n    forwa..."
          ],
          [
           "#### More details on how to create a debugging environment for [camelcase name of model] \n\n[TODO FIL..."
          ],
          [
           "```\n    git checkout -b add_[lowercase name of model]\n```\n\n2.  Commit the automatically generated co..."
          ],
          [
           "```\n\n5.  Once you are satisfied, go to the webpage of your fork on GitHub.\n    Click on \"Pull reques..."
          ],
          [
           "**5. Adapt the generated models code for [camelcase name of model]**\n\nAt first, we will focus only o..."
          ],
          [
           "```python\nfrom transformers import [camelcase name of model]Model, [camelcase name of model]Config\nm..."
          ],
          [
           "```\n\nThe above command will create a model according to the default\nparameters as defined in `[camel..."
          ],
          [
           "In the following, we'll quickly explain how PyTorch models store layer\nweights and define layer name..."
          ],
          [
           "```\n\nNow we can create an instance of this model definition which will fill\nall weights: `dense`, `i..."
          ],
          [
           "```\n\nto see that the weights were randomly initialized..."
          ],
          [
           "```bash\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\n         -0..."
          ],
          [
           "[-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,\n          0.1829, -0.1568],..."
          ],
          [
           "```\n\nIn the conversion script, you should fill those randomly initialized\nweights with the exact wei..."
          ],
          [
           "```\n\nIf either the shape or the name doesn't match, you probably assigned\nthe wrong checkpoint weigh..."
          ],
          [
           "```\n\n[TODO FILL: Here the mentor should add very specific information on what exactly has to be done..."
          ],
          [
           "```\n\nIt is very likely that the ðŸ¤— Transformers implementation and the\noriginal model implementation ..."
          ],
          [
           "The best way to fix the problem is usually to look at the forward pass\nof the original implementatio..."
          ],
          [
           "```\n\n[TODO FILL: Here the mentor should add very specific information on what tests are likely to fa..."
          ],
          [
           "```\n\n**Note:** In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\n..."
          ],
          [
           "```\n\nYou might have to take a deeper look again into the original repository\nto find the correct tok..."
          ],
          [
           "```\n\nWhen both `input_ids` yield the same values, as a final step a tokenizer\ntest file should also ..."
          ],
          [
           "**11. Add Docstring**\n\nNow, all the necessary functionality for *[camelcase name of model]* is added..."
          ],
          [
           "```\n\nand verify that your coding style passes the quality check:\n\n```bash\nmake quality..."
          ],
          [
           "```\n\nThere are a couple of other very strict design tests in ðŸ¤— Transformers\nthat might still be fail..."
          ],
          [
           "### Share your work!!\n\nNow, it's time to get some credit from the community for your work!\nHaving co..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nFor Chinese models, we need to generate a reference files (which requires the ltp library), bec..."
          ],
          [
           "```\n\nThen you can run the script like this: \n\n\n```bash\nexport TRAIN_FILE=/path/to/train/file\nexport ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Implementation Notes\n\n- Each model is about 298 MB on disk, there are more than 1,000 models.\n- T..."
          ],
          [
           "## Examples\n\n- Since Marian models are smaller than many other translation models available in the l..."
          ],
          [
           ">>> model_name = \"Helsinki-NLP/opus-mt-en-roa\"\n>>> tokenizer = MarianTokenizer.from_pretrained(model..."
          ],
          [
           "```\n\nHere is the code to see all available pretrained models on the hub:\n\n```python\nfrom huggingface..."
          ],
          [
           "```\n\n## Old Style Multi-Lingual Models\n\nThese are the old style multi-lingual models ported from the..."
          ],
          [
           "```python no-style\n['Helsinki-NLP/opus-mt-NORTH_EU-NORTH_EU',\n 'Helsinki-NLP/opus-mt-ROMANCE-en',\n '..."
          ],
          [
           "'ROMANCE': ['fr', 'fr_BE', 'fr_CA', 'fr_FR', 'wa', 'frp', 'oc', 'ca', 'rm', 'lld', 'fur', 'lij', 'lm..."
          ],
          [
           "```\n\nExample of translating english to many romance languages, using old-style 2 character language ..."
          ],
          [
           "```\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage tips\n\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Att..."
          ],
          [
           "## Wav2Vec2ConformerForSequenceClassification\n\n[[autodoc]] Wav2Vec2ConformerForSequenceClassificatio..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "TAPEX has been fine-tuned on several datasets: \n- [SQA](https://www.microsoft.com/en-us/download/det..."
          ],
          [
           "## Usage tips\n\n- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPE..."
          ],
          [
           ">>> # prepare table + question\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clo..."
          ],
          [
           "```\n\nNote that [`TapexTokenizer`] also supports batched inference. Hence, one can provide a batch of..."
          ],
          [
           "```\n\nIn case one wants to do table verification (i.e. the task of determining whether a given senten..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--..."
          ],
          [
           "```\n\nYou will use [PyTorchVideo](https://pytorchvideo.org/) (dubbed `pytorchvideo`) to process and p..."
          ],
          [
           "```\n\nAt a high level, the dataset is organized like so:\n\n```bash\nUCF101_subset/\n    train/\n        B..."
          ],
          [
           "```\n\nYou will notice that there are video clips belonging to the same group / scene where group is d..."
          ],
          [
           "```\n\nThere are 10 unique classes. For each class, there are 30 videos in the training set.\n\n## Load ..."
          ],
          [
           "```\n\nWhile the model is loading, you might notice the following warning:\n\n```bash\nSome weights of th..."
          ],
          [
           "```\n\nThe warning is telling us we are throwing away some weights (e.g. the weights and bias of the `..."
          ],
          [
           "```\n\nFor the training dataset transformations, use a combination of uniform temporal subsampling, pi..."
          ],
          [
           "```\n\nNow, define the dataset-specific transformations and the datasets respectively. Starting with t..."
          ],
          [
           "```\n\nThe same sequence of workflow can be applied to the validation and evaluation sets: \n\n```py \n>>..."
          ],
          [
           "```\n\n**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https:..."
          ],
          [
           "```\n\n## Visualize the preprocessed video for better debugging \n\n```py \n>>> import imageio\n>>> import..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           ">>> args = TrainingArguments(\n...     new_model_name,\n...     remove_unused_columns=False,\n...     e..."
          ],
          [
           "```\n\nThe dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__` method. As..."
          ],
          [
           "```\n\nThen you just pass all of this along with the datasets to `Trainer`:\n\n```py \n>>> trainer = Trai..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nYou can also manually replicate the results of the `pipeline` if you'd like.\n\n\n```py\n>>> def ru..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Natural language understanding comprises a wide rang..."
          ],
          [
           "Note:\n\nIf you want to reproduce the original tokenization process of the *OpenAI GPT* paper, you wil..."
          ],
          [
           "```\n\nIf you don't install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\nus..."
          ],
          [
           "- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https://www.philschmid.de/..."
          ],
          [
           "- [`OpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https://gi..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\"/>\n\n- A course material on [Byte-Pair Encoding tokenizat..."
          ],
          [
           "## TFOpenAIGPTDoubleHeadsModel\n\n[[autodoc]] TFOpenAIGPTDoubleHeadsModel\n    - call\n\n## TFOpenAIGPTFo..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-..."
          ],
          [
           "</a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">\n       ..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/\">English</..."
          ],
          [
           "ðŸ¤— Transformers æä¾›äº†æ•°ä»¥åƒè®¡çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œæ”¯æŒ 100 å¤šç§è¯­è¨€çš„æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æŠ½å–ã€é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€‚å®ƒçš„å®—æ—¨æ˜¯è®©æœ€å…ˆè¿›çš„ NLP æŠ€æœ¯äººäººæ˜“ç”¨ã€‚\n\nðŸ¤— Transform..."
          ],
          [
           "è¿™é‡Œæ˜¯ä¸€äº›ä¾‹å­ï¼š\n- [ç”¨ BERT åšæŽ©ç å¡«è¯](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F..."
          ],
          [
           "- [ç”¨ RoBERTa åšè‡ªç„¶è¯­è¨€æŽ¨ç†](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+a..."
          ],
          [
           "- [ç”¨ DistilBERT..."
          ],
          [
           "åšé—®ç­”](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+..."
          ],
          [
           "used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+..."
          ],
          [
           "uese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa..."
          ],
          [
           "n%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+..."
          ],
          [
           "d%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+c..."
          ],
          [
           "t+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square..."
          ],
          [
           "0+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100..."
          ],
          [
           "82%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+n..."
          ],
          [
           "ing+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rain..."
          ],
          [
           "the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Ve..."
          ],
          [
           "ts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments..."
          ],
          [
           "artments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+t..."
          ],
          [
           "alf+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+o..."
          ],
          [
           "+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided..."
          ],
          [
           "+divided+into+16%2C000+species)..."
          ],
          [
           "- [ç”¨ T5 åšç¿»è¯‘](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)..."
          ],
          [
           "**[Write With Transformer](https://transformer.huggingface.co)**ï¼Œç”±æŠ±æŠ±è„¸å›¢é˜Ÿæ‰“é€ ï¼Œæ˜¯ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„å®˜æ–¹ demoã€‚\n\n## å¦‚æžœä½ åœ¨å¯»..."
          ],
          [
           "```\n\nç¬¬äºŒè¡Œä»£ç ä¸‹è½½å¹¶ç¼“å­˜äº†æµæ°´çº¿ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œè€Œç¬¬ä¸‰è¡Œä»£ç åˆ™åœ¨ç»™å®šçš„æ–‡æœ¬ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚è¿™é‡Œçš„ç­”æ¡ˆâ€œæ­£é¢â€ (positive) å…·æœ‰ 99 çš„ç½®ä¿¡åº¦ã€‚\n\nè®¸å¤šçš„ NLP ä»»åŠ¡éƒ½æœ‰å¼€ç®±å³ç”¨çš„é¢„..."
          ],
          [
           "```\nè¿™é‡Œæ˜¯ç­‰æ•ˆçš„ TensorFlow ä»£ç ï¼š\n```python\n>>> from transformers import AutoTokenizer, TFAutoModel\n\n>>> tok..."
          ],
          [
           "```\n\nè¯ç¬¦åŒ–å™¨ (tokenizer) ä¸ºæ‰€æœ‰çš„é¢„è®­ç»ƒæ¨¡åž‹æä¾›äº†é¢„å¤„ç†ï¼Œå¹¶å¯ä»¥ç›´æŽ¥å¯¹å•ä¸ªå­—ç¬¦ä¸²è¿›è¡Œè°ƒç”¨ï¼ˆæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼‰æˆ–å¯¹åˆ—è¡¨ (list) è°ƒç”¨ã€‚å®ƒä¼šè¾“å‡ºä¸€ä¸ªä½ å¯ä»¥åœ¨ä¸‹æ¸¸ä»£ç é‡Œä½¿ç”¨æˆ–ç›´æŽ¥é€šè¿‡ ..."
          ],
          [
           "1. å¯¹äºŽæ¨¡åž‹ç”Ÿå‘½å‘¨æœŸçš„æ¯ä¸€ä¸ªéƒ¨åˆ†éƒ½é¢é¢ä¿±åˆ°ï¼š\n    - è®­ç»ƒå…ˆè¿›çš„æ¨¡åž‹ï¼Œåªéœ€ 3 è¡Œä»£ç \n    - æ¨¡åž‹åœ¨ä¸åŒæ·±åº¦å­¦ä¹ æ¡†æž¶é—´ä»»æ„è½¬ç§»ï¼Œéšä½ å¿ƒæ„\n    - ä¸ºè®­ç»ƒã€è¯„ä¼°å’Œç”Ÿäº§é€‰æ‹©æœ€é€‚åˆçš„æ¡†æž¶ï¼Œè¡”..."
          ],
          [
           "è¿™ä¸ªä»“åº“å·²åœ¨ Python 3.8+ã€Flax 0.4.1+ã€PyTorch 1.10+ å’Œ TensorFlow 2.6+ ä¸‹ç»è¿‡æµ‹è¯•ã€‚\n\nä½ å¯ä»¥åœ¨[è™šæ‹ŸçŽ¯å¢ƒ](https://docs.pytho..."
          ],
          [
           "```\n\nå¦‚æžœä½ æƒ³è¦è¯•è¯•ç”¨ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼å‘å¸ƒå‰ä½¿ç”¨æœ€æ–°çš„å¼€å‘ä¸­ä»£ç ï¼Œä½ å¾—[ä»Žæºä»£ç å®‰è£…](https://huggingface.co/docs/transformers/installation#i..."
          ],
          [
           "```\n\nè¦é€šè¿‡ conda å®‰è£… Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè¯·å‚é˜…å®ƒä»¬å„è‡ªå®‰è£…é¡µçš„è¯´æ˜Žã€‚\n\n## æ¨¡åž‹æž¶æž„\n\nðŸ¤— Transformers æ”¯æŒçš„[**æ‰€æœ‰çš„æ¨¡åž‹..."
          ],
          [
           "1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (æ¥è‡ª Google Research and t..."
          ],
          [
           "1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua Un..."
          ],
          [
           "1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (æ¥è‡ª VinAI Research) ä¼´éšè®º..."
          ],
          [
           "1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (æ¥è‡ª Goo..."
          ],
          [
           "1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (æ¥è‡ª Google AI) ä¼´éšè®ºæ–‡ [Big Transf..."
          ],
          [
           "1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (æ¥è‡ª Salesforce) ä¼´éšè®ºæ–‡ [BLI..."
          ],
          [
           "1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (æ¥è‡ª NAVER CLOVA) ä¼´éšè®ºæ–‡ [BROS: ..."
          ],
          [
           "1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (æ¥è‡ª OFA-Sys) ..."
          ],
          [
           "1. **[CLVP](https://huggingface.co/docs/transformers/model_doc/clvp)** released with the paper [Bett..."
          ],
          [
           "1. **[Conditional DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)** (æ¥è‡ª M..."
          ],
          [
           "1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)** (æ¥è‡ª Tsinghua University) ä¼´éšè®ºæ–‡ [..."
          ],
          [
           "1. **[Data2Vec](https://huggingface.co/docs/transformers/model_doc/data2vec)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [D..."
          ],
          [
           "1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (æ¥è‡ª Sen..."
          ],
          [
           "1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [End-to-En..."
          ],
          [
           "1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (æ¥è‡ª Meta AI) ä¼´éšè®ºæ–‡ [DINOv2..."
          ],
          [
           "1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (æ¥è‡ª Microsoft Research) ä¼´éšè®ºæ–‡ [D..."
          ],
          [
           "1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (æ¥è‡ª Sna..."
          ],
          [
           "1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (æ¥è‡ª Goog..."
          ],
          [
           "1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer ..."
          ],
          [
           "1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) releas..."
          ],
          [
           "1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (æ¥è‡ª CNRS) ä¼´éšè®ºæ–‡ [FlauB..."
          ],
          [
           "1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (æ¥è‡ª CMU/Googl..."
          ],
          [
           "1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (æ¥è‡ª OpenAI) ä¼´éšè®ºæ–‡ [Improv..."
          ],
          [
           "1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (æ¥è‡ª OpenAI) ä¼´éšè®ºæ–‡ [Language M..."
          ],
          [
           "1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (æ¥è‡ª BigCode) ä¼´éšè®º..."
          ],
          [
           "1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (æ¥è‡ª UCSD, NVIDIA) ä¼´éšè®º..."
          ],
          [
           "1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) rele..."
          ],
          [
           "1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (æ¥è‡ª Salesforc..."
          ],
          [
           "1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (æ¥è‡ª Microsoft Res..."
          ],
          [
           "1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (æ¥è‡ª Meta AI) ä¼´éšè®ºæ–‡ [LeViT: A..."
          ],
          [
           "1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (æ¥è‡ª The FAIR team of Meta..."
          ],
          [
           "1. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)** (æ¥è‡ª Microsoft Research & Un..."
          ],
          [
           "1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (æ¥è‡ª UNC Chapel Hill) ä¼´éšè®ºæ–‡..."
          ],
          [
           "1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) rel..."
          ],
          [
           "1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (from Meta and UI..."
          ],
          [
           "1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [Mega: Mov..."
          ],
          [
           "1. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)** (from Mistral AI) by Th..."
          ],
          [
           "1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [Scaling Spe..."
          ],
          [
           "1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (æ¥è‡ª Google Inc..."
          ],
          [
           "1. **[MRA](https://huggingface.co/docs/transformers/model_doc/mra)** (æ¥è‡ª the University of Wisconsin..."
          ],
          [
           "1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)** (æ¥è‡ª SHI Labs) ä¼´éšè®ºæ–‡ [Neighborhoo..."
          ],
          [
           "1. **[NystrÃ¶mformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (æ¥è‡ª the Uni..."
          ],
          [
           "1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (æ¥è‡ª Google AI) ä¼´éšè®ºæ–‡ [Sim..."
          ],
          [
           "1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (æ¥è‡ª Google) ä¼´éšè®ºæ–‡ [PEGAS..."
          ],
          [
           "1. **[Phi](https://huggingface.co/docs/transformers/model_doc/phi)** (from Microsoft) released with ..."
          ],
          [
           "1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (æ¥è‡ª UCLA NLP) ä¼´éšè®ºæ–‡ [Unifi..."
          ],
          [
           "1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (æ¥è‡ª Nanjing University, The Uni..."
          ],
          [
           "1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (æ¥è‡ª Google Research) ..."
          ],
          [
           "1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (æ¥è‡ª Facebook), ä¼´éšè®ºæ–‡ [Ro..."
          ],
          [
           "1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (æ¥è‡ª Bo Peng) ä¼´éšè®ºæ–‡ [this repo]..."
          ],
          [
           "1. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (æ¥è‡ª Meta AI) ä¼´éšè®ºæ–‡ ..."
          ],
          [
           "1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** ..."
          ],
          [
           "1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (æ¥è‡ª MBZUAI) ä¼´éšè®º..."
          ],
          [
           "1. **[SwitchTransformers](https://huggingface.co/docs/transformers/model_doc/switch_transformers)** ..."
          ],
          [
           "1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (æ¥è‡ª Google AI) ä¼´éšè®ºæ–‡ [TAPAS:..."
          ],
          [
           "1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (æ¥è‡ª Google/CM..."
          ],
          [
           "1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (from Google Research) released..."
          ],
          [
           "1. **[UnivNet](https://huggingface.co/docs/transformers/model_doc/univnet)** (from Kakao Corporation..."
          ],
          [
           "1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (æ¥è‡ª NAVER AI Lab/Kakao Enterp..."
          ],
          [
           "1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (æ¥è‡ª Google AI) ä¼´éš..."
          ],
          [
           "1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (æ¥è‡ª Meta AI) ä¼´éšè®ºæ–‡ [Maske..."
          ],
          [
           "1. **[Wav2Vec2-Conformer](https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer)** (..."
          ],
          [
           "1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (æ¥è‡ª Microsoft Research) ä¼´éš..."
          ],
          [
           "1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [Cross-lingu..."
          ],
          [
           "1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (æ¥è‡ª Meta AI) ä¼´éšè®ºæ–‡ [XLM-V: O..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (æ¥è‡ª Faceboo..."
          ],
          [
           "è¦æ£€æŸ¥æŸä¸ªæ¨¡åž‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å®žçŽ°ï¼Œæˆ–å…¶æ˜¯å¦åœ¨ ðŸ¤— Tokenizers åº“ä¸­æœ‰å¯¹åº”è¯ç¬¦åŒ–å™¨ï¼ˆtokenizerï¼‰ï¼Œæ•¬è¯·å‚é˜…[æ­¤è¡¨](https://h..."
          ],
          [
           "## å¼•ç”¨\n\næˆ‘ä»¬å·²å°†æ­¤åº“çš„[è®ºæ–‡](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)æ­£å¼å‘è¡¨ï¼Œå¦‚æžœä½ ä½¿ç”¨äº† ðŸ¤— Transformers åº“..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\n[Activation-aware Weight Quantization (AWQ)](https://hf.co/papers/2306.00978) doesn't quanti..."
          ],
          [
           "```\n\nAWQ-quantized models can be identified by checking the `quantization_config` attribute in the m..."
          ],
          [
           "```\n\nAWQ quantization can also be combined with [FlashAttention-2](perf_infer_gpu_one#flashattention..."
          ],
          [
           "```\n\n### Fused modules\n\nFused modules offers improved accuracy and performance and it is supported o..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"unsupported architectures\">\n\nFor architectures that don't support fus..."
          ],
          [
           "```\n\nThe parameter `modules_to_fuse` should include:\n\n- `\"attention\"`: The names of the attention la..."
          ],
          [
           "</Tip>\n\nThe [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) library implements the GPTQ algorithm, ..."
          ],
          [
           "```\n\nTo quantize a model (currently only supported for text models), you need to create a [`GPTQConf..."
          ],
          [
           "```\n\nIf you're running out of memory because a dataset is too large, disk offloading is not supporte..."
          ],
          [
           "```\n\nYou could also save your quantized model locally with the [`~PreTrainedModel.save_pretrained`] ..."
          ],
          [
           "```\n\n### ExLlama\n\n[ExLlama](https://github.com/turboderp/exllama) is a Python/C++/CUDA implementatio..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nOnly 4-bit models are supported, and we recommend deactivating the ExLlam..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"4-bit\">\n\n```bash\npip install bitsandbytes>=0.39.0\npip install --upgra..."
          ],
          [
           "```\n\nOnce a model is quantized to 8-bit, you can't push the quantized weights to the Hub unless you'..."
          ],
          [
           "```\n\nOnce a model is quantized to 4-bit, you can't push the quantized weights to the Hub.\n\n</hfoptio..."
          ],
          [
           "```\n\nDesign a custom device map to fit everything on your GPU except for the `lm_head`, which you'll..."
          ],
          [
           "```\n\n#### Skip module conversion\n\nFor some models, like [Jukebox](model_doc/jukebox), you don't need..."
          ],
          [
           "```\n\n#### Finetuning\n\nWith the [PEFT](https://github.com/huggingface/peft) library, you can finetune..."
          ],
          [
           "```\n\n#### Normal Float 4 (NF4)\n\nNF4 is a 4-bit data type from the [QLoRA](https://hf.co/papers/2305...."
          ],
          [
           "```\n\nFor inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, t..."
          ],
          [
           "```\n\n## Optimum\n\nThe [Optimum](https://huggingface.co/docs/optimum/index) library supports quantizat..."
          ],
          [
           "<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/hu..."
          ],
          [
           "The benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the ..."
          ],
          [
           "<figcaption class=\"text-center text-gray-500 text-lg\">Unfused module</figcaption>\n\n|   Batch Size | ..."
          ],
          [
           "<figcaption class=\"text-center text-gray-500 text-lg\">Fused module</figcaption>\n\n|   Batch Size |   ..."
          ],
          [
           "The speed and throughput of fused and unfused modules were also tested with the [optimum-benchmark](..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The model is trained end-to-end with a combination of losses derived from variational lower bound an..."
          ],
          [
           "## Usage examples\n\nBoth the VITS and MMS-TTS checkpoints can be used with the same API. Since the fl..."
          ],
          [
           "```\n\nThe resulting waveform can be saved as a `.wav` file:\n\n```python\nimport scipy\n\nscipy.io.wavfile..."
          ],
          [
           "```\n\nYou can then pre-process the text input using the following code snippet. You can either rely o..."
          ],
          [
           "```\n\n## VitsConfig\n\n[[autodoc]] VitsConfig\n\n## VitsTokenizer\n\n[[autodoc]] VitsTokenizer\n    - __call..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Overview\n\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learni..."
          ],
          [
           "This model was contributed by [stefan-it](https://huggingface.co/stefan-it). The original code can b..."
          ],
          [
           "<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on how to [finetune XLM RoBERTa for mul..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\"/>\n\n- [`XLMRobertaForTokenClassification`] is supported ..."
          ],
          [
           "<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`XLMRobertaForMaskedLM`] is supported by this [example scrip..."
          ],
          [
           "<PipelineTag pipeline=\"question-answering\"/>\n\n- [`XLMRobertaForQuestionAnswering`] is supported by t..."
          ],
          [
           "**Multiple choice**\n\n- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https:/..."
          ],
          [
           "[[autodoc]] XLMRobertaTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n## XLMRobertaModel\n\n[[autodoc]] XLMRob..."
          ],
          [
           "[[autodoc]] TFXLMRobertaForSequenceClassification\n    - call\n\n## TFXLMRobertaForMultipleChoice\n\n[[au..."
          ],
          [
           "## FlaxXLMRobertaForQuestionAnswering\n\n[[autodoc]] FlaxXLMRobertaForQuestionAnswering\n    - __call__..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/imagegpt_arc..."
          ],
          [
           "- ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that a different activati..."
          ],
          [
           "easily obtained by first forwarding the image through the model, then specifying `output_hidden_stat..."
          ],
          [
           "| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size** | **Params (M)** | **I..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Pop2Piano is an encoder-decoder Transformer model based on [T5](https://arxiv.org/pdf/1910.10683.pdf..."
          ],
          [
           "```\npip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy\n```\nPlease note that you ma..."
          ],
          [
           "```\n\n- Example using your own audio file:\n\n```python\n>>> import librosa\n>>> from transformers import..."
          ],
          [
           "```\n\n- Example of processing multiple audio files in batch:\n\n```python\n>>> import librosa\n>>> from t..."
          ],
          [
           "```\n\n\n- Example of processing multiple audio files in batch (Using `Pop2PianoFeatureExtractor` and `..."
          ],
          [
           ">>> # Since we now have 2 generated MIDI files\n>>> tokenizer_output[0].write(\"./Outputs/midi_output1..."
          ],
          [
           "```\n\n\n## Pop2PianoConfig\n\n[[autodoc]] Pop2PianoConfig\n\n## Pop2PianoFeatureExtractor\n\n[[autodoc]] Pop..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/deformable_d..."
          ],
          [
           "[[autodoc]] DeformableDetrImageProcessor\n    - preprocess\n    - post_process_object_detection\n\n## De..."
          ],
          [
           "### Fine-tuning BERT on SQuAD1.0 with relative position embeddings\n\nThe following examples show how ..."
          ],
          [
           "##### Base models fine-tuning\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\ntorchrun --nproc_..."
          ],
          [
           "```\nTraining with the above command leads to the following results. It boosts the BERT default from ..."
          ],
          [
           "```\nTraining with the above command leads to the f1 score of 93.52, which is slightly better than th..."
          ],
          [
           "```\n\n##### Results for SQuAD2.0 with the previously defined hyper-parameters:\n\n```python\n{\n\"exact\": ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Detection Transformer (DETR) directly transforms que..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Large Transformer models routinely achieve state-of-..."
          ],
          [
           "### Axial Positional Encodings\n\nAxial Positional Encodings were first implemented in Google's [trax ..."
          ],
          [
           "with:\n\n$$d = d^1 + d^2 \\text{ and } n_s = n_s^1 \\times n_s^2 .$$\n\nTherefore the following holds:\n\n$$..."
          ],
          [
           "In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple \\\\((d^1, d^2)\\\\) which sum..."
          ],
          [
           "For more information, see the [original Paper](https://arxiv.org/abs/2001.04451) or this great [blog..."
          ],
          [
           "### Local Self Attention\n\nLocal self attention is essentially a \"normal\" self attention layer with k..."
          ],
          [
           "```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Question ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## {{cookiecutter.camelcase_modelname}}TokenizerFast\n\n[[autodoc]] {{cookiecutter.camelcase_modelname..."
          ],
          [
           "[[autodoc]] {{cookiecutter.camelcase_modelname}}ForQuestionAnswering\n    - forward\n\n{%- else %}\n## {..."
          ],
          [
           "[[autodoc]] TF{{cookiecutter.camelcase_modelname}}ForCausalLM\n    - call\n\n\n## TF{{cookiecutter.camel..."
          ],
          [
           "{% if cookiecutter.is_encoder_decoder_model == \"False\" %}\n## Flax{{cookiecutter.camelcase_modelname}..."
          ],
          [
           "[[autodoc]] Flax{{cookiecutter.camelcase_modelname}}ForQuestionAnswering\n    - call\n\n\n## Flax{{cooki..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pre-trained language models have attracted increasin..."
          ],
          [
           "## Usage tips\n\n- BioGPT is a model with absolute position embeddings so it's usually advised to pad ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recently, plain vision Transformers (ViTs) have show..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Randomly initializing `SpeechEncoderDecoderModel` from model configurations.\n\n[`SpeechEncoderDeco..."
          ],
          [
           "```\n\n## Initialising `SpeechEncoderDecoderModel` from a pretrained encoder and a pretrained decoder...."
          ],
          [
           "```\n\n## Loading an existing `SpeechEncoderDecoderModel` checkpoint and perform inference.\n\nTo load f..."
          ],
          [
           "```\n\n## Training\n\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e..."
          ],
          [
           ">>> # load its corresponding transcription and tokenize to generate labels\n>>> labels = tokenizer(ds..."
          ],
          [
           "```\n\n## SpeechEncoderDecoderConfig\n\n[[autodoc]] SpeechEncoderDecoderConfig\n\n## SpeechEncoderDecoderM..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Backward\n\nThe last addition is to replace the typical `loss.backward()` in your training loo..."
          ],
          [
           "```\n\nAs you can see in the following code, you only need to add four additional lines of code to you..."
          ],
          [
           "```\n\nThen launch your training with:\n\n```bash\naccelerate launch train.py\n```\n\n### Train with a noteb..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We propose focal modulation networks (FocalNets in s..."
          ],
          [
           "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be foun..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n## Pipeline\n\n<Youtube id=\"tiZFewofSLM\"/>\n\nThe [`pipeline`] is the eas..."
          ],
          [
           "| **Task**                     | **Description**                                                    ..."
          ],
          [
           "| Audio classification         | assign a label to some audio data                                  ..."
          ],
          [
           "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this ..."
          ],
          [
           "```\n\nThe [`pipeline`] downloads and caches a default [pretrained model](https://huggingface.co/disti..."
          ],
          [
           "```\n\nLoad an audio dataset (see the ðŸ¤— Datasets [Quick Start](https://huggingface.co/docs/datasets/qu..."
          ],
          [
           "```\n\nFor larger datasets where the inputs are big (like in speech or vision), you'll want to pass a ..."
          ],
          [
           "```\n</pt>\n<tf>\nUse [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretra..."
          ],
          [
           "```\n\nIf you can't find a model for your use-case, you'll need to finetune a pretrained model on your..."
          ],
          [
           "```\n\nPass your text to the tokenizer:\n\n```py\n>>> encoding = tokenizer(\"We are very happy to show you..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n<Tip>\n\nCheck out the [preprocess](./preprocessing) tutorial for more ..."
          ],
          [
           "```\n\nThe model outputs the final activations in the `logits` attribute. Apply the softmax function t..."
          ],
          [
           "```\n\nThe model outputs the final activations in the `logits` attribute. Apply the softmax function t..."
          ],
          [
           "```\n</pt>\n<tf>\nOnce your model is fine-tuned, you can save it with its tokenizer using [`TFPreTraine..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n## Custom model builds\n\nYou can modify the model's configuration clas..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\nTake a look at the [Create a custom architecture](./create_a_model) g..."
          ],
          [
           "```\n\n3. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processo..."
          ],
          [
           "```\n\n<Tip>\n\nFor tasks - like translation or summarization - that use a sequence-to-sequence model, u..."
          ],
          [
           "```py\n   >>> from transformers import TFAutoModelForSequenceClassification\n\n   >>> model = TFAutoMod..."
          ],
          [
           "```\n\n2. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processo..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Randomly initializing `EncoderDecoderModel` from model configurations.\n\n[`EncoderDecoderModel`] c..."
          ],
          [
           "```\n\n## Initialising `EncoderDecoderModel` from a pretrained encoder and a pretrained decoder.\n\n[`En..."
          ],
          [
           "```\n\n## Loading an existing `EncoderDecoderModel` checkpoint and perform inference.\n\nTo load fine-tu..."
          ],
          [
           "```\n\n## Loading a PyTorch checkpoint into `TFEncoderDecoderModel`.\n\n[`TFEncoderDecoderModel.from_pre..."
          ],
          [
           "```\n\n## Training\n\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e..."
          ],
          [
           ">>> labels = tokenizer(\n...     \"the eiffel tower surpassed the washington monument to become the ta..."
          ],
          [
           "```\n\nDetailed [colab](https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Self-attention has become a defacto choice for captu..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "-->\n\n# Llama2\n\n## Overview\n\nThe Llama2 model was proposed in [LLaMA: Open Foundation and Fine-Tuned ..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In this work, we develop and release Llama 2, a coll..."
          ],
          [
           "The `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\"auto\"` whe..."
          ],
          [
           "</Tip>\n\nTips:\n\n- Weights for the Llama2 models can be obtained by filling out [this form](https://ai..."
          ],
          [
           "```\n\n- After conversion, the model and tokenizer can be loaded via:\n\n```python\nfrom transformers imp..."
          ],
          [
           "```\n\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci..."
          ],
          [
           "<PipelineTag pipeline=\"text-generation\"/>\n\n- A [notebook](https://colab.research.google.com/drive/1P..."
          ],
          [
           "âš¡ï¸ Inference\n- A [notebook](https://colab.research.google.com/drive/1TC56ArKerXUpbgRy5vM3woRsbTEVNq7..."
          ],
          [
           "[[autodoc]] LlamaModel\n    - forward\n\n\n## LlamaForCausalLM\n\n[[autodoc]] LlamaForCausalLM\n    - forwa..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In this work, we present a new network design paradi..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Self-supervised learning (SSL) achieves great succes..."
          ],
          [
           "- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech recognition t..."
          ],
          [
           "Performer fine-tuning\n\nExample authors: @TevenLeScao, @Patrickvonplaten\n\nPaper authors: Krzysztof Ch..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Deeper neural networks are more difficult to train. ..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "End-to-End finetuning of RAG (including DPR retriever) for Question Answering.\n\nThis finetuning scri..."
          ],
          [
           "To start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script als..."
          ],
          [
           "We conducted a simple experiment to investigate the effectiveness of this end2end training extension..."
          ],
          [
           "!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n\nLicensed under ..."
          ],
          [
           "This model was contributed by [shangz](https://huggingface.co/shangz).\n\n## Usage tips\n\n- QDQBERT mod..."
          ],
          [
           "Before creating QDQBERT model, one has to set the default `QuantDescriptor` defining default tensor ..."
          ],
          [
           "```\n\n### Calibration\n\nCalibration is the terminology of passing data samples to the quantizer and de..."
          ],
          [
           "```\n\n### Export to ONNX\n\nThe goal of exporting to ONNX is to deploy inference by [TensorRT](https://..."
          ],
          [
           "```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token cla..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The Autho..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Depth estimation pipeline\n\nThe simplest way to try out inference with a model supporting dep..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "```\n\nVisualize the results:\n\n```py\n>>> import numpy as np\n\n>>> # interpolate to original size\n>>> pr..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Most neural vocoders employ band-limited mel-spectro..."
          ],
          [
           "Tips:\n\n- The `noise_sequence` argument for [`UnivNetModel.forward`] should be standard Gaussian nois..."
          ],
          [
           "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n# Resamp..."
          ],
          [
           "```\n\nThis model was contributed by [dg845](https://huggingface.co/dg845).\nTo the best of my knowledg..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Understanding document images (e.g., invoices) is a ..."
          ],
          [
           "## Usage tips\n\n- The quickest way to get started with Donut is by checking the [tutorial\n  notebooks..."
          ],
          [
           ">>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model.to(device)  # doctest: +IGNORE..."
          ],
          [
           "```\n\n- Step-by-step Document Parsing\n\n```py\n>>> import re\n\n>>> from transformers import DonutProcess..."
          ],
          [
           ">>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> outputs = model.generate(..."
          ],
          [
           "```\n\n- Step-by-step Document Visual Question Answering (DocVQA)\n\n```py\n>>> import re\n\n>>> from trans..."
          ],
          [
           ">>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n>>> outputs = model.generate(..."
          ],
          [
           "```\n\nSee the [model hub](https://huggingface.co/models?filter=donut) to look for Donut checkpoints.\n..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformer based models, like BERT and RoBERTa, hav..."
          ],
          [
           "## IBertConfig\n\n[[autodoc]] IBertConfig\n\n## IBertModel\n\n[[autodoc]] IBertModel\n    - forward\n\n## IBe..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*While the Transformer architecture has become the de..."
          ],
          [
           "- [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperfo..."
          ],
          [
           "- To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-o..."
          ],
          [
           "use a higher resolution than pre-training [(Touvron et al., 2019)](https://arxiv.org/abs/1906.06423)..."
          ],
          [
           "## Resources\n\nDemo notebooks regarding inference as well as fine-tuning ViT on custom data can be fo..."
          ],
          [
           "âš—ï¸ Optimization\n\n- A blog post on how to [Accelerate Vision Transformer (ViT) with Quantization usin..."
          ],
          [
           "[[autodoc]] ViTForImageClassification\n    - forward\n\n</pt>\n<tf>\n\n## TFViTModel\n\n[[autodoc]] TFViTMod..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [Patrick von Platen](https://huggingface.co/patrickvonplaten).\n\nThe or..."
          ],
          [
           "- Step-by-step Speech Translation\n\n```python\n>>> import torch\n>>> from transformers import Speech2Te..."
          ],
          [
           "```\n\n- Speech Translation via Pipelines\n\n  The automatic speech recognition pipeline can also be use..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The `padding` argument controls padding. It can be a boolean or a string:\n\n  - `True` or `'longest'`..."
          ],
          [
           "The `max_length` argument controls the length of the padding and truncation. It can be an integer or..."
          ],
          [
           "| Truncation                           | Padding                           | Instruction            ..."
          ],
          [
           "|                                      |                                   | `tokenizer(batch_senten..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nwhere task name can be one of cola, sst2, mrpc, stsb, qqp, mnli, qnli, rte, wnli.\n\nWe get the f..."
          ],
          [
           "The following example fine-tunes BERT on the `imdb` dataset hosted on our [hub](https://huggingface...."
          ],
          [
           "```\n\n> If your model classification head dimensions do not fit the number of labels in the dataset, ..."
          ],
          [
           "```\nTraining for 1 epoch results in acc of around 0.5958 for review_body only and 0.659 for title+bo..."
          ],
          [
           "```\n It results in a Micro F1 score of around 0.82 without any text and label filtering. Note that y..."
          ],
          [
           "Using mixed precision training usually results in 2x-speedup for training with the same final result..."
          ],
          [
           "Like `run_glue.py`, this script allows you to fine-tune any of the models on the [hub](https://huggi..."
          ],
          [
           "```\n\nthen\n\n```bash\nexport TASK_NAME=mrpc\n\npython run_glue_no_trainer.py \\\n  --model_name_or_path ber..."
          ],
          [
           "```\n\nThis command is the same and will work for:\n\n- a CPU-only setup\n- a setup with one GPU\n- a dist..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Vision Transformers (ViT) have shown rapid progress ..."
          ],
          [
           "## Documentation resources\n\n- [Image classification task guide](../tasks/image_classification)\n\n## E..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Fine-tuning ViLT\n\nViLT model incorporates text embeddings into a Vision Transformer (ViT), allowi..."
          ],
          [
           "```\n\nWe encourage you to share your model with the community. Log in to your Hugging Face account to..."
          ],
          [
           "```\n\nLet's take a look at an example to understand the dataset's features:\n\n```py\n>>> dataset[0]\n{'q..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "```\n\nNow that we have the mappings, we can replace the string answers with their ids, and flatten th..."
          ],
          [
           "```\n\nTo preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. ..."
          ],
          [
           "```\n\nTo apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [`~datasets.map`] f..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n\nThe model in this guide has only been trained on 200 examples, so don't expect a lot from it. L..."
          ],
          [
           "```\n\n## Zero-shot VQA\n\nThe previous model treated VQA as a classification task. Some recent models, ..."
          ],
          [
           "```\n\nNow we need to preprocess the image/prompt with the model's processor, pass the processed input..."
          ],
          [
           "!---\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\n\nLicense..."
          ],
          [
           "```\n\nwhere task name can be one of cola, mnli, mnli_mismatched, mnli_matched, mrpc, qnli, qqp, rte, ..."
          ],
          [
           "```\n\nor directly on the hub under *Training metrics*.\n\n### Accuracy Evaluation\n\nWe train five replic..."
          ],
          [
           "| Task  | Metric                       | Acc (best run) | Acc (avg/5runs) | Stdev     | Metrics     ..."
          ],
          [
           "| QQP   | Accuracy/F1                  | 90.81/87.58    | 90.76/87.51     | 0.05/0.06 | [tfhub.dev](..."
          ],
          [
           "Some of these results are significantly different from the ones reported on the test set of GLUE ben..."
          ],
          [
           "| Task  | TPU v3-8  | 8 GPU      | [1 GPU](https://tensorboard.dev/experiment/mkPS4Zh8TnGe1HB6Yzwj4Q..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We propose an efficient design of Transformer-based ..."
          ],
          [
           "## PatchTSTConfig\n\n[[autodoc]] PatchTSTConfig\n\n## PatchTSTModel\n\n[[autodoc]] PatchTSTModel\n    - for..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/\">English</..."
          ],
          [
           "ðŸ¤— TransformersëŠ” ë¶„ë¥˜, ì •ë³´ ì¶”ì¶œ, ì§ˆë¬¸ ë‹µë³€, ìš”ì•½, ë²ˆì—­, ë¬¸ìž¥ ìƒì„± ë“±ì„ 100ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ìˆ˜ì²œê°œì˜ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ëª©..."
          ],
          [
           "ðŸ¤— TransformersëŠ” ê°€ìž¥ ìœ ëª…í•œ 3ê°œì˜ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ë“¤ì€ ì„œë¡œ ì™„ë²½ížˆ ì—°ë™ë©ë‹ˆë‹¤ â€” [Jax](https://jax.readthedocs.io/en/..."
          ],
          [
           "ì˜ˆì‹œ:\n- [BERTë¡œ ë§ˆìŠ¤í‚¹ëœ ë‹¨ì–´ ì™„ì„±í•˜ê¸°](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+..."
          ],
          [
           "- [BARTë¥¼ ì´ìš©í•œ ìš”ì•½](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C..."
          ],
          [
           "- [DistilBERTë¥¼ ì´ìš©í•œ ì§ˆë¬¸..."
          ],
          [
           "ë‹µë³€](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+d..."
          ],
          [
           "- [T5ë¡œ ë²ˆì—­í•˜ê¸°](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)..."
          ],
          [
           "**[Transformerì™€ ê¸€ì“°ê¸°](https://transformer.huggingface.co)** ëŠ” ì´ ì €ìž¥ì†Œì˜ í…ìŠ¤íŠ¸ ìƒì„± ëŠ¥ë ¥ì— ê´€í•œ Hugging Face íŒ€ì˜ ê³µì‹..."
          ],
          [
           "## í€µ íˆ¬ì–´\n\nì›í•˜ëŠ” í…ìŠ¤íŠ¸ì— ë°”ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡, ìš°ë¦¬ëŠ” `pipeline` APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. Pipelineì€ ì‚¬ì „í•™ìŠµ ëª¨ë¸ê³¼ ê·¸ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ì ìš©í•œ ì „ì²˜..."
          ],
          [
           "```\n\nì½”ë“œì˜ ë‘ë²ˆì§¸ ì¤„ì€ pipelineì´ ì‚¬ìš©í•˜ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìºì‹œë¡œ ì €ìž¥í•©ë‹ˆë‹¤. ì„¸ë²ˆì§¸ ì¤„ì—ì„  ê·¸ ëª¨ë¸ì´ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ëª¨ë¸ì€ 99.9..."
          ],
          [
           "```\n\në‹µë³€ë¿ë§Œ ì•„ë‹ˆë¼, ì—¬ê¸°ì— ì‚¬ìš©ëœ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ í™•ì‹ ë„ì™€ í† í¬ë‚˜ì´ì¦ˆëœ ë¬¸ìž¥ ì† ë‹µë³€ì˜ ì‹œìž‘ì , ëì ê¹Œì§€ ë°˜í™˜í•©ë‹ˆë‹¤. [ì´ íŠœí† ë¦¬ì–¼](https://huggingface.c..."
          ],
          [
           "```\n\ní† í¬ë‚˜ì´ì €ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸ì˜ ëª¨ë“  ì „ì²˜ë¦¬ë¥¼ ì±…ìž„ì§‘ë‹ˆë‹¤. ê·¸ë¦¬ê³  (ìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼) 1ê°œì˜ ìŠ¤íŠ¸ë§ì´ë‚˜ ë¦¬ìŠ¤íŠ¸ë„ ì²˜ë¦¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ëŠ”ë°, ì´ëŠ” ..."
          ],
          [
           "ëª¨ë¸ ìžì²´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ë‚˜ [T..."
          ],
          [
           "1. ë” ì ì€ ê³„ì‚° ë¹„ìš©, ë” ì ì€ íƒ„ì†Œ ë°œìžêµ­:\n    - ì—°êµ¬ìžë“¤ì€ ëª¨ë¸ì„ ê³„ì† ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ëŠ” ëŒ€ì‹  í•™ìŠµëœ ëª¨ë¸ì„ ê³µìœ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n    - ì‹¤ë¬´ìžë“¤ì€ í•™ìŠµì— í•„ìš”í•œ ì‹œ..."
          ],
          [
           "1. í•„ìš”í•œ ëŒ€ë¡œ ëª¨ë¸ì´ë‚˜ ì˜ˆì‹œë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•˜ì„¸ìš”:\n    - ìš°ë¦¬ëŠ” ì €ìžê°€ ê³µê°œí•œ ê²°ê³¼ë¥¼ ìž¬í˜„í•˜ê¸° ìœ„í•´ ê° ëª¨ë¸ êµ¬ì¡°ì˜ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n    - ëª¨ë¸ ë‚´ë¶€ êµ¬ì¡°ëŠ” ê°€ëŠ¥í•œ ..."
          ],
          [
           "## ì™œ transformersë¥¼ ì‚¬ìš©í•˜ì§€ ë§ì•„ì•¼ í• ê¹Œìš”?\n\n- ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì‹ ê²½ë§ ë¸”ë¡ì„ ë§Œë“¤ê¸° ìœ„í•œ ëª¨ë“ˆì´ ì•„ë‹™ë‹ˆë‹¤. ì—°êµ¬ìžë“¤ì´ ì—¬ëŸ¬ íŒŒì¼ì„ ì‚´íŽ´ë³´ì§€ ì•Šê³  ë°”ë¡œ ê° ëª¨ë¸ì„ ..."
          ],
          [
           "## ì„¤ì¹˜\n\n### pipë¡œ ì„¤ì¹˜í•˜ê¸°\n\nì´ ì €ìž¥ì†ŒëŠ” Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, TensorFlow 2.6+ì—ì„œ í…ŒìŠ¤íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n..."
          ],
          [
           "ì´ë“¤ ì¤‘ ì ì–´ë„ í•˜ë‚˜ê°€ ì„¤ì¹˜ë˜ì—ˆë‹¤ë©´, ðŸ¤— TransformersëŠ” ë‹¤ìŒê³¼ ê°™ì´ pipì„ ì´ìš©í•´ ì„¤ì¹˜í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:\n\n```bash\npip install transformers..."
          ],
          [
           "```\n\nì˜ˆì‹œë“¤ì„ ì²´í—˜í•´ë³´ê³  ì‹¶ê±°ë‚˜, ìµœìµœìµœì²¨ë‹¨ ì½”ë“œë¥¼ ì›í•˜ê±°ë‚˜, ìƒˆë¡œìš´ ë²„ì „ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦´ ìˆ˜ ì—†ë‹¤ë©´ [ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì†ŒìŠ¤ì—ì„œ ë°”ë¡œ ì„¤ì¹˜](https://huggingfac..."
          ],
          [
           "```\n\nFlax, PyTorch, TensorFlow ì„¤ì¹˜ íŽ˜ì´ì§€ì—ì„œ ì´ë“¤ì„ condaë¡œ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.\n\n## ëª¨ë¸ êµ¬ì¡°\n\n**ðŸ¤— Transformersê°€ ì œê³µí•˜ëŠ”..."
          ],
          [
           "1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and..."
          ],
          [
           "1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua Un..."
          ],
          [
           "1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (from Google AI) released with ..."
          ],
          [
           "1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (Salesforce ì—ì„œ ì œê³µ)ì€ Junna..."
          ],
          [
           "1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (NAVER CLOVA ì—ì„œ ì œê³µ)ì€ Teakgyu ..."
          ],
          [
           "1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (Google Research ì—ì„œ) Jona..."
          ],
          [
           "1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (OpenAI ì—ì„œ) Alec Radford, Jon..."
          ],
          [
           "1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (MetaAI ì—ì„œ ì œê³µ)ì€ Ba..."
          ],
          [
           "1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (Facebook AI ì—ì„œ) Zhua..."
          ],
          [
           "1. **[CPM-Ant](https://huggingface.co/docs/transformers/model_doc/cpmant)** (from OpenBMB) released ..."
          ],
          [
           "1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (Microsoft ì—ì„œ) Pengchen..."
          ],
          [
           "1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (Facebook ì—ì„œ) Hugo Touvron, M..."
          ],
          [
           "1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (Microsoft Research ì—..."
          ],
          [
           "1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (HuggingFace ì—ì„œ) ..."
          ],
          [
           "1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (Facebook ì—ì„œ) Vladimir Karpukhi..."
          ],
          [
           "1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (Google Research/Stanfo..."
          ],
          [
           "1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (Baidu ì—ì„œ ì œê³µ)ì€ Xuan Ouya..."
          ],
          [
           "1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (from Google Research) releas..."
          ],
          [
           "1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (EleutherAI ì—ì„œ) Sid B..."
          ],
          [
           "1. **[GPT-Sw3](https://huggingface.co/docs/transformers/model_doc/gpt-sw3)** (AI-Sweden ì—ì„œ) Ariel Ek..."
          ],
          [
           "1. **[GPTSAN-japanese](https://huggingface.co/docs/transformers/model_doc/gptsan-japanese)** release..."
          ],
          [
           "1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (Facebook ì—ì„œ) Wei-Ning Hs..."
          ],
          [
           "1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang Univers..."
          ],
          [
           "1. **[KOSMOS-2](https://huggingface.co/docs/transformers/model_doc/kosmos-2)** (from Microsoft Resea..."
          ],
          [
           "1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (Microsoft Resear..."
          ],
          [
           "1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (South China University of Te..."
          ],
          [
           "1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (The FAIR team of Meta AI..."
          ],
          [
           "1. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)** (Microsoft Research & Unive..."
          ],
          [
           "1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (UNC Chapel Hill ì—ì„œ) Hao ..."
          ],
          [
           "1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) rel..."
          ],
          [
           "1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (Meta and UIUC ì—ì„œ..."
          ],
          [
           "1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (Facebook ì—ì„œ) Yuqing Tan..."
          ],
          [
           "1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (NVIDIA ì—ì„œ)..."
          ],
          [
           "1. **[Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral)** (from Mistral AI) by Th..."
          ],
          [
           "1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (CMU/Google Brain..."
          ],
          [
           "1. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)** (Apple ì—ì„œ ì œê³µ)ì€ ..."
          ],
          [
           "1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (Google AI ì—ì„œ) Linting Xue, Noa..."
          ],
          [
           "1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (Huawei Noahâ€™s Ark Lab ì—ì„œ) ..."
          ],
          [
           "1. **[NystrÃ¶mformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (the Univer..."
          ],
          [
           "1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (Google AI ì—ì„œ) Matthias ..."
          ],
          [
           "1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ì—ì„œ ì œê³µ)ì€ Yuqi Nie..."
          ],
          [
           "1. **[Persimmon](https://huggingface.co/docs/transformers/model_doc/persimmon)** (ADEPT ì—ì„œ ì œê³µ)ì€ Eric..."
          ],
          [
           "1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (Google ì—ì„œ ì œê³µ)ì€ K..."
          ],
          [
           "1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (Microsoft Resear..."
          ],
          [
           "1. **[RAG](https://huggingface.co/docs/transformers/model_doc/rag)** (Facebook ì—ì„œ) Patrick Lewis, Et..."
          ],
          [
           "1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (Google Research ì—ì„œ) Hy..."
          ],
          [
           "1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)** (WeChatAI ì—ì„œ) HuiSu, W..."
          ],
          [
           "1. **[SeamlessM4Tv2](https://huggingface.co/docs/transformers/model_doc/seamless_m4t_v2)** (from Met..."
          ],
          [
           "1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** (ASAPP ì—ì„œ) Felix Wu, Kwangy..."
          ],
          [
           "1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)..."
          ],
          [
           "1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (Microsoft ì—ì„œ) Ze..."
          ],
          [
           "1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (Google AI ì—ì„œ) Colin Raffel and N..."
          ],
          [
           "1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (Microsoft Research ì—ì„œ) Qia..."
          ],
          [
           "1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (Google/CMU ì—..."
          ],
          [
           "1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (Google Research ì—ì„œ) Yi Tay, Mo..."
          ],
          [
           "1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (Microsoft R..."
          ],
          [
           "1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (Multimedia Computing..."
          ],
          [
           "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (Google AI..."
          ],
          [
           "1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (Meta AI ì—ì„œ ì œê³µ)ì€ Yanghao ..."
          ],
          [
           "1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (Kakao Enterprise ì—ì„œ ì œê³µ)ì€ Jae..."
          ],
          [
           "1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)** (Faceb..."
          ],
          [
           "1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (Microsoft Research ì—ì„œ) Bo..."
          ],
          [
           "1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (Facebook ì—ì„œ) Guillaume Lample ..."
          ],
          [
           "1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (Meta AI ì—ì„œ) Davis Liang, H..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (Facebook A..."
          ],
          [
           "1. ìƒˆë¡œìš´ ëª¨ë¸ì„ ì˜¬ë¦¬ê³  ì‹¶ë‚˜ìš”? ìš°ë¦¬ê°€ **ìƒì„¸í•œ ê°€ì´ë“œì™€ í…œí”Œë¦¿** ìœ¼ë¡œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì˜¬ë¦¬ë„ë¡ ë„ì™€ë“œë¦´ê²Œìš”. ê°€ì´ë“œì™€ í…œí”Œë¦¿ì€ ì´ ì €ìž¥ì†Œì˜ [`templates`](./te..."
          ],
          [
           "ê° ëª¨ë¸ì´ Flax, PyTorch, TensorFlowìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ ë˜ëŠ” ðŸ¤— Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì§€ì›í•˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´, [ì´ í‘œ](https..."
          ],
          [
           "| ì„¹ì…˜ | ì„¤ëª… |\n|-|-|\n| [ë„íë¨¼íŠ¸](https://huggingface.co/transformers/) | ì „ì²´ API ë„íë¨¼íŠ¸ì™€ íŠœí† ë¦¬ì–¼ |\n| [ê³¼ì œ ìš”ì•½](htt..."
          ],
          [
           "| [ë§ˆì´ê·¸ë ˆì´ì…˜](https://huggingface.co/docs/transformers/migration) | `pytorch-transformers`ë‚˜ `pytorch-pr..."
          ],
          [
           "## ì¸ìš©\n\nðŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¸ìš©í•˜ê³  ì‹¶ë‹¤ë©´, ì´ [ë…¼ë¬¸](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)ì„..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Check more detailed information for [oneccl_bind_pt](https://github.com/intel/torch-ccl).\n\n### Intel..."
          ],
          [
           "```\npip install oneccl_bind_pt=={pytorch_version} -f https://developer.intel.com/ipex-whl-stable-cpu..."
          ],
          [
           "```\n\n#### IntelÂ® Extension for PyTorch installation\n\nIntel Extension for PyTorch (IPEX) provides per..."
          ],
          [
           "```\nThe following command enables training with a total of four processes on two Xeons (node0 and no..."
          ],
          [
           "```\n\n## Usage with Kubernetes\n\nThe same distributed training job from the previous section can be de..."
          ],
          [
           "```\nFROM intel/ai-workflows:torch-2.0.1-huggingface-multinode-py3.9\n\nWORKDIR /workspace\n\n# Download ..."
          ],
          [
           "```\nThe image needs to be built and copied to the cluster's nodes or pushed to a container registry ..."
          ],
          [
           "The snippet below is an example of a yaml file for a PyTorchJob with 4 workers running the\n[question..."
          ],
          [
           "- \"3e-5\"\n                - --num_train_epochs\n                - \"2\"\n                - --max_seq_leng..."
          ],
          [
           "- name: pvc-volume\n                mountPath: /tmp/pvc-mount\n              - mountPath: /dev/shm\n   ..."
          ],
          [
           "```\nTo run this example, update the yaml based on your training script and the nodes in your cluster..."
          ],
          [
           "```\nNAME                                                     READY   STATUS                  RESTART..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper states the following:\n\n*Visual language data such as plots, charts, and in..."
          ],
          [
           "## Usage\n\nCurrently 6 checkpoints are available for MatCha:\n\n- `google/matcha`: the base MatCha mode..."
          ],
          [
           "inputs = processor(images=image, text=\"Is the sum of all 4 places greater than Laos?\", return_tensor..."
          ],
          [
           "```\n\n## Fine-tuning\n\nTo fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https://git..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team and The OpenBMB Team. All rights reserved.\n\nLicensed under th..."
          ],
          [
           "## CpmAntConfig\n\n[[autodoc]] CpmAntConfig\n    - all\n\n## CpmAntTokenizer\n\n[[autodoc]] CpmAntTokenizer..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The recent breakthroughs in natural language process..."
          ],
          [
           "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\nmodel = AutoModel.from_pretra..."
          ],
          [
           "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](http..."
          ],
          [
           "Before you begin, make sure you have all the necessary libraries installed. \n\n```bash\npip install -q..."
          ],
          [
           "```\n\n<Tip>\nTo run the following examples with a non-quantized version of the model checkpoint you wi..."
          ],
          [
           "```\n\nSetting `device_map` to `\"auto\"` will automatically determine how to load and store the model w..."
          ],
          [
           "```\n\nNow that you have the model loaded in one of the suggested ways, let's move on to exploring tas..."
          ],
          [
           ">>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokeniz..."
          ],
          [
           "```\n\n<Tip>\n\nIt is a good idea to include the `bad_words_ids` in the call to `generate` to avoid erro..."
          ],
          [
           ">>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokeniz..."
          ],
          [
           "```\n\n## Few-shot prompting\n\nWhile IDEFICS demonstrates great zero-shot results, your task may requir..."
          ],
          [
           "Photo by [Juan Mayobre](https://unsplash.com/@jmayobres).\n  \n```py\n>>> prompt = [\"User:\",\n...       ..."
          ],
          [
           ">>> generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)\n>>> gen..."
          ],
          [
           "```\n\nNotice that just from a single example (i.e., 1-shot) the model has learned how to perform the ..."
          ],
          [
           ">>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokeniz..."
          ],
          [
           "```\n\n## Image classification\n\nIDEFICS is capable of classifying images into different categories wit..."
          ],
          [
           ">>> generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)\n>>> gene..."
          ],
          [
           ">>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokeniz..."
          ],
          [
           "```\n\nLooks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky Halloween story a..."
          ],
          [
           "## Running inference in batch mode\n\nAll of the earlier sections illustrated IDEFICS for a single exa..."
          ],
          [
           ">>> inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\n>>> bad_words_ids = processor.tokeni..."
          ],
          [
           "```\n\n## IDEFICS instruct for conversational use\n\nFor conversational use cases, you can find fine-tun..."
          ],
          [
           "...         \"\\nUser:\",\n...         \"https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/r..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface..."
          ],
          [
           "<h3 align=\"center\">\n    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>\n</h..."
          ],
          [
           "## Online demos\n\nYou can test most of our models directly on their pages from the [model hub](https:..."
          ],
          [
           "In Natural Language Processing:\n- [Masked word completion with BERT](https://huggingface.co/bert-bas..."
          ],
          [
           "- [Natural Language Inference with RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+w..."
          ],
          [
           "- [Question answering with..."
          ],
          [
           "- [Translation with T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin..."
          ],
          [
           "In Computer Vision:\n- [Image classification with ViT](https://huggingface.co/google/vit-base-patch16..."
          ],
          [
           "In Multimodal tasks:\n- [Table Question Answering with TAPAS](https://huggingface.co/google/tapas-bas..."
          ],
          [
           "## Quick tour\n\nTo immediately use a model on a given input (text, image, audio, ...), we provide the..."
          ],
          [
           "```\n\nThe second line of code downloads and caches the pretrained model used by the pipeline, while t..."
          ],
          [
           "# Allocate a pipeline for object detection\n>>> object_detector = pipeline('object-detection')\n>>> ob..."
          ],
          [
           "```\n\nHere, we get a list of objects detected in the image, with a box surrounding the object and a c..."
          ],
          [
           "```\n\nThe tokenizer is responsible for all the preprocessing the pretrained model expects and can be ..."
          ],
          [
           "1. Easily customize a model or an example to your needs:\n    - We provide examples for each architec..."
          ],
          [
           "First, create a virtual environment with the version of Python you're going to use and activate it.\n..."
          ],
          [
           "```\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait fo..."
          ],
          [
           "```\n\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with co..."
          ],
          [
           "1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua Un..."
          ],
          [
           "1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (from VinAI Research) r..."
          ],
          [
           "1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with..."
          ],
          [
           "1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) relea..."
          ],
          [
           "1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) relea..."
          ],
          [
           "1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)** (from Tsinghua University) rele..."
          ],
          [
           "1. **[Data2Vec](https://huggingface.co/docs/transformers/model_doc/data2vec)** (from Facebook) relea..."
          ],
          [
           "1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (from S..."
          ],
          [
           "1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (from Facebook) released with..."
          ],
          [
           "1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (from Meta AI) released w..."
          ],
          [
           "1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) relea..."
          ],
          [
           "1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from S..."
          ],
          [
           "1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (from Baidu) released with ..."
          ],
          [
           "1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer ..."
          ],
          [
           "1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (from CNRS) released ..."
          ],
          [
           "1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (from CMU/Goo..."
          ],
          [
           "1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (from OpenAI) released w..."
          ],
          [
           "1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with ..."
          ],
          [
           "1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) r..."
          ],
          [
           "1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) r..."
          ],
          [
           "1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) rele..."
          ],
          [
           "1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (from OpenAI) released ..."
          ],
          [
           "1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (from Microsoft R..."
          ],
          [
           "1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University ..."
          ],
          [
           "1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Me..."
          ],
          [
           "1. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)** (from Microsoft Research & ..."
          ],
          [
           "1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (from UNC Chapel Hill) re..."
          ],
          [
           "1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) rel..."
          ],
          [
           "1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (from Meta and UI..."
          ],
          [
           "1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (from Meta/USC/CMU/SJTU) rele..."
          ],
          [
           "1. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)** (from Mistral AI) by Th..."
          ],
          [
           "1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (from Facebook) released with t..."
          ],
          [
           "1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) releas..."
          ],
          [
           "1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with ..."
          ],
          [
           "1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (from Huawei Noahâ€™s Ark Lab..."
          ],
          [
           "1. **[NystrÃ¶mformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (from the U..."
          ],
          [
           "1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) release..."
          ],
          [
           "1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released ..."
          ],
          [
           "1. **[Phi](https://huggingface.co/docs/transformers/model_doc/phi)** (from Microsoft) released with ..."
          ],
          [
           "1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (from UCLA NLP) released ..."
          ],
          [
           "1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (from Nanjing University, The U..."
          ],
          [
           "1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research..."
          ],
          [
           "1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)..."
          ],
          [
           "1. **[SeamlessM4T](https://huggingface.co/docs/transformers/model_doc/seamless_m4t)** (from Meta AI)..."
          ],
          [
           "1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** (from ASAPP) released with the ..."
          ],
          [
           "1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)..."
          ],
          [
           "1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) ..."
          ],
          [
           "1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with th..."
          ],
          [
           "1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from Microsoft Research) r..."
          ],
          [
           "1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft), released ..."
          ],
          [
           "1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (from Google Research) releas..."
          ],
          [
           "1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (from Peking University..."
          ],
          [
           "1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (from University of W..."
          ],
          [
           "1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) ..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Faceb..."
          ],
          [
           "To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated to..."
          ],
          [
           "## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cit..."
          ],
          [
           "CodeParrot ðŸ¦œ\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/lvwerra/repo-images/ra..."
          ],
          [
           "```\n\nAdditionally, sure you have git-lfs installed. You can find instructions for how to install it ..."
          ],
          [
           "- exact deduplication using each file's hash after having removed whistespaces.\n- near deduplication..."
          ],
          [
           "```\nDuring preprocessing the dataset is downloaded and stored locally as well as caches of the compu..."
          ],
          [
           "```\nThis will initialize a new model with the architecture and configuration of `gpt2-large` and use..."
          ],
          [
           "```\n\nRecall that you can see the full set of possible options with descriptions (for all scripts) by..."
          ],
          [
           "```\nIn addition we evaluate the model on OpenAI's _HumanEval_ benchmark. You can run the evaluation ..."
          ],
          [
           "```\n\nThe results as well as reference values are shown in the following table:\n\n| Model | pass@1 | p..."
          ],
          [
           "## Training with Megatron\n[Megatron](https://github.com/NVIDIA/Megatron-LM) is a framework developed..."
          ],
          [
           "```\n\nYou also need to add the vocabulary file and merges table of the tokenizer that you trained on ..."
          ],
          [
           "```\nThis outputs two files `codeparrot_content_document.idx` and `codeparrot_content_document.bin` w..."
          ],
          [
           "### Training\nYou can configure the model architecture and training parameters as shown below, or put..."
          ],
          [
           "--lr-warmup-iters 2000\n--weight-decay .1\n--adam-beta2 .999\n--fp16\n--log-interval 10\n--save-interval ..."
          ],
          [
           "```\nThe training takes almost 12 hours in this setting.\n\n### Convert model to `transformers`\nAfter t..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Check more detailed information for [Auto Mixed Precision](https://intel.github.io/intel-extension-f..."
          ],
          [
           "```\npip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nTo print summary statistics for the GPU utilization and the training run with the [`Trainer`] w..."
          ],
          [
           "```\n\nWe see that the kernels alone take up 1.3GB of GPU memory. Now let's see how much space the mod..."
          ],
          [
           "```\n\n```bash\nTue Jan 11 08:58:05 2022\n+-------------------------------------------------------------..."
          ],
          [
           "+-----------------------------------------------------------------------------+\n| Processes:        ..."
          ],
          [
           "```\n\nWe get the same number as before and you can also see that we are using a V100 GPU with 16GB of..."
          ],
          [
           "```\n\nWe see that already a relatively small batch size almost fills up our GPU's entire memory. Howe..."
          ],
          [
           "1. model weights\n2. optimizer states\n3. gradients\n4. forward activations saved for gradient computat..."
          ],
          [
           "**Functionality-specific memory**\n\nThen, your software could have special memory needs. For example,..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert), [BigBird](../mo..."
          ],
          [
           "[FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [GPT-Sw3](../model_doc/gpt-sw3..."
          ],
          [
           "[Megatron-BERT](../model_doc/megatron-bert), [Mistral](../model_doc/mistral), [Mixtral](../model_doc..."
          ],
          [
           "[RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoForm..."
          ],
          [
           "<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libr..."
          ],
          [
           "```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model w..."
          ],
          [
           "```\n\nThen take a look at an example:\n\n```py\n>>> imdb[\"test\"][0]\n{\n    \"label\": 0,\n    \"text\": \"I lov..."
          ],
          [
           "```\n\nCreate a preprocessing function to tokenize `text` and truncate sequences to be no longer than ..."
          ],
          [
           "```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule...."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n\n<Tip>\n\n[`Trainer`] applies dynamic padding by default when you pass `tokenizer` to it. In this ..."
          ],
          [
           "```\n\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr..."
          ],
          [
           "```\n\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```..."
          ],
          [
           "```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ..."
          ],
          [
           "```\n</pt>\n<tf>\nTokenize the text and return TensorFlow tensors:\n\n```py\n>>> from transformers import ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n *The design choices in the Transformer attention mec..."
          ],
          [
           "## MegaConfig\n\n[[autodoc]] MegaConfig\n\n## MegaModel\n\n[[autodoc]] MegaModel\n    - forward\n\n## MegaFor..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Overview\n\nThe XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequ..."
          ],
          [
           "The Authors' code can be found [here](https://github.com/microsoft/ProphetNet).\n\n## Resources\n\n- [Ca..."
          ],
          [
           "!--Copyright 2022 NVIDIA and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache L..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Grouping and recognition are important components of..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "## TFGroupViTVisionModel\n\n[[autodoc]] TFGroupViTVisionModel\n    - call\n\n</tf>\n</frameworkcontent>..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nThis will create a `imagenette2` dir with two subdirectories `train` and `val` each with multip..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## PreTrainedModel\n\n[[autodoc]] PreTrainedModel\n    - push_to_hub\n    - all\n\n<a id='from_pretrained-..."
          ],
          [
           "```\n\nMoreover, you can directly place the model on different devices if it doesn't fully fit in RAM ..."
          ],
          [
           "```\n\nYou can inspect how the model was split across devices by looking at its `hf_device_map` attrib..."
          ],
          [
           "```\n\nYou can also write your own device map following the same format (a dictionary layer name to de..."
          ],
          [
           "```\n\nDue to Pytorch design, this functionality is only available for floating dtypes.\n\n\n## ModuleUti..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The \"Roaring 20s\" of visual recognition began with t..."
          ],
          [
           "<small> ConvNeXT architecture. Taken from the <a href=\"https://arxiv.org/abs/2201.03545\">original pa..."
          ],
          [
           "## ConvNextFeatureExtractor\n\n[[autodoc]] ConvNextFeatureExtractor\n\n## ConvNextImageProcessor\n\n[[auto..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "In this example we will use the vision model from [CLIP](https://huggingface.co/models?filter=clip)\n..."
          ],
          [
           "```\nhuggingface-cli repo create clip-roberta-base\n```\nNext we clone the model repository to add the ..."
          ],
          [
           "```\n\nIf the checkpoints are in PyTorch then one could pass `text_from_pt=True` and `vision_from_pt=T..."
          ],
          [
           "```\n\n### Prepare dataset files and split the dataset.\n\n```python\nimport json\nimport collections\n\nima..."
          ],
          [
           "```\n\n> Note: The data loading and processing part of this script can still be improved for maximum p..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Vision-and-Language Pre-training (VLP) has improved ..."
          ],
          [
           "## Usage tips\n\n- The quickest way to get started with ViLT is by checking the [example notebooks](ht..."
          ],
          [
           "## ViltForMaskedLM\n\n[[autodoc]] ViltForMaskedLM\n    - forward\n\n## ViltForQuestionAnswering\n\n[[autodo..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Zero-shot image classification pipeline\n\nThe simplest way to try out inference with a model ..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "```\n\nLet's take a different image to switch things up.\n\n```py\n>>> from PIL import Image\n>>> import r..."
          ],
          [
           "```\n\nPass the inputs through the model, and post-process the results:\n\n```py\n>>> import torch\n\n>>> w..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The recently-proposed Perceiver model obtains good r..."
          ],
          [
           "Internally, [`PerceiverModel`] will create the latents, which is a tensor of shape `(batch_size, num..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perceiver_ar..."
          ],
          [
           "## Perceiver specific outputs\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverModelOutput\n..."
          ],
          [
           "## PerceiverProjectionDecoder\n\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverProjectionDe..."
          ],
          [
           "## PerceiverForMaskedLM\n\n[[autodoc]] PerceiverForMaskedLM\n    - forward\n\n## PerceiverForSequenceClas..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Development of the model was led by [Shinya Otani](https://github.com/SO0529), [Takayoshi Makabe](ht..."
          ],
          [
           ">>> print(gen_text)\näººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€AIã¨äººãŒå…±å­˜ã—ã€AIã‚’æ­£ã—ãç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚..."
          ],
          [
           "```\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## GPTNeoXJa..."
          ],
          [
           "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pled..."
          ],
          [
           "## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported ..."
          ],
          [
           "**Consequence**: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attri..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_d..."
          ],
          [
           "```\n\nOnly T5 models `t5-small`, `t5-base`, `t5-large`, `t5-3b` and `t5-11b` must use an additional a..."
          ],
          [
           "```\n\nThe task of summarization supports custom CSV and JSONLINES formats.\n\n#### Custom CSV Files\n\nIf..."
          ],
          [
           "```\n\nand you wanted to select only `text` and `summary`, then you'd pass these additional arguments:..."
          ],
          [
           "```\n\n## With Accelerate\n\nBased on the script [`run_summarization_no_trainer.py`](https://github.com/..."
          ],
          [
           "```\n\nand reply to the questions asked. Then\n\n```bash\naccelerate test\n```\n\nthat will check everything..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Many real-world applications require the prediction ..."
          ],
          [
           "- Check out the Informer blog-post in HuggingFace blog: [Multivariate Probabilistic Time Series Fore..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nHaving downloaded COCO dataset manually you should be able to load with the `ydshieh/coc_datase..."
          ],
          [
           "```\n\n### Create a model from a vision encoder model and a text encoder model\nWe can either load a CL..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "However, if the preferred batch size fits into memory, there's no reason to apply memory-optimizing ..."
          ],
          [
           "Note: when using mixed precision with a small model and a large batch size, there will be some memor..."
          ],
          [
           "[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-mu..."
          ],
          [
           "```\n\nIn the above example, your effective batch size becomes 4. \n\nAlternatively, use ðŸ¤— Accelerate to..."
          ],
          [
           "**Gradient checkpointing** offers a compromise between these two approaches and saves strategically ..."
          ],
          [
           "```\n\nAlternatively, use ðŸ¤— Accelerate - find the ðŸ¤— Accelerate example [further in this guide](#using-..."
          ],
          [
           "```\n\nIf you prefer to use ðŸ¤— Accelerate, find the ðŸ¤— Accelerate example [further in this guide](#using..."
          ],
          [
           "```\nimport torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True..."
          ],
          [
           "```\n\n<Tip>\n\ntf32 can't be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an inter..."
          ],
          [
           "[`Trainer`] integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch..."
          ],
          [
           "```\n\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precis..."
          ],
          [
           "```\n\nHowever, we can also use a third-party implementation of the 8-bit optimizer for demonstration ..."
          ],
          [
           "optimizer_kwargs = {\n    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n    \"eps\": t..."
          ],
          [
           "```\n\nFinally, pass the custom optimizer as an argument to the `Trainer`:\n\n```py\ntrainer = Trainer(mo..."
          ],
          [
           "```\n\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precis..."
          ],
          [
           "DeepSpeed is an open-source deep learning optimization library that is integrated with ðŸ¤— Transformer..."
          ],
          [
           "```\n\n`torch.compile` uses Python's frame evaluation API to automatically create a graph from existin..."
          ],
          [
           "**Training & inference backends**:\n* `dynamo.optimize(\"inductor\")` - Uses TorchInductor backend with..."
          ],
          [
           "**Inference-only backend**s:\n* `dynamo.optimize(\"ofi\")` -  Uses Torchscript optimize_for_inference. ..."
          ],
          [
           "```\n\nThe full example training loop with ðŸ¤— Accelerate is only a handful of lines of code long:\n\n```p..."
          ],
          [
           "```\n\nFirst we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.u..."
          ],
          [
           "At times, additional efforts may be required to pre-build some components. For instance, if you're u..."
          ],
          [
           "![MoE Transformer 2x block](https://huggingface.co/datasets/huggingface/documentation-images/resolve..."
          ],
          [
           "And for Pytorch DeepSpeed has built one as well: [DeepSpeed-MoE: Advancing Mixture-of-Experts Infere..."
          ],
          [
           "```\n\nOnce converted, train the model as usual.\n\n<Tip warning={true}>\n\nThe PyTorch-native `scaled_dot..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "## Motivation\nWithout processing, english-> romanian mbart-large-en-ro gets BLEU score 26.8 on the W..."
          ],
          [
           "```\n\n(2) define a function for post processing.\n It removes diacritics and does other things I don't..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "In particular all \"Please explain\" questions or objectively very user-specific feature requests belo..."
          ],
          [
           "```\n    \"huggingface\" \"transformers\" your query\n    ```\n\n    The first two quoted words tell Google ..."
          ],
          [
           "```\n\n    And now we can use it to do the searching on your favorite search engine:\n\n    1. first for..."
          ],
          [
           "```\n\n   then you'd search for `\"ValueError\" \"cannot be found\"`\n\n   As you search you will notice tha..."
          ],
          [
           "```\n   ```\n   git clone https://github.com/huggingface/transformers\n   cd transformers\n   pip instal..."
          ],
          [
           "```\n\n   which would result in the following entry, which can be opened if desired, but otherwise tak..."
          ],
          [
           "7. If you forked off some of this project's code or example applications, please, do not ask us to g..."
          ],
          [
           "We currently don't have a triage service and we trust your capacity to identify the right domain and..."
          ],
          [
           "For example, the very first comment is the most important one. If while the thread unfolds you reali..."
          ],
          [
           "13. If you are replying to a last comment, it's totally fine to make your reply with just your comme..."
          ],
          [
           "```\n    > How big is your gpu cluster?\n\n    Our cluster is made of 256 gpus.\n    ```\n\n    If you are..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*With the success of language pretraining, it is high..."
          ],
          [
           "## Usage tips\n\n- Since Funnel Transformer uses pooling, the sequence length of the hidden states cha..."
          ],
          [
           "## FunnelConfig\n\n[[autodoc]] FunnelConfig\n\n## FunnelTokenizer\n\n[[autodoc]] FunnelTokenizer\n    - bui..."
          ],
          [
           "[[autodoc]] TFFunnelBaseModel\n    - call\n\n## TFFunnelModel\n\n[[autodoc]] TFFunnelModel\n    - call\n\n##..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nor for an editable install:\n\n```bash\npip install -e .[quality]\n```\n\n\n## Tests\n\nAll the jobs tha..."
          ],
          [
           "```\n\nJust in case anything slipped through the cracks, the full test suite is also run daily.\n\n## Do..."
          ],
          [
           "```\n\nThis can take a lot of time, so to run the same thing on only the files you modified in the cur..."
          ],
          [
           "```\n\nAdditional checks concern PRs that add new models, mainly that:\n\n- All models added are in an A..."
          ],
          [
           "```\n\nNote that instead of applying this to a whole class, you can apply it to the relevant methods t..."
          ],
          [
           "```\n\nNote that there shouldn't be any spaces around the arrow (unless that space is part of the patt..."
          ],
          [
           "```\n\nIn this case, the code is copied from `BertForSequenceClassification` by replacing:\n- `Bert` by..."
          ],
          [
           "Movement Pruning: Adaptive Sparsity by Fine-Tuning\n\nAuthor: @VictorSanh\n\n*Magnitude pruning is a wid..."
          ],
          [
           "| Fine-pruning+Distillation<br>(Teacher=BERT-base fine-tuned) | BERT base<br>fine-tuned | Remaining<..."
          ],
          [
           "For more information, we invite you to check out [our paper](https://arxiv.org/abs/2005.07683).\nYou ..."
          ],
          [
           "While movement pruning does not directly optimize for memory footprint (but rather the number of non..."
          ],
          [
           "As examples, we release two English PruneBERT checkpoints (models fine-pruned from a pre-trained `BE..."
          ],
          [
           "Note that we built our experiments on top of a stabilized version of the library (commit https://git..."
          ],
          [
           "```\n\n### Fine-pruning with other methods\n\nWe can also explore other fine-pruning methods by changing..."
          ],
          [
           "```\n\nL0 regularization\n```bash\npython examples/movement-pruning/masked_run_squad.py \\\n    --output_d..."
          ],
          [
           "```\n\n### After fine-pruning\n\n**Counting parameters**\n\nRegularization based pruning methods (soft mov..."
          ],
          [
           "```\n@article{sanh2020movement,\n    title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},\n    a..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<div align=\"center\">\n    <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/H39Z_72..."
          ],
          [
           "### Encoder[[cv-encoder]]\n\nThe [Vision Transformer (ViT)](model_doc/vit) opened the door to computer..."
          ],
          [
           "### Decoder[[cv-decoder]]\n\nDecoder-only vision models are rare because most vision models rely on an..."
          ],
          [
           "### Encoder[[nlp-encoder]]\n\n[BERT](model_doc/bert) is an encoder-only Transformer that randomly mask..."
          ],
          [
           "### Decoder[[nlp-decoder]]\n\n[GPT-2](model_doc/gpt2) is a decoder-only Transformer that predicts the ..."
          ],
          [
           "### Encoder-decoder[[nlp-encoder-decoder]]\n\n[BART](model_doc/bart) keeps the original Transformer ar..."
          ],
          [
           "### Encoder[[audio-encoder]]\n\n[Wav2Vec2](model_doc/wav2vec2) uses a Transformer encoder to learn spe..."
          ],
          [
           "## Multimodal\n\n<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height=\"450\" src=\"..."
          ],
          [
           "[CLIP](model_doc/clip) takes a different approach and makes a pair prediction of (`image`, `text`) ...."
          ],
          [
           "## Reinforcement learning\n\n<iframe style=\"border: 1px solid rgba(0, 0, 0, 0.1);\" width=\"1000\" height..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Vers..."
          ],
          [
           "```\n\nIf you want to try out the brand new features, you might be interested in installing the librar..."
          ],
          [
           "```\n\n<Tip>\n\nYou can load a PEFT adapter with either an `AutoModelFor` class or the base model class ..."
          ],
          [
           "```\n\n## Add a new adapter\n\nYou can use [`~peft.PeftModel.add_adapter`] to add a new adapter to a mod..."
          ],
          [
           "```\n\n## Enable and disable adapters\n\nOnce you've added an adapter to a model, you can enable or disa..."
          ],
          [
           "```\n\nTo disable the adapter module:\n\n```py\nmodel.disable_adapters()\noutput = model.generate(**inputs..."
          ],
          [
           "```\n\n## Add additional trainable layers to a PEFT adapter\n\nYou can also fine-tune additional trainab..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [jegormeister](https://huggingface.co/jegormeister). The original code..."
          ],
          [
           "!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apac..."
          ],
          [
           "Both `Mistral-7B-v0.1` and `Mistral-7B-Instruct-v0.1` are released under the Apache 2.0 license.\n\n##..."
          ],
          [
           "```\n\nRaw weights for `Mistral-7B-v0.1` and `Mistral-7B-Instruct-v0.1` can be downloaded from:\n\n| Mod..."
          ],
          [
           "```\n\n## Combining Mistral and Flash Attention 2\n\nFirst, make sure to install the latest version of F..."
          ],
          [
           "```\n\n### Expected speedups\n\nBelow is a expected speedup diagram that compares pure inference time be..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Previous approaches preprocessed the audio to extract useful features from it. It is now more common..."
          ],
          [
           "```\n\n### Automatic speech recognition\n\nAutomatic speech recognition (ASR) transcribes speech into te..."
          ],
          [
           "```\n\n## Computer vision\n\nOne of the first and earliest successful computer vision tasks was recogniz..."
          ],
          [
           "```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"image-classification\")\n..."
          ],
          [
           "```\n\n### Object detection\n\nUnlike image classification, object detection identifies multiple objects..."
          ],
          [
           "```\n\n### Image segmentation\n\nImage segmentation is a pixel-level task that assigns every pixel in an..."
          ],
          [
           "```\n\n### Depth estimation\n\nDepth estimation predicts the distance of each pixel in an image from the..."
          ],
          [
           "```\n\n## Natural language processing\n\nNLP tasks are among the most common types of tasks because text..."
          ],
          [
           "```\n\n### Token classification\n\nIn any NLP task, text is preprocessed by separating the sequence of t..."
          ],
          [
           ">>> classifier = pipeline(task=\"ner\")\n>>> preds = classifier(\"Hugging Face is a French company based..."
          ],
          [
           "{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n{'entity': '..."
          ],
          [
           "```\n\n### Question answering\n\nQuestion answering is another token-level task that returns an answer t..."
          ],
          [
           "```\n\n### Summarization\n\nSummarization creates a shorter version of a text from a longer one while tr..."
          ],
          [
           "```\n\n### Translation\n\nTranslation converts a sequence of text in one language to another. It is impo..."
          ],
          [
           "```\n\n* masked: the model's objective is to predict a masked token in a sequence with full access to ..."
          ],
          [
           "```\n\n## Multimodal\n\nMultimodal tasks require a model to process multiple data modalities (text, imag..."
          ],
          [
           "```\n\nHopefully, this page has given you some more background information about all the types of task..."
          ],
          [
           "!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n\nLicensed under ..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recent work in language modeling demonstrates that t..."
          ],
          [
           "## Usage tips\n\nWe have provided pretrained [GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:..."
          ],
          [
           "```\n\nOnce you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a ..."
          ],
          [
           "Summarization (Seq2Seq model) training examples\n\nThe following example showcases how to finetune a s..."
          ],
          [
           "```\n\nThis should finish in 37min, with validation loss and ROUGE2 score of 1.7785 and 17.01 respecti..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The `generate()` method can be used to generate text using GPTSAN-Japanese model.\n\n```python\n>>> fro..."
          ],
          [
           "```\n\n## GPTSAN Features\n\nGPTSAN has some unique features. It has a model structure of Prefix-LM. It ..."
          ],
          [
           ">>> x_token = tokenizer(\"\", prefix_text=\"ï½±ï½²ï½³ï½´\")\ninput_ids:      | SOT | ï½± | ï½² | ï½³ | ï½´ | SEG |\ntoken_..."
          ],
          [
           "## GPTSanJapaneseModel\n\n[[autodoc]] GPTSanJapaneseModel\n\n## GPTSanJapaneseForConditionalGeneration\n\n..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image..."
          ],
          [
           "## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProce..."
          ],
          [
           "```\n\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],..."
          ],
          [
           "```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pre..."
          ],
          [
           "```\n\n**Use case 2: web page classification (training, inference) + token classification (inference),..."
          ],
          [
           "```\n\n**Use case 3: token classification (training), parse_html=False**\n\nFor token classification tas..."
          ],
          [
           "```\n\n**Use case 4: web page question answering (inference), parse_html=True**\n\nFor question answerin..."
          ],
          [
           "```\n\n**Use case 5: web page question answering (inference), parse_html=False**\n\nFor question answeri..."
          ],
          [
           "```\n\n## Resources\n\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/mast..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage tips\n\n- The implementation is the same as [Roberta](roberta) except instead of using _Add a..."
          ],
          [
           "[[autodoc]] RobertaPreLayerNormForMultipleChoice\n    - forward\n\n## RobertaPreLayerNormForTokenClassi..."
          ],
          [
           "[[autodoc]] FlaxRobertaPreLayerNormModel\n    - __call__\n\n## FlaxRobertaPreLayerNormForCausalLM\n\n[[au..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "### Summary\nIn Phi-1 and Phi-1.5 papers, the authors showed how important the quality of the data is..."
          ],
          [
           "The abstract from the Phi-1.5 paper is the following:\n\n*We continue the investigation into the power..."
          ],
          [
           "### Example :\n\n```python\n>>> from transformers import PhiForCausalLM, AutoTokenizer\n\n>>> # define th..."
          ],
          [
           "```\n\n\n## Combining Phi and Flash Attention 2\n\nFirst, make sure to install the latest version of Flas..."
          ],
          [
           "```\n\n### Expected speedups\nBelow is an expected speedup diagram that compares pure inference time be..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n{\"sentence1\": \"COVID-19 vaccine updates: How is the rollout proceeding?\", \"label\": \"news\"}\n{\"sen..."
          ],
          [
           "```\npython run_text_classification.py \\\n--model_name_or_path distilbert-base-cased \\\n--train_file tr..."
          ],
          [
           "```\n\n## run_glue.py\n\nThis script handles training on the GLUE dataset for various text classificatio..."
          ],
          [
           "```\npython run_glue.py \\\n--model_name_or_path distilbert-base-cased \\\n--task_name mnli \\\n--do_train ..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*State-of-the-art computer vision systems are trained..."
          ],
          [
           "To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-ove..."
          ],
          [
           "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help ..."
          ],
          [
           "**Image retrieval**\n\n- A [notebook](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9..."
          ],
          [
           "## CLIPConfig\n\n[[autodoc]] CLIPConfig\n    - from_text_vision_configs\n\n## CLIPTextConfig\n\n[[autodoc]]..."
          ],
          [
           "## TFCLIPModel\n\n[[autodoc]] TFCLIPModel\n    - call\n    - get_text_features\n    - get_image_features\n..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "For something slightly more challenging, you can also take a look at the [Good Second Issue](https:/..."
          ],
          [
           "To get the OS and software versions automatically, run the following command:\n\n```bash\ntransformers-..."
          ],
          [
           "```\n\nYou can also run the same command from the root of the repository:\n\n```bash\npython src/transfor..."
          ],
          [
           "```\n\n### Do you want a new feature?\n\nIf there is a new feature you'd like to see in ðŸ¤— Transformers, ..."
          ],
          [
           "## Do you want to add documentation?\n\nWe're always looking for improvements to the documentation tha..."
          ],
          [
           "```\n\n3. Create a new branch to hold your development changes:\n\n   ```bash\n   git checkout -b a-descr..."
          ],
          [
           "```\n\n   ðŸ¤— Transformers also uses `ruff` and a few custom scripts to check for coding mistakes. Quali..."
          ],
          [
           "```\n\n   If you've already opened a pull request, you'll need to force push with the `--force` flag. ..."
          ],
          [
           "### Pull request checklist\n\nâ˜ The pull request title should summarize your contribution.<br>\nâ˜ If yo..."
          ],
          [
           "â˜ All public methods must have informative docstrings (see\n[`modeling_bert.py`](https://github.com/h..."
          ],
          [
           "```\n\nSimilarly, for the `examples` directory, specify a *path to a subfolder or test file* to run th..."
          ],
          [
           "```\n\nLike the slow tests, there are other environment variables available which not enabled by defau..."
          ],
          [
           "```\n\nOne way to run the `make` command on Windows is with MSYS2:\n\n1. [Download MSYS2](https://www.ms..."
          ],
          [
           "!--Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace I..."
          ],
          [
           "This paper has been accepted to the [AAAI'23](https://aaai.org/Conferences/AAAI-23/) conference. \n\nT..."
          ],
          [
           "<small> BridgeTower architecture. Taken from the <a href=\"https://arxiv.org/abs/2206.08657\">original..."
          ],
          [
           ">>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n>>..."
          ],
          [
           "```\n\nThe following example shows how to run image-text retrieval using [`BridgeTowerProcessor`] and ..."
          ],
          [
           "```\n\nThe following example shows how to run masked language modeling using [`BridgeTowerProcessor`] ..."
          ],
          [
           "```\n\nTips:\n\n- This implementation of BridgeTower uses [`RobertaTokenizer`] to generate text embeddin..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2021 NVIDIA Corporation. All rights reserved.\nLicensed under the Apache License, Vers..."
          ],
          [
           "```\n\nIn the container:\n```\ncd transformers/examples/research_projects/quantization-qdqbert/\n```\n\n## ..."
          ],
          [
           "```\n\nUse `--recalibrate-weights` to calibrate the weight ranges according to the quantizer axis. Use..."
          ],
          [
           "```\npython3 ../../pytorch/question-answering/run_qa.py \\\n  --model_name_or_path bert-base-uncased \\\n..."
          ],
          [
           "```\n\n### Evaluate the INT8 PTQ ONNX model inference with TensorRT\n\n```\npython3 evaluate-hf-trt-qa.py..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Contrastive learning has shown remarkable success in..."
          ],
          [
           "[[autodoc]] ClapFeatureExtractor\n\n## ClapProcessor\n\n[[autodoc]] ClapProcessor\n\n## ClapModel\n\n[[autod..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "### XLM with language embeddings\n\nThe following XLM models use language embeddings to specify the la..."
          ],
          [
           "```\n\nThe `lang2id` attribute of the tokenizer displays this model's languages and their ids:\n\n```py\n..."
          ],
          [
           "```\n\nThe [run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/..."
          ],
          [
           "- `facebook/m2m100_418M` (Translation)\n- `facebook/m2m100_1.2B` (Translation)\n\nIn this example, load..."
          ],
          [
           "```\n\nTokenize the text:\n\n```py\n>>> encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n```\n\nM2..."
          ],
          [
           "```\n\n## MBart\n\nThe following MBart models can be used for multilingual translation:\n\n- `facebook/mba..."
          ],
          [
           "```\n\nTokenize the text:\n\n```py\n>>> encoded_en = tokenizer(en_text, return_tensors=\"pt\")\n```\n\nMBart f..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n\n## Generate text\n\nA language model trained for [causal language modeling](tasks/language_model..."
          ],
          [
           "Properly setting up the token selection step and the stopping condition is essential to make your mo..."
          ],
          [
           "```\n\nYou'll notice two flags in the `from_pretrained` call:\n\n - `device_map` ensures the model is mo..."
          ],
          [
           "```\n\nFinally, you don't need to do it one sequence at a time! You can batch your inputs, which will ..."
          ],
          [
           "```\n\n### Generated output is too short/long\n\nIf not specified in the [`~generation.GenerationConfig`..."
          ],
          [
           "```\n\n### Incorrect generation mode\n\nBy default, and unless specified in the [`~generation.Generation..."
          ],
          [
           "```\n\n### Wrong padding side\n\nLLMs are [decoder-only](https://huggingface.co/learn/nlp-course/chapter..."
          ],
          [
           "```\n\n### Wrong prompt\n\nSome models and tasks expect a certain input prompt format to work properly. ..."
          ],
          [
           ">>> set_seed(0)\n>>> messages = [\n...     {\n...         \"role\": \"system\",\n...         \"content\": \"You..."
          ],
          [
           "```\n\n## Further resources\n\nWhile the autoregressive generation process is relatively straightforward..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## DataCollatorForTokenClassification\n\n[[autodoc]] data.data_collator.DataCollatorForTokenClassifica..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We release Code Llama, a family of large language mo..."
          ],
          [
           "The `Llama2` family models, on which Code Llama is based, were trained using `bfloat16`, but the ori..."
          ],
          [
           "Here is a sample usage:\n\n```bash\npython src/transformers/models/llama/convert_llama_weights_to_hf.py..."
          ],
          [
           "```\n\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci..."
          ],
          [
           "```\n\nIf you only want the infilled part:\n```python\n>>> from transformers import pipeline\n>>> import ..."
          ],
          [
           "```\n\nUnder the hood, the tokenizer [automatically splits by `<FILL_ME>`](https://huggingface.co/docs..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nIf you get a terrible BLEU score, make sure that you didn't forget to use the `--source_prefix`..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Illustrating feature maps of the first stage looks like below.\n<div style=\"text-align: center\">\n<img..."
          ],
          [
           "```\n`feature_maps` object now has three feature maps, each can be accessed like below. Say we would ..."
          ],
          [
           "```\n\n`timm` models are also supported in transformers through `TimmBackbone` and `TimmBackboneConfig..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ), [Younes Belkada](http..."
          ],
          [
           "<PipelineTag pipeline=\"text-generation\" />\n\n- A notebook on [fine-tuning OPT with PEFT, bitsandbytes..."
          ],
          [
           "<PipelineTag pipeline=\"text-classification\" />\n\n- [Text classification task guide](sequence_classifi..."
          ],
          [
           "```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab..."
          ],
          [
           "```\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time b..."
          ],
          [
           "## TFOPTModel\n\n[[autodoc]] TFOPTModel\n    - call\n\n## TFOPTForCausalLM\n\n[[autodoc]] TFOPTForCausalLM\n..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Existing work in translation demonstrated the potent..."
          ],
          [
           "**Supervised Training**\n\n```python\nfrom transformers import M2M100Config, M2M100ForConditionalGenera..."
          ],
          [
           "```\n\n**Generation**\n\nM2M100 uses the `eos_token_id` as the `decoder_start_token_id` for generation w..."
          ],
          [
           "```\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nFLAN-T5 includes the same improvements as T5 version 1.1 (see [here](https://huggingface.co/doc..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Overview\n\nThe RoBERTa model was proposed in [RoBERTa: A Robustly Optimized BERT Pretraining Appro..."
          ],
          [
           "## Usage tips\n\n- This implementation is the same as [`BertModel`] with a tiny embeddings tweak as we..."
          ],
          [
           "<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog on [Getting Started with Sentiment Analysis ..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\"/>\n\n- [`RobertaForTokenClassification`] is supported by ..."
          ],
          [
           "<PipelineTag pipeline=\"fill-mask\"/>\n\n- A blog on [How to train a new language model from scratch usi..."
          ],
          [
           "<PipelineTag pipeline=\"question-answering\"/>\n\n- A blog on [Accelerated Inference with Optimum and Tr..."
          ],
          [
           "**Multiple choice**\n- [`RobertaForMultipleChoice`] is supported by this [example script](https://git..."
          ],
          [
           "[[autodoc]] RobertaForSequenceClassification\n    - forward\n\n## RobertaForMultipleChoice\n\n[[autodoc]]..."
          ],
          [
           "[[autodoc]] FlaxRobertaForCausalLM\n    - __call__\n\n## FlaxRobertaForMaskedLM\n\n[[autodoc]] FlaxRobert..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Ve..."
          ],
          [
           "Please discuss on the [forum](https://discuss.huggingface.co/) or in an [issue](https://github.com/h..."
          ],
          [
           "```\nThen cd in the example folder of your choice and run\n```bash\npip install -r requirements.txt..."
          ],
          [
           "```\n\nTo browse the examples corresponding to released versions of ðŸ¤— Transformers, click on the line ..."
          ],
          [
           "<details>\n  <summary>Examples for older versions of ðŸ¤— Transformers</summary>\n\t<ul>\n\t    <li><a href=..."
          ],
          [
           "<li><a href=\"https://github.com/huggingface/transformers/tree/v4.13.0/examples\">v4.13.0</a></li>\n\t\t<..."
          ],
          [
           "<li><a href=\"https://github.com/huggingface/transformers/tree/v4.4.2/examples\">v4.4.2</a></li>\n\t\t<li..."
          ],
          [
           "<li><a href=\"https://github.com/huggingface/transformers/tree/v3.1.0/examples\">v3.1.0</a></li>\n\t\t<li..."
          ],
          [
           "<li><a href=\"https://github.com/huggingface/transformers/tree/v2.4.0/examples\">v2.4.0</a></li>\n\t\t<li..."
          ],
          [
           "Alternatively, you can switch your cloned ðŸ¤— Transformers to a specific version (for instance with v3..."
          ],
          [
           "```\nand run the example command as usual afterward.\n\n## Running the Examples on Remote Hardware with..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Large multimodal models trained on natural documents..."
          ],
          [
           "## IdeficsImageProcessor\n\n[[autodoc]] IdeficsImageProcessor\n    - preprocess\n\n## IdeficsProcessor\n\n[..."
          ],
          [
           "!---\nCopyright 2022 The Microsoft Inc. and The HuggingFace Inc. Team. All rights reserved.\n\nLicensed..."
          ],
          [
           "### What Questions Can be Answered\n\nBenefiting from the powerfulness of generative models, TAPEX can..."
          ],
          [
           "```bash\nexport EXP_NAME=wikisql_tapex_base\n\npython run_wikisql_with_tapex.py \\\n  --do_train \\\n  --do..."
          ],
          [
           "```\n\n#### TAPEX-Large on WikiSQL\n\nHere is how to run the script on the WikiSQL with `tapex-large`:\n>..."
          ],
          [
           "```\n\n#### TAPEX-Base on WikiTableQuestions\n\nHere is how to run the script on the WikiTableQuestions ..."
          ],
          [
           "```\n\n#### TAPEX-Large on WikiTableQuestions\n\nHere is how to run the script on the WikiTableQuestions..."
          ],
          [
           "```\n\n### How to Evaluate TAPEX Fine-tuned Models on TableQA\n\nWe provide fine-tuned model weights to ..."
          ],
          [
           "```\n\n## Table Fact Verification Tasks\n\n### What is Table Fact Verification\n\n![Example](https://table..."
          ],
          [
           "```\n\n#### TAPEX-Large on TabFact\n\nHere is how to run the script on the TabFact:\n> The default hyper-..."
          ],
          [
           "```\n\n### How to Evaluate TAPEX Fine-tuned Models on TableFV\n\nWe provide fine-tuned model weights to ..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/\">English</..."
          ],
          [
           "ðŸ¤— Transformers aporta miles de modelos preentrenados Para realizar tareas en diferentes modalidades ..."
          ],
          [
           "ðŸ¤— Transformers estÃ¡ respaldado por las tres bibliotecas de deep learning mÃ¡s populares â€” [Jax](https..."
          ],
          [
           "En procesamiento del lenguaje natural:\n- [TerminaciÃ³n de palabras enmascaradas con BERT](https://hug..."
          ],
          [
           "- [Inferencia del lenguaje natural con RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+d..."
          ],
          [
           "- [Responder a preguntas con..."
          ],
          [
           "- [TraducciÃ³n con T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)..."
          ],
          [
           "En visiÃ³n de ordenador:\n- [ClasificaciÃ³n de imÃ¡genes con ViT](https://huggingface.co/google/vit-base..."
          ],
          [
           "## Si estÃ¡ buscando soporte personalizado del equipo de Hugging Face\n\n<a target=\"_blank\" href=\"https..."
          ],
          [
           "```\n\nLa segunda lÃ­nea de cÃ³digo descarga y almacena en cachÃ© el modelo previamente entrenado que usa..."
          ],
          [
           "# Allocate a pipeline for object detection\n>>> object_detector = pipeline('object_detection')\n>>> ob..."
          ],
          [
           "```\n\nAquÃ­ obtenemos una lista de objetos detectados en la imagen, con un cuadro que rodea el objeto ..."
          ],
          [
           "```\n\nY aquÃ­ estÃ¡ el cÃ³digo equivalente para TensorFlow:\n```python\n>>> from transformers import AutoT..."
          ],
          [
           "```\n\nEl tokenizador es responsable de todo el preprocesamiento que espera el modelo preentrenado y s..."
          ],
          [
           "1. Menores costes de cÃ³mputo, menor huella de carbono:\n    - Los investigadores pueden compartir mod..."
          ],
          [
           "## Â¿Por quÃ© no deberÃ­a usar transformers?\n\n- Esta biblioteca no es una caja de herramientas modular ..."
          ],
          [
           "Primero, crea un entorno virtual con la versiÃ³n de Python que vas a usar y actÃ­valo.\n\nLuego, deberÃ¡s..."
          ],
          [
           "```\n\nSi deseas jugar con los ejemplos o necesitas la Ãºltima versiÃ³n del cÃ³digo y no puedes esperar a..."
          ],
          [
           "```\n\nSigue las pÃ¡ginas de instalaciÃ³n de Flax, PyTorch o TensorFlow para ver cÃ³mo instalarlos con co..."
          ],
          [
           "1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (from OpenAI) released w..."
          ],
          [
           "1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with ..."
          ],
          [
           "1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) rele..."
          ],
          [
           "1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Me..."
          ],
          [
           "1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (from Facebook) released with..."
          ],
          [
           "1. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)** (from Mistral AI) by Th..."
          ],
          [
           "1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) releas..."
          ],
          [
           "1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) release..."
          ],
          [
           "1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released ..."
          ],
          [
           "1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (from UCLA NLP) released ..."
          ],
          [
           "1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)..."
          ],
          [
           "1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from Microsoft Research) r..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Faceb..."
          ],
          [
           "Para comprobar si cada modelo tiene una implementaciÃ³n en Flax, PyTorch o TensorFlow, o tiene un tok..."
          ],
          [
           "## Aprender mÃ¡s\n\n| SecciÃ³n | DescripciÃ³n |\n|-|-|\n| [DocumentaciÃ³n](https://huggingface.co/docs/trans..."
          ],
          [
           "## CitaciÃ³n\n\nAhora nosotros tenemos un [papel](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Development of the model was led by Sid Black, Stella Biderman and Eric Hallahan, and the model was ..."
          ],
          [
           "```\n\nGPT-NeoX-20B also has a different tokenizer from the one used in GPT-J-6B and GPT-Neo. The new ..."
          ],
          [
           "```\n\n## Using Flash Attention 2\n\nFlash Attention 2 is an faster, optimized version of the model.\n\n##..."
          ],
          [
           "```\n\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\n\nWhile the main concepts discussed in this guide are likely applicable across frameworks, here..."
          ],
          [
           "TP is almost always used within a single node. That is TP size <= GPUs per node.\n\n**Case 3: Largest ..."
          ],
          [
           "### DataParallel vs DistributedDataParallel\n\nTo understand the key differences in inter-GPU communic..."
          ],
          [
           "This is not an exhaustive list of differences between DP and DDP, however, other nuances are out of ..."
          ],
          [
           "```\nrm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \\\npython examples/pytorch/language-modeling/run_cl..."
          ],
          [
           "```\n\n**DDP w/o NVlink**\n\n```\nrm -r /tmp/test-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \\\ntorc..."
          ],
          [
           "```\n\nHere are the same benchmarking results gathered in a table for convenience:\n\n| Type   | NVlink ..."
          ],
          [
           "```\n\nIf we have 3 GPUs, ZeRO-DP splits the model onto 3 GPUs like so:\n\n```\nGPU0:\nLa | Lb | Lc\n---|--..."
          ],
          [
           "```\n\nThe inputs are passed without modifications as if they would be processed by the original model..."
          ],
          [
           "</Tip>\n\nWhile reading the literature on this topic you may encounter the following synonyms: Sharded..."
          ],
          [
           "```\n===================  ===================\n|  0 | 1 | 2 | 3  |  |  4 | 5 | 6 | 7  |\n==============..."
          ],
          [
           "```\n\nIn this example, when data moves from layer 0 to 3, it's no different from regular forward pass..."
          ],
          [
           "The following illustration from the [GPipe paper](https://ai.googleblog.com/2019/03/introducing-gpip..."
          ],
          [
           "Note that this is the same concept as gradient accumulation steps. PyTorch uses `chunks`, while Deep..."
          ],
          [
           "Pipeline API solutions have been implemented in:\n- PyTorch\n- DeepSpeed\n- Megatron-LM\n\nThese come wit..."
          ],
          [
           "We have not experimented with Varuna and SageMaker but their papers report that they have overcome t..."
          ],
          [
           "<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documen..."
          ],
          [
           "If we split the weight matrix `A` column-wise across `N` GPUs and perform matrix multiplications `XA..."
          ],
          [
           "This section is based on the original much more [detailed TP overview](https://github.com/huggingfac..."
          ],
          [
           "## Data Parallelism + Pipeline Parallelism\n\nThe following diagram from the DeepSpeed [pipeline tutor..."
          ],
          [
           "To get an even more efficient training a 3D parallelism is used where PP is combined with TP and DP...."
          ],
          [
           "When ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1 (optimiz..."
          ],
          [
           "## FlexFlow\n\n[FlexFlow](https://github.com/flexflow/FlexFlow) also solves the parallelization proble..."
          ],
          [
           "The significance of this framework is that it takes resources like (1) GPU/TPU/CPU vs. (2) RAM/DRAM ..."
          ],
          [
           "### Number of GPUs\n\nFor example, if you have 4 GPUs and you only want to use the first 2:\n\n<hfoption..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Accelerate\">\n\nUse `--num_processes` to select how many GPUs to use.\n\n..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nAs with any environment variable, they can be exported instead of being a..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "### Memory usage and data loading\n\nOne thing to note is that all data is loaded into memory in this ..."
          ],
          [
           "```\npython run_qa.py \\\n--model_name_or_path distilbert-base-cased \\\n--output_dir output \\\n--dataset_..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In this paper, we design and train a Generative Imag..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The origi..."
          ],
          [
           ">>> # train...\n>>> loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).lo..."
          ],
          [
           "```\n\nPretrained [`EncoderDecoderModel`] are also directly available in the model hub, e.g.:\n\n```pyth..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pre-trained Language Models (PLMs) have proven to be..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The recently-developed DETR approach applies the tra..."
          ],
          [
           "## Resources\n\n- [Object detection task guide](../tasks/object_detection)\n\n## ConditionalDetrConfig\n\n..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Convolutional Neural Networks (ConvNets) are commonl..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nWe now have a tokenizer trained on the files we defined. We can either continue using it in tha..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\n**Note:** This script only works with models that have a fast tokenizer (backed by the [ðŸ¤— Token..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```bash\npython run_audio_classification.py \\\n    --model_name_or_path facebook/wav2vec2-base \\\n    -..."
          ],
          [
           "```\n\nOn a single V100 GPU (16GB), this script should run in ~14 minutes and yield accuracy of **98.2..."
          ],
          [
           "```\n\nOn 4 V100 GPUs (16GB), this script should run in ~1 hour and yield accuracy of **79.45%**.\n\nðŸ‘€ S..."
          ],
          [
           "```\n\n### Examples\n\nThe following table shows a couple of demonstration fine-tuning runs.\nIt has been..."
          ],
          [
           "| Dataset | Pretrained Model | # transformer layers | Accuracy on eval | GPU setup | Training time |..."
          ],
          [
           "| Keyword Spotting | [asapp/sew-mid-100k](https://huggingface.co/asapp/sew-mid-100k) | 24 | 0.9757 |..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "*Recent advancements in automatic speech translation have dramatically expanded language coverage, i..."
          ],
          [
           "model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. Th..."
          ],
          [
           "oneâ€™s voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attenti..."
          ],
          [
           "naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we impl..."
          ],
          [
           "a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a sci..."
          ],
          [
           "## Usage\n\nIn the following example, we'll load an Arabic audio sample and an English text sample and..."
          ],
          [
           "```\n\nYou can seamlessly use this model on text or on audio, to generated either translated text or t..."
          ],
          [
           "```\n\nWith basically the same code, I've translated English text and Arabic speech to Russian speech ..."
          ],
          [
           "```\n\nOr you can replace the text-to-text generation snippet with the model dedicated to the T2TT tas..."
          ],
          [
           "```\n\nFeel free to try out [`SeamlessM4Tv2ForSpeechToText`] and [`SeamlessM4Tv2ForTextToSpeech`] as w..."
          ],
          [
           "### Difference with SeamlessM4T-v1\n\nThe architecture of this new version differs from the first in a..."
          ],
          [
           "This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be ..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\n## Usage..."
          ],
          [
           "<PipelineTag pipeline=\"automatic-speech-recognition\"/>\n\n- A blog post on [boosting Wav2Vec2 with n-g..."
          ],
          [
           "## Wav2Vec2Config\n\n[[autodoc]] Wav2Vec2Config\n\n## Wav2Vec2CTCTokenizer\n\n[[autodoc]] Wav2Vec2CTCToken..."
          ],
          [
           ">>> # import model, feature extractor, tokenizer\n>>> model = AutoModelForCTC.from_pretrained(\"patric..."
          ],
          [
           "...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text\n...     batch[\"trans..."
          ],
          [
           "```\n\n## Wav2Vec2 specific outputs\n\n[[autodoc]] models.wav2vec2_with_lm.processing_wav2vec2_with_lm.W..."
          ],
          [
           "## Wav2Vec2ForPreTraining\n\n[[autodoc]] Wav2Vec2ForPreTraining\n    - forward\n\n</pt>\n<tf>\n\n## TFWav2Ve..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "where \\\\(\\log p_\\theta (x_i|x_{<i})\\\\) is the log-likelihood of the ith token conditioned on the pre..."
          ],
          [
           "Instead, the sequence is typically broken into subsequences equal to the model's maximum input size...."
          ],
          [
           "## Example: Calculating perplexity with GPT-2 in ðŸ¤— Transformers\n\nLet's demonstrate this process with..."
          ],
          [
           "```\n\nWe'll load in the WikiText-2 dataset and evaluate the perplexity using a few different sliding-..."
          ],
          [
           "```\n\nWith ðŸ¤— Transformers, we can simply pass the `input_ids` as the `labels` to our model, and the a..."
          ],
          [
           "```\n\nRunning this with the stride length equal to the max input length is equivalent to the suboptim..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Ve..."
          ],
          [
           "| Task | Example model | Example dataset | ðŸ¤— Datasets | Colab\n|---|---|---|:---:|:---:|\n| [**`causal..."
          ],
          [
           "## Intro: JAX and Flax\n\n[JAX](https://github.com/google/jax) is a numerical computation library that..."
          ],
          [
           "Each example README contains more details on the specific model and training\nprocedure.\n\n\n## Running..."
          ],
          [
           "To specify a given repository name, use the `--hub_model_id` argument. You will need to specify the ..."
          ],
          [
           "# ðŸ”¥ Model cards now live inside each huggingface.co model repo ðŸ”¥\n\n\nFor consistency, ease of use and ..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "Dictionary\n\nHugging Face: à¤—à¤²à¥‡ à¤²à¤—à¤¾à¤“ à¤šà¥‡à¤¹à¤°à¤¾\ntoken: à¤¶à¤¬à¥à¤¦ (à¤”à¤° à¤®à¥‚à¤² à¤…à¤‚à¤—à¥à¤°à¥‡à¤œà¥€ à¤•à¥‹ à¤•à¥‹à¤·à¥à¤ à¤• à¤®à¥‡à¤‚ à¤šà¤¿à¤¹à¥à¤¨à¤¿à¤¤ à¤•à¤°à¥‡à¤‚ï¼‰\nto..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/\">English</..."
          ],
          [
           "ðŸ¤— Transformers 100 à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤®à¥‡à¤‚ à¤ªà¤¾à¤  à¤µà¤°à¥à¤—à¥€à¤•à¤°à¤£, à¤¸à¥‚à¤šà¤¨à¤¾ à¤¨à¤¿à¤·à¥à¤•à¤°à¥à¤·à¤£, à¤ªà¥à¤°à¤¶à¥à¤¨ à¤‰à¤¤à¥à¤¤à¤°, à¤¸à¤¾à¤°à¤¾à¤‚à¤¶à¥€à¤•à¤°à¤£, à¤…à¤¨à¥à¤µà¤¾..."
          ],
          [
           "ðŸ¤— Transformers à¤¤à¥€à¤¨ à¤¸à¤¬à¤¸à¥‡ à¤²à¥‹à¤•à¤ªà¥à¤°à¤¿à¤¯ à¤—à¤¹à¤¨ à¤¶à¤¿à¤•à¥à¤·à¤£ à¤ªà¥à¤¸à¥à¤¤à¤•à¤¾à¤²à¤¯à¥‹à¤‚ à¤•à¤¾ à¤¸à¤®à¤°à¥à¤¥à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆï¼š [Jax](https://jax.readthe..."
          ],
          [
           "à¤¯à¤¹à¤¾à¤ à¤•à¥à¤› à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤¹à¥ˆà¤‚ï¼š\n- [à¤¶à¤¬à¥à¤¦ à¤•à¥‹ à¤­à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤®à¤¾à¤¸à¥à¤• à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ BERT à¤•à¤¾ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚](https://huggingfac..."
          ],
          [
           "- [à¤¬à¤¾à¤°à¥à¤Ÿ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤ªà¤¾à¤  à¤¸à¤¾à¤°à¤¾à¤‚à¤¶](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+met..."
          ],
          [
           "- [à¤¡à¤¿à¤¸à¥à¤Ÿà¤¿à¤²à¤¬à¤°à¥à¤Ÿ à¤•à¥‡ à¤¸à¤¾à¤¥..."
          ],
          [
           "à¤ªà¥à¤°à¤¶à¥à¤¨à¥‹à¤¤à¥à¤¤à¤°](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+..."
          ],
          [
           "is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portug..."
          ],
          [
           "28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazo..."
          ],
          [
           "2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwou..."
          ],
          [
           "regenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+fores..."
          ],
          [
           "af+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C00..."
          ],
          [
           "000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%2..."
          ],
          [
           "etres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belong..."
          ],
          [
           "y+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+..."
          ],
          [
           "0%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amoun..."
          ],
          [
           "or+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+dep..."
          ],
          [
           "s+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+h..."
          ],
          [
           "s+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse..."
          ],
          [
           "odiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees..."
          ],
          [
           "al+trees+divided+into+16%2C000+species)..."
          ],
          [
           "- [à¤…à¤¨à¥à¤µà¤¾à¤¦ à¤•à¥‡ à¤²à¤¿à¤ T5 à¤•à¤¾ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+li..."
          ],
          [
           "**[Write With Transformer](https://transformer.huggingface.co)**ï¼Œà¤¹à¤—à¤¿à¤‚à¤— à¤«à¥‡à¤¸ à¤Ÿà¥€à¤® à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤¬à¤¨à¤¾à¤¯à¤¾ à¤—à¤¯à¤¾, à¤¯à¤¹ ..."
          ],
          [
           "```python\n>>> from transformers import pipeline\n\n# à¤­à¤¾à¤µà¤¨à¤¾ à¤µà¤¿à¤¶à¥à¤²à¥‡à¤·à¤£ à¤ªà¤¾à¤‡à¤ªà¤²à¤¾à¤‡à¤¨ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾\n>>> classif..."
          ],
          [
           "```\n\nà¤•à¥‹à¤¡ à¤•à¥€ à¤¦à¥‚à¤¸à¤°à¥€ à¤ªà¤‚à¤•à¥à¤¤à¤¿ à¤ªà¤¾à¤‡à¤ªà¤²à¤¾à¤‡à¤¨ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤ à¤—à¤ à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤¡à¤¾à¤‰à¤¨à¤²à¥‹à¤¡ à¤”à¤° à¤•à¥ˆà¤¶ à¤•à¤°à¤¤à¥€ à¤¹..."
          ],
          [
           "```\n\nà¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤¨à¥‡ à¤•à¥‡ à¤…à¤²à¤¾à¤µà¤¾, à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤² à¤¸à¤‚à¤—à¤¤ à¤†à¤¤à¥à¤®à¤µà¤¿à¤¶à¥à¤µà¤¾à¤¸ à¤¸à¥à¤•à¥‹à¤° à¤­à¥€ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¹à¤¾à¤‚ à¤‰à¤¤à¥à¤¤à¤° à¤Ÿà¥‹à¤•à¤¨à¤¯à¥..."
          ],
          [
           "```\n\nà¤Ÿà¥‹à¤•à¤¨à¤¨à¤¾à¤‡à¤œà¤¼à¤° à¤¸à¤­à¥€ à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤²à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥à¤°à¥€à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸à¤¿à¤‚à¤— à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤”à¤° à¤‡à¤¸à¥‡ à¤¸à¥€à¤§à¥‡ à¤à¤• à¤¸à¥à¤Ÿà¥..."
          ],
          [
           "## à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¤¾à¤°à¥à¤®à¤° à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥à¤¯à¥‹à¤‚ à¤•à¤°à¥‡à¤‚?\n\n1. à¤‰à¤ªà¤¯à¥‹à¤— à¤®à¥‡à¤‚ à¤†à¤¸à¤¾à¤¨à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤¨à¥à¤¨à¤¤ à¤®à¥‰à¤¡à¤²:\n    - à¤à¤¨à¤à¤²à¤¯à¥‚ à¤”à¤° à¤à¤¨à¤à¤²à¤œà¥€ à¤ª..."
          ],
          [
           "1. à¤†à¤¸à¤¾à¤¨à¥€ à¤¸à¥‡ à¤…à¤¨à¤¨à¥à¤¯ à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤…à¤¨à¥à¤•à¥‚à¤²à¤¿à¤¤ à¤•à¤°à¥‡à¤‚ à¤”à¤° à¤…à¤ªà¤¨à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾à¤“à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤®à¤¾à¤®à¤²à¥‹à¤‚ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚:\n    - à¤¹à¤® à¤®à¥‚à¤²..."
          ],
          [
           "## à¤®à¥à¤à¥‡ à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¥‰à¤°à¥à¤®à¤° à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¬ à¤¨à¤¹à¥€à¤‚ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤?\n\n- à¤¯à¤¹ à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤®à¥‰à¤¡à¥à¤¯à¥‚à¤²à¤° à¤¨à¥à¤¯à¥‚à¤°à¤² à¤¨à¥‡à¤Ÿà¤µà¤°à¥à¤• à¤Ÿà¥‚à¤²à¤¬à¥‰à¤•à¥à¤¸ à¤¨..."
          ],
          [
           "## à¤¸à¥à¤¥à¤¾à¤ªà¤¿à¤¤ à¤•à¤°à¤¨à¤¾\n\n### à¤ªà¤¿à¤ª à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾\n\nà¤‡à¤¸ à¤°à¤¿à¤ªà¥‰à¤œà¤¿à¤Ÿà¤°à¥€ à¤•à¤¾ à¤ªà¤°à¥€à¤•à¥à¤·à¤£ Python 3.8+, Flax 0.4.1+, PyTorch 1...."
          ],
          [
           "à¤¦à¥‡à¤–à¥‡à¤‚ start-locally à¤¯à¤¾ [Flax à¤¸à¥à¤¥à¤¾à¤ªà¤¨à¤¾ à¤ªà¥ƒà¤·à¥à¤ ](https://github.com/google/flax#quick-install).\n\nà¤œà¤¬ à¤‡à¤¨à¤®à¥‡à¤‚..."
          ],
          [
           "```\n\nà¤¯à¤¦à¤¿ à¤†à¤ª à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥‡ à¤®à¤¾à¤®à¤²à¥‹à¤‚ à¤•à¥‹ à¤†à¤œà¤¼à¤®à¤¾à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤¯à¤¾ à¤†à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤• à¤°à¤¿à¤²à¥€à¤œà¤¼ à¤¸à¥‡ à¤ªà¤¹à¤²à¥‡ à¤¨à¤µà¥€à¤¨à¤¤à¤® à¤‡à¤¨-à¤¡à¥‡à¤µà¤²à¤ªà¤®à¥‡à¤‚à¤Ÿ à¤•à¥‹à¤¡ ..."
          ],
          [
           "```\n\nà¤•à¥‹à¤‚à¤¡à¤¾ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ Flax, PyTorch, à¤¯à¤¾ TensorFlow à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¤¿à¤¸à¥€ à¤à¤• à¤•à¥‹ à¤¸à¥à¤¥à¤¾à¤ªà¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶à¥‹..."
          ],
          [
           "1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (Google Research and the ..."
          ],
          [
           "1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectr..."
          ],
          [
           "1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (à¤¸à¥‡ Ã‰cole polytechnique..."
          ],
          [
           "1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (à¤—à¥‚à¤—à¤² à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° [à¤¬à¥€à¤ˆà¤†..."
          ],
          [
           "1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (à¤—à¥‚à¤—à¤² à¤°..."
          ],
          [
           "1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (from Microsoft Research ..."
          ],
          [
           "1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (à¤«à¥‡à¤¸à¤¬à¥..."
          ],
          [
           "1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (à¤à¤²à¥‡à¤•à¥à¤¸à¤¾ à¤¸à¥‡) à¤•à¤¾à¤—à¤œ à¤•à¥‡ à¤¸à¤¾à¤¥ [à¤¬à¥€à¤ˆ..."
          ],
          [
           "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (Google à¤…à¤¨à¥à¤¸à¤‚à¤§à¤¾à¤¨ à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ ..."
          ],
          [
           "1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (Google à¤°à¤¿à¤¸à¤°à¥à¤š à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡..."
          ],
          [
           "1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (OpenAI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° [à¤²à¤°..."
          ],
          [
           "1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (à¤¸à¥‡à¤²à¥à¤¸à¤«à¥‹à¤°à¥à¤¸ à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚..."
          ],
          [
           "1. **[Conditional DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)** (à¤®à¤¾à¤‡à¤•..."
          ],
          [
           "1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI..."
          ],
          [
           "1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)** (à¤¸à¥‡à¤²à¥à¤¸à¤«à¥‹à¤°à¥à¤¸ à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° ..."
          ],
          [
           "1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (Microsoft à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ ..."
          ],
          [
           "1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (à¤¸à¥‡à¤‚à¤¸à¤Ÿà¤¾..."
          ],
          [
           "1. **[DETA](https://huggingface.co/docs/transformers/model_doc/deta)** (from The University of Texas..."
          ],
          [
           "1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)** (from SHI Labs) released wi..."
          ],
          [
           "1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (à¤¹à¤—à¤¿à¤‚à¤—à¤«à¥‡à¤¸ à¤¸à¥‡), à¤¸à¤¾..."
          ],
          [
           "1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (NAVER à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤•à¤¾à¤—à¤œ [OC..."
          ],
          [
           "1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from S..."
          ],
          [
           "1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (Meta AI à¤¸à¥‡) Alexandre ..."
          ],
          [
           "1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (Baidu à¤¸à¥‡) Xuan Ouyang, ..."
          ],
          [
           "1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (à¤®à¥‡à¤Ÿà¤¾ AI à¤¸à¥‡) à¤Ÿà¥à¤°à¤¾à¤‚à¤¸à¤«à¥‰à¤°à¥à¤®à¤° à¤ªà¥à¤°à¥‹à¤Ÿ..."
          ],
          [
           "à¤œà¤¼à¤¿à¤Ÿà¤¨à¤¿à¤•, à¤œà¥‡à¤°à¥€ à¤®à¤¾ à¤”à¤° à¤°à¥‰à¤¬ à¤«à¤°à¥à¤—à¤¸à¥¤ **ESM-1v** à¤•à¥‹ à¤ªà¥‡à¤ªà¤° à¤•à¥‡ à¤¸à¤¾à¤¥ à¤œà¤¾à¤°à¥€ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¥à¤¾ [à¤­à¤¾à¤·à¤¾ à¤®à¥‰à¤¡à¤² à¤ªà¥à¤°à¥‹à¤Ÿà¥€à¤¨ à¤«à¤¼à¤‚à¤•à¥à¤¶à¤¨..."
          ],
          [
           "à¤ªà¥à¤°à¥‹à¤Ÿà¥€à¤¨ à¤…à¤¨à¥à¤•à¥à¤°à¤® à¤¸à¤Ÿà¥€à¤• à¤¸à¤‚à¤°à¤šà¤¨à¤¾ à¤­à¤µà¤¿à¤·à¥à¤¯à¤µà¤¾à¤£à¥€ à¤•à¥‹ à¤¸à¤•à¥à¤·à¤® à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚](https://doi.org/10.1101/2022.07.20.500902)..."
          ],
          [
           "1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) rele..."
          ],
          [
           "1. **[FLAVA](https://huggingface.co/docs/transformers/model_doc/flava)** (FLAVA: A à¤«à¤¾à¤‰à¤‚à¤¡à¥‡à¤¶à¤¨à¤² à¤²à¥ˆà¤‚à¤—à¥à¤µà¥‡..."
          ],
          [
           "1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (à¤¸à¥€à¤à¤®à¤¯à¥‚/à¤—à¥‚à¤—à¤² ..."
          ],
          [
           "1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (KAIST à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° [à¤µà¤°à¥..."
          ],
          [
           "1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (EleutherAI à¤¸à¥‡) à¤ªà¥‡à¤ªà¤° ..."
          ],
          [
           "1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (EleutherAI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ª..."
          ],
          [
           "1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (BigCode à¤¸à¥‡) Lou..."
          ],
          [
           "1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)** (from Microsoft) ..."
          ],
          [
           "1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° ..."
          ],
          [
           "1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** (from OpenAI) release..."
          ],
          [
           "1. **[KOSMOS-2](https://huggingface.co/docs/transformers/model_doc/kosmos-2)** (from Microsoft Resea..."
          ],
          [
           "1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (à¤®à¤¾à¤‡à¤•à¥à¤°à¥‹à¤¸à¥‰à¤«à¥à¤Ÿ à¤°à¤¿à¤¸..."
          ],
          [
           "1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (à¤®à¥‡à¤Ÿà¤¾ AI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° ..."
          ],
          [
           "1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (The FAIR team of Meta AI..."
          ],
          [
           "1. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)** (Microsoft Research & Unive..."
          ],
          [
           "1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (UNC à¤šà¥ˆà¤ªà¤² à¤¹à¤¿à¤² à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚..."
          ],
          [
           "1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) rel..."
          ],
          [
           "1. **[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former)** (FAIR and UIUC ..."
          ],
          [
           "1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° [à¤¨..."
          ],
          [
           "1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (NVIDIA à¤¸à¥‡)..."
          ],
          [
           "1. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)** (from Mistral AI) by Th..."
          ],
          [
           "1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (Facebook à¤¸à¥‡) Vineel Pratap, An..."
          ],
          [
           "1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (from Google I..."
          ],
          [
           "1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (MosaiML à¤¸à¥‡) the MosaicML NLP T..."
          ],
          [
           "1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released ..."
          ],
          [
           "1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (à¤«à¥à¤°à¥‰à¤® à¤®à¥‡à¤Ÿà¤¾) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤ªà¥‡à¤ªà¤° [à¤¨à¥‹..."
          ],
          [
           "1. **[NystrÃ¶mformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (à¤µà¤¿à¤¸à¥à¤•à¥‰à¤¨à¥à¤¸à¤¿..."
          ],
          [
           "1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released ..."
          ],
          [
           "1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Researc..."
          ],
          [
           "1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (à¤¦à¥€à¤ªà¤®à¤¾à¤‡à¤‚à¤¡ à¤¸à¥‡) à¤¸à¤¾..."
          ],
          [
           "1. **[Phi](https://huggingface.co/docs/transformers/model_doc/phi)** (from Microsoft) released with ..."
          ],
          [
           "1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (Google à¤¸à¥‡) Kento..."
          ],
          [
           "1. **[Pop2Piano](https://huggingface.co/docs/transformers/model_doc/pop2piano)** released with the p..."
          ],
          [
           "1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (NVIDIA à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡..."
          ],
          [
           "1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research..."
          ],
          [
           "1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (à¤®à¤¾à¤‡à¤•à¥à¤°à¥‹à¤¸à¥‰à¤«à¥à¤Ÿ à¤°à¤¿à¤¸à¤°à¥à¤š à¤¸à¥‡) ..."
          ],
          [
           "1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)** (from WeChatAI) releas..."
          ],
          [
           "1. **[SeamlessM4Tv2](https://huggingface.co/docs/transformers/model_doc/seamless_m4t_v2)** (from Met..."
          ],
          [
           "1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** (ASAPP à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤¦à¥‡à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° [..."
          ],
          [
           "1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** ..."
          ],
          [
           "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (à¤¬à¤°à¥à¤•à¤²à¥‡ à¤¸à¥‡) à¤•à¤¾à¤—..."
          ],
          [
           "1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (Microsoft à¤¸..."
          ],
          [
           "1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (æ¥è‡ª Google AI)à¤•à¥‰à¤²à¤¿à¤¨ à¤°à¥ˆà¤«à¥‡à¤² à¤”à¤° à¤¨à¥‹à¤® ..."
          ],
          [
           "1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (Google AI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤•à¤¾à¤—à¤œ..."
          ],
          [
           "1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transfor..."
          ],
          [
           "1. **[TVP](https://huggingface.co/docs/transformers/model_doc/tvp)** (from Intel) released with the ..."
          ],
          [
           "1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (à¤®à¤¾à¤‡à¤•à¥à¤°à¥‹à¤¸à¥‰à¤«à¥à¤Ÿ à¤°à¤¿à¤¸à¤°à¥..."
          ],
          [
           "1. **[UnivNet](https://huggingface.co/docs/transformers/model_doc/univnet)** (from Kakao Corporation..."
          ],
          [
           "1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (à¤®à¤²à¥à¤Ÿà¥€à¤®à¥€à¤¡à¤¿à¤¯à¤¾ à¤•à¤®à¥à¤ªà¥à¤¯à¥‚à¤Ÿ..."
          ],
          [
           "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (à¤—à¥‚à¤—à¤² à¤à¤†à¤ˆ ..."
          ],
          [
           "1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) ..."
          ],
          [
           "1. **[ViTMatte](https://huggingface.co/docs/transformers/model_doc/vitmatte)** (HUST-VL à¤¸à¥‡) Jingfeng..."
          ],
          [
           "1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)** (from Google Research) rele..."
          ],
          [
           "1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)** (Faceb..."
          ],
          [
           "1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (OpenAI à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤•à¤¾à¤—..."
          ],
          [
           "1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)** (From Facebook AI) released w..."
          ],
          [
           "1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤à¤†à¤ˆ à¤¸à¥‡)..."
          ],
          [
           "1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** (Google/CMU à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤à¤†à¤ˆ..."
          ],
          [
           "1. à¤à¤• à¤¨à¤ à¤®à¥‰à¤¡à¤² à¤®à¥‡à¤‚ à¤¯à¥‹à¤—à¤¦à¤¾à¤¨ à¤¦à¥‡à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚? à¤¨à¤ à¤®à¥‰à¤¡à¤² à¤œà¥‹à¤¡à¤¼à¤¨à¥‡ à¤®à¥‡à¤‚ à¤†à¤ªà¤•à¤¾ à¤®à¤¾à¤°à¥à¤—à¤¦à¤°à¥à¤¶à¤¨ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¤®à¤¾à¤°à¥‡ à¤ªà¤¾à¤¸ à¤à¤•..."
          ],
          [
           "à¤¯à¤¹ à¤œà¤¾à¤‚à¤šà¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿ à¤•à¥à¤¯à¤¾ à¤•à¤¿à¤¸à¥€ à¤®à¥‰à¤¡à¤² à¤®à¥‡à¤‚ à¤ªà¤¹à¤²à¥‡ à¤¸à¥‡ à¤¹à¥€ Flax, PyTorch à¤¯à¤¾ TensorFlow à¤•à¤¾ à¤•à¤¾à¤°à¥à¤¯à¤¾à¤¨à¥à¤µà¤¯à¤¨ à¤¹à¥ˆ, à¤¯à¤¾ ..."
          ],
          [
           "## à¤…à¤§à¤¿à¤• à¤¸à¤®à¤à¥‡à¤‚\n\n|à¤…à¤§à¥à¤¯à¤¾à¤¯ | à¤µà¤¿à¤µà¤°à¤£ |\n|-|-|\n| [à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¤¼à¥€à¤•à¤°à¤£](https://huggingface.co/transformers/) | à¤ªà¥‚à¤°..."
          ],
          [
           "## à¤‰à¤¦à¥à¤§à¤°à¤£\n\nà¤¹à¤®à¤¨à¥‡ à¤†à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤• à¤¤à¥Œà¤° à¤ªà¤° à¤‡à¤¸ à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤•à¤¾ [à¤ªà¥‡à¤ªà¤°](https://www.aclweb.org/anthology/2020.emnlp-d..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Language models have become a key step to achieve st..."
          ],
          [
           "## FlaubertConfig\n\n[[autodoc]] FlaubertConfig\n\n## FlaubertTokenizer\n\n[[autodoc]] FlaubertTokenizer\n\n..."
          ],
          [
           "[[autodoc]] TFFlaubertForMultipleChoice\n    - call\n\n## TFFlaubertForTokenClassification\n\n[[autodoc]]..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "DeepSpeed ZeRO-3 can be used for inference as well, since it allows huge models to be loaded on mult..."
          ],
          [
           "```\n\nor via `transformers`' `extras`:\n\n```bash\npip install transformers[deepspeed]\n```\n\nor find more..."
          ],
          [
           "```\n\nSo if you get `8, 6`, then use `TORCH_CUDA_ARCH_LIST=\"8.6\"`. If you have multiple different car..."
          ],
          [
           "```\n\nIf the output is:\n\n```bash\n_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, tot..."
          ],
          [
           "```\nor use the launcher provided by `deepspeed`:\n\n```bash\ndeepspeed --num_gpus=2 your_program.py <no..."
          ],
          [
           "```\n\nNote that in the DeepSpeed documentation you are likely to see `--deepspeed --deepspeed_config ..."
          ],
          [
           "```\n\nThis is almost the same as with multiple-GPUs, but here we tell DeepSpeed explicitly to use jus..."
          ],
          [
           "```\n\nwhich enables optimizer offload and some other important features. You may experiment with the ..."
          ],
          [
           "```\n\n  In this example, we tell DeepSpeed to use GPU 1 (second gpu).\n\n\n\n<a id='deepspeed-multi-node'..."
          ],
          [
           "```\nhostname1 slots=8\nhostname2 slots=8\n```\nand then you can launch it as:\n\n```bash\ndeepspeed --num_..."
          ],
          [
           "```\n\nUnlike the `torch.distributed.run` launcher, `deepspeed` will automatically launch this command..."
          ],
          [
           "```\n\nAll is left is to schedule it to run:\n```bash\nsbatch launch.slurm\n```\n\n`srun` will take care of..."
          ],
          [
           "```\n\nNote: `...` stands for the normal arguments that you'd pass to the functions.\n\nIf you want to u..."
          ],
          [
           "\"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu..."
          ],
          [
           "```\n\nIf the training script is in a normal file and not in the notebook cells, you can launch `deeps..."
          ],
          [
           "```\n\nSome more examples are to be found in the [main repo](https://github.com/microsoft/DeepSpeed) a..."
          ],
          [
           "```\n\nWhen you execute the program, DeepSpeed will log the configuration it received from the [`Train..."
          ],
          [
           "```\n\n<a id='deepspeed-config-shared'></a>\n\n### Shared Configuration\n\n\n<Tip warning={true}>\n\nThis sec..."
          ],
          [
           "<a id='deepspeed-zero'></a>\n\n### ZeRO\n\n[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/t..."
          ],
          [
           "```\n\n**Performance tuning:**\n\n- enabling `offload_optimizer` should reduce GPU RAM usage (it require..."
          ],
          [
           "```\n\nThis is a stage 2 optimization for CPU offloading that parallelizes gradient copying to CPU mem..."
          ],
          [
           "```\n\nIf you are getting OOMs, because your model or activations don't fit into the GPU memory and yo..."
          ],
          [
           "The following configuration values depend on the model's hidden size:\n\n- `reduce_bucket_size`: `hidd..."
          ],
          [
           "You can leave `sub_group_size` to its default value of *1e9* when not using NVMe offload. You may wa..."
          ],
          [
           "```\n\nThis will essentially disable ZeRO without you needing to change anything else.\n\n\n#### ZeRO-1 C..."
          ],
          [
           "```\n\n\n\n<a id='deepspeed-nvme'></a>\n\n### NVMe Support\n\nZeRO-Infinity allows for training incredibly l..."
          ],
          [
           "```\n\nYou can choose to offload both optimizer states and params to NVMe, or just one of them or none..."
          ],
          [
           "It's possible to adjust ZeRO-3 configuration to make it perform closer to ZeRO-2:\n\n- set `stage3_par..."
          ],
          [
           "\"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu..."
          ],
          [
           "```\n\nHere is a full ZeRO-2 all-enabled manually set configuration file. It is here mainly for you to..."
          ],
          [
           "```\n\n<a id='deepspeed-zero3-example'></a>\n\n#### ZeRO-3 Example\n\nHere is a full ZeRO-3 auto-configura..."
          ],
          [
           "\"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,..."
          ],
          [
           "```\n\nHere is a full ZeRO-3 all-enabled manually set configuration file. It is here mainly for you to..."
          ],
          [
           "```\n\n#### How to Choose Which ZeRO Stage and Offloads To Use For Best Performance\n\nSo now you know t..."
          ],
          [
           "8. Definitely use mixed half-precision over fp32 - so bf16 on Ampere and higher GPUs and fp16 on old..."
          ],
          [
           "These notes were written primarily for the training mode, but they should mostly apply for inference..."
          ],
          [
           "Therefore you have two ways to take advantage of this very beneficial feature:\n\n1. If you want to us..."
          ],
          [
           "<a id='deepspeed-optimizer'></a>\n\n#### Optimizer\n\n\nDeepSpeed's main optimizers are Adam, AdamW, OneB..."
          ],
          [
           "```\n\nNote that the command line arguments will set the values in the configuration file. This is so ..."
          ],
          [
           "```\nto the top level configuration.\n\n\n\n<a id='deepspeed-scheduler'></a>\n\n#### Scheduler\n\nDeepSpeed s..."
          ],
          [
           "```\n\nSince *\"auto\"* is used the [`Trainer`] arguments will set the correct values in the configurati..."
          ],
          [
           "```\n\nand `total_num_steps`, `warmup_max_lr`, `warmup_num_steps` and `total_num_steps` will be set at..."
          ],
          [
           "```\n\nIf you're using the Ampere-architecture based GPU, pytorch version 1.7 and higher will automati..."
          ],
          [
           "```\n\nand the [`Trainer`] will automatically enable or disable it based on the value of\n`args.fp16_ba..."
          ],
          [
           "```\n\n<Tip>\n\nAs of `deepspeed==0.6.0` the bf16 support is new and experimental.\n\nIf you use [gradient..."
          ],
          [
           "```\nThe valid values as of this writing are \"fp16\", \"bfp16\", \"fp32\".\n\nnote: stage zero 3 had a bug w..."
          ],
          [
           "```\n\nand the [`Trainer`] will automatically set `train_micro_batch_size_per_gpu` to the value of\n`ar..."
          ],
          [
           "```\n\nBut then you're on your own synchronizing the [`Trainer`] command line arguments and the DeepSp..."
          ],
          [
           "```\n\n**FP32 Weights:**\n\nWhile the fp16 weights are fine for resuming training, if you finished finet..."
          ],
          [
           "```\n\n<Tip>\n\nNote, that once `load_state_dict_from_zero_checkpoint` was run, the `model` will no long..."
          ],
          [
           "```\n\n**Offline FP32 Weights Recovery:**\n\nDeepSpeed creates a special conversion script `zero_to_fp32..."
          ],
          [
           "```\n\nIn this example there is just one DeepSpeed checkpoint sub-folder *global_step1*. Therefore to ..."
          ],
          [
           "```\n\nAs you can see this gives you a randomly initialized model.\n\nIf you want to use a pretrained mo..."
          ],
          [
           "```\n\nIf you're using the official example scripts and your command line arguments include `--deepspe..."
          ],
          [
           "```\n\nstress on `tensor([1.])`, or if you get an error where it says the parameter is of size `1`, in..."
          ],
          [
           "```\n\nSince for inference there is no need for additional large memory used by the optimizer states a..."
          ],
          [
           "Let's estimate how much memory is needed to finetune \"bigscience/T0_3B\" on a single GPU:\n\n```bash\n$ ..."
          ],
          [
           "```\n\nSo you can fit it on a single 80GB GPU and no CPU offload, or a tiny 8GB GPU but then need ~60G..."
          ],
          [
           "If you have enough GPU memory make sure to disable the CPU/NVMe offload as it'll make everything fas..."
          ],
          [
           "```\n\nSo here you'd want 2x 32GB GPUs or higher without offloading to CPU.\n\nFor full information plea..."
          ],
          [
           "```\n\n4. If possible include a link to a Google Colab notebook that we can reproduce the problem with..."
          ],
          [
           "### Troubleshooting\n\n#### the `deepspeed` process gets killed at startup without a traceback\n\nIf the..."
          ],
          [
           "```\n\nand you see in your log that Deepspeed reports `OVERFLOW!` as follows:\n\n```\n0%|                ..."
          ],
          [
           "```\n\nthat means that the Deepspeed loss scaler can't figure out a scaling co-efficient that overcome..."
          ],
          [
           "If you're using Deepspeed ZeRO-1 or ZeRO-2 you don't need to use `HfDeepSpeedConfig` at all.\n\nFor ex..."
          ],
          [
           "```\n\nor for non-pretrained model:\n\n```python\nfrom transformers.integrations import HfDeepSpeedConfig..."
          ],
          [
           "```\n\nPlease note that if you're not using the [`Trainer`] integration, you're completely on your own..."
          ],
          [
           "```python\n#!/usr/bin/env python\n\n# This script demonstrates how to use Deepspeed ZeRO in an inferenc..."
          ],
          [
           "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To avoid warnings about parallelism in tokenizers\n..."
          ],
          [
           "# keeping the same format as json for consistency, except it uses lower case for true/false\n# fmt: o..."
          ],
          [
           "# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at onc..."
          ],
          [
           "```\n\nLet's save it as `t0.py` and run it:\n```\n$ deepspeed --num_gpus 2 t0.py\nrank0:\n   in=Is this re..."
          ],
          [
           "```\nRUN_SLOW=1 pytest tests/deepspeed\n```\n\n\n\n\n## Main DeepSpeed Resources\n\n- [Project's github](http..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n### Model checkpoints\n\n|     Model Name      | Language |           Description           |\n|:-..."
          ],
          [
           "## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classifi..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Compression plays an important role on the efficient..."
          ],
          [
           "## Resources\n\nDemo notebooks for Swin2SR can be found [here](https://github.com/NielsRogge/Transform..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\npython run_mlm.py \\\n--model_name_or_path distilbert-base-cased \\\n--output_dir output \\\n--dataset..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Natural Language Processing (NLP) has recently achie..."
          ],
          [
           "## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classifi..."
          ],
          [
           "## MobileBertForTokenClassification\n\n[[autodoc]] MobileBertForTokenClassification\n    - forward\n\n## ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The recent \"Text-to-Text Transfer Transformer\" (T5) ..."
          ],
          [
           "- [google/mt5-xxl](https://huggingface.co/google/mt5-xxl).\n\nThis model was contributed by [patrickvo..."
          ],
          [
           "[[autodoc]] TFMT5Model\n\n## TFMT5ForConditionalGeneration\n\n[[autodoc]] TFMT5ForConditionalGeneration\n..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recent studies have demonstrated the efficiency of g..."
          ],
          [
           "* Causal language modeling (CLM) which is the traditional autoregressive training (so this model cou..."
          ],
          [
           "[[autodoc]] XLMModel\n    - forward\n\n## XLMWithLMHeadModel\n\n[[autodoc]] XLMWithLMHeadModel\n    - forw..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\nNew behaviour\n\n```python\n>>> from transformers import NllbTokenizer\n\n>>> tokenizer = NllbTokeniz..."
          ],
          [
           "```\n\nFor more details, feel free to check the linked [PR](https://github.com/huggingface/transformer..."
          ],
          [
           "The abstract of the paper is the following:\n\n*Driven by the goal of eradicating language barriers on..."
          ],
          [
           "Note that we're using the BCP-47 code for French `fra_Latn`. See [here](https://github.com/facebookr..."
          ],
          [
           "```\n\n### Generating from any other language than English\n\nEnglish (`eng_Latn`) is set as the default..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [zphang](https://huggingface.co/zphang) with contributions from [Black..."
          ],
          [
           "```\n\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci..."
          ],
          [
           "<PipelineTag pipeline=\"text-classification\"/>\n\n- A [notebook](https://colab.research.google.com/gith..."
          ],
          [
           "ðŸš€ Deploy\n- A [notebook](https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/mas..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*This paper shows that masked autoencoders (MAE) are ..."
          ],
          [
           "<small> MAE architecture. Taken from the <a href=\"https://arxiv.org/abs/2111.06377\">original paper.<..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "ESM-2 outperforms all tested single-sequence protein language models across a range of structure pre..."
          ],
          [
           "The abstract from \n\"Biological structure and function emerge from scaling unsupervised learning to 2..."
          ],
          [
           "The original code can be found [here](https://github.com/facebookresearch/esm) and was\nwas developed..."
          ],
          [
           "## EsmForMaskedLM\n\n[[autodoc]] EsmForMaskedLM\n    - forward\n\n## EsmForSequenceClassification\n\n[[auto..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We design a family of image classification architect..."
          ],
          [
           "- Compared to ViT, LeViT models use an additional distillation head to effectively learn from a teac..."
          ],
          [
           "contrast with the original ViT model, which used external data like the JFT-300M dataset/Imagenet-21..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer with..."
          ],
          [
           "Examples are \"day of the month\", \"month of the year\", etc. as scalar values (and then stacked togeth..."
          ],
          [
           "- At inference time, we give the final value of the `past_values` as input to the decoder. Next, we ..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "## Note on custom data\n\nIn case you'd like to use the script with custom data, there are 2 things re..."
          ],
          [
           "# step 2: create DatasetDict\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": v..."
          ],
          [
           "```\n\nAn example of such a dataset can be seen at [nielsr/ade20k-demo](https://huggingface.co/dataset..."
          ],
          [
           "```\n\nYou can easily upload this by clicking on \"Add file\" in the \"Files and versions\" tab of your re..."
          ],
          [
           "```\n\nThe resulting model can be seen here: https://huggingface.co/nielsr/segformer-finetuned-sidewal..."
          ],
          [
           "The script leverages [ðŸ¤— `Accelerate`](https://github.com/huggingface/accelerate), which allows to wr..."
          ],
          [
           "```\n\nand reply to the questions asked regarding the environment on which you'd like to train. Then\n\n..."
          ],
          [
           "```\n\nand perform inference as follows:\n\n```python\nfrom PIL import Image\nimport requests\nimport torch..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nIf you run into any issues running this model, please reinstall the last version that supported..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformers have a potential of learning longer-ter..."
          ],
          [
           "## Usage tips\n\n- Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done ..."
          ],
          [
           "## TransfoXLTokenizer\n\n[[autodoc]] TransfoXLTokenizer\n    - save_vocabulary\n\n## TransfoXL specific o..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[[autodoc]] pytorch_utils.find_pruneable_heads_and_indices\n\n[[autodoc]] pytorch_utils.prune_layer\n\n[..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The example scripts are not expected to work out-of-the-box on every problem, and you may need to ad..."
          ],
          [
           "```\n\nFor older versions of the example scripts, click on the toggle below:..."
          ],
          [
           "<details>\n  <summary>Examples for older versions of ðŸ¤— Transformers</summary>\n\t<ul>\n\t\t<li><a href=\"ht..."
          ],
          [
           "<li><a href=\"https://github.com/huggingface/transformers/tree/v3.3.1/examples\">v3.3.1</a></li>\n\t\t<li..."
          ],
          [
           "<li><a href=\"https://github.com/huggingface/transformers/tree/v2.6.0/examples\">v2.6.0</a></li>\n\t\t<li..."
          ],
          [
           "<li><a href=\"https://github.com/huggingface/transformers/tree/v1.0.0/examples\">v1.0.0</a></li>\n\t</ul..."
          ],
          [
           "Then switch your current clone of ðŸ¤— Transformers to a specific version, like v3.5.1 for example:\n\n``..."
          ],
          [
           "```\n\nAfter you've setup the correct library version, navigate to the example folder of your choice a..."
          ],
          [
           "```\n</pt>\n<tf>\nThe example script downloads and preprocesses a dataset from the ðŸ¤— [Datasets](https:/..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n## Distributed training and mixed precision\n\nThe [Trainer](https://hu..."
          ],
          [
           "```\n\nTensorFlow scripts utilize a [`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_..."
          ],
          [
           "```\n</pt>\n<tf>\nTensor Processing Units (TPUs) are specifically designed to accelerate performance. T..."
          ],
          [
           "```\n\nTest your setup to make sure it is configured correctly:\n\n```bash\naccelerate test\n```\n\nNow you ..."
          ],
          [
           "```\n\n## Test a script\n\nIt is often a good idea to run your script on a smaller number of dataset exa..."
          ],
          [
           "```\n\n## Resume training from checkpoint\n\nAnother helpful option to enable is resuming training from ..."
          ],
          [
           "```\n\n## Share your model\n\nAll scripts can upload your final model to the [Model Hub](https://hugging..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Get a closer look at [DistilBERT](model_doc/distilbert) by accessing [`DistilBertConfig`] to inspect..."
          ],
          [
           "```\n\n[`DistilBertConfig`] displays all the default attributes used to build a base [`DistilBertModel..."
          ],
          [
           "```\n\nTo reuse the configuration file, load it with [`~PretrainedConfig.from_pretrained`]:\n\n```py\n>>>..."
          ],
          [
           "```\n\nThis creates a model with random values instead of pretrained weights. You won't be able to use..."
          ],
          [
           "```\n\nWhen you load pretrained weights, the default model configuration is automatically loaded if th..."
          ],
          [
           "```\n</pt>\n<tf>\nFor example, [`TFDistilBertForSequenceClassification`] is a base DistilBERT model wit..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n## Tokenizer\n\nThe last base class you need before using a model for t..."
          ],
          [
           "```\n\nCreate a fast tokenizer with the [`DistilBertTokenizerFast`] class:\n\n```py\n>>> from transformer..."
          ],
          [
           "```\n\n<Tip>\n\nIf you aren't looking for any customization, just use the `from_pretrained` method to lo..."
          ],
          [
           "```\n\n## Feature Extractor\n\nA feature extractor processes audio inputs. It inherits from the base [`~..."
          ],
          [
           "```\n\n<Tip>\n\nIf you aren't looking for any customization, just use the `from_pretrained` method to lo..."
          ],
          [
           "```\n\nCombine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:\n\n```py\n>>> from transform..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [stas](https://huggingface.co/stas). The original code can be found\n[h..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the..."
          ],
          [
           "New in v2:\n\n- **Vocabulary** In v2 the tokenizer is changed to use a new vocabulary of size 128K bui..."
          ],
          [
           "## DebertaV2Config\n\n[[autodoc]] DebertaV2Config\n\n## DebertaV2Tokenizer\n\n[[autodoc]] DebertaV2Tokeniz..."
          ],
          [
           "## TFDebertaV2PreTrainedModel\n\n[[autodoc]] TFDebertaV2PreTrainedModel\n    - call\n\n## TFDebertaV2ForM..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Multimodal pre-training with text, layout, and image..."
          ],
          [
           "```\n\nNote that LayoutXLM has its own tokenizer, based on\n[`LayoutXLMTokenizer`]/[`LayoutXLMTokenizer..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We present in this paper a new architecture, named C..."
          ],
          [
           "## Usage tips\n\n- CvT models are regular Vision Transformers, but trained with convolutions. They out..."
          ],
          [
           "If you're interested in submitting a resource to be included here, please feel free to open a Pull R..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\nthis should make a directory called `xsum/` with files like `test.source`.\nTo use your own data,..."
          ],
          [
           "```\ntrain.source\ntrain.target\nval.source\nval.target\ntest.source\ntest.target..."
          ],
          [
           "```\nThe `.source` files are the input, the `.target` files are the desired output.\n\n### Potential is..."
          ],
          [
           "Summarization Tips:\n- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB..."
          ],
          [
           "### Fine-tuning using Seq2SeqTrainer\nTo use `Seq2SeqTrainer` for fine-tuning you should use the `fin..."
          ],
          [
           "```\n\nFor multi-gpu training use `torch.distributed.launch`, e.g. with 2 gpus:\n```bash\ntorchrun --npr..."
          ],
          [
           "```\n\n## Evaluation Commands\n\nTo create summaries for each article in dataset, we use `run_eval.py`, ..."
          ],
          [
           "```\n\n### Multi-GPU Evaluation\nhere is a command to run xsum evaluation on 8 GPUS. It is more than li..."
          ],
          [
           "```\n\n   `--info` is an additional argument available for the same purpose of tracking the conditions..."
          ],
          [
           "```\n    --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\n   ```\n   whi..."
          ],
          [
           "```\n\nIf you pass `--info \"some experiment-specific info\"` it will get printed before the results tab..."
          ],
          [
           "```\nsplits `wmt_en_ro/train` into 11,197 uneven lengthed batches and can finish 1 epoch in 8 minutes..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper is the following:\n\n*Driven by the goal of eradicating language barriers on..."
          ],
          [
           "## Implementation differences with SwitchTransformers\n\nThe biggest difference is the way the tokens ..."
          ],
          [
           ">>> translated_tokens = model.generate(\n...     **inputs, forced_bos_token_id=tokenizer.lang_code_to..."
          ],
          [
           "```\n\n### Generating from any other language than English\n\nEnglish (`eng_Latn`) is set as the default..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by repl..."
          ],
          [
           "## FNetConfig\n\n[[autodoc]] FNetConfig\n\n## FNetTokenizer\n\n[[autodoc]] FNetTokenizer\n    - build_input..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We tackle the task of conditional music generation. ..."
          ],
          [
           "```\n\n## Generation\n\nMusicGen is compatible with two generation modes: greedy and sampling. In practi..."
          ],
          [
           "```\n\nThe audio outputs are a three-dimensional Torch tensor of shape `(batch_size, num_channels, seq..."
          ],
          [
           "```\n\nThe `guidance_scale` is used in classifier free guidance (CFG), setting the weighting between t..."
          ],
          [
           "```\n\nFor batched audio-prompted generation, the generated `audio_values` can be post-processed to re..."
          ],
          [
           "```\n\n### Generation Configuration\n\nThe default parameters that control the generation process, such ..."
          ],
          [
           "```\n\nNote that any arguments passed to the generate method will **supersede** those in the generatio..."
          ],
          [
           "```\n\nSince the text encoder and audio encoder/decoder models are frozen during training, the MusicGe..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[[autodoc]] generation.GenerationConfig\n\t- from_pretrained\n\t- from_model_config\n\t- save_pretrained\n\n..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## AutoTokenizer\n\nNearly every NLP task begins with a tokenizer. A tokenizer converts your input int..."
          ],
          [
           "```\n\nThen tokenize your input as shown below:\n\n```py\n>>> sequence = \"In a hole in the ground there l..."
          ],
          [
           "```\n\n## AutoProcessor\n\nMultimodal tasks require a processor that combines two types of preprocessing..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nFor PyTorch models, the `from_pretrained()` method uses `torch.load()` wh..."
          ],
          [
           "```\n\nEasily reuse the same checkpoint to load an architecture for a different task:\n\n```py\n>>> from ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert), [BigBird](../mo..."
          ],
          [
           "[Megatron-BERT](../model_doc/megatron-bert), [MobileBERT](../model_doc/mobilebert), [MPNet](../model..."
          ],
          [
           "<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libr..."
          ],
          [
           "```\n\nWe encourage you to log in to your Hugging Face account so you can upload and share your model ..."
          ],
          [
           "```\n\nThen take a look at an example:\n\n```py\n>>> eli5[\"train\"][0]\n{'answers': {'a_id': ['c3d1aib', 'c..."
          ],
          [
           "```\n\nWhile this may look like a lot, you're only really interested in the `text` field. What's cool ..."
          ],
          [
           "```\n\nYou'll notice from the example above, the `text` field is actually nested inside `answers`. Thi..."
          ],
          [
           "```py\n>>> eli5 = eli5.flatten()\n>>> eli5[\"train\"][0]\n{'answers.a_id': ['c3d1aib', 'c3d4lya'],\n 'answ..."
          ],
          [
           "```\n\nEach subfield is now a separate column as indicated by the `answers` prefix, and the `text` fie..."
          ],
          [
           "```\n\nThis dataset contains the token sequences, but some of these are longer than the maximum input ..."
          ],
          [
           "```\n\nNow create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient t..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the ..."
          ],
          [
           "```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_ap..."
          ],
          [
           "```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use..."
          ],
          [
           "```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ..."
          ],
          [
           "```\n\nPass your inputs to the model and return the `logits` of the masked token:\n\n```py\n>>> from tran..."
          ],
          [
           "```\n\nPass your inputs to the model and return the `logits` of the masked token:\n\n```py\n>>> from tran..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Existing pre-trained models are generally geared tow..."
          ],
          [
           "## Usage tips\n\n- UL2 is an encoder-decoder model pre-trained on a mixture of denoising functions as ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "- [self-hosted (push)](https://github.com/huggingface/transformers/tree/main/.github/workflows/self-..."
          ],
          [
           "```\n\n   The results can be observed [here](https://github.com/huggingface/transformers/actions).\n\n\n\n..."
          ],
          [
           "```\n\nHere:\n\n- `tests/test_optimization.py` - the file with tests\n- `OptimizationTest` - the name of ..."
          ],
          [
           "```\n\n### Run `accelerate` tests\n\nSometimes you need to run `accelerate` tests on your models. For th..."
          ],
          [
           "```\n\nJust run the following line to automatically test every docstring example in the desired file: ..."
          ],
          [
           "```\n\nor `pytest.ini`/``tox.ini`` files:\n\n```ini\n[pytest]\nlooponfailroots = transformers tests\n```\n\nT..."
          ],
          [
           "```\n\n### Clearing state\n\nCI builds and when isolation is important (against speed), cache should be ..."
          ],
          [
           "```\n\nImportant: the presence of `pytest-random-order` will automatically randomize tests, no configu..."
          ],
          [
           "```\n\nTo disable the shuffling for all tests:\n\n```bash\npytest --random-order-bucket=none\n```\n\nBy defa..."
          ],
          [
           "```\n\n```bash\npytest --instafail\n```\n\n### To GPU or not to GPU\n\nOn a GPU-enabled setup, to test in CP..."
          ],
          [
           "```\n\nThis is handy when you want to run different tasks on different GPUs.\n\nSome tests must be run o..."
          ],
          [
           "```\n\nThese decorators can be stacked. For example, if a test is slow and requires at least one GPU u..."
          ],
          [
           "```\nAlternative backends may also require the replacement of device-specific functions. For example ..."
          ],
          [
           "```\nThis format also allows for specification of any additional imports required. To use this file t..."
          ],
          [
           "```\n\nTo send test results to JUnit format output:\n\n```bash\npy.test tests --junitxml=result.xml\n```\n\n..."
          ],
          [
           "```\n\nNow, by default this test will be run 3 times, each time with the last 3 arguments of `test_flo..."
          ],
          [
           "```\n\nThe module [parameterized](https://pypi.org/project/parameterized/) which is already in the dev..."
          ],
          [
           "```\n\nas in the previous example.\n\n\n\n### Files and directories\n\nIn tests often we need to know where ..."
          ],
          [
           "```\n\nIf you don't need to manipulate paths via `pathlib` or you just need a path as a string, you ca..."
          ],
          [
           "```\n\n`tmp_dir` will contain the path to the created temporary dir. It will be automatically removed ..."
          ],
          [
           "```\n\n### Skipping tests\n\nThis is useful when a bug is found and a new test is written, yet the bug i..."
          ],
          [
           "```\n\nor the whole module:\n\n```python\nimport pytest\n\nif not pytest.config.getoption(\"--custom-flag\"):..."
          ],
          [
           "```\n\nOnce a test is marked as `@slow`, to run such tests set `RUN_SLOW=1` env var, e.g.:\n\n```bash\nRU..."
          ],
          [
           "```\n\nAs explained at the beginning of this document, slow tests get to run on a scheduled basis, rat..."
          ],
          [
           "```\n\nHere is a an example of a [script](https://github.com/huggingface/transformers/tree/main/script..."
          ],
          [
           "```\n\nAnd, of course, most of the time, `stderr` will come as a part of an exception, so try/except h..."
          ],
          [
           "```\n\nHere is a full test example:\n\n```python\nfrom transformers.testing_utils import CaptureStdout\n\nm..."
          ],
          [
           "```\n\n### Testing with environment variables\n\nIf you want to test the impact of environment variables..."
          ],
          [
           "```\n\nDepending on whether the test file was under the `tests` test suite or `examples` it'll correct..."
          ],
          [
           "```\n\n## Working with github actions workflows\n\nTo trigger a self-push workflow CI job, you must:\n\n1...."
          ],
          [
           "That way experiments on CI functionality itself won't interfere with the normal workflow.\n\nNow how c..."
          ],
          [
           "```\n\nFor simple commands you could also do:\n\n```bash\ncmd_that_may_fail || true\n```\n\nOf course, once ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We introduce dense vision transformers, an architect..."
          ],
          [
           "## Usage tips\n\nDPT is compatible with the [`AutoBackbone`] class. This allows to use the DPT framewo..."
          ],
          [
           "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help ..."
          ],
          [
           "Awesome projects built with Transformers\n\nThis page lists awesome projects built on top of Transform..."
          ],
          [
           "Keywords: inpainting, SD, Stable Diffusion\n\n## [flair](https://github.com/flairNLP/flair)\n\nFLAIR is ..."
          ],
          [
           "Keywords: LLMs, Large Language Models, Data Retrieval, Indices, Knowledge Augmentation \n\n## [ParlAI]..."
          ],
          [
           "Keywords: Stable-Diffusion, WebUI, CLI\n\n## [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n\n[..."
          ],
          [
           "## [imagen-pytorch](https://github.com/lucidrains/imagen-pytorch)\n\nAn open-source Implementation of ..."
          ],
          [
           "Keywords: Conversational, ASR, TTS, LLMs, NLP\n\n## [Runhouse](https://github.com/run-house/runhouse)\n..."
          ],
          [
           "Keywords: LLM, Agents, HF Hub\n\n## [transformers.js](https://xenova.github.io/transformers.js/)\n\n[tra..."
          ],
          [
           "Keywords: NLP, Framework\n\n## [speechbrain](https://github.com/speechbrain/speechbrain)\n\nSpeechBrain ..."
          ],
          [
           "Keywords: Haiku, Model parallelism, LLM, TPU\n\n## [deepchem](https://github.com/deepchem/deepchem)\n\nD..."
          ],
          [
           "Keywords: Stable-Diffusion, Blender\n\n## [seldon-core](https://github.com/SeldonIO/seldon-core)\n\nSeld..."
          ],
          [
           "Stable-Dreamfusion is a pytorch implementation of the text-to-3D model Dreamfusion, powered by the S..."
          ],
          [
           "Keywords: LLM, Evaluation, Few-shot\n\n## [gpt-neox](https://github.com/EleutherAI/gpt-neox)\n\nThis rep..."
          ],
          [
           "Keywords: Training, Inference, Sequence Processing, Sequence Generation\n\n## [LaTeX-OCR](https://gith..."
          ],
          [
           "Keywords: Federated Learning, Analytics, Collaborative ML, Decentralized\n\n## [gpt-code-clippy](https..."
          ],
          [
           "Keywords: LLM, WebUI\n\n## [libra](https://github.com/Palashio/libra)\n\nAn ergonomic machine learning [..."
          ],
          [
           "Keywords: Deployment, BERT, XLNet\n\n## [towhee](https://github.com/towhee-io/towhee)\n\nTowhee makes it..."
          ],
          [
           "Keywords: Training, Generation\n\n## [diffgram](https://github.com/diffgram/diffgram)\n\nDiffgram aims t..."
          ],
          [
           "Keywords: Knowledge Extraction, Knowledge Graphs\n\n## [Nebuly](https://github.com/nebuly-ai/nebuly)\n\n..."
          ],
          [
           "Keywords: Differential privacy\n\n## [LAVIS](https://github.com/salesforce/LAVIS)\n\n[LAVIS](https://git..."
          ],
          [
           "Keywords: Rust, BERT, Inference\n\n## [EasyNLP](https://github.com/alibaba/EasyNLP)\n\n[EasyNLP](https:/..."
          ],
          [
           "Keywords: Semi-structured documents, Unstructured documents, LLM, Document Question Answering\n\n## [C..."
          ],
          [
           "Keywords: Model deployment, CLoud, Mobile, Edge\n\n## [underthesea](https://github.com/undertheseanlp/..."
          ],
          [
           "Keywords: Model interpretation, Visualization\n\n## [mlrun](https://github.com/mlrun/mlrun)\n\nMLRun is ..."
          ],
          [
           "Keywords: Thai, NLP, NLTK\n\n## [FlagAI](https://github.com/FlagAI-Open/FlagAI)\n\n[FlagAI](https://gith..."
          ],
          [
           "Keywords: Active Learning, Research, Labeling\n\n## [cleanlab](https://github.com/cleanlab/cleanlab)\n\n..."
          ],
          [
           "Keywords: PEFT, fine-tuning, LLaMA-2, ChatGLM, Qwen..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface..."
          ],
          [
           "<h3 align=\"center\">\n    <p>Aprendizado de mÃ¡quina de Ãºltima geraÃ§Ã£o para JAX, PyTorch e TensorFlow</..."
          ],
          [
           "A biblioteca ðŸ¤— Transformers oferece APIs para baixar e usar rapidamente esses modelos prÃ©-treinados ..."
          ],
          [
           "- [Completar palavra mascarada com BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+..."
          ],
          [
           "- [SumarizaÃ§Ã£o com BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres..."
          ],
          [
           "- [Resposta a perguntas com..."
          ],
          [
           "- [TraduÃ§Ã£o com T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)..."
          ],
          [
           "Em VisÃ£o Computacional:\n- [ClassificaÃ§Ã£o de Imagens com ViT](https://huggingface.co/google/vit-base-..."
          ],
          [
           "Em Tarefas Multimodais:\n- [Respostas de Perguntas em Tabelas com TAPAS](https://huggingface.co/googl..."
          ],
          [
           "## Se vocÃª estÃ¡ procurando suporte personalizado da equipe Hugging Face\n\n<a target=\"_blank\" href=\"ht..."
          ],
          [
           "```\n\nA segunda linha de cÃ³digo baixa e armazena em cache o modelo prÃ©-treinado usado pelo pipeline, ..."
          ],
          [
           "```\n\n\nAqui obtemos uma lista de objetos detectados na imagem, com uma caixa envolvendo o objeto e um..."
          ],
          [
           "```\n\nE aqui estÃ¡ o cÃ³digo equivalente para TensorFlow:\n\n```python\n>>> from transformers import AutoT..."
          ],
          [
           "```\n\nO tokenizador Ã© responsÃ¡vel por todo o prÃ©-processamento que o modelo prÃ©-treinado espera, e po..."
          ],
          [
           "1. Menores custos de computaÃ§Ã£o, menor pegada de carbono:\n    - Pesquisadores podem compartilhar mod..."
          ],
          [
           "## Por que nÃ£o devo usar transformers?\n\n- Esta biblioteca nÃ£o Ã© uma caixa de ferramentas modular par..."
          ],
          [
           "### Com pip\n\nEste repositÃ³rio Ã© testado no Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ e TensorFlow 2.6+..."
          ],
          [
           "```\nSe vocÃª deseja experimentar com os exemplos ou precisa da versÃ£o mais recente do cÃ³digo e nÃ£o po..."
          ],
          [
           "```\n\nSiga as pÃ¡ginas de instalaÃ§Ã£o do Flax, PyTorch ou TensorFlow para ver como instalÃ¡-los com cond..."
          ],
          [
           "1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (from Google Research) releas..."
          ],
          [
           "1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (from KAIST) released with th..."
          ],
          [
           "1. **[GPT NeoX Japanese](https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese)** (fr..."
          ],
          [
           "1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (from OpenAI) released ..."
          ],
          [
           "1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (from Microsoft Res..."
          ],
          [
           "1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta..."
          ],
          [
           "1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDI..."
          ],
          [
           "1. **[Persimmon](https://huggingface.co/docs/transformers/model_doc/persimmon)** (from ADEPT) releas..."
          ],
          [
           "1. **[PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer)** (from Sea AI Labs..."
          ],
          [
           "1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (from NVIDIA) released ..."
          ],
          [
           "1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)** (from META Platforms) rel..."
          ],
          [
           "1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) relea..."
          ],
          [
           "1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (from Microsoft Resea..."
          ],
          [
           "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley)..."
          ],
          [
           "1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (from University of WÃ¼r..."
          ],
          [
           "1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (fr..."
          ],
          [
           "1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transfor..."
          ],
          [
           "1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (from Google Research) released..."
          ],
          [
           "1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Goog..."
          ],
          [
           "1. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)** (from Meta AI) released ..."
          ],
          [
           "1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)** (from Google Research) rele..."
          ],
          [
           "1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (from Microsoft Research) r..."
          ],
          [
           "1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)** (From Facebook AI) released w..."
          ],
          [
           "1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (from Facebook ..."
          ],
          [
           "1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)** (from Facebook AI) released..."
          ],
          [
           "Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh...."
          ],
          [
           "1. Quer contribuir com um novo modelo? Adicionamos um **guia detalhado e modelos de exemplo** para o..."
          ],
          [
           "## Saiba mais\n\n| SeÃ§Ã£o | DescriÃ§Ã£o |\n|-|-|\n| [DocumentaÃ§Ã£o](https://huggingface.co/docs/transformers..."
          ],
          [
           "## CitaÃ§Ã£o\n\nAgora temos um [artigo](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) que vocÃª p..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We propose Masked Siamese Networks (MSN), a self-sup..."
          ],
          [
           "## Usage tips\n\n- MSN (masked siamese networks) is a method for self-supervised pre-training of Visio..."
          ],
          [
           "## ViTMSNConfig\n\n[[autodoc]] ViTMSNConfig\n\n## ViTMSNModel\n\n[[autodoc]] ViTMSNModel\n    - forward\n\n##..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Although convolutional neural networks (CNNs) have a..."
          ],
          [
           "- PVTv1 on ImageNet-1K\n\n| **Model variant**  |**Size** |**Acc@1**|**Params (M)**|\n|-----------------..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->..."
          ],
          [
           "[BART](../model_doc/bart), [BERT](../model_doc/bert), [Bert Generation](../model_doc/bert-generation..."
          ],
          [
           "[GPTBigCode](../model_doc/gpt_bigcode), [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt..."
          ],
          [
           "[PLBart](../model_doc/plbart), [ProphetNet](../model_doc/prophetnet), [QDQBert](../model_doc/qdqbert..."
          ],
          [
           "```\n\nWe encourage you to log in to your Hugging Face account so you can upload and share your model ..."
          ],
          [
           "```\n\nWhile this may look like a lot, you're only really interested in the `text` field. What's cool ..."
          ],
          [
           "```\n\nYou'll notice from the example above, the `text` field is actually nested inside `answers`. Thi..."
          ],
          [
           "```\n\nThis dataset contains the token sequences, but some of these are longer than the maximum input ..."
          ],
          [
           "```\n\nNow create a batch of examples using [`DataCollatorForLanguageModeling`]. It's more efficient t..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the ..."
          ],
          [
           "```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_ap..."
          ],
          [
           "```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use..."
          ],
          [
           "```\n\nUse the [`~transformers.generation_utils.GenerationMixin.generate`] method to generate text.\nFo..."
          ],
          [
           "```\n\nUse the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method to create the s..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Semi-supervised learning through pseudo-labeling has..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "The example script uses the ðŸ¤— Datasets library. You can easily customize them to your needs if you n..."
          ],
          [
           "```\nhuggingface-cli repo create english-roberta-base-dummy\n```\n\nNext we clone the model repository t..."
          ],
          [
           "```\n\n### Train model\n\nNext we can run the example script to pretrain the model.\nCompared to the defa..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformer-based models are widely used in natural ..."
          ],
          [
           "## Usage tips\n\n- The YOSO attention algorithm is implemented through custom CUDA kernels, functions ..."
          ],
          [
           "## YosoForMaskedLM\n\n[[autodoc]] YosoForMaskedLM\n    - forward\n\n## YosoForSequenceClassification\n\n[[a..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage tips\n\n1. CLVP is an integral part of the Tortoise TTS model.\n2. CLVP can be used to compare..."
          ],
          [
           ">>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using ..."
          ],
          [
           "```\n\n\n## ClvpConfig\n\n[[autodoc]] ClvpConfig\n    - from_sub_model_configs\n\n## ClvpEncoderConfig\n\n[[au..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The tremendous success of CLIP (Radford et al., 2021..."
          ],
          [
           ">>> url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n>>> image = Image.open(..."
          ],
          [
           "```\n\nCurrently, following scales of pretrained Chinese-CLIP models are available on ðŸ¤— Hub:\n\n- [OFA-S..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pretrained multilingual large language models have t..."
          ],
          [
           "## Usage tips \n\n- UMT5 was only pre-trained on [mC4](https://huggingface.co/datasets/mc4) excluding ..."
          ],
          [
           "```\n\n<Tip> \n\nRefer to [T5's documentation page](t5) for more tips, code examples and notebooks.\n</Ti..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling..."
          ],
          [
           "## ProphetNetConfig\n\n[[autodoc]] ProphetNetConfig\n\n## ProphetNetTokenizer\n\n[[autodoc]] ProphetNetTok..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "According to the abstract,\n\n- Pegasus' pretraining task is intentionally similar to summarization: i..."
          ],
          [
           "All the [checkpoints](https://huggingface.co/models?search=pegasus) are fine-tuned for summarization..."
          ],
          [
           "## Usage Example\n\n```python\n>>> from transformers import PegasusForConditionalGeneration, PegasusTok..."
          ],
          [
           "```\n\n## Resources\n\n- [Script](https://github.com/huggingface/transformers/tree/main/examples/researc..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [anton-l](https://huggingface.co/anton-l).\n\n## Usage tips\n\n- SEW-D is ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*While the general idea of self-supervised learning i..."
          ],
          [
           "## Usage tips\n\n- Data2VecAudio, Data2VecText, and Data2VecVision have all been trained using the sam..."
          ],
          [
           "**Data2VecAudio documentation resources**\n- [Audio classification task guide](../tasks/audio_classif..."
          ],
          [
           "## Data2VecTextModel\n\n[[autodoc]] Data2VecTextModel\n    - forward\n\n## Data2VecTextForCausalLM\n\n[[aut..."
          ],
          [
           "## TFData2VecVisionForImageClassification\n\n[[autodoc]] TFData2VecVisionForImageClassification\n    - ..."
          ],
          [
           "Token classification\n\n## PyTorch version, no Trainer\n\nFine-tuning (m)LUKE for token classification t..."
          ],
          [
           "```\n\nand reply to the questions asked. Then\n\n```bash\naccelerate test\n```\n\nthat will check everything..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper is the following:\n\n*Building open-domain chatbots is a challenging area fo..."
          ],
          [
           "## BlenderbotSmallTokenizerFast\n\n[[autodoc]] BlenderbotSmallTokenizerFast\n\n<frameworkcontent>\n<pt>\n\n..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n\n[BART](../mo..."
          ],
          [
           "<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libr..."
          ],
          [
           "```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model w..."
          ],
          [
           "```\n\nThe preprocessing function you want to create needs to:\n\n1. Prefix the input with a prompt so T..."
          ],
          [
           "```\n</pt>\n<tf>\n\n```py\n>>> from transformers import DataCollatorForSeq2Seq\n\n>>> data_collator = DataC..."
          ],
          [
           "```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule...."
          ],
          [
           "```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`Seq2SeqTr..."
          ],
          [
           "```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t..."
          ],
          [
           "```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_ap..."
          ],
          [
           "```\n\nThen bundle your callbacks together:\n\n```py\n>>> callbacks = [metric_callback, push_to_hub_callb..."
          ],
          [
           "```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ..."
          ],
          [
           "```\n</pt>\n<tf>\nTokenize the text and return the `input_ids` as TensorFlow tensors:\n\n```py\n>>> from t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n| **Input**                                                                                    ..."
          ],
          [
           "```\n| **Input**                                                                                     ..."
          ],
          [
           "```\n| **Input**                                                                                     ..."
          ],
          [
           "```\n\nTo use BigCode or OpenAssistant, start by logging in to have access to the Inference API:\n\n```p..."
          ],
          [
           "```\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transfo..."
          ],
          [
           "```\n\nHere, the model could interpret in two ways:\n- Have the `text-to-image` generate a capybara swi..."
          ],
          [
           "```\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transfo..."
          ],
          [
           "#### Tools\n\nTools are very simple: they're a single function, with a name, and a description. We the..."
          ],
          [
           "### A curated set of tools\n\nWe identify a set of tools that can empower such agents. Here is an upda..."
          ],
          [
           "```\n\n### Custom tools\n\nWhile we identify a curated set of tools, we strongly believe that the main v..."
          ],
          [
           "```\n\nreturns the following code\n\n```python\nfrom transformers import load_tool\n\nimage_generator = loa..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The paper aims at creating a single unified foundation model which can work across vision, language\n..."
          ],
          [
           "## FlavaFeatureExtractor\n\n[[autodoc]] FlavaFeatureExtractor\n\n## FlavaImageProcessor\n\n[[autodoc]] Fla..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Large-scale language models show promising text gene..."
          ],
          [
           "## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Causal languag..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nT5 Version 1.1 includes the following improvements compared to the original T5 model:\n\n- GEGLU ..."
          ],
          [
           "- [google/t5-v1_1-xl](https://huggingface.co/google/t5-v1_1-xl)\n\n- [google/t5-v1_1-xxl](https://hugg..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThe encoded versions have different lengths:\n\n```python\n>>> len(encoded_sequence_a), len(encode..."
          ],
          [
           "```\n\n### autoencoding models\n\nSee [encoder models](#encoder-models) and [masked language modeling](#..."
          ],
          [
           "### convolution\n\nA type of layer in a neural network where the input matrix is multiplied element-wi..."
          ],
          [
           "### deep learning (DL)\n\nMachine learning algorithms which uses neural networks with several layers.\n..."
          ],
          [
           "For an input of size `[batch_size, sequence_length]`, the memory required to store the intermediate ..."
          ],
          [
           "### head\n\nThe model head refers to the last layer of a neural network that accepts the raw hidden st..."
          ],
          [
           "```python\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(..."
          ],
          [
           "```\n\nThe tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocab..."
          ],
          [
           "```\n\nbecause this is the way a [`BertModel`] is going to expect its inputs.\n\n## L\n\n### labels\n\nThe l..."
          ],
          [
           "- For sequence classification models, ([`BertForSequenceClassification`]), the model expects a tenso..."
          ],
          [
           "- For object detection models, ([`DetrForObjectDetection`]), the model expects a list of dictionarie..."
          ],
          [
           "Each model's labels may be different, so be sure to always check the documentation of each model for..."
          ],
          [
           "For more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tu..."
          ],
          [
           "### preprocessing\n\nThe task of preparing raw data into a format that can be easily consumed by machi..."
          ],
          [
           "### self-attention\n\nEach element of the input finds out which other elements of the input they shoul..."
          ],
          [
           "Another name for the foundational [ZeRO](#zero-redundancy-optimizer--zero-) concept as used by vario..."
          ],
          [
           "```python\n>>> # [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]..."
          ],
          [
           "```\n\nWe can use our tokenizer to automatically generate such a sentence by passing the two sequences..."
          ],
          [
           "```\n\nThe first sequence, the \"context\" used for the question, has all its tokens represented by a `0..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "Try out the inference widget here: https://huggingface.co/google/vit-base-patch16-224\n\nContent:\n- [P..."
          ],
          [
           "```\n\nðŸ‘€ See the results here: [nateraw/vit-base-beans](https://huggingface.co/nateraw/vit-base-beans)..."
          ],
          [
           "```\n\nInternally, the script will use the [`ImageFolder`](https://huggingface.co/docs/datasets/v2.0.0..."
          ],
          [
           "# example 4: providing several splits\ndataset = load_dataset(\"imagefolder\", data_files={\"train\": [\"p..."
          ],
          [
           "```\n\n`ImageFolder` will create a `label` column, and the label name is based on the directory name.\n..."
          ],
          [
           "```\n\n## PyTorch version, no Trainer\n\nBased on the script [`run_image_classification_no_trainer.py`](..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nHere's a code snippet you can use to listen to the resulting audio in a notebook: \n\n```python\n>..."
          ],
          [
           "```\n\nor alternatively for AMD GPUs:\n\n```bash\n!rocm-smi\n```\n\n</Tip>\n\nWe encourage you to log in to yo..."
          ],
          [
           "```\n\n### Text cleanup for SpeechT5 tokenization \n\nStart by cleaning up the text data. You'll need th..."
          ],
          [
           "```\n\nThe dataset examples contain `raw_text` and `normalized_text` features. When deciding which fea..."
          ],
          [
           "```\n\nNow you have two sets of characters: one with the vocabulary from the dataset and one with the ..."
          ],
          [
           "```\n\nNow that you have dealt with special characters in the text, it's time to shift focus to the au..."
          ],
          [
           "```\n\nLet's check how many speakers remain: \n\n```py\n>>> len(set(dataset[\"speaker_id\"]))\n42\n```\n\nLet's..."
          ],
          [
           "```\n\nYou are left with just under 10,000 examples from approximately 40 unique speakers, which shoul..."
          ],
          [
           ">>> def create_speaker_embedding(waveform):\n...     with torch.no_grad():\n...         speaker_embedd..."
          ],
          [
           "```\n\nIt's important to note that the `speechbrain/spkrec-xvect-voxceleb` model was trained on Englis..."
          ],
          [
           "```\n\nSpeaker embeddings should be a 512-element vector:\n\n```py\n>>> processed_example[\"speaker_embedd..."
          ],
          [
           "```\n\nNext, create a basic train/test split: \n\n```py\n>>> dataset = dataset.train_test_split(test_size..."
          ],
          [
           "```\n\n### Data collator\n\nIn order to combine multiple examples into a batch, you need to define a cus..."
          ],
          [
           "...         # not used during fine-tuning\n...         del batch[\"decoder_attention_mask\"]\n\n...      ..."
          ],
          [
           "```\n\nIn SpeechT5, the input to the decoder part of the model is reduced by a factor 2. In other word..."
          ],
          [
           "```\n\nThe `use_cache=True` option is incompatible with gradient checkpointing. Disable it for trainin..."
          ],
          [
           "```\n\nAnd with that, you're ready to start training! Training will take several hours. Depending on y..."
          ],
          [
           "```\n\nNow you can pass the text and speaker embeddings to the pipeline, and it will take care of the ..."
          ],
          [
           "```\n\nCreate a spectrogram with your model: \n\n```py\n>>> spectrogram = model.generate_speech(inputs[\"i..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Self-supervised learning (SSL) is a long-standing go..."
          ],
          [
           "## Resources\n\n- [Audio classification task guide](../tasks/audio_classification)\n- [Automatic speech..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "For question answering, TAPAS has 2 heads on top: a cell selection head and an aggregation head, for..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Answering natural language questions over tables is ..."
          ],
          [
           "In addition, the authors have further pre-trained TAPAS to recognize **table entailment**, by creati..."
          ],
          [
           "- TAPAS is a model that uses relative position embeddings by default (restarting the position embedd..."
          ],
          [
           "- TAPAS is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It ..."
          ],
          [
           "## Usage: fine-tuning\n\nHere we explain how you can fine-tune [`TapasForQuestionAnswering`] on your o..."
          ],
          [
           "To summarize:\n\n| **Task**                            | **Example dataset** | **Description**        ..."
          ],
          [
           ">>> # or, the base sized model with WikiSQL configuration\n>>> config = TapasConfig(\"google-base-fine..."
          ],
          [
           "```\n\nOf course, you don't necessarily have to follow one of these three ways in which TAPAS was fine..."
          ],
          [
           "```\n</pt>\n<tf>\nInitializing a model with a pre-trained base and randomly initialized classification ..."
          ],
          [
           "```\n\nOf course, you don't necessarily have to follow one of these three ways in which TAPAS was fine..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\nWhat you can also do is start from an already fine-tuned checkpoint. ..."
          ],
          [
           "- `id`: optional, id of the table-question pair, for bookkeeping purposes.\n- `annotator`: optional, ..."
          ],
          [
           "**STEP 3: Convert your data into tensors using TapasTokenizer**\n\n<frameworkcontent>\n<pt>\nThird, give..."
          ],
          [
           "[`TapasTokenizer`] creates the `labels`, `numeric_values` and `numeric_values_scale` based on the `a..."
          ],
          [
           ">>> model_name = \"google/tapas-base\"\n>>> tokenizer = TapasTokenizer.from_pretrained(model_name)\n\n>>>..."
          ],
          [
           "```\n\nNote that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.a..."
          ],
          [
           "...     def __len__(self):\n...         return len(self.data)\n\n\n>>> data = pd.read_csv(tsv_path, sep=..."
          ],
          [
           "```\n</pt>\n<tf>\nThird, given that you've prepared your data in this TSV/CSV format (and corresponding..."
          ],
          [
           "```py\n>>> from transformers import TapasTokenizer\n>>> import pandas as pd\n\n>>> model_name = \"google/..."
          ],
          [
           "```\n\nNote that [`TapasTokenizer`] expects the data of the table to be **text-only**. You can use `.a..."
          ],
          [
           ">>> class TableDataset:\n...     def __init__(self, data, tokenizer):\n...         self.data = data\n....."
          ],
          [
           "...     def __len__(self):\n...         return len(self.data)\n\n\n>>> data = pd.read_csv(tsv_path, sep=..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\nNote that here, we encode each table-question pair independently. Thi..."
          ],
          [
           "```py\n>>> from transformers import TapasConfig, TapasForQuestionAnswering, AdamW\n\n>>> # this is the ..."
          ],
          [
           "...         # zero the parameter gradients\n...         optimizer.zero_grad()\n\n...         # forward ..."
          ],
          [
           "```\n</pt>\n<tf>\nYou can then fine-tune [`TFTapasForQuestionAnswering`] as follows (shown here for the..."
          ],
          [
           "...         # forward + backward + optimize\n...         with tf.GradientTape() as tape:\n...         ..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n## Usage: inference\n\n<frameworkcontent>\n<pt>\nHere we explain how you ..."
          ],
          [
           ">>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"8..."
          ],
          [
           ">>> display(table)\n>>> print(\"\")\n>>> for query, answer, predicted_agg in zip(queries, answers, aggre..."
          ],
          [
           "```\n</pt>\n<tf>\nHere we explain how you can use [`TFTapasForQuestionAnswering`] for inference (i.e. m..."
          ],
          [
           ">>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"8..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\nIn case of a conversational set-up, then each table-question pair mus..."
          ],
          [
           "</pt>\n<tf>\n\n## TFTapasModel\n[[autodoc]] TFTapasModel\n    - call\n    \n## TFTapasForMaskedLM\n[[autodoc..."
          ],
          [
           "## Translating the Transformers documentation into your language\n\nAs part of our mission to democrat..."
          ],
          [
           "```\n\nHere, `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https:..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[ALBERT](../model_doc/albert), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [CamemBE..."
          ],
          [
           "[MRA](../model_doc/mra), [Nezha](../model_doc/nezha), [NystrÃ¶mformer](../model_doc/nystromformer), [..."
          ],
          [
           "```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model w..."
          ],
          [
           "```\n\nWhile it looks like there are a lot of fields here, it is actually pretty straightforward:\n\n- `..."
          ],
          [
           "```\n\nThe preprocessing function you want to create needs to:\n\n1. Make four copies of the `sent1` fie..."
          ],
          [
           "```\n\nðŸ¤— Transformers doesn't have a data collator for multiple choice, so you'll need to adapt the [`..."
          ],
          [
           "...         batch = self.tokenizer.pad(\n...             flattened_features,\n...             padding=..."
          ],
          [
           "```\n</pt>\n<tf>\n```py\n>>> from dataclasses import dataclass\n>>> from transformers.tokenization_utils_..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n## Evaluate\n\nIncluding a metric during training is often helpful for ..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the ..."
          ],
          [
           "```\n\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr..."
          ],
          [
           "```\n\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```..."
          ],
          [
           "```\n\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use..."
          ],
          [
           "```\n\nPass your inputs and labels to the model and return the `logits`:\n\n```py\n>>> from transformers ..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "### Fine-tuning BERT on SQuAD1.0\n\nThe [`run_qa.py`](https://github.com/huggingface/transformers/blob..."
          ],
          [
           "This example code fine-tunes BERT on the SQuAD1.0 dataset. It runs in 24 min (with BERT-base) or 68 ..."
          ],
          [
           "```\n\nTraining with the previously defined hyper-parameters yields the following results:\n\n```bash\nf1..."
          ],
          [
           "```\n\n#### Command for SQuAD2.0:\n\n```bash\nexport SQUAD_DIR=/path/to/SQUAD\n\npython run_qa_beam_search...."
          ],
          [
           "```\n\n### Fine-tuning T5 on SQuAD2.0\n\nThe [`run_seq2seq_qa.py`](https://github.com/huggingface/transf..."
          ],
          [
           "```\n\n## Accelerate-based scripts\n\nBased on the scripts `run_qa_no_trainer.py` and `run_qa_beam_searc..."
          ],
          [
           "```\n\nThis command is the same and will work for:\n\n- a CPU-only setup\n- a setup with one GPU\n- a dist..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Large-scale pretrained language models have achieved..."
          ],
          [
           "## RoCBertModel\n\n[[autodoc]] RoCBertModel\n    - forward\n\n## RoCBertForPreTraining\n\n[[autodoc]] RoCBe..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transfer of pre-trained representations improves sam..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Local attention\n\n[Longformer](#longformer) uses local attention: often, the local context (e.g., ..."
          ],
          [
           "Using those attention matrices with less parameters then allows the model to have inputs having a bi..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Types of Segmentation\n\nSemantic segmentation assigns a label or class to every single pixel ..."
          ],
          [
           "```\n\nThe segmentation pipeline output includes a mask for every predicted class. \n```bash\n[{'score':..."
          ],
          [
           "```\n\nTaking a look at the mask for the car class, we can see every car is classified with the same m..."
          ],
          [
           "```\nChecking out one of the car masks below.\n\n```python\nresults[2][\"mask\"]\n```\n<div class=\"flex just..."
          ],
          [
           "```\nAs you can see below, we have more classes. We will later illustrate to see that every pixel is ..."
          ],
          [
           "```\n\nLet's have a side by side comparison for all types of segmentation.\n\n<div class=\"flex justify-c..."
          ],
          [
           "<!--End of the generated tip-->\n\n</Tip>\n\n\n### Load SceneParse150 dataset\n\nStart by loading a smaller..."
          ],
          [
           "```\n\nSplit the dataset's `train` split into a train and test set with the [`~datasets.Dataset.train_..."
          ],
          [
           "```\n\n- `image`: a PIL image of the scene.\n- `annotation`: a PIL image of the segmentation map, which..."
          ],
          [
           "```\n\n#### Custom dataset\n\nYou could also create and use your own dataset if you prefer to train with..."
          ],
          [
           "# step 2: create DatasetDict\n     dataset = DatasetDict({\n          \"train\": train_dataset,\n        ..."
          ],
          [
           "```\n\n2. an id2label dictionary mapping the class integers to their class names\n\n     ```py\n     impo..."
          ],
          [
           "```\n\nNow create two preprocessing functions to prepare the images and annotations for the model. The..."
          ],
          [
           "```\n\n</pt>\n</frameworkcontent>\n\n<frameworkcontent>\n<tf>\nIt is common to apply some data augmentation..."
          ],
          [
           "```\n\nNext, create two preprocessing functions to prepare batches of images and annotations for the m..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n### Evaluate\n\nIncluding a metric during training is often helpful for..."
          ],
          [
           "```\n\nThen create a function to [`~evaluate.EvaluationModule.compute`] the metrics. Your predictions ..."
          ],
          [
           "```\n\n</pt>\n</frameworkcontent>\n\n\n<frameworkcontent>\n<tf>\n\n```py\n>>> def compute_metrics(eval_pred):\n..."
          ],
          [
           "```\n\n</tf>\n</frameworkcontent>\n\nYour `compute_metrics` function is ready to go now, and you'll retur..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t..."
          ],
          [
           "```\n\nThen, load SegFormer with [`TFAutoModelForSemanticSegmentation`] along with the label mappings,..."
          ],
          [
           "```\n\nTo compute the accuracy from the predictions and push your model to the ðŸ¤— Hub, use [Keras callb..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nPass your input to the model and return the `logits`:\n\n```py\n>>> from transformers import TFAut..."
          ],
          [
           "```\n\n</tf>\n</frameworkcontent>\n\nTo visualize the results, load the [dataset color palette](https://g..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Scene text recognition (STR) has been an active rese..."
          ],
          [
           "MGP-STR is trained on two synthetic datasets [MJSynth]((http://www.robots.ox.ac.uk/~vgg/data/text/))..."
          ],
          [
           "```py\n>>> from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\n>>> import request..."
          ],
          [
           "```\n\n## MgpstrConfig\n\n[[autodoc]] MgpstrConfig\n\n## MgpstrTokenizer\n\n[[autodoc]] MgpstrTokenizer\n    ..."
          ],
          [
           "Zero-shot classifier distillation\n\nAuthor: @joeddav \n\nThis script provides a way to improve the spee..."
          ],
          [
           "```\n\n`<unlabeled_data.txt>` should be a text file with a single unlabeled example per line. `<class_..."
          ],
          [
           "Any of the arguments in the ðŸ¤— Trainer's\n[`TrainingArguments`](https://huggingface.co/transformers/ma..."
          ],
          [
           ">>> zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"roberta-large-mnli\")\n>>> zero..."
          ],
          [
           "```\n\nUnfortunately, inference is slow since each of our 4 class names must be fed through the large ..."
          ],
          [
           "```\n\nand even used trivially with a `TextClassificationPipeline`:\n\n```python\n>>> distilled_classifie..."
          ],
          [
           "```\n\n```python\n%%time\nfor _ in range(1000):\n    distilled_classifier([sequence] * 16)\n# runs in 10.3..."
          ],
          [
           "!--Copyright 2023 Mistral AI and The HuggingFace Team. All rights reserved.\n\nLicensed under the Apac..."
          ],
          [
           "This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada) and [Arthur Zucker](..."
          ],
          [
           "These ready-to-use checkpoints can be downloaded and used via the HuggingFace Hub:\n\n```python\n>>> fr..."
          ],
          [
           "```\n\nTo use the raw checkpoints with HuggingFace you can use the `convert_mixtral_weights_to_hf.py` ..."
          ],
          [
           "```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab..."
          ],
          [
           "```\n\n### Expected speedups\n\nBelow is a expected speedup diagram that compares pure inference time be..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "### Structure of the prompt\n\nLet's take a closer look at how the prompt is structured to understand ..."
          ],
          [
           "```\n\nTask: \"Identify the oldest person in the `document` and create an image showcasing the result a..."
          ],
          [
           "```\n\nWe can see that the tool name is short and precise. The description includes two parts, the fir..."
          ],
          [
           "```\n\n````\n\nThe pattern the model is prompted to repeat has three parts: The task statement, the agen..."
          ],
          [
           "```\n\nHuman: I tried this code, it worked but didn't give me a good result. The question is in French..."
          ],
          [
           "```\nwhich the agent completes. Contrary to the `run` command, the `chat` command then appends the co..."
          ],
          [
           "```\n\n```text\n'This is a tool that creates an image according to a prompt, which is a text descriptio..."
          ],
          [
           "```\nreturns\n```text\n==Explanation from the agent== \nI will use the following tools `image_generator`..."
          ],
          [
           "```\n\nwhich is definitely closer to what we had in mind! However, we want to have both the house and ..."
          ],
          [
           "```\n\nTherefore it is important that the examples of the custom `chat` prompt template also make use ..."
          ],
          [
           "```\ntemplate = \"\"\" [...] \"\"\"\n\nagent = HfAgent(url_endpoint=your_endpoint, chat_prompt_template=templ..."
          ],
          [
           "```\n\nUpon adding custom tools to an agent, the tools' descriptions and names are automatically\ninclu..."
          ],
          [
           "```\n\nThe set of curated tools already has an `image_transformer` tool which is hereby replaced with ..."
          ],
          [
           "```\n\n```text\n==Explanation from the agent==\nI will use the following tool: `image_transformer` to tr..."
          ],
          [
           "```\n\nFor the task `text-classification`, this returns `'facebook/bart-large-mnli'`, for `translation..."
          ],
          [
           "```\n\nWe now have our tool handy. Save it in a file and import it from your main script. Let's name t..."
          ],
          [
           "```\n\nand generates the following audio.\n\n| **Audio**                                                ..."
          ],
          [
           "```\n\n<Tip>\n\nBeware when replacing tools with others! This will also adjust the agent's prompt. This ..."
          ],
          [
           "```\n\nThe model adequately leverages the tool:\n```text\n==Explanation from the agent==\nI will use the ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\n\nHereby, _inference_ is defined by a single forward pass, and _training_ is defined by a singl..."
          ],
          [
           "```\n</pt>\n<tf>\n```py\n>>> from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments\n..."
          ],
          [
           "```\n\nAn instantiated benchmark object can then simply be run by calling `benchmark.run()`.\n\n```py\n>>..."
          ],
          [
           "====================      INFERENCE - MEMORY - RESULT       ====================\n-------------------..."
          ],
          [
           "====================        ENVIRONMENT INFORMATION         ====================\n\n- transformers_ver..."
          ],
          [
           "```\n</pt>\n<tf>\n```bash\npython examples/tensorflow/benchmarking/run_benchmark_tf.py --help..."
          ],
          [
           "```\n\nAn instantiated benchmark object can then simply be run by calling `benchmark.run()`.\n\n```py\n>>..."
          ],
          [
           "====================      INFERENCE - MEMORY - RESULT       ====================\n-------------------..."
          ],
          [
           "====================        ENVIRONMENT INFORMATION         ====================\n\n- transformers_ver..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\nBy default, the _time_ and the _required memory_ for _inference_ are ..."
          ],
          [
           ">>> benchmark = PyTorchBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\n>>> benc..."
          ],
          [
           "====================      INFERENCE - MEMORY - RESULT       ====================\n-------------------..."
          ],
          [
           "====================        ENVIRONMENT INFORMATION         ====================\n\n- transformers_ver..."
          ],
          [
           "```\n</pt>\n<tf>\n```py\n>>> from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments,..."
          ],
          [
           ">>> benchmark = TensorFlowBenchmark(args, configs=[config_base, config_384_hid, config_6_lay])\n>>> b..."
          ],
          [
           "====================      INFERENCE - MEMORY - RESULT       ====================\n-------------------..."
          ],
          [
           "====================        ENVIRONMENT INFORMATION         ====================\n\n- transformers_ver..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\nAgain, _inference time_ and _required memory_ for _inference_ are mea..."
          ],
          [
           "With the new _benchmark_ tools, it is easier than ever to share your benchmark results with the comm..."
          ],
          [
           "!--Copyright 2023 IBM and HuggingFace Inc. team. All rights reserved.\n\nLicensed under the Apache Lic..."
          ],
          [
           "The abstract from the paper is the following:\n\n*TSMixer is a lightweight neural architecture exclusi..."
          ],
          [
           "## Sample usage \n```python\n\nfrom transformers import PatchTSMixerConfig, PatchTSMixerForPrediction\nf..."
          ],
          [
           "```\n\n## Usage tips\n\nThe model can also be used for time series classification and time series regres..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Overview\n\nOpenAI GPT-2 model was proposed in [Language Models are Unsupervised Multitask Learners..."
          ],
          [
           "## Usage tips\n\n- GPT-2 is a model with absolute position embeddings so it's usually advised to pad t..."
          ],
          [
           "- [`GPT2LMHeadModel`] is supported by this [causal language modeling example script](https://github...."
          ],
          [
           "## GPT2Config\n\n[[autodoc]] GPT2Config\n\n## GPT2Tokenizer\n\n[[autodoc]] GPT2Tokenizer\n    - save_vocabu..."
          ],
          [
           "## TFGPT2LMHeadModel\n\n[[autodoc]] TFGPT2LMHeadModel\n    - call\n\n## TFGPT2DoubleHeadsModel\n\n[[autodoc..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## RetriBertConfig\n\n[[autodoc]] RetriBertConfig\n\n## RetriBertTokenizer\n\n[[autodoc]] RetriBertTokeniz..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nAct..."
          ],
          [
           "```\n\n## Install from source\n\nInstall ðŸ¤— Transformers from source with the following command:\n\n```bash..."
          ],
          [
           "```\n\nYour Python environment will find the `main` version of ðŸ¤— Transformers on the next run.\n\n## Ins..."
          ],
          [
           "```\n\nThis script should run without hanging or waiting to timeout because it won't attempt to downlo..."
          ],
          [
           "```\n\n    3. Now when you're offline, reload your files with [`PreTrainedModel.from_pretrained`] from..."
          ],
          [
           "```\n\n<Tip>\n\nSee the [How to download files from the Hub](https://huggingface.co/docs/hub/how-to-down..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           ">>> prompt = (\n...     \"In a shocking finding, scientists discovered a herd of unicorns living in a ..."
          ],
          [
           "```\n\n## Combining GPT-Neo and Flash Attention 2\n\nFirst, make sure to install the latest version of F..."
          ],
          [
           "```\n\n### Expected speedups\n\nBelow is an expected speedup diagram that compares pure inference time b..."
          ],
          [
           "## GPTNeoForTokenClassification\n\n[[autodoc]] GPTNeoForTokenClassification\n    - forward\n\n</pt>\n<jax>..."
          ],
          [
           "DeeBERT: Early Exiting for *BERT\n\nThis is the code base for the paper [DeeBERT: Dynamic Early Exitin..."
          ],
          [
           "```\n@inproceedings{xin-etal-2020-deebert,\n    title = \"{D}ee{BERT}: Dynamic Early Exiting for Accele..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--..."
          ],
          [
           "```\n\nYou'll use ðŸ¤— Datasets to load a dataset from the Hugging Face Hub, ðŸ¤— Transformers to train your..."
          ],
          [
           "```\n\nYou'll see that this dataset already comes with a training set containing 1000 images and a tes..."
          ],
          [
           "```\n\nThe examples in the dataset have the following fields:\n- `image_id`: the example image id\n- `im..."
          ],
          [
           ">>> for i in range(len(annotations[\"id\"])):\n...     box = annotations[\"bbox\"][i]\n...     class_idx =..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://i.imgur.com/TdaqPJO.png\" alt=\"CPPE-5 Im..."
          ],
          [
           "```\n\n## Preprocess the data\n\nTo finetune a model, you must preprocess the data you plan to use to ma..."
          ],
          [
           "```\n\nBefore passing the images to the `image_processor`, apply two preprocessing transformations to ..."
          ],
          [
           "```\n\nThe `image_processor` expects the annotations to be in the following format: `{'image_id': int,..."
          ],
          [
           "```\n\nNow you can combine the image and annotation transformations to use on a batch of examples:\n\n``..."
          ],
          [
           "```\n\nApply this preprocessing function to the entire dataset using ðŸ¤— Datasets [`~datasets.Dataset.wi..."
          ],
          [
           "[[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\n          [ 1.3081,  1.3081,  1.3081,..."
          ],
          [
           "[[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\n          [ 1.4200,  1.4200,  1.4200,..."
          ],
          [
           "```\n\nYou have successfully augmented the individual images and prepared their annotations. However, ..."
          ],
          [
           "```\n\n## Training the DETR model\nYou have done most of the heavy lifting in the previous sections, so..."
          ],
          [
           "```\n\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, then configu..."
          ],
          [
           "```\n\n## Evaluate\nObject detection models are commonly evaluated with a set of <a href=\"https://cocod..."
          ],
          [
           "...     return annotations\n\n\n>>> # Save images and annotations into the files torchvision.datasets.C..."
          ],
          [
           "...     with open(path_anno, \"w\") as file:\n...         json.dump(output_json, file, ensure_ascii=Fal..."
          ],
          [
           "```\n\nNext, prepare an instance of a `CocoDetection` class that can be used with `cocoevaluator`.\n\n``..."
          ],
          [
           "```\n\nFinally, load the metrics and run the evaluation.\n\n```py\n>>> import evaluate\n>>> from tqdm impo..."
          ],
          [
           "...         module.add(prediction=results, reference=labels)\n...         del batch\n\n>>> results = mo..."
          ],
          [
           "```\nThese results can be further improved by adjusting the hyperparameters in [`~transformers.Traini..."
          ],
          [
           "```\n\nYou can also manually replicate the results of the pipeline if you'd like:\n\n```py\n>>> image_pro..."
          ],
          [
           "```\n\nLet's plot the result:\n```py\n>>> draw = ImageDraw.Draw(image)\n\n>>> for score, label, box in zip..."
          ],
          [
           "Fine-Tuning week of XLSR-Wav2Vec2 on 60 languages ðŸŒ\n\nWelcome to the fine-tuning week! The goal of th..."
          ],
          [
           "**Please keep in mind:**\nThe spirit of the fine-tuning week is to provide state-of-the-art speech re..."
          ],
          [
           "## Table of Contents\n\n- [Organization of the fine tuning week](#organization-of-the-fine-tuning-week..."
          ],
          [
           "## Organization of the fine tuning week\n\nThe week officially starts on 22.03.2021 and ends on 29.03...."
          ],
          [
           "Two possible setups can be used to fine-tune Wav2Vec2. The easiest setup is to simply use [google co..."
          ],
          [
           "**2.**: Next, head over to the official [Fine-Tune XLSR-Wav2Vec2 with ðŸ¤— Transformes](https://colab.r..."
          ],
          [
           "**5.**: It is time to start running the google colab! Make sure that you have selected \"GPU\" as your..."
          ],
          [
           "When running the google colab make sure that you uncomment the cell corresponding to mounting your g..."
          ],
          [
           "```\n\nand the line:\n\n```python\n  output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo..."
          ],
          [
           "```\n\nfurther below (which should already be uncommented).\n\nHaving finished the training you should f..."
          ],
          [
           "### Local machine\n\nWe have provided `run_common_voice.py` script to run fine-tuning on local machine..."
          ],
          [
           "```\n$ git clone https://github.com/huggingface/transformers.git\n```\n\nSecond, head over to the `examp..."
          ],
          [
           "```\n\n\t**Note**: Installing the latest version of `torchaudio` will also upgrade `torch` to it's late..."
          ],
          [
           "```\n\n\t**To lanuch fine-tuninig on multiple GPUs:**\n\t\n\t```bash\n\tpython -m torch.distributed.launch \\\n..."
          ],
          [
           "```\n\n\tThe above command will launch the training on 4 GPUs. Use the `--nproc_per_node` option to spe..."
          ],
          [
           "```\n\nThen and add the following files that fully define a XLSR-Wav2Vec2 checkpoint into the reposito..."
          ],
          [
           "```\n\nThe next **very important** step is to create the model card. For people to use your fine-tuned..."
          ],
          [
           "<======================Copy **raw** version from here=========================\n---\nlanguage: {lang_i..."
          ],
          [
           "type: common_voice\n      args: {lang_id} #TODO: replace {lang_id} in your language code here. Make s..."
          ],
          [
           "# Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, *e.g.* French\n\nFin..."
          ],
          [
           "processor = Wav2Vec2Processor.from_pretrained(\"{model_id}\") #TODO: replace {model_id} with your mode..."
          ],
          [
           "```\n\n\n## Evaluation\n\nThe model can be evaluated as follows on the {language} test data of Common Voi..."
          ],
          [
           "# Preprocessing the datasets.\n# We need to read the aduio files as arrays\ndef speech_file_to_array_f..."
          ],
          [
           "```\n\n**Test Result**: XX.XX %  # TODO: write output of print here. IMPORTANT: Please remember to als..."
          ],
          [
           "In this section, we will quickly go over what data is allowed to be used as training \ndata, what kin..."
          ],
          [
           "## Tips and tricks\n\nThis section summarizes a couple of tips and tricks across various topics. It wi..."
          ],
          [
           "If you are interested in learning more about the model though, here are a couple of resources that a..."
          ],
          [
           "- What data was used to XLSR-Wav2Vec2? The checkpoint we will use for further fine-tuning was pretra..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Read more about it [in the release blogpost](https://www.mosaicml.com/blog/mpt-7b)\n\n## Usage tips\n\n-..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Open-vocabulary object detection has benefited great..."
          ],
          [
           "## Usage example\n\nOWLv2 is, just like its predecessor [OWL-ViT](owlvit), a zero-shot text-conditione..."
          ],
          [
           ">>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.g..."
          ],
          [
           "```\n\n## Resources\n\n- A demo notebook on using OWLv2 for zero- and one-shot (image-guided) object det..."
          ],
          [
           "Flax/JAX community week ðŸ¤—\n\nWelcome to the Flax/JAX community week! The goal of this week is to make ..."
          ],
          [
           "Don't forget to sign up [here](https://forms.gle/tVGPhjKXyEsSgUcs8)! \n\n## Table of Contents\n\n- [Orga..."
          ],
          [
           "## Organization\n\nParticipants can propose ideas for an interesting NLP and/or CV project. Teams of 3..."
          ],
          [
           "## Important dates\n\n- **23.06.** Official announcement of the community week. Make sure to sign-up i..."
          ],
          [
           "For issues with Flax/JAX, Transformers, Datasets or for questions that are specific to your project ..."
          ],
          [
           "## Projects\n\nDuring the first week after the community week announcement, **23.06. - 30.06.**, teams..."
          ],
          [
           "### How to form a team around a project\n\nYou can check out all existing projects ideas on the forum ..."
          ],
          [
           "Once created, the team can start refining their project:\n\n- What is the goal of the project? *E.g.*,..."
          ],
          [
           "## Tips on how to organize the project\n\nThis section gives you some tips on how to most efficiently ..."
          ],
          [
           "By \"has defined\" we don't meant that the corresponding code already has to be written and ready \nto ..."
          ],
          [
           "As a conclusion, being honest about one's expected involvement is crucial so that \nthe workload can ..."
          ],
          [
           "### Other tips\n\nHere is a collection of some more tips:\n\n- We strongly recommend to work as publicly..."
          ],
          [
           "```\n- Ask for help. If you are stuck, use the public Slack channel or the [forum](https://discuss.hu..."
          ],
          [
           "```\n\nYou can activate your venv by running\n\n```bash\nsource ~/<your-venv-name>/bin/activate\n```\n\nWe s..."
          ],
          [
           "```\n\n4. Set up a flax environment by running the following command in a virtual environment:\n\n   ```..."
          ],
          [
           "```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \"..."
          ],
          [
           "```\nlibtpu.so already in used by another process. Not attempting to load libtpu.so in this process.\n..."
          ],
          [
           "```\n\nNext you should install JAX's TPU version on TPU by running the following command: \n\n```\n$ pip ..."
          ],
          [
           "```\n\n**Note**: Running this command might actually throw an error, such as:\n```\n Building wheel for ..."
          ],
          [
           "```\nJax should have been installed correctly nevertheless.\n\nTo verify that JAX was correctly install..."
          ],
          [
           "```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \"..."
          ],
          [
           "```\n\n## Quickstart flax and jax\n\n[JAX](https://jax.readthedocs.io/en/latest/index.html) is Autograd ..."
          ],
          [
           "- [BART](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/modeling..."
          ],
          [
           "- [T5](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_fla..."
          ],
          [
           "You can find all available training scripts for JAX/Flax under the \nofficial [flax example folder](h..."
          ],
          [
           "### **Flax design philosophy in Transformers**\n\nThis section will explain how Flax models are implem..."
          ],
          [
           "In PyTorch, the weights matrices would be stored as `torch.nn.Linear` objects alongside the model's ..."
          ],
          [
           "```\n\nInstantiating an object `model_pytorch` of the class `ModelPyTorch` would actually allocate mem..."
          ],
          [
           "```\n\nIn a more abstract way, this can be represented as passing the word embeddings to the model fun..."
          ],
          [
           "```\n\nAt first glance the linear layer class `flax.linen.Dense` looks very similar to PyTorch's `torc..."
          ],
          [
           "```\n\nVisually, the forward pass would now be represented as passing all tensors required for the com..."
          ],
          [
           "The `FlaxPreTrainedModel` is an abstract class that holds a Flax module, handles weights initializat..."
          ],
          [
           "First, write a Flax module that will declare the layers and computation.\n\n```python\nimport flax.line..."
          ],
          [
           "```\n\nNow let's define the `FlaxPreTrainedModel` model class.\n\n```python\nfrom transformers.modeling_f..."
          ],
          [
           "```\n\nNow the `FlaxMLPModel` will have a similar interface as PyTorch or Tensorflow models and allows..."
          ],
          [
           "Another significant difference between Flax and PyTorch models is that, we can pass the `labels` dir..."
          ],
          [
           "model = FlaxRobertaModel.from_pretrained(\"julien-c/dummy-unknown\")\n\n@jax.jit\ndef run_model(input_ids..."
          ],
          [
           "```\n\nWe use `jax.jit` to compile the function to get maximum performance. Note that in the above exa..."
          ],
          [
           "```\n\nAs explained above we don't compute the loss inside the model, but rather in the task-specific ..."
          ],
          [
           "```\n\nFinally, let's run our training loop.\n\n```python\n# train loop\nfor i in range(10):\n   params, op..."
          ],
          [
           "```\n\nNote that, as JAX is backed by the [XLA](https://www.tensorflow.org/xla) compiler any JAX/Flax ..."
          ],
          [
           "Speaker        | Topic                           | Time                  |  Video |\n|-------------|-..."
          ],
          [
           "### Thursday, July 1st\n- [Watch the talks on YouTube](https://www.youtube.com/watch?v=__eG63ZP_5g)\n-..."
          ],
          [
           "Speaker        | Topic                           | Time                  | Video |\n|-------------|--..."
          ],
          [
           "| Rohan Anil, Google Brain | Scalable Second Order Optimization for Deep Learning      | 7.00pm-7.30..."
          ],
          [
           "### Friday, July 2nd\n- [Watch the talks on YouTube](https://www.youtube.com/watch?v=ZCMOPkcTu3s)\n- [..."
          ],
          [
           "Speaker        | Topic                           | Time                  |  Video |\n|-------------|-..."
          ],
          [
           "| Siddhartha Kamalakara, Joanna Yoo & JoÃ£o G M AraÃºjo, Cohere | Training large scale language models..."
          ],
          [
           "### Talks & Speakers\n\n#### Skye Wanderman-Milne, JAX developer, Google Brain\n- Talk: Intro to JAX on..."
          ],
          [
           "#### Pablo Castro, Staff Research Software Developer; Google Research, Brain Team\n- Talk: Using Jax ..."
          ],
          [
           "#### Sabrina J. Mielke, PhD student at The Johns Hopkins University & Part-time research intern at H..."
          ],
          [
           "#### Mostafa Dehghani, Research Scientist, Google Brain\n- Talk: Long Range Arena: Benchmarking Effic..."
          ],
          [
           "#### Rohan Anil, Senior Staff Software Engineer, Google Research, Brain Team\n- Talk: Scalable Second..."
          ],
          [
           "#### Ben Wang, Independent AI Researcher, EleutherAI\n- Talk: Multihost Training in Mesh Transformer ..."
          ],
          [
           "#### Siddhartha Kamalakara, Joanna Yoo, JoÃ£o G M AraÃºjo, MLE at Cohere\n- Talk: Training large scale ..."
          ],
          [
           "Now let's explain in more detail how a project can be created on the hub. Having an officially defin..."
          ],
          [
           "Great, now we have a project directory with integrated git version control and a public model page, ..."
          ],
          [
           "```\n\nNext we can clone the repo:\n\n```bash\n$ git clone https://huggingface.co/flax-community/roberta-..."
          ],
          [
           "```\n\nCool! The file is now displayed on the model page under the [files tab](https://huggingface.co/..."
          ],
          [
           "```\n\nThis creates and saves our tokenizer directly in the cloned repository.\nFinally, we can start t..."
          ],
          [
           "```\n\nSince the dataset is tiny this command should actually run in less than 5 minutes. Note that we..."
          ],
          [
           "bytes_output = serialization.to_bytes(params)\n\nrepo = Repository(\"flax-model\", clone_from=\"flax-comm..."
          ],
          [
           "```\n\n**Note**: Make sure to have `huggingface_hub >= 0.0.13` to make this command work.\n\nFor more in..."
          ],
          [
           "```\nYou don't have sufficient permission to view this page\n```\n- this is expected! \n\nGreat, now you ..."
          ],
          [
           "```\n\t\nThis should ssh you into the TPU VM!\nNow you can follow the steps of the section [How to insta..."
          ],
          [
           "**NLP**\n* **Conversational:** To have the best conversations!. [Example](https://huggingface.co/micr..."
          ],
          [
           "**Speech**\n* **Audio to Audio:** For tasks such as audio source separation or speech enhancement. \n*..."
          ],
          [
           "```\npip install huggingface_hub\n```\n\nHere is an example downloading (and caching!) a specific file d..."
          ],
          [
           "```\n\n\nWe'll provide more examples on Streamlit demos next week. Stay tuned!\n\n### Using a Gradio demo..."
          ],
          [
           "### Jury\n\n* [Niki Parmar](https://research.google/people/NikiParmar/): Staff Research Scientist at G..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The cost of vision-and-language pre-training has bec..."
          ],
          [
           "## Usage tips\n\n- BLIP-2 can be used for conditional text generation given an image and an optional t..."
          ],
          [
           "[[autodoc]] Blip2Model\n    - forward\n    - get_text_features\n    - get_image_features\n    - get_qfor..."
          ],
          [
           "Text Summarization with Pretrained Encoders\n\nThis folder contains part of the code necessary to repr..."
          ],
          [
           "```\n\nAnd move all the stories to the same folder. We will refer as `$DATA_PATH` the path to where yo..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*This paper presents a new vision Transformer, called..."
          ],
          [
           "<small> Swin Transformer architecture. Taken from the <a href=\"https://arxiv.org/abs/2102.03334\">ori..."
          ],
          [
           "If you're interested in submitting a resource to be included here, please feel free to open a Pull R..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[[autodoc]] onnx.config.OnnxConfigWithPast\n\n### OnnxSeq2SeqConfigWithPast\n\n[[autodoc]] onnx.config.O..."
          ],
          [
           "!--Copyright 2023 The Intel Team Authors and HuggingFace Inc. team. All rights reserved.\n\nLicensed u..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In this paper, we study the problem of temporal vide..."
          ],
          [
           "This research addresses temporal video grounding (TVG), which is the process of pinpointing the star..."
          ],
          [
           "The goal of this model is to incorporate trainable prompts into both visual inputs and textual featu..."
          ],
          [
           "def pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n    '''\n    ..."
          ],
          [
           "def decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n    '''\n    Decod..."
          ],
          [
           "decoder_kwargs = dict(\n    container=av.open(file, metadata_errors=\"ignore\"),\n    sampling_rate=1,\n ..."
          ],
          [
           "```\n\nTips:\n\n- This implementation of TVP uses [`BertTokenizer`] to generate text embeddings and Resn..."
          ],
          [
           "# Adversarial evaluation of model performances\n\nHere is an example on evaluating a model using adver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original checkpoints can be found [here](https://github.com/google-research/t5x/blob/main/docs/m..."
          ],
          [
           "```\n\n<Tip>\n\nRefer to [T5's documentation page](t5) for API reference, tips, code examples and notebo..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "FlashAttention-2 is currently supported for the following architectures:\n* [Bark](https://huggingfac..."
          ],
          [
           "* [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n* ..."
          ],
          [
           "You can request to add FlashAttention-2 support for another model by opening a GitHub Issue or Pull ..."
          ],
          [
           "```\n\nWe strongly suggest referring to the detailed [installation instructions](https://github.com/Da..."
          ],
          [
           "```\n\n<Tip>\n\nFlashAttention-2 can only be used when the model's dtype is `fp16` or `bf16`. Make sure ..."
          ],
          [
           "```\n\n### Expected speedups\n\nYou can benefit from considerable speedups for inference, especially for..."
          ],
          [
           "For sequences with padding tokens (generating with padding tokens), you need to unpad/pad the input ..."
          ],
          [
           "For now, Transformers supports SDPA inference and training for the following architectures:\n* [Bart]..."
          ],
          [
           "input_text = \"Hello my dog is cute and\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda..."
          ],
          [
           "```\n\nIf you see a bug with the traceback below, try using the nightly version of PyTorch which may h..."
          ],
          [
           "```\n\n## BetterTransformer\n\n<Tip warning={true}>\n\nSome BetterTransformer features are being upstreame..."
          ],
          [
           "```\n\nYou can return the original Transformers model with the [`~PreTrainedModel.reverse_bettertransf..."
          ],
          [
           "```\n\n### 8-bit\n\n<Tip>\n\nIf you're curious and interested in learning more about the concepts underlyi..."
          ],
          [
           "```\n\nTo load a model in 4-bit for inference with multiple GPUs, you can control how much GPU RAM you..."
          ],
          [
           "```\n\n<Tip>\n\nFeel free to try running a 11 billion parameter [T5 model](https://colab.research.google..."
          ],
          [
           "ORT is supported by ðŸ¤— Optimum which can be used in ðŸ¤— Transformers. You'll need to use an [`~optimum...."
          ],
          [
           "```\n\nNow you're free to use the model for inference:\n\n```py\nfrom optimum.pipelines import pipeline\nf..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Introduction\n\nSplitting a text into smaller chunks is a task that is harder than it looks, and th..."
          ],
          [
           "```\n[\"Don't\", \"you\", \"love\", \"ðŸ¤—\", \"Transformers?\", \"We\", \"sure\", \"do.\"]\n```\n\nThis is a sensible firs..."
          ],
          [
           "```\n\nAs can be seen space and punctuation tokenization, as well as rule-based tokenization, is used ..."
          ],
          [
           "## Subword tokenization\n\n<Youtube id=\"zHvTiHr506c\"/>\n\nSubword tokenization algorithms rely on the pr..."
          ],
          [
           "```\n\nBecause we are considering the uncased model, the sentence was lowercased first. We can see tha..."
          ],
          [
           "```\n\nWe'll get back to the meaning of those `\"â–\"` when we look at [SentencePiece](#sentencepiece). A..."
          ],
          [
           "```\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```\n\nConsequently, the base vocabu..."
          ],
          [
           "```\n\nBPE then identifies the next most common symbol pair. It's `\"u\"` followed by `\"n\"`, which occur..."
          ],
          [
           "```\n\nAssuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rule..."
          ],
          [
           "<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BER..."
          ],
          [
           "<a id='unigram'></a>\n\n### Unigram\n\nUnigram is a subword tokenization algorithm introduced in [Subwor..."
          ],
          [
           "```\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],..."
          ],
          [
           "```\n\n`\"hugs\"` could be tokenized both as `[\"hug\", \"s\"]`, `[\"h\", \"ug\", \"s\"]` or `[\"h\", \"u\", \"g\", \"s\"]..."
          ],
          [
           "The [`XLNetTokenizer`] uses SentencePiece for example, which is also why in the example earlier the\n..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pre-trained language models like BERT and its varian..."
          ],
          [
           "## ConvBertConfig\n\n[[autodoc]] ConvBertConfig\n\n## ConvBertTokenizer\n\n[[autodoc]] ConvBertTokenizer\n ..."
          ],
          [
           "## TFConvBertForMultipleChoice\n\n[[autodoc]] TFConvBertForMultipleChoice\n    - call\n\n## TFConvBertFor..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "After such a [`VisionEncoderDecoderModel`] has been trained/fine-tuned, it can be saved/loaded just ..."
          ],
          [
           "```\n\n## Initialising `VisionEncoderDecoderModel` from a pretrained encoder and a pretrained decoder...."
          ],
          [
           "```\n\n## Loading an existing `VisionEncoderDecoderModel` checkpoint and perform inference.\n\nTo load f..."
          ],
          [
           "```\n\n## Loading a PyTorch checkpoint into `TFVisionEncoderDecoderModel`.\n\n[`TFVisionEncoderDecoderMo..."
          ],
          [
           "```\n\n## Training\n\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e..."
          ],
          [
           "```\n\nThis model was contributed by [nielsr](https://github.com/nielsrogge). This model's TensorFlow ..."
          ],
          [
           "Distil*\n\nAuthor: @VictorSanh\n\nThis folder contains the original code used to train Distil* as well a..."
          ],
          [
           "**October 23, 2019 - Update** We release **DistilRoBERTa**: 95% of `RoBERTa-base`'s performance on G..."
          ],
          [
           "## What is Distil*\n\nDistil* is a class of compressed models that started with DistilBERT. DistilBERT..."
          ],
          [
           "For more information on DistilBERT, please refer to our [NeurIPS workshop paper](https://arxiv.org/a..."
          ],
          [
           "| Model                     | Macro-score                    | CoLA | MNLI | MRPC | QNLI | QQP  | RT..."
          ],
          [
           "| DistilRoBERTa<sup>1</sup> |  **79.0**/**82.3**<sup>2</sup> | 59.3 | 84.0 | 86.6 | 90.8 | 89.4 | 67..."
          ],
          [
           "<sup>1</sup> We did not use the MNLI checkpoint for fine-tuning but directly perform transfer learni..."
          ],
          [
           "- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to p..."
          ],
          [
           "- `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1/2 of the data use..."
          ],
          [
           "- `distilbert-base-multilingual-cased`: DistilmBERT multilingual model pretrained with the supervisi..."
          ],
          [
           "Using DistilBERT is very similar to using BERT. DistilBERT share the same tokenizer as BERT's `bert-..."
          ],
          [
           "```\n\nSimilarly, using the other Distil* models simply consists in calling the base classes with a di..."
          ],
          [
           "```\n\nOur implementation of masked language modeling loss follows [XLM](https://github.com/facebookre..."
          ],
          [
           "```\n\nBy default, this will launch a training on a single GPU (even if more are available on the clus..."
          ],
          [
           "```\n\n**Tips:** Starting distilled training with good initialization of the model weights is crucial ..."
          ],
          [
           "How to propose a Flax/JAX + Transformers project \n\nGreat that you've opened this document! \nWhile we..."
          ],
          [
           "## How to submit a project proposal\n\nFirst, you should make sure that you are [logged in](https://hu..."
          ],
          [
           "1. *A clear description of the project*\n2. *In which language should the project be conducted?* Engl..."
          ],
          [
           "4. *What data should be used?* It is important to state at least what kind of data you would like to..."
          ],
          [
           "Feel free to copy-paste the following format for your project proposal and fill out the respective s..."
          ],
          [
           "```\n# <FILL ME: Name of project>\n\n<FILL ME: A clear description of the project>\n\n## 2. Language\n\nThe..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage tips\n\n- We have released a series of models [here](https://huggingface.co/models?filter=mvp..."
          ],
          [
           ">>> generated_ids = model_with_prompt.generate(**inputs)\n>>> tokenizer.batch_decode(generated_ids, s..."
          ],
          [
           "```\n\nFor data-to-text generation, it is an example to use MVP and multi-task pre-trained variants.\n`..."
          ],
          [
           "```\n\nFor lightweight tuning, *i.e.*, fixing the model and only tuning prompts, you can load MVP with..."
          ],
          [
           "```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Question ..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformers-based models, such as BERT, have been o..."
          ],
          [
           "The original code can be found [here](https://github.com/google-research/bigbird).\n\n## Usage tips\n\n-..."
          ],
          [
           "[[autodoc]] BigBirdPegasusForConditionalGeneration\n    - forward\n\n## BigBirdPegasusForSequenceClassi..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--..."
          ],
          [
           "```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model w..."
          ],
          [
           "```\n\nTake a look at the example again:\n\n```py\n>>> minds[\"train\"][0]\n{'audio': {'array': array([-0.00..."
          ],
          [
           "```\n\nThe MInDS-14 dataset has a sampling rate of 8000kHz (you can find this information in its [data..."
          ],
          [
           "```\n\nAs you can see in the `transcription` above, the text contains a mix of upper and lowercase cha..."
          ],
          [
           "```\n\nðŸ¤— Transformers doesn't have a data collator for ASR, so you'll need to adapt the [`DataCollator..."
          ],
          [
           "```\n\nNow instantiate your `DataCollatorForCTCWithPadding`:\n\n```py\n>>> data_collator = DataCollatorCT..."
          ],
          [
           "```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           ">>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=encode..."
          ],
          [
           "```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t..."
          ],
          [
           "```\n\n<Tip>\n\nThe transcription is decent, but it could be better! Try finetuning your model on more e..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [junnyu](https://huggingface.co/junnyu). The original code can be foun..."
          ],
          [
           "## RoFormerForSequenceClassification\n\n[[autodoc]] RoFormerForSequenceClassification\n    - forward\n\n#..."
          ],
          [
           "## FlaxRoFormerModel\n\n[[autodoc]] FlaxRoFormerModel\n    - __call__\n\n## FlaxRoFormerForMaskedLM\n\n[[au..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*This paper presents XLSR which learns cross-lingual ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Program synthesis strives to generate a computer pro..."
          ],
          [
           "## Checkpoint Naming\n\n* CodeGen model [checkpoints](https://huggingface.co/models?other=codegen) are..."
          ],
          [
           "```\n\n## Resources\n\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## CodeGenCo..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "In the remainder of this guide, you will learn what's needed to add a new TensorFlow model architect..."
          ],
          [
           "- Don't reinvent the wheel! More often than not, there are at least two reference implementations yo..."
          ],
          [
           "<Tip>\n\nBefore starting the work on a TensorFlow model architecture, double-check that there is no on..."
          ],
          [
           "```\n\n3. Set up a development environment, for instance by running the following command:\n\n```bash\npy..."
          ],
          [
           "```\n\n8. Once you are satisfied, go to the webpage of your fork on GitHub. Click on â€œPull requestâ€. M..."
          ],
          [
           "### 4. Model implementation\n\nNow it's time to finally start coding. Our suggested starting point is ..."
          ],
          [
           "Sadly, there is no prescription to convert a PyTorch model into TensorFlow. You can, however, follow..."
          ],
          [
           "Double-check the documentation!\n- PyTorch's `nn.Parameter` variables typically need to be initialize..."
          ],
          [
           "`TFBrandNewBertModel` will simply be a wrapper around this layer.\n- Keras models need to be built in..."
          ],
          [
           "In addition to the model file itself, you will also need to add the pointers to the model classes an..."
          ],
          [
           "When you're happy with your implementation, run the following checklist to confirm that your model a..."
          ],
          [
           "```\n\nThe most likely outcome is that you'll see a bunch of errors. Don't worry, this is expected! De..."
          ],
          [
           "```\n\nand we will merge your PR! Congratulations on the milestone ðŸŽ‰\n\n**7. (Optional) Build demos and ..."
          ],
          [
           "First of all, let's talk about why understanding these mismatches matters. Many community members wi..."
          ],
          [
           "In some cases, in discussion with the ðŸ¤— Transformers team, we might find that fixing the mismatch is..."
          ],
          [
           "Image Captioning (vision-encoder-text-decoder model) training example\n\nThe following example showcas..."
          ],
          [
           "```\n\n### Create a model from a vision encoder model and a text decoder model\nNext, we create a [Flax..."
          ],
          [
           "```\n\n### Train the model\nFinally, we can run the example script to train the model:\n\n```bash\npython3..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/simmim_archi..."
          ],
          [
           "```\n\nHere, we train for 100 epochs with a learning rate of 2e-5. Note that the SimMIM authors used a..."
          ],
          [
           "```\n\nNext, we can run the script by providing the path to this custom configuration (replace `path_t..."
          ],
          [
           "```\n\n## MAE\n\nThe `run_mae.py` script can be used to pre-train a Vision Transformer as a masked autoe..."
          ],
          [
           "```bash\npython run_mae.py \\\n    --dataset_name cifar10 \\\n    --output_dir ./vit-mae-demo \\\n    --rem..."
          ],
          [
           "```\n\nHere we set:\n- `mask_ratio` to 0.75 (to mask 75% of the patches for each image)\n- `norm_pix_los..."
          ],
          [
           "```\n\nNote that you can put images in dummy subfolders, whose names will be ignored by default (as la..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [sijunhe](https://huggingface.co/sijunhe). The original code can be fo..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In deep learning, models typically reuse the same pa..."
          ],
          [
           "## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](../tasks..."
          ],
          [
           "Simple VQGAN CLIP\n\nAuthor: @ErwannMillon \n\nThis is a very simple VQGAN-CLIP implementation that was ..."
          ],
          [
           "```\nfrom VQGAN_CLIP import VQGAN_CLIP\nvqgan_clip = VQGAN_CLIP()\nvqgan_clip.generate(\"a picture of a ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recent work has shown that either (1) increasing the..."
          ],
          [
           "## Usage tips\n\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`..."
          ],
          [
           "```python\n>>> import evaluate\n>>> from datasets import load_dataset\n>>> from transformers import Aut..."
          ],
          [
           "```\n\n\n## Resources\n\n- [Translation task guide](../tasks/translation)\n- [Summarization task guide](....."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [novice03](https://huggingface.co/novice03).\nThe original code can be ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note tha..."
          ],
          [
           "```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two f..."
          ],
          [
           "```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown abo..."
          ],
          [
           "```\n\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrain..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [matthijs](https://huggingface.co/Matthijs). The original code and wei..."
          ],
          [
           "Unsupported features:\n\n- The [`MobileNetV1Model`] outputs a globally pooled version of the last hidd..."
          ],
          [
           "## MobileNetV1Config\n\n[[autodoc]] MobileNetV1Config\n\n## MobileNetV1FeatureExtractor\n\n[[autodoc]] Mob..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "<frameworkcontent>\n<pt>\n\n## BloomModel\n\n[[autodoc]] BloomModel\n    - forward\n\n## BloomForCausalLM\n\n[..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Language model pre-training has been shown to captur..."
          ],
          [
           "## RealmRetriever\n\n[[autodoc]] RealmRetriever\n\n## RealmEmbedder\n\n[[autodoc]] RealmEmbedder\n    - for..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/\">English</..."
          ],
          [
           "<h3 align=\"center\">\n    <p>JAX, PyTorch à°®à°°à°¿à°¯à± TensorFlow à°•à±‹à°¸à°‚ à°…à°¤à±à°¯à°¾à°§à±à°¨à°¿à°• à°¯à°‚à°¤à±à°° à°…à°­à±à°¯à°¾à°¸à°‚</p>\n</h3>\n\n<h..."
          ],
          [
           "à°¸à°¹à°œ à°­à°¾à°·à°¾ à°ªà±à°°à°¾à°¸à±†à°¸à°¿à°‚à°—à±â€Œà°²à±‹:\n- [BERT à°¤à±‹ à°®à°¾à°¸à±à°•à±â€Œà°¡à± à°µà°°à±à°¡à± à°•à°‚à°ªà±à°²à±€à°·à°¨à±](https://huggingface.co/bert-base-unca..."
          ],
          [
           "- [RoBERTa à°¤à±‹ à°¸à°¹à°œ à°­à°¾à°·à°¾ à°…à°¨à±à°®à°¿à°¤à°¿](https://huggingface.co/roberta-large-mnli?text=The+dog+was+Lost.+Nob..."
          ],
          [
           "- [DistilBERT à°¤à±‹ à°ªà±à°°à°¶à±à°¨..."
          ],
          [
           "à°¸à°®à°¾à°§à°¾à°¨à°‚](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used..."
          ],
          [
           "lso+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese..."
          ],
          [
           "rtuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3..."
          ],
          [
           "mazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29..."
          ],
          [
           "nwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+th..."
          ],
          [
           "orest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+sq..."
          ],
          [
           "2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2..."
          ],
          [
           "s+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+..."
          ],
          [
           "longing+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+..."
          ],
          [
           "+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+i..."
          ],
          [
           "mounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departm..."
          ],
          [
           "+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+..."
          ],
          [
           "er+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tra..."
          ],
          [
           "erse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+div..."
          ],
          [
           "rees+divided+into+16%2C000+species)..."
          ],
          [
           "- [T5 à°¤à±‹ à°…à°¨à±à°µà°¾à°¦à°‚](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)..."
          ],
          [
           "à°•à°‚à°ªà±à°¯à±‚à°Ÿà°°à± à°¦à±ƒà°·à±à°Ÿà°¿à°²à±‹:\n- [VIT à°¤à±‹ à°šà°¿à°¤à±à°° à°µà°°à±à°—à±€à°•à°°à°£](https://huggingface.co/google/vit-base-patch16-224)\n- ..."
          ],
          [
           "à°®à°²à±à°Ÿà±€à°®à±‹à°¡à°²à± à°Ÿà°¾à°¸à±à°•à±â€Œà°²à°²à±‹:\n- [TAPAS à°¤à±‹ à°Ÿà±‡à°¬à±à°²à± à°ªà±à°°à°¶à±à°¨ à°¸à°®à°¾à°§à°¾à°¨à°¾à°²à±](https://huggingface.co/google/tapas-base..."
          ],
          [
           "```python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\n>>> cl..."
          ],
          [
           "```\n\nà°°à±†à°‚à°¡à°µ à°²à±ˆà°¨à± à°•à±‹à°¡à± à°¡à±Œà°¨à±â€Œà°²à±‹à°¡à± à°®à°°à°¿à°¯à± à°ªà±ˆà°ªà±â€Œà°²à±ˆà°¨à± à°‰à°ªà°¯à±‹à°—à°¿à°‚à°šà±‡ à°ªà±à°°à±€à°Ÿà±à°°à±ˆà°¨à±à°¡à± à°®à±‹à°¡à°²à±â€Œà°¨à± à°•à°¾à°·à± à°šà±‡à°¸à±à°¤à±à°‚à°¦à°¿, à°®à±‚à°¡à°µà°¦..."
          ],
          [
           "```\n\nà°‡à°•à±à°•à°¡ à°®à°¨à°‚ à°†à°¬à±à°œà±†à°•à±à°Ÿà± à°šà±à°Ÿà±à°Ÿà±‚ à°‰à°¨à±à°¨ à°¬à°¾à°•à±à°¸à± à°®à°°à°¿à°¯à± à°•à°¾à°¨à±à°«à°¿à°¡à±†à°¨à±à°¸à± à°¸à±à°•à±‹à°°à±â€Œà°¤à±‹ à°šà°¿à°¤à±à°°à°‚à°²à±‹ à°—à±à°°à±à°¤à°¿à°‚à°šà°¬à°¡à°¿à°¨ à°µà°¸à±à°¤à±..."
          ],
          [
           "```\n\nà°ªà±à°°à°¿à°Ÿà±à°°à±ˆà°¨à±à°¡à± à°®à±‹à°¡à°²à± à°†à°¶à°¿à°‚à°šà±‡ à°…à°¨à±à°¨à°¿ à°ªà±à°°à±€à°ªà±à°°à°¾à°¸à±†à°¸à°¿à°‚à°—à±â€Œà°²à°•à± à°Ÿà±‹à°•à±†à°¨à±ˆà°œà°°à± à°¬à°¾à°§à±à°¯à°¤ à°µà°¹à°¿à°¸à±à°¤à±à°‚à°¦à°¿ à°®à°°à°¿à°¯à± à°¨à±‡à°°à±à°—à°¾ à°’à°•..."
          ],
          [
           "## à°¨à±‡à°¨à± à°Ÿà±à°°à°¾à°¨à±à°¸à±â€Œà°«à°¾à°°à±à°®à°°à±â€Œà°²à°¨à± à°Žà°‚à°¦à±à°•à± à°‰à°ªà°¯à±‹à°—à°¿à°‚à°šà°•à±‚à°¡à°¦à±?\n\n- à°ˆ à°²à±ˆà°¬à±à°°à°°à±€ à°¨à±à°¯à±‚à°°à°²à± à°¨à±†à°Ÿà±â€Œà°² à°•à±‹à°¸à°‚ à°¬à°¿à°²à±à°¡à°¿à°‚à°—à± à°¬à±à°²à°¾à°•à±..."
          ],
          [
           "```\n\nà°®à±€à°°à± à°‰à°¦à°¾à°¹à°°à°£à°²à°¤à±‹ à°ªà±à°²à±‡ à°šà±‡à°¯à°¾à°²à°¨à±à°•à±à°‚à°Ÿà±‡ à°²à±‡à°¦à°¾ à°•à±‹à°¡à± à°¯à±Šà°•à±à°• à°¬à±à°²à±€à°¡à°¿à°‚à°—à± à°Žà°¡à±à°œà± à°…à°µà°¸à°°à°‚ à°®à°°à°¿à°¯à± à°•à±Šà°¤à±à°¤ à°µà°¿à°¡à±à°¦à°² à°•à±‹à°¸à°‚ ..."
          ],
          [
           "```\n\nFlax, PyTorch à°²à±‡à°¦à°¾ TensorFlow à°¯à±Šà°•à±à°• à°‡à°¨à±â€Œà°¸à±à°Ÿà°¾à°²à±‡à°·à°¨à± à°ªà±‡à°œà±€à°²à°¨à± à°•à±Šà°‚à°¡à°¾à°¤à±‹ à°Žà°²à°¾ à°‡à°¨à±â€Œà°¸à±à°Ÿà°¾à°²à± à°šà±‡à°¯à°¾à°²à±‹ à°šà±‚à°¡à°Ÿà°¾à°¨à°¿..."
          ],
          [
           "1. **[GIT](https://huggingface.co/docs/transformers/model_doc/git)** (from Microsoft Research) relea..."
          ],
          [
           "1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from EleutherAI) rel..."
          ],
          [
           "1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) release..."
          ],
          [
           "1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) ..."
          ],
          [
           "1. **[SeamlessM4T](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t)** (from Met..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Faceb..."
          ],
          [
           "à°ªà±à°°à°¤à°¿ à°®à±‹à°¡à°²à± à°«à±à°²à°¾à°•à±à°¸à±, à°ªà±ˆà°Ÿà°¾à°°à±à°šà± à°²à±‡à°¦à°¾ à°Ÿà±†à°¨à±à°¸à°°à±â€Œà°«à±à°²à±‹à°²à±‹ à°…à°®à°²à± à°šà±‡à°¯à°¬à°¡à°¿à°‚à°¦à°¾ à°²à±‡à°¦à°¾ ðŸ¤— Tokenizers à°²à±ˆà°¬à±à°°à°°à±€ à°¦à±à°µà°¾à°°à°¾ à°…..."
          ],
          [
           "## à°…à°¨à±à°²à±‡à°–à°¨à°‚\n\nðŸ¤— à°Ÿà±à°°à°¾à°¨à±à°¸à±â€Œà°«à°¾à°°à±à°®à°°à±à°¸à± à°²à±ˆà°¬à±à°°à°°à±€ à°•à±‹à°¸à°‚ à°®à±€à°°à± à°‰à°¦à°¹à°°à°¿à°‚à°šà°—à°² [à°ªà±‡à°ªà°°à±](https://www.aclweb.org/antholo..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Most widely-used pre-trained language models operate..."
          ],
          [
           "## Usage example\n\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n\n```python\n>..."
          ],
          [
           "```\n\nFor batched inference and training it is however recommended to make use of the tokenizer:\n\n```..."
          ],
          [
           "```\n\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \nsince the mode..."
          ],
          [
           ">>> input_ids = torch.tensor([input_ids[:8] + [258] + input_ids[14:21] + [257] + input_ids[28:]])\n>>..."
          ],
          [
           ">>> # ^- Note how 258 descends to 257, 256, 255\n\n>>> # Now we need to split on the sentinel tokens, ..."
          ],
          [
           "```\n\n\n## ByT5Tokenizer\n\n[[autodoc]] ByT5Tokenizer\n\nSee [`ByT5Tokenizer`] for all details...."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage example\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n..."
          ],
          [
           "```\n\n## Usage tips\n\n- Following mBART, BARTpho uses the \"large\" architecture of BART with an additio..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "Dictionary\n\nAPI: API (ä¸ç¿»è­¯ï¼‰\nadd: åŠ å…¥\ncheckpoint: æª¢æŸ¥é»ž\ncode: ç¨‹å¼ç¢¼\ncommunity: ç¤¾ç¾¤\nconfidence: ä¿¡è³´åº¦\ndataset: ..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/\">English</..."
          ],
          [
           "ðŸ¤— Transformers æä¾›äº†æ•¸ä»¥åƒè¨ˆçš„é è¨“ç·´æ¨¡åž‹ï¼Œæ”¯æ´ 100 å¤šç¨®èªžè¨€çš„æ–‡æœ¬åˆ†é¡žã€è³‡è¨Šæ“·å–ã€å•ç­”ã€æ‘˜è¦ã€ç¿»è­¯ã€æ–‡æœ¬ç”Ÿæˆã€‚å®ƒçš„å®—æ—¨æ˜¯è®“æœ€å…ˆé€²çš„ NLP æŠ€è¡“äººäººæ˜“ç”¨ã€‚\n\nðŸ¤— Transform..."
          ],
          [
           "é€™è£¡æ˜¯ä¸€äº›ç¯„ä¾‹ï¼š\n- [ç”¨ BERT åšé®è“‹å¡«è©ž](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F..."
          ],
          [
           "- [ç”¨ RoBERTa åšè‡ªç„¶èªžè¨€æŽ¨è«–](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+a..."
          ],
          [
           "åšå•ç­”](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+..."
          ],
          [
           "- [ç”¨ T5 åšç¿»è­¯](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)..."
          ],
          [
           "**[Write With Transformer](https://transformer.huggingface.co)**ï¼Œç”± Hugging Face åœ˜éšŠæ‰€æ‰“é€ ï¼Œæ˜¯ä¸€å€‹æ–‡æœ¬ç”Ÿæˆçš„å®˜æ–¹ dem..."
          ],
          [
           "```\n\nç¬¬äºŒè¡Œç¨‹å¼ç¢¼ä¸‹è¼‰ä¸¦å¿«å– pipeline ä½¿ç”¨çš„é è¨“ç·´æ¨¡åž‹ï¼Œè€Œç¬¬ä¸‰è¡Œç¨‹å¼ç¢¼å‰‡åœ¨çµ¦å®šçš„æ–‡æœ¬ä¸Šé€²è¡Œäº†è©•ä¼°ã€‚é€™è£¡çš„ç­”æ¡ˆâ€œæ­£é¢â€ (positive) å…·æœ‰ 99.97% çš„ä¿¡è³´åº¦ã€‚\n\nè¨±å¤šçš„ NL..."
          ],
          [
           "```\né€™è£¡æ˜¯å°æ‡‰çš„ TensorFlow ç¨‹å¼ç¢¼ï¼š\n```python\n>>> from transformers import AutoTokenizer, TFAutoModel\n\n>>> to..."
          ],
          [
           "```\n\nTokenizer ç‚ºæ‰€æœ‰çš„é è¨“ç·´æ¨¡åž‹æä¾›äº†é è™•ç†ï¼Œä¸¦å¯ä»¥ç›´æŽ¥è½‰æ›å–®ä¸€å­—ä¸²ï¼ˆæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼‰æˆ–ä¸²åˆ— (list)ã€‚å®ƒæœƒè¼¸å‡ºä¸€å€‹çš„å­—å…¸ (dict) è®“ä½ å¯ä»¥åœ¨ä¸‹æ¸¸ç¨‹å¼ç¢¼è£¡ä½¿ç”¨æˆ–ç›´æŽ¥è—‰ç”± `*..."
          ],
          [
           "1. å°æ–¼æ¨¡åž‹ç”Ÿå‘½é€±æœŸçš„æ¯ä¸€å€‹éƒ¨åˆ†éƒ½é¢é¢ä¿±åˆ°ï¼š\n    - è¨“ç·´å…ˆé€²çš„æ¨¡åž‹ï¼Œåªéœ€ 3 è¡Œç¨‹å¼ç¢¼\n    - æ¨¡åž‹å¯ä»¥åœ¨ä¸åŒæ·±åº¦å­¸ç¿’æ¡†æž¶ä¹‹é–“ä»»æ„è½‰æ›\n    - ç‚ºè¨“ç·´ã€è©•ä¼°å’Œç”Ÿç”¢é¸æ“‡æœ€é©åˆçš„æ¡†æž¶ï¼Œä¸¦å®Œ..."
          ],
          [
           "é€™å€‹ Repository å·²åœ¨ Python 3.8+ã€Flax 0.4.1+ã€PyTorch 1.10+ å’Œ TensorFlow 2.6+ ä¸‹ç¶“éŽæ¸¬è©¦ã€‚\n\nä½ å¯ä»¥åœ¨[è™›æ“¬ç’°å¢ƒ](https://..."
          ],
          [
           "```\n\nå¦‚æžœä½ æƒ³è¦è©¦è©¦ç¯„ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼ç™¼å¸ƒå‰ä½¿ç”¨æœ€æ–°é–‹ç™¼ä¸­çš„ç¨‹å¼ç¢¼ï¼Œä½ å¿…é ˆ[å¾žåŽŸå§‹ç¢¼å®‰è£](https://huggingface.co/docs/transformers/installation..."
          ],
          [
           "```\n\nè¦è—‰ç”± conda å®‰è£ Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè«‹åƒé–±å®ƒå€‘å„è‡ªå®‰è£é é¢çš„èªªæ˜Žã€‚\n\n## æ¨¡åž‹æž¶æ§‹\n\n**ðŸ¤— Transformers æ”¯æ´çš„[æ‰€æœ‰çš„æ¨¡..."
          ],
          [
           "1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (from Meta AI) released w..."
          ],
          [
           "1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) relea..."
          ],
          [
           "1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with ..."
          ],
          [
           "1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) r..."
          ],
          [
           "1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (from Meta and UI..."
          ],
          [
           "1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) releas..."
          ],
          [
           "1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released ..."
          ],
          [
           "1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research..."
          ],
          [
           "1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)..."
          ],
          [
           "1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)..."
          ],
          [
           "1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with th..."
          ],
          [
           "1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft) released w..."
          ],
          [
           "1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (from Peking University..."
          ],
          [
           "1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released togeth..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Faceb..."
          ],
          [
           "è¦æª¢æŸ¥æŸå€‹æ¨¡åž‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å¯¦ä½œï¼Œæˆ–å…¶æ˜¯å¦åœ¨ðŸ¤— Tokenizers å‡½å¼åº«ä¸­æœ‰å°æ‡‰çš„ tokenizerï¼Œæ•¬è«‹åƒé–±[æ­¤è¡¨](https://hugg..."
          ],
          [
           "## å¼•ç”¨\n\næˆ‘å€‘å·²å°‡æ­¤å‡½å¼åº«çš„[è«–æ–‡](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)æ­£å¼ç™¼è¡¨ã€‚å¦‚æžœä½ ä½¿ç”¨äº† ðŸ¤— Transformers..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pre-training techniques have been verified successfu..."
          ],
          [
           "```python\ndef normalize_bbox(bbox, width, height):\n    return [\n        int(1000 * (bbox[0] / width)..."
          ],
          [
           "```\n\nHere, `width` and `height` correspond to the width and height of the original document in which..."
          ],
          [
           "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help ..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\" />\n\n- A notebook on how to [ fine-tune LayoutLM for tok..."
          ],
          [
           "## TFLayoutLMModel\n\n[[autodoc]] TFLayoutLMModel\n\n## TFLayoutLMForMaskedLM\n\n[[autodoc]] TFLayoutLMFor..."
          ],
          [
           "Wav2Vec2 Contrastive Loss PreTraining examples\n\nThe following example showcases how to pretrain a wa..."
          ],
          [
           "```\n\nTo ensure that all tensorboard traces will be uploaded correctly, we need to \ntrack them. You c..."
          ],
          [
           "```\n\n### Create the model configuration\n\nLet's first create the model configuration and store it in ..."
          ],
          [
           "```\n\n### Create a feature extractor configuration\n\nBefore we can start the training, we need to defi..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pre-training video transformers on extra large-scale..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original code can be found [here](https://github.com/microsoft/DialoGPT).\n\n## Usage tips\n\n- Dial..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Humans read and write hundreds of billions of messag..."
          ],
          [
           "## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classifi..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [s-JoL](https://huggingface.co/s-JoL).\nThe original code was released ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "### Agent\n\n[[autodoc]] Agent\n    - chat\n    - run\n    - prepare_for_new_chat\n\n## Tools\n\n### load_too..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This guide demonstrates how you can distill a [fine-tuned ViT model](https://huggingface.co/merve/vi..."
          ],
          [
           "```\n\nIn this example, we are using the `merve/beans-vit-224` model as teacher model. It's an image c..."
          ],
          [
           "```\n\nEssentially, we want the student model (a randomly initialized MobileNet) to mimic the teacher ..."
          ],
          [
           "with torch.no_grad():\n          teacher_output = self.teacher(**inputs)\n\n        # Compute soft targ..."
          ],
          [
           "```\n\nWe will now login to Hugging Face Hub so we can push our model to the Hugging Face Hub through ..."
          ],
          [
           "```\n\nWe can use `compute_metrics` function to evaluate our model on the test set. This function will..."
          ],
          [
           "```\n\nWe can evaluate the model on the test set.\n\n```python\ntrainer.evaluate(processed_datasets[\"test..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "## Connectionist Temporal Classification\n\nThe script [`run_speech_recognition_ctc.py`](https://githu..."
          ],
          [
           "```bash\nOMP_NUM_THREADS=1 python run_speech_recognition_ctc ......"
          ],
          [
           "```\n\nIf the environment variable is not set, the training script might freeze, *i.e.* see: https://g..."
          ],
          [
           "```\n\nOn a single V100 GPU, this script should run in *ca.* 1 hour 20 minutes and yield a CTC loss of..."
          ],
          [
           "```\n\nOn 8 V100 GPUs, this script should run in *ca.* 18 minutes and yield a CTC loss of **0.39** and..."
          ],
          [
           "```bash\n**torchrun \\\n\t--nproc_per_node 4 run_speech_recognition_ctc_streaming.py \\\n\t--dataset_name=\"..."
          ],
          [
           "```\n\nOn 4 V100 GPUs, this script should run in *ca.* 3h 31min and yield a CTC loss of **0.35** and w..."
          ],
          [
           "| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval..."
          ],
          [
           "| [TIMIT](https://huggingface.co/datasets/timit_asr)| -  | [wav2vec2-base](https://huggingface.co/fa..."
          ],
          [
           "| [TIMIT](https://huggingface.co/datasets/timit_asr)| -  | [ntu-spml/distilhubert](https://huggingfa..."
          ],
          [
           "#### Librispeech CTC\n\n- [Librispeech](https://huggingface.co/datasets/librispeech_asr)..."
          ],
          [
           "| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval..."
          ],
          [
           "| [Librispeech](https://huggingface.co/datasets/librispeech_asr)| `\"clean\"` - `\"train.100\"` |  [micr..."
          ],
          [
           "| [Librispeech](https://huggingface.co/datasets/librispeech_asr)| `\"clean\"` - `\"train.100\"` |  [face..."
          ],
          [
           "| [Librispeech](https://huggingface.co/datasets/librispeech_asr)| `\"clean\"` - `\"train.100\"` |  [asap..."
          ],
          [
           "#### Common Voice CTC\n\n- [Common Voice](https://huggingface.co/datasets/common_voice)..."
          ],
          [
           "| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval..."
          ],
          [
           "| [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_3_0)| `\"it\"`  | [fa..."
          ],
          [
           "| [Common Voice](https://huggingface.co/datasets/common_voice)| `\"tr\"`  | [facebook/wav2vec2-large-x..."
          ],
          [
           "| [Common Voice](https://huggingface.co/datasets/common_voice)| `\"tr\"`  | [facebook/wav2vec2-large-x..."
          ],
          [
           "| [Common Voice](https://huggingface.co/datasets/common_voice)| `\"tr\"`  | [facebook/wav2vec2-xls-r-1..."
          ],
          [
           "#### Multilingual Librispeech CTC\n\n- [Multilingual Librispeech](https://huggingface.co/datasets/mult..."
          ],
          [
           "| Dataset | Dataset Config | Pretrained Model | Word error rate on eval | Phoneme error rate on eval..."
          ],
          [
           "| [Multilingual Librispeech](https://huggingface.co/datasets/multilingual_librispeech)| `\"german\"`  ..."
          ],
          [
           "## Connectionist Temporal Classification With Adapters\n\nThe script [`run_speech_recognition_ctc_adap..."
          ],
          [
           "#### Common Voice CTC Adapter\n\nAs in the examples [above](#examples-ctc), we fine-tune on Common Voi..."
          ],
          [
           "```\nhuggingface-cli login\n```\n\nNow, let's run an example and upload it to the Hub under `wav2vec2-co..."
          ],
          [
           "```\n\nThis should take less than 10 minutes on most GPUs and you should very quickly get word error r..."
          ],
          [
           "```sh\npython run_speech_recognition_ctc.py \\\n\t--dataset_name=\"common_voice\" \\\n\t--model_name_or_path=..."
          ],
          [
           "```\n\nNow you should have both `adapter.tur.safetensors` and `adapter.swe.safetensors` in the model r..."
          ],
          [
           "```\nrespectively.\n\n## Sequence to Sequence\n\nThe script [`run_speech_recognition_seq2seq.py`](https:/..."
          ],
          [
           "#### Single GPU Whisper Training\nThe following example shows how to fine-tune the [Whisper small](ht..."
          ],
          [
           "```\nOn a single V100, training should take approximately 8 hours, with a final cross-entropy loss of..."
          ],
          [
           "#### Multi GPU Whisper Training\nThe following example shows how to fine-tune the [Whisper small](htt..."
          ],
          [
           "```\nOn two V100s, training should take approximately 4 hours, with a final cross-entropy loss of **1..."
          ],
          [
           "```bash\nhuggingface-cli repo create wav2vec2-2-bart-base\ngit clone https://huggingface.co/<your-user..."
          ],
          [
           "```\n\nNext, run the following script **inside** the just cloned repo:\n\n```python\nfrom transformers im..."
          ],
          [
           "```\n\nNote that we have added a randomly initialized _adapter layer_ to `wav2vec2-base` with the argu..."
          ],
          [
           "In the script [`run_speech_recognition_seq2seq`], we load the warm-started model, \nfeature extractor..."
          ],
          [
           "```\n\nIf the environment variable is not set, the training script might freeze, *i.e.* see: https://g..."
          ],
          [
           "```bash\npython run_speech_recognition_seq2seq.py \\\n\t--dataset_name=\"librispeech_asr\" \\\n\t--model_name..."
          ],
          [
           "```\n\nOn a single V100 GPU, this script should run in *ca.* 5 hours and yield a \ncross-entropy loss o..."
          ],
          [
           "```\n\nOn 8 V100 GPUs, this script should run in *ca.* 45 minutes and yield a cross-entropy loss of **..."
          ],
          [
           "| Dataset                                                        | Dataset Config            | Pretr..."
          ],
          [
           "|----------------------------------------------------------------|---------------------------|------..."
          ],
          [
           "-|--------------------------------------------------------------------------------------------------..."
          ],
          [
           "-----------------------------------------------------------------|-------------------------|--------..."
          ],
          [
           "----------------------------|------------|---------------|------------------------------------------..."
          ],
          [
           "-------------------------------------|--------------------------------------------------------------..."
          ],
          [
           "-----------------------------------------------------------------|..."
          ],
          [
           "| [Librispeech](https://huggingface.co/datasets/librispeech_asr) | `\"clean\"` - `\"train.100\"` | [face..."
          ],
          [
           "| [Librispeech](https://huggingface.co/datasets/librispeech_asr) | `\"clean\"` - `\"train.100\"` | [face..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "MobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-atten..."
          ],
          [
           "## Usage tips\n\n- MobileViTV2 is more like a CNN than a Transformer model. It does not work on sequen..."
          ],
          [
           "LXMERT DEMO\n\n1. make a virtualenv: ``virtualenv venv`` and activate ``source venv/bin/activate``\n2. ..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\n### Train tokenizer\n\nIn the first step, we train a tokenizer to efficiently process the text in..."
          ],
          [
           "```\n\nGreat, we have set up our model repository. During training, we will automatically\npush the tra..."
          ],
          [
           "```\n\nTraining should converge at a loss and accuracy \nof 1.78 and 0.64 respectively after 18 epochs ..."
          ],
          [
           "```\n\n### Train tokenizer\n\nIn the first step, we train a tokenizer to efficiently process the text in..."
          ],
          [
           "```\n\n### Create configuration\n\nNext, we create the model's configuration file. This is as simple \nas..."
          ],
          [
           "```\n\nTraining should converge at a loss and perplexity \nof 3.24 and 25.72 respectively after 20 epoc..."
          ],
          [
           "```\n\n### Train tokenizer\n\nIn the first step, we train a tokenizer to efficiently process the text in..."
          ],
          [
           "# Train tokenizer\ntokenizer.train_from_iterator(\n    iterator=batch_iterator(input_sentence_size=inp..."
          ],
          [
           "```\n\n### Create configuration\n\nNext, we create the model's configuration file. This is as simple \nas..."
          ],
          [
           "```\n\nTraining should converge at a loss and accuracy \nof 2.36 and 57.0 respectively after 3 epochs o..."
          ],
          [
           "```\n\n### Train tokenizer\nIn the first step, we train a tokenizer to efficiently process the text inp..."
          ],
          [
           "```\n\nGreat, we have set up our model repository. During training, we will automatically\npush the tra..."
          ],
          [
           "```\n\nTraining should converge at a loss and accuracy \nof 1.36 and 0.77 respectively after 3 epochs o..."
          ],
          [
           "### Script to run MLM with PyTorch/XLA on TPUv3-8\n\nFor comparison one can run the same pre-training ..."
          ],
          [
           "```\n\n, set the following environment variables:\n\n```bash\nexport XRT_TPU_CONFIG=\"localservice;0;local..."
          ],
          [
           "```\n\n### Script to compare pre-training with PyTorch on 8 GPU V100's\n\nFor comparison you can run the..."
          ],
          [
           "```\n\n, and can start training as follows:\n\n```bash\npython3 -m torch.distributed.launch --nproc_per_n..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Large pre-trained language models have been shown to..."
          ],
          [
           "## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outp..."
          ],
          [
           "Robust Speech Challenge ðŸ¤—\n\nWelcome to the robust speech recognition challenge ðŸŽ™ï¸ !\n\nThe goal of this..."
          ],
          [
           "## TLDR\n\nParticipants are encouraged to leverage pre-trained speech recognition checkpoints,\nprefera..."
          ],
          [
           "For training, it is recommended to use the [official training script](https://github.com/huggingface..."
          ],
          [
           "During the event, the speech recognition system will be evaluated on both the Common Voice `\"test\"` ..."
          ],
          [
           "## Important dates\n\n![timeline](https://github.com/patrickvonplaten/scientific_images/raw/master/Rob..."
          ],
          [
           "```\n\nincludes more or less the same data as\n\n```python\nload_dataset(\"mozilla-foundation/common_voice..."
          ],
          [
           "```\n\nHowever, we strongly encourage participants to make use of Common Voice's other splits, *e.g.* ..."
          ],
          [
           "Next, let's talk about preprocessing. Audio data and transcriptions have to be brought into the corr..."
          ],
          [
           "It is allowed (and recommended) to normalize the data to only have lower-case characters. It is also..."
          ],
          [
           "Since those choices are not always obvious when in doubt feel free to ask on Discord or even better ..."
          ],
          [
           "```\n\nYou can activate your venv by running\n\n```bash\nsource ~/<your-venv-name>/bin/activate\n```\n\nTo b..."
          ],
          [
           "```\n\n4. Set up a PyTorch environment by running the following command your virtual environment:\n\n   ..."
          ],
          [
           "```\n$ cd ~/\n$ git clone https://github.com/huggingface/datasets.git\n$ cd datasets\n$ pip install -e \"..."
          ],
          [
           "```\n\n## How to finetune an acoustic model\n\nIn this section, we show you how to fine-tune a pre-train..."
          ],
          [
           "The blog can also be opened and directly fine-tuned in a google colab notebook.\nIn this section, we ..."
          ],
          [
           "```\n\nto login. It is recommended to login with your access token that can be found under your huggin..."
          ],
          [
           "```\n\n3. **Add your training script and `run`-command to the repository**\n\nWe encourage participants ..."
          ],
          [
           "```\n\nAlright, finally we can define the training script. We'll simply use some \ndummy hyper-paramete..."
          ],
          [
           "```\n\n4. **Start training**\n\nNow all that is left to do is to start training the model by executing t..."
          ],
          [
           "```\n\n, clone it locally (assuming the `<username>` is `hf-test`)\n\n```bash\ngit clone hf-test/xls-r-30..."
          ],
          [
           "```\n\n, and, define the following hyperparameters for training\n\n```bash\necho '''python run_speech_rec..."
          ],
          [
           "```\n\nThe training takes *ca.* 7 hours and yields a reasonable test word \nerror rate of 27% as can be..."
          ],
          [
           "### Setting up an AI notebook\n1. Go to the `Public Cloud` page and select `Project Management` -> `U..."
          ],
          [
           "For more quick tutorials about OVHcloud AI products, check out the showcase https://vimeo.com/showca..."
          ],
          [
           "## Evaluation\n\nFinally, we have arrived at the most fun part of the challenge - sitting back and\nwat..."
          ],
          [
           "```\n\nNext, we should adapt `eval.py` so that it fits our evaluation data. Here it is \nimportant to k..."
          ],
          [
           "- 1. The following input arguments should not be changed and keep their original functionality/meani..."
          ],
          [
           "- a. Somehow giving the model access to the target transcriptions to improve performance. The model ..."
          ],
          [
           "Uff, that was a lot of text describing how to make sure your `eval.py` script \nis in the correct for..."
          ],
          [
           "```\n\nTo log each of the model's predictions with the target transcriptions, you can just \nadd the `-..."
          ],
          [
           "```\n- \"sv\"\n- \"robust-speech-event\"\n```\n\nunder `tags:` as done [here](https://huggingface.co/hf-test/..."
          ],
          [
           "```\n\nThe dataset `WER_REAL_AUDIO_TEST` is hidden and will only be published \nat the end of the robus..."
          ],
          [
           "The following table summarizes what platform to use for which problem.\n\n- Problem/question/bug with ..."
          ],
          [
           "We are very excited to be hosting 2 days of talks from Kensho-Technologies, Mozilla's Common Voice, ..."
          ],
          [
           "### Friday, January 21th\n\n Speaker        | Topic                           | Time                  ..."
          ],
          [
           "#### Raymond Grossman, Jeremy Lopez, Machine Learning Engineer, Kensho Technologies\n- Talk: PyCTCDec..."
          ],
          [
           "#### Changhan Wang, Main author of XLS-R and Research Engineer, Meta AI Research\n- Talk: XLS-R: Larg..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [AI Sweden](https://huggingface.co/AI-Sweden).\n\n## Usage example\n\n```p..."
          ],
          [
           "```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token cla..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Ve..."
          ],
          [
           "Testing mixed int8 quantization\n\n![HFxbitsandbytes.png](https://cdn-uploads.huggingface.co/productio..."
          ],
          [
           "```\nFor the latest pytorch instructions please see [this](https://pytorch.org/get-started/locally/)\n..."
          ],
          [
           "```\nls -l $CONDA_PREFIX/lib/libcudart.so\n```\nor \n```\nls -l $LD_LIBRARY_PATH\n```\nCheck if `libcudart...."
          ],
          [
           "```\n\nOn each path (`$path`) separated by `:`.\nIf not, simply run\n```bash\nls -l $LD_LIBRARY_PATH/libc..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Driven by improved architectures and better represen..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pipelined NLP systems have largely been superseded b..."
          ],
          [
           "## Usage tips\n\n- CANINE uses no less than 3 Transformer encoders internally: 2 \"shallow\" encoders (w..."
          ],
          [
           ">>> model = CanineModel.from_pretrained(\"google/canine-c\")  # model pre-trained with autoregressive ..."
          ],
          [
           "```\n\nFor batched inference and training, it is however recommended to make use of the tokenizer (to ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Image segmentation groups pixels with different sema..."
          ],
          [
           "## Usage tips\n\n- Mask2Former uses the same preprocessing and postprocessing steps as [MaskFormer](ma..."
          ],
          [
           "## Mask2FormerModel\n\n[[autodoc]] Mask2FormerModel\n    - forward\n\n## Mask2FormerForUniversalSegmentat..."
          ],
          [
           "Plug and Play Language Models: a Simple Approach to Controlled Text Generation\n\nAuthors: [Sumanth Da..."
          ],
          [
           "```\n\n### Tuning hyperparameters for bag-of-words control\n\n1. Increase `--stepsize` to intensify topi..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "### GPT-2/GPT and causal language modeling\n\nThe following example fine-tunes GPT-2 on WikiText-2. We..."
          ],
          [
           "```\n\nThis takes about half an hour to train on a single K80 GPU and about one minute for the evaluat..."
          ],
          [
           "```\n\n### RoBERTa/BERT/DistilBERT and masked language modeling\n\nThe following example fine-tunes RoBE..."
          ],
          [
           "```\n\nIf your dataset is organized with one sample per line, you can use the `--line_by_line` flag (o..."
          ],
          [
           "```\n\n**Note:** On TPU, you should use the flag `--pad_to_max_length` in conjunction with the `--line..."
          ],
          [
           "```\n\nIf your dataset is organized with one sample per line, you can use the `--line_by_line` flag (o..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Reinforcement learning (RL) is typically concerned w..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nFor example:\n\n```bash\ndoc-builder preview transformers docs/source/en/\n```\n\nThe docs will be vi..."
          ],
          [
           "```\n\nUse the relative style to link to the new file so that the versioned docs continue to work.\n\nFo..."
          ],
          [
           "### Adding a new model\n\nWhen adding a new model:\n\n- Create a file `xxx.md` or under `./source/model_..."
          ],
          [
           "```\n## XXXConfig\n\n[[autodoc]] XXXConfig\n```\n\nThis will include every public method of the configurat..."
          ],
          [
           "```\n## XXXTokenizer\n\n[[autodoc]] XXXTokenizer\n    - all\n    - __call__\n```\n\n### Writing source docum..."
          ],
          [
           "```\n\nIf the description is too long to fit in one line, another indentation is necessary before writ..."
          ],
          [
           "```\n```\n# first line of code\n# second line\n# etc\n```\n````\n\nWe follow the [doctest](https://docs.pyth..."
          ],
          [
           "```\n\n#### Adding an image\n\nDue to the rapidly growing repository, it is important to make sure that ..."
          ],
          [
           "```\n    Example:\n\n    ```python\n    >>> from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n ..."
          ],
          [
           "```\n```\n\nThe docstring should give a minimal, clear example of how the respective model \nis to be us..."
          ],
          [
           "```\n\n### For Markdown files\n\nYou can test locally a given file with this command (here testing the q..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Those processors inherit from the following base class that implements the saving and loading functi..."
          ],
          [
           "Those processors are:\n\n- [`~data.processors.utils.MrpcProcessor`]\n- [`~data.processors.utils.MnliPro..."
          ],
          [
           "- [`~data.processors.utils.XnliProcessor`]\n\nPlease note that since the gold labels are available on ..."
          ],
          [
           "[[autodoc]] data.processors.squad.squad_convert_examples_to_features\n\n\nThese processors as well as t..."
          ],
          [
           "```\n\nUsing *tensorflow_datasets* is as easy as using a data file:\n\n```python\n# tensorflow_datasets o..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "`BrosForTokenClassification` has a simple linear layer on top of BrosModel. It predicts the label of..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Key information extraction (KIE) from document image..."
          ],
          [
           "```python\ndef expand_and_normalize_bbox(bboxes, doc_width, doc_height):\n    # here, bboxes are numpy..."
          ],
          [
           "```\n\n- [`~transformers.BrosForTokenClassification.forward`, `~transformers.BrosSpadeEEForTokenClassi..."
          ],
          [
           "```\n\n## Resources\n\n- Demo scripts can be found [here](https://github.com/clovaai/bros).\n\n## BrosConf..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*While large pretrained Transformer models have prove..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage example\n\n```py\nimport torch\nfrom transformers import AutoTokenizer, RwkvConfig, RwkvModel\n\n..."
          ],
          [
           "```\n\nIf you want to make sure the model stops generating when `'\\n\\n'` is detected, we recommend usi..."
          ],
          [
           "```\n\n## RwkvConfig\n\n[[autodoc]] RwkvConfig\n\n## RwkvModel\n\n[[autodoc]] RwkvModel\n    - forward\n\n## Rw..."
          ],
          [
           "In comparison, the RWKV attention is given by\n\n$$O_{i} = \\sigma(R_{i}) \\frac{\\sum_{j=1}^{i} e^{W_{i-..."
          ],
          [
           "so \\\\(\\hat{N}_{i}\\\\) (called `numerator_state` in the code) satistfies\n\n$$\\hat{N}_{0} = 0 \\hbox{  an..."
          ],
          [
           "with \\\\(M\\\\) the maximum of all \\\\(x_{j}\\\\). So here on top of saving the numerator state (\\\\(\\hat{N..."
          ],
          [
           "and\n\n$$D_{i} = e^{u + K_{i} - q} + e^{M_{i}} \\tilde{D}_{i} \\hbox{  where  } q = \\max(u + K_{i}, M_{i..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Universal Image Segmentation is not a new concept. P..."
          ],
          [
           "This model was contributed by [Jitesh Jain](https://huggingface.co/praeclarumjj3). The original code..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\n## With Accelerate\n\nBased on the script [run_swag_no_trainer.py](https://github.com/huggingface..."
          ],
          [
           "```\n\nand reply to the questions asked. Then\n\n```bash\naccelerate test\n```\n\nthat will check everything..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```python\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"..."
          ],
          [
           "```\n\nNotice how the entire chat is condensed into a single string. If we use `tokenize=True`, which ..."
          ],
          [
           "```\n\nNote that this time, the tokenizer has added the control tokens [INST] and [/INST] to indicate ..."
          ],
          [
           "```\n\nNow that our input is formatted correctly for Zephyr, we can use the model to generate a respon..."
          ],
          [
           "```\n\n```text\nConversation id: 76d886a0-74bd-454e-9804-0467041a63dc\nsystem: You are a friendly chatbo..."
          ],
          [
           "```\n\nAnd here's what it looks like **with** a generation prompt:\n\n```python\ntokenizer.apply_chat_tem..."
          ],
          [
           "```\n\nNote that this time, we've added the tokens that indicate the start of a bot response. This ens..."
          ],
          [
           "```\nAnd we get:\n```text\n<|user|>\nWhich is bigger, the moon or the sun?</s>\n<|assistant|>\nThe sun.</s..."
          ],
          [
           "```\n\nIf you've never seen one of these before, this is a [Jinja template](https://jinja.palletsproje..."
          ],
          [
           "```\n{% for message in messages %}\n    {% if message['role'] == 'user' %}\n        {{ bos_token + '[IN..."
          ],
          [
           "```\n{% for message in messages %}\n    {% if message['role'] == 'user' %}\n        {{ bos_token + '[IN..."
          ],
          [
           "```\n\nThe method [`~PreTrainedTokenizer.apply_chat_template`] which uses your chat template is called..."
          ],
          [
           "If you're training a model from scratch, or fine-tuning a base language model for chat, on the other..."
          ],
          [
           "```\n{% for message in messages %}\n    {{'<|im_start|>' + message['role'] + '\\n' + message['content']..."
          ],
          [
           "```\n\nThe \"user\", \"system\" and \"assistant\" roles are the standard for chat, and we recommend using th..."
          ],
          [
           "If you're unfamiliar with Jinja, we generally find that the easiest way to write a chat template is ..."
          ],
          [
           "```\n{% for message in messages %}\n{{ message['content'] }}\n{% endfor %}\n```\n\nNote that whatever's in..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/transformers/\">English</..."
          ],
          [
           "ðŸ¤—Transformersã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€è¦–è¦šã€éŸ³å£°ãªã©ã®ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«å¯¾ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€äº‹å‰ã«å­¦ç¿’ã•ã›ãŸæ•°åƒã®ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚\n\nã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯æ¬¡ã®ã‚ˆã†ãªå ´åˆã«é©ç”¨ã§ãã¾ã™:\n\n..."
          ],
          [
           "ðŸ¤—Transformersã¯[Jax](https://jax.readthedocs.io/en/latest/)ã€[PyTorch](https://pytorch.org/)ã€[TensorFl..."
          ],
          [
           "è‡ªç„¶è¨€èªžå‡¦ç†ã«ã¦:\n- [BERTã«ã‚ˆã‚‹ãƒžã‚¹ã‚¯ãƒ‰ãƒ¯ãƒ¼ãƒ‰è£œå®Œ](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D..."
          ],
          [
           "- [RoBERTaã«ã‚ˆã‚‹è‡ªç„¶è¨€èªžæŽ¨è«–](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+an..."
          ],
          [
           "-..."
          ],
          [
           "[DistilBERTã«ã‚ˆã‚‹è³ªå•å¿œç­”](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+i..."
          ],
          [
           "h+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%2..."
          ],
          [
           "orest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2..."
          ],
          [
           "B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoner..."
          ],
          [
           "Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadlea..."
          ],
          [
           "broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C0..."
          ],
          [
           "es+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilome..."
          ],
          [
           "e+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory..."
          ],
          [
           "erritory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60..."
          ],
          [
           "+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+mino..."
          ],
          [
           "ith+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States..."
          ],
          [
           ".+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents..."
          ],
          [
           "presents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+bio..."
          ],
          [
           "most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individua..."
          ],
          [
           "ndividual+trees+divided+into+16%2C000+species)..."
          ],
          [
           "- [T5ã«ã‚ˆã‚‹ç¿»è¨³](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)..."
          ],
          [
           "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã«ã¦:\n- [ViTã«ã‚ˆã‚‹ç”»åƒåˆ†é¡ž](https://huggingface.co/google/vit-base-patch16-224)\n- [DETRã«ã‚ˆã‚‹ç‰©ä½“æ¤œå‡º](htt..."
          ],
          [
           "## Hugging Faceãƒãƒ¼ãƒ ã«ã‚ˆã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ»ã‚µãƒãƒ¼ãƒˆã‚’ã”å¸Œæœ›ã®å ´åˆ\n\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\n   ..."
          ],
          [
           "```\n\n2è¡Œç›®ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€pipelineã§ä½¿ç”¨ã•ã‚Œã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€3è¡Œç›®ã§ã¯ä¸Žãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ãã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€ç­”ãˆã¯99.97%ã®ä¿¡..."
          ],
          [
           "```\n\nã“ã“ã§ã¯ã€ç”»åƒã‹ã‚‰æ¤œå‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆãŒå¾—ã‚‰ã‚Œã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å›²ã‚€ãƒœãƒƒã‚¯ã‚¹ã¨ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚å·¦å´ãŒå…ƒç”»åƒã€å³å´ãŒäºˆæ¸¬çµæžœã‚’è¡¨ç¤ºã—ãŸã‚‚ã®ã§ã™:\n\n<h3 align=\"c..."
          ],
          [
           "```\n\nãã—ã¦ã“ã¡ã‚‰ã¯TensorFlowã¨åŒç­‰ã®ã‚³ãƒ¼ãƒ‰ã¨ãªã‚Šã¾ã™:\n```python\n>>> from transformers import AutoTokenizer, TFAutoMode..."
          ],
          [
           "```\n\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ã™ã¹ã¦ã®å‰å‡¦ç†ã‚’æ‹…å½“ã—ã€å˜ä¸€ã®æ–‡å­—åˆ— (ä¸Šè¨˜ã®ä¾‹ã®ã‚ˆã†ã«) ã¾ãŸã¯ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦ç›´æŽ¥å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã¯ä¸‹æµã®ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã§ãã‚‹è¾žæ›¸ã‚’å‡ºåŠ›ã—ã¾..."
          ],
          [
           "## ãªãœtransformersã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n\n1. ä½¿ã„ã‚„ã™ã„æœ€æ–°ãƒ¢ãƒ‡ãƒ«:\n    - è‡ªç„¶è¨€èªžç†è§£ãƒ»ç”Ÿæˆã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã®å„ã‚¿ã‚¹ã‚¯ã§é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚’ç™ºæ®ã—..."
          ],
          [
           "1. ãƒ¢ãƒ‡ãƒ«ã‚„ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ‹ãƒ¼ã‚ºã«åˆã‚ã›ã¦ç°¡å˜ã«ã‚«ã‚¹ã‚¿ãƒžã‚¤ã‚ºå¯èƒ½:\n    - åŽŸè‘—è€…ãŒç™ºè¡¨ã—ãŸçµæžœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä¾‹ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\n    - ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã¯å¯èƒ½ãªé™ã‚Šä¸€è²«ã—ã¦å…¬..."
          ],
          [
           "## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\n### pipã«ã¦\n\nã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, TensorFlow 2.6+ ã§ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n..."
          ],
          [
           "```\n\nã‚‚ã—ã‚µãƒ³ãƒ—ãƒ«ã‚’è©¦ã—ãŸã„ã€ã¾ãŸã¯ã‚³ãƒ¼ãƒ‰ã®æœ€å…ˆç«¯ãŒå¿…è¦ã§ã€æ–°ã—ã„ãƒªãƒªãƒ¼ã‚¹ã‚’å¾…ã¦ãªã„å ´åˆã¯ã€[ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](https://huggingface.co/docs/tran..."
          ],
          [
           "```\n\nFlaxã€PyTorchã€TensorFlowã‚’condaã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹æ–¹æ³•ã¯ã€ãã‚Œãžã‚Œã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«å¾“ã£ã¦ãã ã•ã„ã€‚\n\n> **_æ³¨æ„:_**  Windowsã§ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥..."
          ],
          [
           "1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (Google Research and the ..."
          ],
          [
           "1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua Un..."
          ],
          [
           "1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (VinAI Research ã‹ã‚‰) Ngu..."
          ],
          [
           "1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (VinAI Research ã‹ã‚‰) D..."
          ],
          [
           "1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (Microsoft Research AI4Sc..."
          ],
          [
           "1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (Faceb..."
          ],
          [
           "1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (Alexa ã‹ã‚‰) Adrian de Wynter a..."
          ],
          [
           "1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (Inria/Facebook/Sor..."
          ],
          [
           "1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (OpenAI ã‹ã‚‰) Alec Radford, Jon..."
          ],
          [
           "1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (MetaAI ã‹ã‚‰) Baptis..."
          ],
          [
           "1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (Facebook AI ã‹ã‚‰) Zhua..."
          ],
          [
           "1. **[CPM-Ant](https://huggingface.co/docs/transformers/model_doc/cpmant)** (OpenBMB ã‹ã‚‰) [OpenBMB](h..."
          ],
          [
           "1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (Microsoft ã‹ã‚‰) Pengchen..."
          ],
          [
           "1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (Facebook ã‹ã‚‰) Hugo Touvron, M..."
          ],
          [
           "1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (Microsoft Research ã‹..."
          ],
          [
           "1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (HuggingFace ã‹ã‚‰),..."
          ],
          [
           "1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (NAVER ã‹ã‚‰), Geewook Kim, Te..."
          ],
          [
           "1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google ..."
          ],
          [
           "1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (Baidu ã‹ã‚‰) Yu Sun, Shuohuan..."
          ],
          [
           "1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (Meta AI ã‹ã‚‰) ã¯ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒžãƒ¼ãƒ—ãƒ­ãƒ†ã‚¤ãƒ³è¨€èªžãƒ¢..."
          ],
          [
           "1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (Google AI ã‹ã‚‰) Hyung Wo..."
          ],
          [
           "1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (CNRS ã‹ã‚‰) Hang Le, Lo..."
          ],
          [
           "1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (CMU/Google B..."
          ],
          [
           "1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (OpenAI ã‹ã‚‰) Alec Radford..."
          ],
          [
           "1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (OpenAI ã‹ã‚‰) Alec Radford*, J..."
          ],
          [
           "1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (BigCode ã‹ã‚‰) Lou..."
          ],
          [
           "1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)** (Microsoft ã‹ã‚‰) Ch..."
          ],
          [
           "1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (Berkeley ã‹ã‚‰) Sehoon Kim, ..."
          ],
          [
           "1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (Salesforce ã‹..."
          ],
          [
           "1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (Microsoft Resear..."
          ],
          [
           "1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (Meta AI ã‹ã‚‰) Ben Graham, Al..."
          ],
          [
           "1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (The FAIR team of Meta AI..."
          ],
          [
           "1. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)** (Microsoft Research & Unive..."
          ],
          [
           "1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (UNC Chapel Hill ã‹ã‚‰) Hao ..."
          ],
          [
           "1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) rel..."
          ],
          [
           "1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (Meta and UIUC ã‹ã‚‰..."
          ],
          [
           "1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (Facebook ã‹ã‚‰) Xuezhe Ma, Chun..."
          ],
          [
           "1. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)** (from Mistral AI) by Th..."
          ],
          [
           "1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (Facebook ã‹ã‚‰) Vineel Pratap, An..."
          ],
          [
           "1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (Google Inc. ã‹..."
          ],
          [
           "1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (MosaiML ã‹ã‚‰) the MosaicML NLP T..."
          ],
          [
           "1. **[MVP](https://huggingface.co/docs/transformers/model_doc/mvp)** (RUC AI Box ã‹ã‚‰) Tianyi Tang, Ju..."
          ],
          [
           "1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (Meta ã‹ã‚‰) the NLLB te..."
          ],
          [
           "1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](http..."
          ],
          [
           "1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Researc..."
          ],
          [
           "1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (Deepmind ã‹ã‚‰) An..."
          ],
          [
           "1. **[Phi](https://huggingface.co/docs/transformers/model_doc/phi)** (from Microsoft) released with ..."
          ],
          [
           "1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (UCLA NLP ã‹ã‚‰) Wasi Uddin ..."
          ],
          [
           "1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (Nanjing University, The Univer..."
          ],
          [
           "1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (Google Research ã‹ã‚‰) ..."
          ],
          [
           "1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (Facebook ã‹ã‚‰), Yinhan L..."
          ],
          [
           "1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (Bo Peng ã‹ã‚‰) Bo Peng. ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸ..."
          ],
          [
           "1. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (Meta AI ã‹ã‚‰) Alexa..."
          ],
          [
           "1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (Microsoft Research ã‹..."
          ],
          [
           "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (Berkeley ã‹ã‚‰) F..."
          ],
          [
           "1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (University of WÃ¼rzburg..."
          ],
          [
           "1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (Google AI ã‹ã‚‰) Colin Raff..."
          ],
          [
           "1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transf..."
          ],
          [
           "1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (Microsoft ã‹ã‚‰), Minghao Li,..."
          ],
          [
           "1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (Google Research ã‹ã‚‰) Hyung Wo..."
          ],
          [
           "1. **[UnivNet](https://huggingface.co/docs/transformers/model_doc/univnet)** (from Kakao Corporation..."
          ],
          [
           "1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (NAVER AI Lab/Kakao Enterpris..."
          ],
          [
           "1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (UCLA NLP ã‹ã‚‰) Li..."
          ],
          [
           "1. **[ViTMatte](https://huggingface.co/docs/transformers/model_doc/vitmatte)** (HUST-VL ã‹ã‚‰) Jingfeng..."
          ],
          [
           "1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (Facebook AI ã‹ã‚‰) Alex..."
          ],
          [
           "1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (Microsoft Research ã‹ã‚‰) San..."
          ],
          [
           "1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (Meta AI ã‹ã‚‰) Jonas Pfeiffer,..."
          ],
          [
           "1. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlm-prophetnet)** (Microsof..."
          ],
          [
           "1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (Meta AI ã‹ã‚‰) Davis Liang, H..."
          ],
          [
           "1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (Facebook A..."
          ],
          [
           "1. æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’æŠ•ç¨¿ã—ãŸã„ã§ã™ã‹ï¼Ÿæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ã¨ã—ã¦ã€**è©³ç´°ãªã‚¬ã‚¤ãƒ‰ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã¯ãƒªãƒã‚¸ãƒˆãƒªã®[`templates`](./template..."
          ],
          [
           "å„ãƒ¢ãƒ‡ãƒ«ãŒFlaxã€PyTorchã€TensorFlowã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‹ã€ðŸ¤—Tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«æ”¯ãˆã‚‰ã‚ŒãŸé–¢é€£ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’æŒã£ã¦ã„ã‚‹ã‹ã¯ã€[ã“ã®è¡¨](https://huggingfa..."
          ],
          [
           "## ã•ã‚‰ã«è©³ã—ã\n\n| ã‚»ã‚¯ã‚·ãƒ§ãƒ³ | æ¦‚è¦ |\n|-|-|\n| [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/transformers/) | å®Œå…¨ãªAPIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ..."
          ],
          [
           "## å¼•ç”¨\n\nðŸ¤— ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒžãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å¼•ç”¨ã§ãã‚‹[è«–æ–‡](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)ãŒå‡ºæ¥ã¾ã—ãŸ:\n```bi..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "It should be noted that each of the first three modules can support conditional speaker embeddings t..."
          ],
          [
           "```\n\n#### Using CPU offload\n\nAs mentioned above, Bark is made up of 4 sub-models, which are called u..."
          ],
          [
           "```\n\nNote that ðŸ¤— Optimum must be installed before using this feature. [Here's how to install it.](ht..."
          ],
          [
           "```\n\n##### Performance comparison\n\n\nThe following diagram shows the latency for the native attention..."
          ],
          [
           "```\n\nFind out more on inference optimization techniques [here](https://huggingface.co/docs/transform..."
          ],
          [
           "```\n\nBark can generate highly realistic, **multilingual** speech as well as other audio - including ..."
          ],
          [
           "```\n\n## BarkConfig\n\n[[autodoc]] BarkConfig\n    - all\n\n## BarkProcessor\n\n[[autodoc]] BarkProcessor\n  ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*BERT adopts masked language modeling (MLM) for pre-t..."
          ],
          [
           "## MPNetConfig\n\n[[autodoc]] MPNetConfig\n\n## MPNetTokenizer\n\n[[autodoc]] MPNetTokenizer\n    - build_i..."
          ],
          [
           "[[autodoc]] TFMPNetForMultipleChoice\n    - call\n\n## TFMPNetForTokenClassification\n\n[[autodoc]] TFMPN..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Vision-Language Pre-training (VLP) has advanced the ..."
          ],
          [
           "## BlipConfig\n\n[[autodoc]] BlipConfig\n    - from_text_vision_configs\n\n## BlipTextConfig\n\n[[autodoc]]..."
          ],
          [
           "## TFBlipVisionModel\n\n[[autodoc]] TFBlipVisionModel\n    - call\n\n## TFBlipForConditionalGeneration\n\n[..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recent studies have shown that multilingual pretrain..."
          ],
          [
           "```\n\nNote that mLUKE has its own tokenizer, [`MLukeTokenizer`]. You can initialize it as follows:\n\n`..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper is the following:\n\n*Building open-domain chatbots is a challenging area fo..."
          ],
          [
           "An example:\n\n```python\n>>> from transformers import BlenderbotTokenizer, BlenderbotForConditionalGen..."
          ],
          [
           "```\n\n## Implementation Notes\n\n- Blenderbot uses a standard [seq2seq model transformer](https://arxiv..."
          ],
          [
           "## BlenderbotForCausalLM\n\n[[autodoc]] BlenderbotForCausalLM\n    - forward\n\n</pt>\n<tf>\n\n## TFBlenderb..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "A Hugging Face team member will be available to help you along the way so you'll never be alone. ðŸ¤— â¤..."
          ],
          [
           "With this in mind, let's go a bit deeper into the general library design.\n\n### Overview of models\n\nT..."
          ],
          [
           "```python\nmodel = BrandNewBertModel.from_pretrained(\"brandy/brand_new_bert\")\nmodel.config  # model h..."
          ],
          [
           "```\n\nSimilar to the model, the configuration inherits basic serialization and deserialization functi..."
          ],
          [
           "### Overview of tokenizers\n\nNot quite ready yet :-( This section will be added soon!\n\n## Step-by-ste..."
          ],
          [
           "In the following, we try to give you a general recipe that we found most useful when porting a model..."
          ],
          [
           "-  What type of model is *brand_new_bert*? BERT-like encoder-only model? GPT2-like decoder-only mode..."
          ],
          [
           "```\n\n3. Set up a development environment, for instance by running the following command:\n\n```bash\npy..."
          ],
          [
           "```\n\nNow you have set up a development environment to port *brand_new_bert* to ðŸ¤— Transformers.\n\n### ..."
          ],
          [
           "It is very important that before you start the porting process, you can **efficiently** debug code i..."
          ],
          [
           "```python\nmodel = BrandNewBertModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\ninput_ids = [..."
          ],
          [
           "```\n\nNext, regarding the debugging strategy, there are generally a few from which to choose from:\n\n-..."
          ],
          [
           "However, if the original code-base is very complex or only allows intermediate components to be run ..."
          ],
          [
           "```\n[[\n [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\n [-0.4417, -0.5920,  0.3450,  ..."
          ],
          [
           "```\n\nWe expect that every model added to ðŸ¤— Transformers passes a couple of integration tests, meanin..."
          ],
          [
           "- Find the best way of debugging intermediate results. Is the original repository written in PyTorch..."
          ],
          [
           "you have to input a string, then try to find out where in the forward call the string input is chang..."
          ],
          [
           "The following section gives you more specific details/tips on how you can do this for *brand_new_ber..."
          ],
          [
           "```\n\nIn the special case that you are adding a model whose architecture exactly matches the model ar..."
          ],
          [
           "```\n\n4. Push the changes to your account using:\n\n```bash\ngit push -u origin a-descriptive-name-for-m..."
          ],
          [
           "```\n\nIn general, all questions you might have regarding the model or your implementation should be a..."
          ],
          [
           "**Note** that at this point, you don't have to be very sure that your code is fully correct or clean..."
          ],
          [
           "```\n\nThe above command will create a model according to the default parameters as defined in `BrandN..."
          ],
          [
           "```\n\nYou can have some more custom schemes if you need a special initialization for some modules. Fo..."
          ],
          [
           "```\n\nThe `_is_hf_initialized` flag is internally used to make sure we only initialize a submodule on..."
          ],
          [
           "In the following, we'll quickly explain how PyTorch models store layer weights and define layer name..."
          ],
          [
           "```\n\nNow we can create an instance of this model definition which will fill all weights: `dense`, `i..."
          ],
          [
           "```\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\n         -0.207..."
          ],
          [
           "```\n\nIn the conversion script, you should fill those randomly initialized weights with the exact wei..."
          ],
          [
           "```\n\nIf either the shape or the name doesn't match, you probably assigned the wrong checkpoint weigh..."
          ],
          [
           "```\n\n**7. Implement the forward pass**\n\nHaving managed to correctly load the pretrained weights into..."
          ],
          [
           "```\n\nIt is very likely that the ðŸ¤— Transformers implementation and the original model implementation ..."
          ],
          [
           "The best way to fix the problem is usually to look at the forward pass of the original implementatio..."
          ],
          [
           "```\n\nHaving fixed all common tests, it is now crucial to ensure that all the nice work you have done..."
          ],
          [
           "```\n\n<Tip>\n\nIn case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\n\n</..."
          ],
          [
           "```\n\nYou might have to take a deeper look again into the original repository to find the correct tok..."
          ],
          [
           "```\n\nWhen both `input_ids` yield the same values, as a final step a tokenizer test file should also ..."
          ],
          [
           "Next, make sure that the docstring added to `src/transformers/models/brand_new_bert/modeling_brand_n..."
          ],
          [
           "```\n\nand verify that your coding style passes the quality check:\n\n```bash\nmake quality\n```\n\nThere ar..."
          ],
          [
           "```\n\nIt is worth spending some time to create fitting model cards for each checkpoint. The model car..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "Self-training\n\nThis is an implementation of the self-training algorithm (without task augmentation) ..."
          ],
          [
           "```\nThis will install PyTorch as a backend.\n\n## Self-training\n### Running self-training with a base ..."
          ],
          [
           "```python\nimport os\nfrom selftraining import selftrain\n\ndata_dir = '/path/to/your/data/dir'\nparamete..."
          ],
          [
           "```\n\n**Note**: We checkpoint periodically during self-training. In case of preemptions, just re-run ..."
          ],
          [
           "```\n\n2. Run your script with the following command:\n\n```sh\npython -m torch.distributed.launch --nnod..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n2. Pass your input to the [`pipeline`]. In the case of speech recognition, this is an audio inp..."
          ],
          [
           "```\n\nNow this result looks more accurate! For a deep-dive comparison on Wav2Vec2 vs Whisper, refer t..."
          ],
          [
           "```\n\nLet's check out 3 important ones:\n\n### Device\n\nIf you use `device=n`, the pipeline automaticall..."
          ],
          [
           "```\n\nThis runs the pipeline on the 4 provided audio files, but it will pass them in batches of 2\nto ..."
          ],
          [
           "```\n\nAs you can see, the model inferred the text and also outputted **when** the various sentences w..."
          ],
          [
           "```\n\nThe iterator `data()` yields each result, and the pipeline automatically\nrecognizes the input i..."
          ],
          [
           "```\n\n\n## Using pipelines for a webserver\n\n<Tip>\nCreating an inference engine is a complex topic whic..."
          ],
          [
           "```\n\n## Text pipeline\n\nUsing a [`pipeline`] for NLP tasks is practically identical.\n\n```py\n>>> from ..."
          ],
          [
           "```\n\n## Multimodal pipeline\n\nThe [`pipeline`] supports more than one modality. For example, a visual..."
          ],
          [
           "```\n\n</Tip>\n\n## Using `pipeline` on large models with ðŸ¤— `accelerate`:\n\nYou can easily run `pipeline`..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "From the abstract of the XLM-V paper:\n\n*Large multilingual language models typically rely on a singl..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Combining simple architectures with large-scale pre-..."
          ],
          [
           "## Usage tips\n\nOWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses [CLIP](c..."
          ],
          [
           ">>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.g..."
          ],
          [
           "```\n\n## Resources\n\nA demo notebook on using OWL-ViT for zero- and one-shot (image-guided) object det..."
          ],
          [
           "Security Policy\n\n## Reporting a Vulnerability\n\nðŸ¤— We have our bug bounty program set up with HackerOn..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "*The MobileNetV2 architecture is based on an inverted residual structure where the input and output ..."
          ],
          [
           "- One can use [`MobileNetV2ImageProcessor`] to prepare images for the model.\n\n- The available image ..."
          ],
          [
           "- The DeepLabV3+ segmentation head does not use the final convolution layer from the backbone, but t..."
          ],
          [
           "## MobileNetV2ForImageClassification\n\n[[autodoc]] MobileNetV2ForImageClassification\n    - forward\n\n#..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "According to the abstract\n\n*Code summarization and generation empower conversion between programming..."
          ],
          [
           "However, for fine-tuning, in some cases no language token is provided in cases where a single langua..."
          ],
          [
           "```\n\n### Generation\n\n  While generating the target text set the `decoder_start_token_id` to the targ..."
          ],
          [
           "```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Causal la..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We introduce a self-supervised vision representation..."
          ],
          [
           "- BEiT models are regular Vision Transformers, but pre-trained in a self-supervised way rather than ..."
          ],
          [
           "- The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/) ..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers..."
          ],
          [
           "[[autodoc]] BeitFeatureExtractor\n    - __call__\n    - post_process_semantic_segmentation\n\n## BeitIma..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This version of the model is for tasks where the state is a vector.\n\nThis model was contributed by [..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*This paper presents XLS-R, a large-scale model for c..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage example\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n..."
          ],
          [
           "```\n\n<Tip> \n\nPhoBERT implementation is the same as BERT, except for tokenization. Refer to [EART doc..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Text recognition is a long-standing research problem..."
          ],
          [
           "## Usage tips\n\n- The quickest way to get started with TrOCR is by checking the [tutorial\n  notebooks..."
          ],
          [
           "<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on [Accelerating Document AI](https://h..."
          ],
          [
           "- [Casual language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) task ..."
          ],
          [
           ">>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n>>> generated_ids = model.gene..."
          ],
          [
           "```\n\nSee the [model hub](https://huggingface.co/models?filter=trocr) to look for TrOCR checkpoints.\n..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n`compile()`Â comes with multiple modes for compiling, which essentially differ in compilation ti..."
          ],
          [
           "```\n\n#### Object Detection with DETR\n\n```python \nfrom transformers import AutoImageProcessor, AutoMo..."
          ],
          [
           "```\n\nBelow you can find the list of the models we benchmarked.\n\n**Image Classification** \n- [google/..."
          ],
          [
           "Below you can find visualization of inference durations with and without `torch.compile()`Â and perce..."
          ],
          [
           "Below you can find inference durations in milliseconds for each model with and without `compile()`. ..."
          ],
          [
           "### A100 (batch size: 4)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>comp..."
          ],
          [
           "### A100 (batch size: 16)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>com..."
          ],
          [
           "### V100 (batch size: 1)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>comp..."
          ],
          [
           "### V100 (batch size: 4)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>comp..."
          ],
          [
           "### V100 (batch size: 16)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>com..."
          ],
          [
           "### T4 (batch size: 1)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compil..."
          ],
          [
           "### T4 (batch size: 4)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compil..."
          ],
          [
           "### T4 (batch size: 16)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compi..."
          ],
          [
           "### A100\n\n| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0 -<br> compile..."
          ],
          [
           "###Â V100\n\n| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>com..."
          ],
          [
           "### T4\n\n| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compi..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\nWith the `mps` device set, you can:\n\n* train larger networks or batch sizes locally\n* reduce..."
          ],
          [
           "```\n\n[`TrainingArguments`] uses the `mps` device by default if it's available which means you don't ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers..."
          ],
          [
           "- Step-by-step PDF transcription\n\n```py\n>>> from huggingface_hub import hf_hub_download\n>>> import r..."
          ],
          [
           ">>> sequence = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n>>> sequence = processor..."
          ],
          [
           "```\n\nSee the [model hub](https://huggingface.co/models?filter=nougat) to look for Nougat checkpoints..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Self-supervised approaches for speech representation..."
          ],
          [
           "## HubertConfig\n\n[[autodoc]] HubertConfig\n\n<frameworkcontent>\n<pt>\n\n## HubertModel\n\n[[autodoc]] Hube..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "It extends [NAT](nat) by adding a Dilated Neighborhood Attention pattern to capture global context,\n..."
          ],
          [
           "<img\nsrc=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dilated-neig..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[`SeamlessM4TModel`] can perform all the above tasks, but each task also has its own dedicated sub-m..."
          ],
          [
           "## Usage\n\nFirst, load the processor and a checkpoint of the model:\n\n```python\n>>> from transformers ..."
          ],
          [
           "```\n\nYou can seamlessly use this model on text or on audio, to generated either translated text or t..."
          ],
          [
           "```\n\nWith basically the same code, I've translated English text and Arabic speech to Russian speech ..."
          ],
          [
           "```\n\nFeel free to try out [`SeamlessM4TForSpeechToText`] and [`SeamlessM4TForTextToSpeech`] as well...."
          ],
          [
           "This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be ..."
          ],
          [
           "[[autodoc]] SeamlessM4TProcessor\n    - __call__\n\n## SeamlessM4TCodeHifiGan\n\n[[autodoc]] SeamlessM4TC..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "3. Check the [Migration](migration) guide if you use an older version of ðŸ¤— Transformers since some i..."
          ],
          [
           "```\nValueError: Connection error, and we cannot find the requested files in the cached path.\nPlease ..."
          ],
          [
           "```\n\nHere are some potential solutions you can try to lessen memory use:\n\n- Reduce the [`per_device_..."
          ],
          [
           "```\n\n- Save the model with [`~TFPretrainedModel.save_pretrained`] and load it again with [`~TFPreTra..."
          ],
          [
           "```\n\n## Incorrect output when padding tokens aren't masked\n\nIn some cases, the output `hidden_state`..."
          ],
          [
           "```\n\nMost of the time, you should provide an `attention_mask` to your model to ignore the padding to..."
          ],
          [
           "```\n\nðŸ¤— Transformers doesn't automatically create an `attention_mask` to mask a padding token if it i..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n> Visually-situated language is ubiquitous -- sources..."
          ],
          [
           "If you want to use the model to perform conditional text captioning, make sure to use the processor ..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Ve..."
          ],
          [
           "## The Big Table of Tasks\n\nHere is the list of all our examples:\n\n| Task | Example datasets |\n|---|-..."
          ],
          [
           "Patience-based Early Exit\n\nPatience-based Early Exit (PABEE) is a plug-and-play inference method for..."
          ],
          [
           "```\n\n## Inference\n\nYou can inference with different patience settings by:\n```bash\nexport GLUE_DIR=/p..."
          ],
          [
           "```\nwhere `patience` can be a list of patience settings, separated by a comma. It will help determin..."
          ],
          [
           "| Model         | \\#Param | Speed\\-up | MNLI  | SST\\-2 | STS\\-B |\n|---------------|---------|-------..."
          ],
          [
           "Long Form Question Answering\n\nAuthor: @yjernite\n\nThis folder contains the code for the Long Form Que..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recently, neural networks purely based on attention ..."
          ],
          [
           "- Compared to ViT, DeiT models use a so-called distillation token to effectively learn from a teache..."
          ],
          [
           "contrast with the original ViT model, which used external data like the JFT-300M dataset/Imagenet-21..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "## DeiTForImageClassification\n\n[[autodoc]] DeiTForImageClassification\n    - forward\n\n## DeiTForImage..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Light-weight convolutional neural networks (CNNs) ar..."
          ],
          [
           "## Usage tips\n\n- MobileViT is more like a CNN than a Transformer model. It does not work on sequence..."
          ],
          [
           "model_ckpt = \"apple/mobilevit-xx-small\"\nmodel = TFMobileViTForImageClassification.from_pretrained(mo..."
          ],
          [
           "```\n\n  The resulting model will be just **about an MB** making it a good fit for mobile applications..."
          ],
          [
           "## MobileViTForImageClassification\n\n[[autodoc]] MobileViTForImageClassification\n    - forward\n\n## Mo..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert), [BigBird](../mo..."
          ],
          [
           "Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [GPT-J](../model_doc/gptj), [I-BERT](..."
          ],
          [
           "[NystrÃ¶mformer](../model_doc/nystromformer), [OPT](../model_doc/opt), [QDQBert](../model_doc/qdqbert..."
          ],
          [
           "```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model w..."
          ],
          [
           "```\n\nThen take a look at an example:\n\n```py\n>>> squad[\"train\"][0]\n{'answers': {'answer_start': [515]..."
          ],
          [
           "```\n\nThere are a few preprocessing steps particular to question answering tasks you should be aware ..."
          ],
          [
           "...         # Find the start and end of the context\n...         idx = 0\n...         while sequence_i..."
          ],
          [
           "```\n\nTo apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [`~datasets.Dataset..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the ..."
          ],
          [
           "```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_ap..."
          ],
          [
           "```\nOnce training is completed, your model is automatically uploaded to the Hub so everyone can use ..."
          ],
          [
           "```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ..."
          ],
          [
           "```\n\nDecode the predicted tokens to get the answer:\n\n```py\n>>> predict_answer_tokens = inputs.input_..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Expanding the language coverage of speech technology..."
          ],
          [
           "Tips:\n\n- All ASR models accept a float array corresponding to the raw waveform of the speech signal...."
          ],
          [
           "```\n\n<Tip>\n\nYou can safely ignore a warning such as:\n\n```text\nSome weights of Wav2Vec2ForCTC were no..."
          ],
          [
           "```\n\n#### Inference\n\nNext, let's look at how we can run MMS in inference and change adapter layers a..."
          ],
          [
           "```\n\nNow we process the audio data, pass the processed audio data to the model and transcribe the mo..."
          ],
          [
           "```\n\nIn the same way the language can be switched out for all other supported languages. Please have..."
          ],
          [
           "```\n\nThe resulting waveform can be saved as a `.wav` file:\n\n```python\nimport scipy\n\nscipy.io.wavfile..."
          ],
          [
           "```\n\n**Tips:**\n\n* The MMS-TTS checkpoints are trained on lower-cased, un-punctuated text. By default..."
          ],
          [
           "```\n\n### Language Identification (LID)\n\nDifferent LID models are available based on the number of la..."
          ],
          [
           "```\n\nNext, we load the model and processor\n\n```py\nfrom transformers import Wav2Vec2ForSequenceClassi..."
          ],
          [
           "```\n\nTo see all the supported languages of a checkpoint, you can print out the language ids as follo..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## BetterTransformer\n\nBetterTransformer accelerates inference with its fastpath (native PyTorch spec..."
          ],
          [
           "```\n\n## TorchScript\n\nTorchScript is an intermediate PyTorch model representation that can be run in ..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nFor PyTorch >= 1.14.0, JIT-mode could benefit any model for prediction an..."
          ],
          [
           "```\n\n## ðŸ¤— Optimum\n\n<Tip>\n\nLearn more details about using ORT with ðŸ¤— Optimum in the [Optimum Inferenc..."
          ],
          [
           "!--Copyright 2023 The HuggingFace and Baidu Team. All rights reserved.\n\nLicensed under the Apache Li..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recent studies have demonstrated that pre-trained cr..."
          ],
          [
           "## ErnieMConfig\n\n[[autodoc]] ErnieMConfig\n\n\n## ErnieMTokenizer\n\n[[autodoc]] ErnieMTokenizer\n    - bu..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Contrastive language-image pretraining has shown gre..."
          ],
          [
           "<small> X-CLIP architecture. Taken from the <a href=\"https://arxiv.org/abs/2208.02816\">original pape..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nHaving downloaded COCO dataset manually you should be able to load with the `ydshieh/coc_datase..."
          ],
          [
           "```\n\nThis loads both the text and vision encoders using pre-trained weights, the projection layers a..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nTo convert lots of models you can pass your list of Tatoeba model names to `resolver.convert_mo..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] thus implement the main\nmethods for using al..."
          ],
          [
           "## PreTrainedTokenizerFast\n\nThe [`PreTrainedTokenizerFast`] depend on the [tokenizers](https://huggi..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```py\nfrom starlette.applications import Starlette\nfrom starlette.responses import JSONResponse\nfrom..."
          ],
          [
           "```\n\nNow you can start it with:\n```bash\nuvicorn server:app\n```\n\nAnd you can query it:\n```bash\ncurl -..."
          ],
          [
           "```\n\nAgain, the proposed code is optimized for readability, not for being the best code.\nFirst of al..."
          ],
          [
           "### Blocking the main thread\n\nCurrently PyTorch is not async aware, and computation will block the m..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be ..."
          ],
          [
           "```python\n>>> import torch\n>>> from transformers import Speech2TextProcessor, Speech2TextForConditio..."
          ],
          [
           "```\n\n- Multilingual speech translation\n\n  For multilingual speech translation models, `eos_token_id`..."
          ],
          [
           "```\n\nSee the [model hub](https://huggingface.co/models?filter=speech_to_text) to look for Speech2Tex..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*While originally designed for natural language proce..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recently, significant progress has been made applyin..."
          ],
          [
           "## Resources\n\n<PipelineTag pipeline=\"object-detection\"/>\n\n- A demo notebook for the Table Transforme..."
          ],
          [
           "# Sequence to Sequence Training and Evaluation\n\nThis directory contains examples for finetuning and ..."
          ],
          [
           "```\nthis should make a directory called `wmt_en_ro/` with 6 files.\n\n#### WMT English-German\n\n```bash..."
          ],
          [
           "```\nThe `.source` files are the input, the `.target` files are the desired output.\n\n### Potential is..."
          ],
          [
           "Summarization Tips:\n- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB..."
          ],
          [
           "To see all the possible command line options, run:\n\n```bash\n./finetune.py --help..."
          ],
          [
           "```\n\n### Finetuning Training Params\n\nTo override the pretrained model's training params, you can pas..."
          ],
          [
           "```\nThis should take < 6h/epoch on a 16GB v100 and achieve test BLEU above 26\nTo get results in line..."
          ],
          [
           "```\n### Finetuning Outputs\nAs you train, `output_dir` will be filled with files, that look kind of l..."
          ],
          [
           "```\n\n### Converting pytorch-lightning checkpoints\npytorch lightning ``-do_predict`` often fails, aft..."
          ],
          [
           "```\nuses 12,723 batches of length 48 and takes slightly more time 9.5 minutes.\n\nThe feature is still..."
          ],
          [
           "![DBART](https://huggingface.co/front/thumbnails/distilbart_large.png)\n\n+ For the CNN/DailyMail data..."
          ],
          [
           "### Evaluation\n\nuse [run_distributed_eval](./run_distributed_eval.py), with the following convenient..."
          ],
          [
           "```\nOn a 1 GPU system, here are four commands (that assume `xsum`, `cnn_dm` are downloaded, cmd-F fo..."
          ],
          [
           "```\n\n### Distillation\n+ For all of the following commands, you can get roughly equivalent result and..."
          ],
          [
           "```\nor for `pegasus-xsum`\n```bash\npython make_student.py google/pegasus-xsum --save_path dpx_xsum_16..."
          ],
          [
           "```\n\n+ Note: The command that produced `sshleifer/distilbart-cnn-12-6` is at [train_distilbart_cnn.s..."
          ],
          [
           "```\n<!--- runtime: 6H on NVIDIA RTX 24GB GPU -->\n+ Tip: You can get the same simple distillation log..."
          ],
          [
           "```\n\n\n\nTo combine datasets, as in Section 6.2, try something like:\n```bash\ncurl -S https://cdn-datas..."
          ],
          [
           "```\n\n+ Expected ROUGE-2 between 21.3 and 21.6, run time ~13H.\n+ direct KD + Pegasus is VERY slow and..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip warning={true}>\n\nThe `Fuyu` models were trained using `bfloat16`, but the original inference us..."
          ],
          [
           "- To convert the model, you need to clone the original repository using `git clone https://github.co..."
          ],
          [
           "```\n\nFor the chat model:\n```bash\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-o..."
          ],
          [
           "```\n\nThis model was contributed by [Molbap](https://huggingface.co/Molbap).\nThe original code can be..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<br>\n\nFeel free to check out the [API reference](./main_classes/trainer) for these other [`Trainer`]..."
          ],
          [
           "```\n\nThis guide provides an overview of the [`Trainer`] class.\n\n## Basic usage\n\n[`Trainer`] includes..."
          ],
          [
           "```\n\nPass `training_args` to the [`Trainer`] along with a model, dataset, something to preprocess th..."
          ],
          [
           "```\n\nYou can save your checkpoints (the optimizer state is not saved by default) to the Hub by setti..."
          ],
          [
           "* [`~Trainer.get_train_dataloader`] creates a training DataLoader\n* [`~Trainer.get_eval_dataloader`]..."
          ],
          [
           "```\n\n### Callbacks\n\nAnother option for customizing the [`Trainer`] is to use [callbacks](callbacks)...."
          ],
          [
           "```\n\n## Logging\n\n<Tip>\n\nCheck out the [logging](./main_classes/logging) API reference for more infor..."
          ],
          [
           "For example, to set your main code and modules to use the same log level according to each node:\n\n``..."
          ],
          [
           "```\n\nUse different combinations of `log_level` and `log_level_replica` to configure what gets logged..."
          ],
          [
           "```\n\nNEFTune is disabled after training to restore the original embedding layer to avoid any unexpec..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"FSDP\">\n\n```yml\ncompute_environment: LOCAL_MACHINE\ndistributed_type: F..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"DeepSpeed with Accelerate plugin\">\n\n```yml\ncompute_environment: LOCAL..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\nThe [`accelerate_launch`](https://huggingface.co/docs/accelerate/pack..."
          ],
          [
           "```\n\nYou could also specify the parameters from the `config_file.yaml` file directly in the command ..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "The following examples show how to fine-tune a `\"base\"`-sized Wav2Vec2 model as well as a `\"large\"`-..."
          ],
          [
           "```bash\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\n\t--dataset_name=\"librispeech_asr\"..."
          ],
          [
           "```\n\nThe results of this run can be seen [here](https://wandb.ai/patrickvonplaten/wav2vec2-pretraine..."
          ],
          [
           "```bash\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\\n\t--dataset_name=librispeech_asr \\..."
          ],
          [
           "```\n\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 4 days. \nIn case you have more than 8..."
          ],
          [
           "```bash\naccelerate launch run_wav2vec2_pretraining_no_trainer.py \\ \n\t--dataset_name=librispeech_asr ..."
          ],
          [
           "```\n\nThe experiment was run on 8 GPU V100 (16 GB RAM each) for 7 days. \nIn case you have more than 8..."
          ],
          [
           "p align=\"center\"> <img src=\"http://sayef.tech:8082/uploads/FSNER-LOGO-2.png\" alt=\"FSNER LOGO\"> </p>\n..."
          ],
          [
           "You can use the FSNER model in 3 ways:\n\n1. Install directly from PyPI: `pip install fsner` and impor..."
          ],
          [
           "# each list in supports are the examples of one entity type\n# wrap entities around with [E] and [/E]..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar t..."
          ],
          [
           "| Notebook     |      Description      |      Author      |      |\n|:----------|:-------------|:----..."
          ],
          [
           "| [Train T5 on TPU](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)  | How ..."
          ],
          [
           "| [Fine-tune DialoGPT on New Datasets and Languages](https://github.com/ncoop57/i-am-a-nerd/blob/mas..."
          ],
          [
           "| [Fine-tune BART for Summarization](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021..."
          ],
          [
           "| [Optimize ðŸ¤— Hugging Face models with Weights & Biases](https://colab.research.google.com/github/wa..."
          ],
          [
           "| [Fine-tune Longformer for QA](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_t..."
          ],
          [
           "| [Fine-tune T5 for Sentiment Span Extraction](https://github.com/enzoampil/t5-intro/blob/master/t5_..."
          ],
          [
           "|[Fine-tune BERT for Multi-label Classification](https://github.com/abhimishra91/transformers-tutori..."
          ],
          [
           "|[Speed up Fine-Tuning in Transformers with Dynamic Padding / Bucketing](https://github.com/ELS-RD/t..."
          ],
          [
           "|[Expand and Fine Tune Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20..."
          ],
          [
           "|[Fine-tune Electra and interpret with Integrated Gradients](https://github.com/elsanns/xai-nlp-note..."
          ],
          [
           "|[Fine-tune a DistilBERT Model for Multi Label Classification task](https://github.com/DhavalTaunk08..."
          ],
          [
           "|[Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master..."
          ],
          [
           "|[Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail](https://github.com/patrickvonpla..."
          ],
          [
           "|[Fine-tune TAPAS on Sequential Question Answering (SQA)](https://github.com/NielsRogge/Transformers..."
          ],
          [
           "|[Fine-tuning mBART for translation](https://colab.research.google.com/github/vasudevgupta7/huggingf..."
          ],
          [
           "|[Fine-Tune DistilGPT2 and Generate Text](https://colab.research.google.com/github/tripathiaakash/Di..."
          ],
          [
           "|[Evaluate LED on Arxiv](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipyn..."
          ],
          [
           "|[Wav2Vec2 CTC decoding with GPT2 adjustment](https://github.com/voidful/huggingface_notebook/blob/m..."
          ],
          [
           "|[Evaluate Big Bird on Trivia QA](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluati..."
          ],
          [
           "| [Fine-tune the Vision Transformer on CIFAR-10 using PyTorch Lightning](https://github.com/NielsRog..."
          ],
          [
           "| [Fine-tune the Vision Transformer on CIFAR-10 using the ðŸ¤— Trainer](https://github.com/NielsRogge/T..."
          ],
          [
           "| [Evaluate LUKE on TACRED, a relation extraction dataset](https://github.com/studio-ousia/luke/blob..."
          ],
          [
           "| [Evaluate BigBird-Pegasus on PubMed dataset](https://github.com/vasudevgupta7/bigbird/blob/main/no..."
          ],
          [
           "| [Detect objects in an image with DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/m..."
          ],
          [
           "| [Finetune T5 for Named Entity Recognition](https://github.com/ToluClassics/Notebooks/blob/main/T5_..."
          ],
          [
           "<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ve..."
          ],
          [
           "ðŸ¤— Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provide..."
          ],
          [
           "- **MAIN CLASSES** details the most important classes like configuration, model, tokenizer, and pipe..."
          ],
          [
           "|                                  Model                                   | PyTorch support | Tenso..."
          ],
          [
           "|                 [BertJapanese](model_doc/bert-japanese)                  |       âœ…        |       ..."
          ],
          [
           "|                          [CLAP](model_doc/clap)                          |       âœ…        |       ..."
          ],
          [
           "|                    [DeBERTa-v2](model_doc/deberta-v2)                    |       âœ…        |       ..."
          ],
          [
           "|                       [ELECTRA](model_doc/electra)                       |       âœ…        |       ..."
          ],
          [
           "|                          [GLPN](model_doc/glpn)                          |       âœ…        |       ..."
          ],
          [
           "|                      [KOSMOS-2](model_doc/kosmos-2)                      |       âœ…        |       ..."
          ],
          [
           "|                        [Marian](model_doc/marian)                        |       âœ…        |       ..."
          ],
          [
           "|                     [MobileViT](model_doc/mobilevit)                     |       âœ…        |       ..."
          ],
          [
           "|                           [OPT](model_doc/opt)                           |       âœ…        |       ..."
          ],
          [
           "|                           [RAG](model_doc/rag)                           |       âœ…        |       ..."
          ],
          [
           "|        [Speech Encoder decoder](model_doc/speech-encoder-decoder)        |       âœ…        |       ..."
          ],
          [
           "|                  [Transformer-XL](model_doc/transfo-xl)                  |       âœ…        |       ..."
          ],
          [
           "|                           [ViT](model_doc/vit)                           |       âœ…        |       ..."
          ],
          [
           "|                [XLM-ProphetNet](model_doc/xlm-prophetnet)                |       âœ…        |       ..."
          ],
          [
           "<!-- End table-->..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "By exposing a graph with standardized operators and data types, ONNX makes it easy to\nswitch between..."
          ],
          [
           "```\n\nTo check out all available arguments, refer to the [ðŸ¤— Optimum docs](https://huggingface.co/docs..."
          ],
          [
           "```\n\nThe example above illustrates exporting a checkpoint from ðŸ¤— Hub. When exporting a local model, ..."
          ],
          [
           "```\n\nThe process is identical for TensorFlow checkpoints on the Hub. For instance, here's how you wo..."
          ],
          [
           "```\n\n### Exporting a model for an unsupported architecture\n\nIf you wish to contribute by adding supp..."
          ],
          [
           "```\n\nThe required output names (like `[\"last_hidden_state\"]`) can be obtained by taking a look at th..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "To explain how tasks are solved, we'll walk through what goes on inside the model to output useful p..."
          ],
          [
           "This model has four main components:\n\n1. A *feature encoder* takes the raw audio waveform, normalize..."
          ],
          [
           "### Automatic speech recognition\n\nTo use the pretrained model for automatic speech recognition, add ..."
          ],
          [
           "ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an ..."
          ],
          [
           "4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer percep..."
          ],
          [
           "<small>A basic convolution without padding or stride, taken from <a href=\"https://arxiv.org/abs/1603..."
          ],
          [
           "The output from the convolution blocks is passed to a classification head which converts the outputs..."
          ],
          [
           "3. DETR uses a *bipartite matching loss* during training to compare a fixed number of predictions wi..."
          ],
          [
           "There are three main components to Mask2Former:\n\n1. A [Swin](model_doc/swin) backbone accepts an ima..."
          ],
          [
           "### Depth estimation\n\n[GLPN](model_doc/glpn), *Global-Local Path Network*, is a Transformer for dept..."
          ],
          [
           "## Natural language processing\n\nThe Transformer was initially designed for machine translation, and ..."
          ],
          [
           "3. The input embeddings are passed through multiple encoder layers to output some final hidden state..."
          ],
          [
           "</Tip>\n\n### Text generation\n\n[GPT-2](model_doc/gpt2) is a decoder-only model pretrained on a large a..."
          ],
          [
           "Ready to try your hand at text generation? Check out our complete [causal language modeling guide](t..."
          ],
          [
           "2. The encoder's output is passed to the decoder, which must predict the masked tokens and any uncor..."
          ],
          [
           "Ready to try your hand at translation? Check out our complete [translation guide](tasks/summarizatio..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) and [stefan-it](http..."
          ],
          [
           "[[autodoc]] XLMRobertaXLForSequenceClassification\n    - forward\n\n## XLMRobertaXLForMultipleChoice\n\n[..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nTo export a model's checkpoint from the ðŸ¤— Hub, for example, `bert-base-uncased`, run the follow..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We present a new method that views object detection ..."
          ],
          [
           "## How DETR works\n\nHere's a TLDR explaining how [`~transformers.DetrForObjectDetection`] works:\n\nFir..."
          ],
          [
           "Next, this is sent through the encoder, outputting `encoder_hidden_states` of the same shape (you ca..."
          ],
          [
           "DETR can be naturally extended to perform panoptic segmentation (which unifies semantic segmentation..."
          ],
          [
           "- DETR uses so-called **object queries** to detect objects in an image. The number of queries determ..."
          ],
          [
           "_num_boxes_ variable in the _DetrLoss_ class of _modeling_detr.py_. When training on multiple nodes,..."
          ],
          [
           "Alternatively, one can also define a custom `collate_fn` in order to batch images together, using\n  ..."
          ],
          [
           "There are three ways to instantiate a DETR model (depending on what you prefer):\n\nOption 1: Instanti..."
          ],
          [
           "```\n\nOption 2: Instantiate DETR with randomly initialized weights for Transformer, but pre-trained w..."
          ],
          [
           "```\n\nAs a summary, consider the following table:..."
          ],
          [
           "| Task | Object detection | Instance segmentation | Panoptic segmentation |\n|------|----------------..."
          ],
          [
           "| **Postprocessing** (i.e. converting the output of the model to Pascal VOC format) | [`~transformer..."
          ],
          [
           "In short, one should prepare the data either in COCO detection or COCO panoptic format, then use\n[`~..."
          ],
          [
           "## DetrConfig\n\n[[autodoc]] DetrConfig\n\n## DetrImageProcessor\n\n[[autodoc]] DetrImageProcessor\n    - p..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "In our example, we will take a couple of arguments of the ResNet class that we might want to tweak. ..."
          ],
          [
           "```\n\nThe three important things to remember when writing you own configuration are the following:\n- ..."
          ],
          [
           "```\n\nYou can also use any other method of the [`PretrainedConfig`] class, like [`~PretrainedConfig.p..."
          ],
          [
           "```\n\nFor the model that will classify images, we just change the forward method:\n\n```py\nimport torch..."
          ],
          [
           "```\n\nIn both cases, notice how we inherit from `PreTrainedModel` and call the superclass initializat..."
          ],
          [
           "```\n\nNow let's see how to make sure that when we do [`~PreTrainedModel.save_pretrained`] or [`~PreTr..."
          ],
          [
           "```\n\nNote that the first argument used when registering your custom config to [`AutoConfig`] needs t..."
          ],
          [
           "```\n\nThen you have to tell the library you want to copy the code files of those objects when using t..."
          ],
          [
           "```\n\n</Tip>\n\nNext, let's create the config and models as we did before:\n\n```py\nresnet50d_config = Re..."
          ],
          [
           "```\n\nOn top of the modeling weights and the configuration in json format, this also copied the model..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Open-domain question answering relies on efficient p..."
          ],
          [
           "## DPRQuestionEncoderTokenizerFast\n\n[[autodoc]] DPRQuestionEncoderTokenizerFast\n\n## DPRReaderTokeniz..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libr..."
          ],
          [
           "```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model w..."
          ],
          [
           "```\n\nThen take a look at an example:..."
          ],
          [
           "```py\n>>> billsum[\"train\"][0]\n{'summary': 'Existing law authorizes state agencies to enter into cont..."
          ],
          [
           "'text': 'The people of the State of California do enact as follows:\\n\\n\\nSECTION 1.\\nSection 10295.3..."
          ],
          [
           "but not limited to, the employeeâ€™s or dependentâ€™s identification as transgender.\\n(2) For purposes o..."
          ],
          [
           "operations that occur under any of the following conditions:\\n(A) Within the state.\\n(B) On real pro..."
          ],
          [
           "contractorâ€™s insurance provider, any request by an employee or applicant for employment benefits or ..."
          ],
          [
           "as determined by the state agency, that endangers the public health, welfare, or safety, or the cont..."
          ],
          [
           "authorize application of this section.\\n(4) The contractor is providing wholesale or bulk water, pow..."
          ],
          [
           "the benefits, pays the actual costs incurred in obtaining the benefit.\\n(2) If a contractor is unabl..."
          ],
          [
           "contractor falsely certifies that it is in compliance with this section, the contract with that cont..."
          ],
          [
           "the application of any existing remedies otherwise available to the department or other contracting ..."
          ],
          [
           "or circumstances, it is the intent of the state that the court or agency sever that clause, sentence..."
          ],
          [
           "may be incurred by a local agency or school district will be incurred because this act creates a new..."
          ],
          [
           "'title': 'An act to add Section 10295.35 to the Public Contract Code, relating to public contracts.'..."
          ],
          [
           "```\n\nThere are two fields that you'll want to use:\n\n- `text`: the text of the bill which'll be the i..."
          ],
          [
           "```\n\nNow create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynami..."
          ],
          [
           "```\n\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule...."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`Seq2SeqTr..."
          ],
          [
           "```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t..."
          ],
          [
           "```\n\nConfigure the model for training with [`compile`](https://keras.io/api/models/model_training_ap..."
          ],
          [
           "```\n\nThen bundle your callbacks together:\n\n```py\n>>> callbacks = [metric_callback, push_to_hub_callb..."
          ],
          [
           "```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ..."
          ],
          [
           "```\n\nDecode the generated token ids back into text:\n\n```py\n>>> tokenizer.decode(outputs[0], skip_spe..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "* [Methods and tools for efficient training on a single GPU](perf_train_gpu_one): start here to lear..."
          ],
          [
           "When making contributions that A is better than B, please try to include a reproducible benchmark an..."
          ],
          [
           "!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\n\nLicensed under ..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Recent work in language modeling demonstrates that t..."
          ],
          [
           "## Usage tips\n\nWe have provided pretrained [BERT-345M](https://ngc.nvidia.com/catalog/models/nvidia:..."
          ],
          [
           "```\n\nBERT-345M-cased:\n\n```bash\nwget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidi..."
          ],
          [
           "```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token cla..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_d..."
          ],
          [
           "```\n\nMBart and some T5 models require special handling.\n\nT5 models `t5-small`, `t5-base`, `t5-large`..."
          ],
          [
           "```\n\nIf you get a terrible BLEU score, make sure that you didn't forget to use the `--source_prefix`..."
          ],
          [
           "```\n\nAnd here is how you would use the translation finetuning on your own files, after adjusting the..."
          ],
          [
           "```\nHere the languages are Romanian (`ro`) and English (`en`).\n\nIf you want to use a pre-processed d..."
          ],
          [
           "```\n\nthen\n\n```bash\npython run_translation_no_trainer.py \\\n    --model_name_or_path Helsinki-NLP/opus..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "Try out the inference widget here: https://huggingface.co/google/vit-base-patch16-224\n\n## TensorFlow..."
          ],
          [
           "```\n\nðŸ‘€ See the results here: [amyeroberts/vit-base-beans](https://huggingface.co/amyeroberts/vit-bas..."
          ],
          [
           "```\n\nInternally, the script will use the [`ImageFolder`](https://huggingface.co/docs/datasets/v2.0.0..."
          ],
          [
           "```\n\n`ImageFolder` will create a `label` column, and the label name is based on the directory name.\n..."
          ],
          [
           "ere is how to convert a GPT2 model generated outside of `transformers`\n\n* [Megatron-LM](https://gith..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "According to the abstract, MBART is a sequence-to-sequence denoising auto-encoder pretrained on larg..."
          ],
          [
           ">>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_romanian, return_ten..."
          ],
          [
           "```\n\n- Generation\n\n  While generating the target text set the `decoder_start_token_id` to the target..."
          ],
          [
           "```\n\n## Overview of MBart-50\n\nMBart-50 was introduced in the [Multilingual Translation with Extensib..."
          ],
          [
           "-  Supervised training\n\n```python\nfrom transformers import MBartForConditionalGeneration, MBart50Tok..."
          ],
          [
           "```\n\n- Generation\n\n  To generate using the mBART-50 multilingual translation models, `eos_token_id` ..."
          ],
          [
           "# translate Arabic to English\ntokenizer.src_lang = \"ar_AR\"\nencoded_ar = tokenizer(article_ar, return..."
          ],
          [
           "```\n\n## Documentation resources\n\n- [Text classification task guide](../tasks/sequence_classification..."
          ],
          [
           "## TFMBartModel\n\n[[autodoc]] TFMBartModel\n    - call\n\n## TFMBartForConditionalGeneration\n\n[[autodoc]..."
          ],
          [
           "# MM-IMDb\n\nBased on the script [`run_mmimdb.py`](https://github.com/huggingface/transformers/blob/ma..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "XTREME-S covers speech recognition with Fleurs, Multilingual LibriSpeech (MLS) and VoxPopuli, speech..."
          ],
          [
           "## Fine-tuning for the XTREME-S tasks\n\nBased on the [`run_xtreme_s.py`](https://github.com/huggingfa..."
          ],
          [
           "```\n\nwhere `TASK_NAME` can be one of: `mls, voxpopuli, covost2, fleurs-asr, fleurs-lang_id, minds14`..."
          ],
          [
           "### Speech Recognition with MLS\n\nThe following command shows how to fine-tune the [XLS-R](https://hu..."
          ],
          [
           "```\n\nOn 8 V100 GPUs, this script should run in ~19 hours and yield a cross-entropy loss of **0.6215*..."
          ],
          [
           "```bash\npython -m torch.distributed.launch \\\n    --nproc_per_node=2 \\\n    run_xtreme_s.py \\\n    --ta..."
          ],
          [
           "```\n\nOn 2 A100 GPUs, this script should run in ~5 hours and yield a cross-entropy loss of **0.4119**..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Example of using a model with MeCab and WordPiece tokenization:\n\n```python\n>>> import torch\n>>> from..."
          ],
          [
           "```\n\nExample of using a model with Character tokenization:\n\n```python\n>>> bertjapanese = AutoModel.f..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers..."
          ],
          [
           "```\nfrom transformers import LiltModel\n\nmodel = LiltModel.from_pretrained(\"path_to_your_files\")\nmode..."
          ],
          [
           "```\n\n- When preparing data for the model, make sure to use the token vocabulary that corresponds to ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Before you can fine-tune a pretrained model, download a dataset and prepare it for training. The pre..."
          ],
          [
           "```\n\nAs you now know, you need a tokenizer to process the text and include a padding and truncation ..."
          ],
          [
           "```\n\n<a id='trainer'></a>\n\n## Train\n\nAt this point, you should follow the section corresponding to t..."
          ],
          [
           "```\n\n<Tip>\n\nYou will see a warning about some of the pretrained weights not being used and some weig..."
          ],
          [
           "```\n\nCall [`~evaluate.compute`] on `metric` to calculate the accuracy of your predictions. Before pa..."
          ],
          [
           "```\n\nThen fine-tune your model by calling [`~transformers.Trainer.train`]:\n\n```py\n>>> trainer.train(..."
          ],
          [
           "```\n\nFinally, load, [`compile`](https://keras.io/api/models/model_training_apis/#compile-method), an..."
          ],
          [
           "```\n\n<Tip>\n\nYou don't have to pass a loss argument to your models when you `compile()` them! Hugging..."
          ],
          [
           "```\n\nRemember that Hugging Face datasets are stored on disk by default, so this will not inflate you..."
          ],
          [
           "```\n\n</tf>\n</frameworkcontent>\n\n<a id='pytorch_native'></a>\n\n## Train in native PyTorch\n\n<frameworkc..."
          ],
          [
           "```\n\n### DataLoader\n\nCreate a `DataLoader` for your training and test datasets so you can iterate ov..."
          ],
          [
           "```\n\nLastly, specify `device` to use a GPU if you have access to one. Otherwise, training on a CPU m..."
          ],
          [
           "```\n\n### Evaluate\n\nJust like how you added an evaluation function to [`Trainer`], you need to do the..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "According to the abstract,\n\n- Bart uses a standard seq2seq/machine translation architecture with a b..."
          ],
          [
           "## Implementation Notes\n\n- Bart doesn't use `token_type_ids` for sequence classification. Use [`Bart..."
          ],
          [
           "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help ..."
          ],
          [
           "- A blog post on [Distributed Training: Train BART/T5 for Summarization using ðŸ¤— Transformers and Ama..."
          ],
          [
           "- An example of how to train [`BartForConditionalGeneration`] with a Hugging Face `datasets` object ..."
          ],
          [
           "<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`BartForConditionalGeneration`] is supported by this [exampl..."
          ],
          [
           "<PipelineTag pipeline=\"translation\"/>\n\n- A notebook on how to [finetune mBART using Seq2SeqTrainer f..."
          ],
          [
           "## BartTokenizerFast\n\n[[autodoc]] BartTokenizerFast\n    - all\n\n\n<frameworkcontent>\n<pt>\n\n## BartMode..."
          ],
          [
           "## FlaxBartForQuestionAnswering\n\n[[autodoc]] FlaxBartForQuestionAnswering\n    - __call__\n    - encod..."
          ],
          [
           "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks ..."
          ],
          [
           "```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream ta..."
          ],
          [
           "```\nYou will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag...."
          ],
          [
           "```\n\nUsing Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load th..."
          ],
          [
           "```\nDoes He Love You\tDoes He Love You\tRed Sandy Spika dress of Reba McEntire\tGreatest Hits Volume Tw..."
          ],
          [
           "```\n   ```bash\n   # EXPLANATION\n    python examples/research_projects/rag/eval_rag.py \\\n        --mo..."
          ],
          [
           "```\n- `ans` - where a single line contains a single expected answer, e.g.:\n```\nXiu Li Dai\n```\n\nPredi..."
          ],
          [
           "```\n\nThe created outputs in `path/to/my_knowledge_dataset` can then be used to finetune RAG as follo..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be foun..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nDeepSpeed compiles CUDA C++ code and it can be a potential source of errors when building PyTor..."
          ],
          [
           "```\n\n`PATH` lists the locations of the executables and `LD_LIBRARY_PATH` lists where to look for sha..."
          ],
          [
           "```\n\n## Multi-GPU Network Issues Debug\n\nWhen training or inferencing with `DistributedDataParallel` ..."
          ],
          [
           "```\n\nThis will dump a lot of NCCL-related debug information, which you can then search online if you..."
          ],
          [
           "```\nDetected inf/nan during batch_number=0\nLast 21 forward frames:\nabs min  abs max  metadata\n      ..."
          ],
          [
           "0.00e+00 8.76e+03 input[0]\n0.00e+00 9.74e+03 output\n                  encoder.block.2.layer.1.DenseR..."
          ],
          [
           "```\n\nThe example output has been trimmed in the middle for brevity.\n\nThe second column shows the val..."
          ],
          [
           "```\nDetected inf/nan during batch_number=0\nLast 21 forward frames:\nabs min  abs max  metadata\n[...]\n..."
          ],
          [
           "```\n\nThe last frame reports for `Dropout.forward` function with the first entry for the only input a..."
          ],
          [
           "def forward(self, hidden_states):\n        hidden_gelu = self.gelu_act(self.wi_0(hidden_states))\n    ..."
          ],
          [
           "```\n\nNow it's easy to see the `dropout` call, and all the previous calls as well.\n\nSince the detecti..."
          ],
          [
           "```\n\nSince the automatic detector only reports on inputs and outputs of full frames, once you know w..."
          ],
          [
           "```\n\nAnd now full batches 1 and 3 will be traced using the same format as the underflow/overflow det..."
          ],
          [
           "# Information Gain Filtration(IGF)\n\nAuthors @Tuko @mraunak\n\nThis folder contains the code how to imp..."
          ],
          [
           "![IGF performance](result_igf.png)\n\nFigure 1: Comparing IGF to Standard Fine-tuning:\nIGF with consta..."
          ],
          [
           "## How to use this project?\n\nTo fine-tune a transformer model with IGF on a language modeling task, ..."
          ],
          [
           "```python\npython run_clm_igf.py\\\n--model_name_or_path \"gpt2\" \\\n--data_file=\"data/tokenized_stories_t..."
          ],
          [
           "```\n\n## Citation\n\nIf you find the resource useful, please cite the following paper\n\n```\n@inproceedin..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Overview\n\nThe T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified..."
          ],
          [
           "## Usage tips\n\n- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised ..."
          ],
          [
           "- [t5-11b](https://huggingface.co/t5-11b).\n\nBased on the original T5 model, Google has released some..."
          ],
          [
           "## Training\n\nT5 is an encoder-decoder model and converts all NLP problems into a text-to-text format..."
          ],
          [
           ">>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_..."
          ],
          [
           "```\n\nIf you're interested in pre-training T5 on a new corpus, check out the [run_t5_mlm_flax.py](htt..."
          ],
          [
           "```\n\nAs you can see, only 2 inputs are required for the model in order to compute a loss: `input_ids..."
          ],
          [
           "```python\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n>>> import torch\n\n>>>..."
          ],
          [
           ">>> # replace padding token id's of the labels by -100 so it's ignored by the loss\n>>> labels[labels..."
          ],
          [
           "```\n\nAdditional training tips:\n\n- T5 models need a slightly higher learning rate than the default on..."
          ],
          [
           "```python\n>>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n>>> tokenizer = T5To..."
          ],
          [
           "```\n\nNote that T5 uses the `pad_token_id` as the `decoder_start_token_id`, so when doing generation ..."
          ],
          [
           "```\n\nBecause T5 has been trained with the span-mask denoising objective,\nit can be used to predict t..."
          ],
          [
           "```\n\n## Performance\n\nIf you'd like a faster training and inference performance, install [NVIDIA APEX..."
          ],
          [
           "<PipelineTag pipeline=\"text-generation\"/>\n\n- A notebook for [Finetuning CodeT5 for generating docstr..."
          ],
          [
           "- A notebook to [Finetune T5-base-dutch to perform Dutch abstractive summarization on a TPU](https:/..."
          ],
          [
           "- [`FlaxT5ForConditionalGeneration`] is supported by this [example script](https://github.com/huggin..."
          ],
          [
           "<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`FlaxT5ForConditionalGeneration`] is supported by this [exam..."
          ],
          [
           "<PipelineTag pipeline=\"question-answering\"/>\n\n- A notebook on how to [finetune T5 for question answe..."
          ],
          [
           "## T5ForSequenceClassification\n\n[[autodoc]] T5ForSequenceClassification\n    - forward\n\n## T5ForQuest..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "The following example shows how to fine-tune the [Whisper small](https://huggingface.co/openai/whisp..."
          ],
          [
           "```\n\nOn a TPU v4-8, training should take approximately 25 minutes, with a final cross-entropy loss o..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Here's an example:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = G..."
          ],
          [
           "```\n\nThe `generation_output` object is a [`~generation.GreedySearchDecoderOnlyOutput`], as we can\nse..."
          ],
          [
           "```\n\nwill return the tuple `(generation_output.sequences, generation_output.scores)` for instance.\n\n..."
          ],
          [
           "[[autodoc]] generation.TFBeamSearchDecoderOnlyOutput\n\n[[autodoc]] generation.TFBeamSampleEncoderDeco..."
          ],
          [
           "[[autodoc]] ForcedEOSTokenLogitsProcessor\n    - __call__\n\n[[autodoc]] ForceTokensLogitsProcessor\n   ..."
          ],
          [
           "[[autodoc]] TopKLogitsWarper\n    - __call__\n\n[[autodoc]] TopPLogitsWarper\n    - __call__\n\n[[autodoc]..."
          ],
          [
           "[[autodoc]] TFTemperatureLogitsWarper\n    - __call__\n\n[[autodoc]] TFTopKLogitsWarper\n    - __call__\n..."
          ],
          [
           "A [`StoppingCriteria`] can be used to change when to stop generation (other than EOS token). Please ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*With the capability of modeling bidirectional contex..."
          ],
          [
           "## Usage tips\n\n- The specific attention pattern can be controlled at training and test time using th..."
          ],
          [
           "## XLNetTokenizerFast\n\n[[autodoc]] XLNetTokenizerFast\n\n## XLNet specific outputs\n\n[[autodoc]] models..."
          ],
          [
           "<frameworkcontent>\n<pt>\n\n## XLNetModel\n\n[[autodoc]] XLNetModel\n    - forward\n\n## XLNetLMHeadModel\n\n[..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<!--End of the generated tip-->\n\n</Tip>\n\nLayoutLMv2 solves the document question-answering task by a..."
          ],
          [
           "```\n\n```bash\npip install 'git+https://github.com/facebookresearch/detectron2.git'\npip install torchv..."
          ],
          [
           "```\n\nAs you can see, the dataset is split into train and test sets already. Take a look at a random ..."
          ],
          [
           "```\n\nNote that the LayoutLMv2 checkpoint that we use in this guide has been trained with `max_positi..."
          ],
          [
           "```\n\nFinally, the data exploration won't be complete if we don't peek at an image example.\n\n```py\n>>..."
          ],
          [
           "```\n\nTo apply this preprocessing to the entire dataset in a fast way, use [`~datasets.Dataset.map`]...."
          ],
          [
           "```\n\nOn top of the preprocessing mentioned above, we also need to add the labels for the model. For ..."
          ],
          [
           "```\n\nTo illustrate how this function finds the position of the answer, let's use it on an example:..."
          ],
          [
           "```py\n>>> example = dataset_with_ocr[\"train\"][1]\n>>> words = [word.lower() for word in example[\"word..."
          ],
          [
           "Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'develo..."
          ],
          [
           "'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from..."
          ],
          [
           "'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', ..."
          ],
          [
           "'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appea..."
          ],
          [
           "'unburned', 'section', 'for', 'future', 'smoking.', 'Â«short', 'cigarette,', 'tobacco', 'section', '3..."
          ],
          [
           "'papers;', 'seasonal', 'promotions,', 'e.g.', 'pastel', 'colored', 'cigarettes', 'for', 'easter', 'o..."
          ],
          [
           "Answer:  T.F. Riehl\nstart_index 17\nend_index 18..."
          ],
          [
           "```\n\nOnce examples are encoded, however, they will look like this:\n\n```py\n>>> encoding = tokenizer(e..."
          ],
          [
           "```\n\nWe'll need to find the position of the answer in the encoded input.\n* `token_type_ids` tells us..."
          ],
          [
           "...         if match:\n...             # if match is found, use `token_type_ids` to find where words ..."
          ],
          [
           "...         else:\n...             start_positions.append(cls_index)\n...             end_positions.ap..."
          ],
          [
           "```\n\nNow that we have this preprocessing function, we can encode the entire dataset:\n\n```py\n>>> enco..."
          ],
          [
           "```\n\n## Evaluation\n\nEvaluation for document question answering requires a significant amount of post..."
          ],
          [
           "```\n\nIn the [`TrainingArguments`] use `output_dir` to specify where to save your model, and configur..."
          ],
          [
           "```\n\nTo add the final model to ðŸ¤— Hub, create a model card and call `push_to_hub`:\n\n```py\n>>> trainer..."
          ],
          [
           "```\n\nYou can also manually replicate the results of the pipeline if you'd like:\n1. Take an image and..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We extract an optimal subset of architectural parame..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among gro..."
          ],
          [
           "## Usage tips\n\n- ALBERT is a model with absolute position embeddings so it's usually advised to pad ..."
          ],
          [
           "- [`TFAlbertForSequenceClassification`] is supported by this [example script](https://github.com/hug..."
          ],
          [
           "<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`AlbertForMaskedLM`] is supported by this [example script](h..."
          ],
          [
           "<PipelineTag pipeline=\"question-answering\"/>\n\n- [`AlbertForQuestionAnswering`] is supported by this ..."
          ],
          [
           "**Multiple choice**\n\n- [`AlbertForMultipleChoice`] is supported by this [example script](https://git..."
          ],
          [
           "## AlbertForPreTraining\n\n[[autodoc]] AlbertForPreTraining\n    - forward\n\n## AlbertForMaskedLM\n\n[[aut..."
          ],
          [
           "## FlaxAlbertModel\n\n[[autodoc]] FlaxAlbertModel\n    - __call__\n\n## FlaxAlbertForPreTraining\n\n[[autod..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[ALBERT](../model_doc/albert), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [BioGpt]..."
          ],
          [
           "[OpenAI GPT-2](../model_doc/gpt2), [GPTBigCode](../model_doc/gpt_bigcode), [GPT Neo](../model_doc/gp..."
          ],
          [
           "[QDQBert](../model_doc/qdqbert), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [..."
          ],
          [
           "<!--End of the generated tip-->\n\n</Tip>\n\nBefore you begin, make sure you have all the necessary libr..."
          ],
          [
           "```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model w..."
          ],
          [
           "```\n\nEach number in `ner_tags` represents an entity. Convert the numbers to their label names to fin..."
          ],
          [
           "```\n\nAs you saw in the example `tokens` field above, it looks like the input has already been tokeni..."
          ],
          [
           "```\n\nHowever, this adds some special tokens `[CLS]` and `[SEP]` and the subword tokenization creates..."
          ],
          [
           "...     labels = []\n...     for i, label in enumerate(examples[f\"ner_tags\"]):\n...         word_ids =..."
          ],
          [
           "```\n\nTo apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [`~datasets.Dataset..."
          ],
          [
           "```\n\nGet the NER labels first, and then create a function that passes your true predictions and true..."
          ],
          [
           "```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your..."
          ],
          [
           "```\n\n<frameworkcontent>\n<pt>\n<Tip>\n\nIf you aren't familiar with finetuning a model with the [`Traine..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n</pt>\n<tf>\n<Tip>\n\nIf you aren't familiar with finetuning a model with Keras, take a look at the ..."
          ],
          [
           "```\n\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr..."
          ],
          [
           "```\n\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n\n```..."
          ],
          [
           "```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ..."
          ],
          [
           "```\n\nPass your inputs to the model and return the `logits`:\n\n```py\n>>> from transformers import Auto..."
          ],
          [
           "```\n\nGet the class with the highest probability, and use the model's `id2label` mapping to convert i..."
          ],
          [
           "## Saved Pseudo-Labels\nThese are the generations of various large models on various large **training..."
          ],
          [
           "### Available Pseudo-labels\n| Dataset | Model                       | Link                          ..."
          ],
          [
           "| CNN/DM  | `sshleifer/pegasus-cnn-ft-v2` | [download](https://cdn-datasets.huggingface.co/pseudo/cn..."
          ],
          [
           "(EN_RO = WMT 2016 English-Romanian).\n\nExample Download Command:\n```bash\ncurl -S https://cdn-datasets..."
          ],
          [
           "```\n### Generating New Pseudolabels\nHere is the command I used to generate the pseudolabels in the s..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "If you would like to list benchmark results on your favorite models of the [model hub](https://huggi..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In the past decade, convolutional neural networks (C..."
          ],
          [
           "## Usage tips\n\n- When fine-tuning the Audio Spectrogram Transformer (AST) on your own dataset, it's ..."
          ],
          [
           "If you're interested in submitting a resource to be included here, please feel free to open a Pull R..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## How to enable Hyperparameter search in example\n\nDefine the hyperparameter search space, diff..."
          ],
          [
           "```\n\nOptuna provides multi-objective HPO. You can pass `direction` in `hyperparameter_search` and de..."
          ],
          [
           "```\n\nFor wandb, see wandb [object_parameter](https://docs.wandb.ai/guides/sweeps/configuration), it'..."
          ],
          [
           "```\n\nCreate a [`Trainer`] with your `model_init` function, training arguments, training and test dat..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Image Transformer has recently achieved significant ..."
          ],
          [
           "```\n\nThis will load the model pre-trained on masked image modeling. Note that this won't include the..."
          ],
          [
           "```\n\nThis particular checkpoint was fine-tuned on [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdi..."
          ],
          [
           "urrently the following model proposals are available:\n\n- <s>[BigBird (Google)](./ADD_BIG_BIRD.md)</s..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We present SegFormer, a simple, efficient yet powerf..."
          ],
          [
           "- SegFormer consists of a hierarchical Transformer encoder, and a lightweight all-MLP decoder head.\n..."
          ],
          [
           "- One can also check out [this interactive demo on Hugging Face Spaces](https://huggingface.co/space..."
          ],
          [
           "used by [`SegformerForSemanticSegmentation`]). However, other datasets use the 0 index as\n  backgrou..."
          ],
          [
           "| **Model variant** | **Depths**    | **Hidden sizes**    | **Decoder hidden size** | **Params (M)**..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "## SegformerConfig\n\n[[autodoc]] SegformerConfig\n\n## SegformerFeatureExtractor\n\n[[autodoc]] Segformer..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Motivated by the success of T5 (Text-To-Text Transfe..."
          ],
          [
           "## SpeechT5FeatureExtractor\n\n[[autodoc]] SpeechT5FeatureExtractor\n    - __call__\n\n## SpeechT5Process..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nNow, let's load an image.\n\n```python\nfrom PIL import Image\nimport requests\n\nurl = \"https://hugg..."
          ],
          [
           "```\n\n`pipeline` abstracts away the preprocessing and postprocessing steps that we have to do ourselv..."
          ],
          [
           "```\n\nWe need to squeeze the output and get rid of axis 0, clip the values, then convert it to be num..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Entity representations are useful in natural languag..."
          ],
          [
           "## Usage tips\n\n- This implementation is the same as [`RobertaModel`] with the addition of entity emb..."
          ],
          [
           "- There are three head models for the former use case:\n\n  - [`LukeForEntityClassification`], for tas..."
          ],
          [
           ">>> text = \"BeyoncÃ© lives in Los Angeles.\"\n>>> entity_spans = [(0, 7)]  # character-based entity spa..."
          ],
          [
           ">>> model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacre..."
          ],
          [
           "```\n\n## Resources\n\n- [A demo notebook on how to fine-tune [`LukeForEntityPairClassification`] for re..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[BEiT](../model_doc/beit), [BiT](../model_doc/bit), [ConvNeXT](../model_doc/convnext), [ConvNeXTV2](..."
          ],
          [
           "[PVT](../model_doc/pvt), [RegNet](../model_doc/regnet), [ResNet](../model_doc/resnet), [SegFormer](...."
          ],
          [
           "```\n\nWe encourage you to log in to your Hugging Face account to upload and share your model with the..."
          ],
          [
           "```\n\nNow you can convert the label id to a label name:\n\n```py\n>>> id2label[str(79)]\n'prime_rib'\n```\n..."
          ],
          [
           "```\n\nTo apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [`~datasets.Dataset..."
          ],
          [
           "```\n</pt>\n</frameworkcontent>\n\n\n<frameworkcontent>\n<tf>\n\nTo avoid overfitting and to make the model ..."
          ],
          [
           "```\n\nNext, create functions to apply appropriate transformations to a batch of images, instead of on..."
          ],
          [
           "```\n\nAs a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike o..."
          ],
          [
           "```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you set up you..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           ">>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     data_collator=data_c..."
          ],
          [
           "```\n\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_t..."
          ],
          [
           "```\n\nThen, load ViT with [`TFAutoModelForImageClassification`] along with the label mappings:\n\n```py..."
          ],
          [
           "```\n\nTo compute the accuracy from the predictions and push your model to the ðŸ¤— Hub, use [Keras callb..."
          ],
          [
           "```\n\nFinally, you are ready to train your model! Call `fit()` with your training and validation data..."
          ],
          [
           "```\n\nCongratulations! You have fine-tuned your model and shared it on the ðŸ¤— Hub. You can now use it ..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nPass your inputs to the model and return the logits:\n\n```py\n>>> from transformers import AutoMo..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage tips\n\n- The model usually performs well without requiring any finetuning.\n- The architectur..."
          ],
          [
           "```\nThe script will automatically determine all necessary parameters from the OpenAI checkpoint. A `..."
          ],
          [
           "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help ..."
          ],
          [
           "```\n\n## WhisperConfig\n\n[[autodoc]] WhisperConfig\n\n## WhisperTokenizer\n\n[[autodoc]] WhisperTokenizer\n..."
          ],
          [
           "## TFWhisperForConditionalGeneration\n\n[[autodoc]] TFWhisperForConditionalGeneration\n    - call\n\n</tf..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```python\n>>> from transformers import GPTJForCausalLM\n>>> import torch\n\n>>> device = \"cuda\"\n>>> mod..."
          ],
          [
           "```\n\n- The model should fit on 16GB GPU for inference. For training/fine-tuning it would take much m..."
          ],
          [
           "```\n\n...or in float16 precision:\n\n```python\n>>> from transformers import GPTJForCausalLM, AutoTokeni..."
          ],
          [
           "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help ..."
          ],
          [
           "- Description of [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B).\n- A blog on how to [Deploy GPT..."
          ],
          [
           "- [`GPTJForCausalLM`] is supported by this [causal language modeling example script](https://github...."
          ],
          [
           "**Documentation resources**\n- [Text classification task guide](../tasks/sequence_classification)\n- [..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThe `outputs` object is a [`~modeling_outputs.SequenceClassifierOutput`], as we can see in the\n..."
          ],
          [
           "```\n\nwill return the tuple `(outputs.loss, outputs.logits)` for instance.\n\nWhen considering our `out..."
          ],
          [
           "## CausalLMOutputWithCrossAttentions\n\n[[autodoc]] modeling_outputs.CausalLMOutputWithCrossAttentions..."
          ],
          [
           "[[autodoc]] modeling_outputs.SemanticSegmenterOutput\n\n## ImageClassifierOutput\n\n[[autodoc]] modeling..."
          ],
          [
           "## TFBaseModelOutputWithPast\n\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPast\n\n## TFBaseMo..."
          ],
          [
           "## TFSeq2SeqSequenceClassifierOutput\n\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqSequenceClassifierOut..."
          ],
          [
           "[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqModelOutput\n\n## FlaxCausalLMOutputWithCrossAttentions\n\n..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the..."
          ],
          [
           "<PipelineTag pipeline=\"text-classification\"/>\n\n- A blog post on how to [Accelerate Large Model Train..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\" />\n\n- [`DebertaForTokenClassification`] is supported by..."
          ],
          [
           "<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`DebertaForMaskedLM`] is supported by this [example script](..."
          ],
          [
           "<PipelineTag pipeline=\"question-answering\"/>\n\n- [`DebertaForQuestionAnswering`] is supported by this..."
          ],
          [
           "## DebertaPreTrainedModel\n\n[[autodoc]] DebertaPreTrainedModel\n\n## DebertaForMaskedLM\n\n[[autodoc]] De..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We introduce a state-of-the-art real-time, high-fide..."
          ],
          [
           ">>> model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n>>> processor = AutoProcessor.fro..."
          ],
          [
           "```\n\n## EncodecConfig\n\n[[autodoc]] EncodecConfig\n\n## EncodecFeatureExtractor\n\n[[autodoc]] EncodecFea..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "emb = jnp.zeros((50264, model.config.hidden_size))\n# update the first 50257 weights using pre-traine..."
          ],
          [
           "```\n\n\n### Train Model\n\n```bash\npython run_clm_mp.py \\\n    --model_name_or_path gpt-neo-1.3B  \\\n    -..."
          ],
          [
           "VisualBERT Demo\n\nThis demo shows usage of VisualBERT VQA model and is adapted from LXMERT demo prese..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can..."
          ],
          [
           "The [`BertTokenizer`] is used to encode the text. A custom detector/image processor must be used\nto ..."
          ],
          [
           ">>> visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n>>> visual_attent..."
          ],
          [
           "```\n\n## VisualBertConfig\n\n[[autodoc]] VisualBertConfig\n\n## VisualBertModel\n\n[[autodoc]] VisualBertMo..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformers have emerged as a powerful tool for a b..."
          ],
          [
           "## NystromformerModel\n\n[[autodoc]] NystromformerModel\n    - forward\n\n## NystromformerForMaskedLM\n\n[[..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We introduce the Segment Anything (SA) project: a ne..."
          ],
          [
           "Below is an example on how to run mask generation given an image and a 2D point:\n\n```python\nimport t..."
          ],
          [
           "```\n\nResources:\n\n- [Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/segme..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers..."
          ],
          [
           "```\n\nTo use another vision backbone, like [ConvNeXt](convnext), simply instantiate the model with th..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformers have shown great potential in computer ..."
          ],
          [
           "This model was contributed by [heytanay](https://huggingface.co/heytanay). The original code can be ..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!---\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\n\nLicense..."
          ],
          [
           "```\n\nor directly on the hub under *Training metrics*.\n\nsample Metrics - [tfhub.dev](https://tensorbo..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nWhen you run `accelerate config`, you'll be prompted with a series of options to configure your..."
          ],
          [
           "### Wrapping policy\n\nFSDP is applied by wrapping each layer in the network. The wrapping is usually ..."
          ],
          [
           "```\n\nHowever, when training ends, you want to save the full state dict because sharded state dict is..."
          ],
          [
           "```\n\nThe [`xla_fsdp_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273d..."
          ],
          [
           "```\n\n```bash\naccelerate launch --fsdp=\"full shard\" --fsdp_config=\"path/to/fsdp_config/ my-trainer-sc..."
          ],
          [
           "Testing new Hugging Face Deep Learning Container.\n\nThis document explains the testing strategy for r..."
          ],
          [
           "```\nThese tests take around 10-15 minutes to finish. Preferably make a screenshot of the successfull..."
          ],
          [
           "repository_info:\n  training_repository: &TRAINING_REPOSITORY\n    image_type: &TRAINING_IMAGE_TYPE tr..."
          ],
          [
           "```\n2. In the PR comment describe what test, we ran and with which package versions. Here you can co..."
          ],
          [
           "```\nThese tests take around 10-15 minutes to finish. Preferably make a screenshot of the successfull..."
          ],
          [
           "repository_info:\n  training_repository: &TRAINING_REPOSITORY\n    image_type: &TRAINING_IMAGE_TYPE tr..."
          ],
          [
           "```\n2. In the PR comment describe what test we ran and with which framework versions. Here you can c..."
          ],
          [
           "| ID                                  | Description                                                 ..."
          ],
          [
           "| tensorflow-transfromers-test-single | Test bert finetuning using BERT from transformer lib+TF     ..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten)\n\nThe origi..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Converting custom checkpoints \n\n<Tip>\n\nFalcon models were initially added to the Hugging Face Hub..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Depth estimation from a single image is an important..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pre-trained representations are becoming crucial for..."
          ],
          [
           "[`AlignProcessor`] wraps [`EfficientNetImageProcessor`] and [`BertTokenizer`] into a single instance..."
          ],
          [
           "```\n\n## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The library was designed with two strong goals in mind:\n\n1. Be as easy and fast to use as possible:\n..."
          ],
          [
           "2. Provide state-of-the-art models with performances as close as possible to the original models:\n\n ..."
          ],
          [
           "## Main concepts\n\nThe library is built around three types of classes for each model:\n\n- **Model clas..."
          ],
          [
           "All these classes can be instantiated from pretrained instances, saved locally, and shared on the Hu..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Natural Language Processing\n\n<Youtube id=\"Yffk5aydLzg\"/>\n\nThe main tool for preprocessing te..."
          ],
          [
           "```\n\nThe tokenizer returns a dictionary with three important items:\n\n* [input_ids](glossary#input-id..."
          ],
          [
           "```\n\nAs you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separa..."
          ],
          [
           "```\n\n### Pad\n\nSentences aren't always the same length which can be an issue because tensors, the mod..."
          ],
          [
           "```\n\nThe first and third sentences are now padded with `0`'s because they are shorter.\n\n### Truncati..."
          ],
          [
           "Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by th..."
          ],
          [
           "```\n\n<Tip>\n\nCheck out the [Padding and truncation](./pad_truncation) concept guide to learn more dif..."
          ],
          [
           "Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow:\n\n<frameworkco..."
          ],
          [
           "```\n</pt>\n<tf>\n```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Do..."
          ],
          [
           "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n 'attention_mask': <tf.Tensor: shape=..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n## Audio\n\nFor audio tasks, you'll need a [feature extractor](main_cla..."
          ],
          [
           "```\n\nThis returns three items:\n\n* `array` is the speech signal loaded - and potentially resampled - ..."
          ],
          [
           "```\n\nNext, load a feature extractor to normalize and pad the input. When padding textual data, a `0`..."
          ],
          [
           "```\n\nCreate a function to preprocess the dataset so the audio samples are the same lengths. Specify ..."
          ],
          [
           "```\n\n## Computer vision\n\nFor computer vision tasks, you'll need an [image processor](main_classes/im..."
          ],
          [
           "```\n\nNext, take a look at the image with ðŸ¤— Datasets [`Image`](https://huggingface.co/docs/datasets/p..."
          ],
          [
           "```\n\nFirst, let's add some image augmentation. You can use any library you prefer, but in this tutor..."
          ],
          [
           "```\n\n2. The model accepts [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoder..."
          ],
          [
           "```\n\nHere is what the image looks like after the transforms are applied. The image has been randomly..."
          ],
          [
           "```\n\n## Multimodal\n\nFor tasks involving multimodal inputs, you'll need a [processor](main_classes/pr..."
          ],
          [
           "```\n\nNow take a look at the `audio` and `text` columns:\n\n```py\n>>> lj_speech[0][\"audio\"]\n{'array': a..."
          ],
          [
           "```\n\n1. Create a function to process the audio data contained in `array` to `input_values`, and toke..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This guide covers the prompt engineering best practices to help you craft better LLM prompts and sol..."
          ],
          [
           "When using a pipeline to generate text with an LLM, it's important to know what type of LLM you are ..."
          ],
          [
           "```\n\nTo run inference with an encoder-decoder, use the `text2text-generation` pipeline:\n\n```python\n>..."
          ],
          [
           "```\n\nNext, let's load the model with the appropriate pipeline (`\"text-generation\"`): \n\n```python\n>>>..."
          ],
          [
           "```\n\n<Tip>\n\nNote that Falcon models were trained using the `bfloat16` datatype, so we recommend you ..."
          ],
          [
           "```\n\nAs a result, the output contains a classification label from the list we have provided in the i..."
          ],
          [
           "```\n\nAs you can see, the model correctly identified two named entities from the given text.\n\n#### Tr..."
          ],
          [
           "```\n\nHere we've added a `do_sample=True` and `top_k=10` to allow the model to be a bit more flexible..."
          ],
          [
           "```\n\n#### Question answering\n\nFor question answering task we can structure the prompt into the follo..."
          ],
          [
           "```\n\n#### Reasoning\n\nReasoning is one of the most difficult tasks for LLMs, and achieving good resul..."
          ],
          [
           "```\n\nThis is a wrong answer, it should be 12. In this case, this can be due to the prompt being too ..."
          ],
          [
           "In few-shot prompting, we provide examples in the prompt giving the model more context to improve th..."
          ],
          [
           "```\n\nIn the above code snippet we used a single example to demonstrate the desired output to the mod..."
          ],
          [
           "```\n\n## Prompting vs fine-tuning\n\nYou can achieve great results by optimizing your prompts, however,..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Several TensorFlow methods in ðŸ¤— Transformers have been rewritten to be XLA-compatible, including tex..."
          ],
          [
           "```\n\nThe above model accepts inputs having a dimension of `(10, )`. We can use the model for running..."
          ],
          [
           "```\n\nAnd then you can run the following code:\n\n```py\nimport tensorflow as tf\nfrom transformers impor..."
          ],
          [
           "```\n\nAs you can notice, enabling XLA on `generate()` is just a single line of code. The rest of the ..."
          ],
          [
           "xla_generate = tf.function(model.generate, jit_compile=True)\n\n# Here, we call the tokenizer with pad..."
          ],
          [
           "```\n\nThis way, you can ensure that the inputs to `xla_generate()` will always receive inputs with th..."
          ],
          [
           "```\nThe first call to `xla_generate()` is time-consuming because of tracing, but the successive call..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Ve..."
          ],
          [
           "| Task | Example datasets | Trainer support | ðŸ¤— Accelerate | ðŸ¤— Datasets | Colab\n|---|---|:---:|:---:..."
          ],
          [
           "| [**`question-answering`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/..."
          ],
          [
           "| [**`text-generation`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/tex..."
          ],
          [
           "| [**`speech-recognition`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/..."
          ],
          [
           "| [**`image-pretraining`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/i..."
          ],
          [
           "## Running quick tests\n\nMost examples are equipped with a mechanism to truncate the number of datase..."
          ],
          [
           "```\nexamples/pytorch/token-classification/run_ner.py \\\n--max_train_samples 50 \\\n--max_eval_samples 5..."
          ],
          [
           "```\n\n## Resuming training\n\nYou can resume training from a previous checkpoint like this:\n\n1. Pass `-..."
          ],
          [
           "A few notes on this integration:\n\n- you will need to be logged in to the Hugging Face website locall..."
          ],
          [
           "```\n\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) o..."
          ],
          [
           "```\n\nIf you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you c..."
          ],
          [
           "```\n\nAs an example, here is how you would fine-tune the BERT large model (with whole word masking) o..."
          ],
          [
           "```\n\nthat will check everything is ready for training. Finally, you can launch training with\n\n```bas..."
          ],
          [
           "```\n\nIf you are in Jupyter or Colab, you should login with:\n\n```python\nimport wandb\nwandb.login()\n``..."
          ],
          [
           "```\n\n`conda`:\n\n```bash\nconda install -c conda-forge neptune\n```\n\nNext, in your model training script..."
          ],
          [
           "```\n\nNow, when you start the training with `trainer.train()`, your metadata will be logged in Neptun..."
          ],
          [
           "```\n\n\nTo enable logging to ClearML, include `\"clearml\"` in the `report_to` of your `TrainingArgument..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "### Power and Cooling\n\nIf you bought an expensive high end GPU make sure you give it the correct pow..."
          ],
          [
           "Next let's have a look at one of the most important aspects when having multiple GPUs: connectivity...."
          ],
          [
           "```\nnvidia-smi topo -m\n```\n\nand it will tell you how the GPUs are inter-connected. On a machine with..."
          ],
          [
           "```\n\nSo the first report `NV2` tells us the GPUs are interconnected with 2 NVLinks, and the second r..."
          ],
          [
           "So the higher `X` you get in the report of `NVX` in the output of `nvidia-smi topo -m` the better. T..."
          ],
          [
           "{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}\n\n# DDP w/o NVLink\n\nrm ..."
          ],
          [
           "```\n\nHardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks (`NV2` in `nvidia-smi topo -m`)\nSoftwa..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Modern approaches typically formulate semantic segme..."
          ],
          [
           "## Usage tips\n\n-  MaskFormer's Transformer decoder is identical to the decoder of [DETR](detr). Duri..."
          ],
          [
           "## MaskFormer specific outputs\n\n[[autodoc]] models.maskformer.modeling_maskformer.MaskFormerModelOut..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*While the Transformer architecture has become the de..."
          ],
          [
           "## ViTHybridConfig\n\n[[autodoc]] ViTHybridConfig\n\n## ViTHybridImageProcessor\n\n[[autodoc]] ViTHybridIm..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Self-supervised pre-training techniques have achieve..."
          ],
          [
           "## Usage tips\n\n- In terms of data processing, LayoutLMv3 is identical to its predecessor [LayoutLMv2..."
          ],
          [
           "<PipelineTag pipeline=\"text-classification\"/>\n\n- [`LayoutLMv2ForSequenceClassification`] is supporte..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\"/>\n\n- [`LayoutLMv3ForTokenClassification`] is supported ..."
          ],
          [
           "<PipelineTag pipeline=\"question-answering\"/>\n\n- [`LayoutLMv2ForQuestionAnswering`] is supported by t..."
          ],
          [
           "## LayoutLMv3ForTokenClassification\n\n[[autodoc]] LayoutLMv3ForTokenClassification\n    - forward\n\n## ..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Large-scale autoregressive language models such as G..."
          ],
          [
           "## XGLMConfig\n\n[[autodoc]] XGLMConfig\n\n## XGLMTokenizer\n\n[[autodoc]] XGLMTokenizer\n    - build_input..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This is a machine translation model that supports many low-resource languages, and that is competiti..."
          ],
          [
           "```\n\nGoogle has released the following variants:\n\n- [google/madlad400-3b-mt](https://huggingface.co/..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```python\nfrom transformers import Pipeline\n\n\nclass MyPipeline(Pipeline):\n    def _sanitize_paramete..."
          ],
          [
           "```\n\nThe structure of this breakdown is to support relatively seamless support for CPU/GPU, while su..."
          ],
          [
           "A classic example would be a `top_k` argument in the post processing in classification tasks.\n\n```py..."
          ],
          [
           "```\n\nIn order to achieve that, we'll update our `postprocess` method with a default parameter to `5`..."
          ],
          [
           "```\n\nYou can specify a default model if you want, in which case it should come with a specific revis..."
          ],
          [
           "```\n\n## Share your pipeline on the Hub\n\nTo share your custom pipeline on the Hub, you just have to s..."
          ],
          [
           "```\n\nThe implementation is framework agnostic, and will work for PyTorch and TensorFlow models. If w..."
          ],
          [
           "```\n\n## Add the pipeline to ðŸ¤— Transformers\n\nIf you want to contribute your pipeline to ðŸ¤— Transformer..."
          ],
          [
           "Training a masked language model end-to-end from scratch on TPUs\n\nIn this example, we're going to de..."
          ],
          [
           "## Setting up a TPU-VM\n\nSince this example focuses on using TPUs, the first step is to set up access..."
          ],
          [
           "```\n\nThe script will automatically load the `train` split of the WikiText dataset and train a [Unigr..."
          ],
          [
           "```\n\n**Notes**:\n\n* While running the above script, you need to specify the `split` accordingly. The ..."
          ],
          [
           "```\n\nIf you had specified a `hub_model_id` while launching training, then your model will be pushed ..."
          ],
          [
           "```\n\nYou can also try out inference using the [Inference Widget](https://huggingface.co/tf-tpu/rober..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Load the PokÃ©mon BLIP captions dataset\n\nUse the ðŸ¤— Dataset library to load a dataset that con..."
          ],
          [
           "```\n\nLet's visualize a couple of samples from the training set. \n\n\n```python\nfrom textwrap import wr..."
          ],
          [
           "```\n\nThe processor will internally pre-process the image (which includes resizing, and pixel scaling..."
          ],
          [
           "```\n\n## Evaluate\n\nImage captioning models are typically evaluated with the [Rouge Score](https://hug..."
          ],
          [
           "```\n\n## Train!\n\nNow, you are ready to start fine-tuning the model. You will use the ðŸ¤— [`Trainer`] fo..."
          ],
          [
           "```\n\n## Inference\n\nTake a sample image from `test_ds` to test the model.\n\n\n```python\nfrom PIL import..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved.\n\nLicensed under the MIT L..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The Transformer architecture has become a dominant c..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Can Transformer perform 2D object- and region-level ..."
          ],
          [
           "If you're interested in submitting a resource to be included here, please feel free to open a Pull R..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Overview\n\nThe ELECTRA model was proposed in the paper [ELECTRA: Pre-training Text Encoders as Dis..."
          ],
          [
           "This model was contributed by [lysandre](https://huggingface.co/lysandre). The original code can be ..."
          ],
          [
           "## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classifi..."
          ],
          [
           "## ElectraForTokenClassification\n\n[[autodoc]] ElectraForTokenClassification\n    - forward\n\n## Electr..."
          ],
          [
           "[[autodoc]] FlaxElectraForCausalLM\n    - __call__\n\n## FlaxElectraForMaskedLM\n\n[[autodoc]] FlaxElectr..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In this work, we present a conceptually simple and e..."
          ],
          [
           "The [`AltCLIPProcessor`] wraps a [`CLIPImageProcessor`] and a [`XLMRobertaTokenizer`] into a single ..."
          ],
          [
           "```\n\n<Tip>\n\nThis model is based on `CLIPModel`, use it like you would use the original [CLIP](clip)...."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*While existing large vision-language multimodal mode..."
          ],
          [
           "```\n\nFor multiple turns conversation:\n\n```bash\nA chat between a curious human and an artificial inte..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nOnce the installation is done, you can use the CLI command `add-new-model` to generate your mod..."
          ],
          [
           "```\n-->\n\nOnce the command has finished, you should have a total of 7 new files spread across the rep..."
          ],
          [
           "```\n\nFeel free to modify each file to mimic the behavior of your model. \n\nâš  You should be careful ab..."
          ],
          [
           "```\n\nThis will start a small questionnaire you have to fill.\n\n```\nWhat identifier would you like to ..."
          ],
          [
           "```\nWill your new model use the same processing class as Xxx (XxxTokenizer/XxxFeatureExtractor/XxxIm..."
          ],
          [
           "```\npython -m pytest ./tests/test_*<model_name>*.py\n```\n\nâš  You should be careful about the classes p..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmu..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We introduce Kosmos-2, a Multimodal Large Language M..."
          ],
          [
           "## Example\n\n```python\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import Aut..."
          ],
          [
           ">>> caption, entities = processor.post_process_generation(generated_text)\n>>> caption\n'An image of a..."
          ],
          [
           "```\n\nThis model was contributed by [Yih-Dar SHIEH](https://huggingface.co/ydshieh). The original cod..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "- information extraction from scanned documents: the [FUNSD](https://guillaumejaume.github.io/FUNSD/..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Pre-training of text and layout has proved effective..."
          ],
          [
           "```\npython -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\npython -m pip ins..."
          ],
          [
           "```\n(If you are developing for LayoutLMv2, note that passing the doctests also requires the installa..."
          ],
          [
           "- The main difference between LayoutLMv1 and LayoutLMv2 is that the latter incorporates visual embed..."
          ],
          [
           "in Detectron2 are pre-trained using the BGR format. The `bbox` input are the bounding boxes (i.e. 2D..."
          ],
          [
           "```\n\nHere, `width` and `height` correspond to the width and height of the original document in which..."
          ],
          [
           "```\n\nHowever, this model includes a brand new [`~transformers.LayoutLMv2Processor`] which can be use..."
          ],
          [
           "## Resources\n\nA list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you g..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\"/>\n\n- A notebook on how to [finetune LayoutLMv2 for toke..."
          ],
          [
           "```\n\nIn short, one can provide a document image (and possibly additional data) to [`LayoutLMv2Proces..."
          ],
          [
           "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n\nimage = Image...."
          ],
          [
           "```\n\n**Use case 2: document image classification (training, inference) + token classification (infer..."
          ],
          [
           "```\n\n**Use case 3: token classification (training), apply_ocr=False**\n\nFor token classification task..."
          ],
          [
           "```\n\n**Use case 4: visual question answering (inference), apply_ocr=True**\n\nFor visual question answ..."
          ],
          [
           "```\n\n**Use case 5: visual question answering (inference), apply_ocr=False**\n\nFor visual question ans..."
          ],
          [
           "```\n\n## LayoutLMv2Config\n\n[[autodoc]] LayoutLMv2Config\n\n## LayoutLMv2FeatureExtractor\n\n[[autodoc]] L..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\n**Note:** This script only works with models that have a fast tokenizer (backed by the ðŸ¤— Tokeni..."
          ],
          [
           "```\n\nthen\n\n```bash\nexport TASK_NAME=ner\n\npython run_ner_no_trainer.py \\\n  --model_name_or_path bert-..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper:\n\n\n*Inductive transfer learning, enabled by self-supervised learning, have..."
          ],
          [
           "## BarthezTokenizer\n\n[[autodoc]] BarthezTokenizer\n\n## BarthezTokenizerFast\n\n[[autodoc]] BarthezToken..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In recent years, a series of Transformer-based model..."
          ],
          [
           ">>> # HerBERT can also be loaded using AutoTokenizer and AutoModel:\n>>> import torch\n>>> from transf..."
          ],
          [
           "```\n\n<Tip>\n\nHerbert implementation is the same as `BERT` except for the tokenization method. Refer t..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [anton-l](https://huggingface.co/anton-l).\n\n## Usage tips\n\n- SEW is a ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "- accessing all the hidden-states of BERT/GPT/GPT-2,\n- accessing all the attention weights for each ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformer-based models are unable to process long ..."
          ],
          [
           "## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" con..."
          ],
          [
           "For more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n..."
          ],
          [
           "```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token cla..."
          ],
          [
           "[[autodoc]] models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutputWithPooling\n\n[[autod..."
          ],
          [
           "## TFLongformerModel\n\n[[autodoc]] TFLongformerModel\n    - call\n\n## TFLongformerForMaskedLM\n\n[[autodo..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\n\nLicense..."
          ],
          [
           "```\n\nor directly on the hub under *Training metrics*.\n\nTraining with the previously defined hyper-pa..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*In this work, we present the Textless Vision-Languag..."
          ],
          [
           "## Usage tips\n\n- TVLT is a model that takes both `pixel_values` and `audio_values` as input. One can..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Overview\n\nThe DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: ..."
          ],
          [
           "This model was contributed by [victorsanh](https://huggingface.co/victorsanh). This model jax versio..."
          ],
          [
           "- A blog post on [Getting Started with Sentiment Analysis using Python](https://huggingface.co/blog/..."
          ],
          [
           "- [`TFDistilBertForSequenceClassification`] is supported by this [example script](https://github.com..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\"/>\n\n- [`DistilBertForTokenClassification`] is supported ..."
          ],
          [
           "<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`DistilBertForMaskedLM`] is supported by this [example scrip..."
          ],
          [
           "<PipelineTag pipeline=\"question-answering\"/>\n\n- [`DistilBertForQuestionAnswering`] is supported by t..."
          ],
          [
           "**Multiple choice**\n- [`DistilBertForMultipleChoice`] is supported by this [example script](https://..."
          ],
          [
           "ðŸš€ Deploy\n\n- A blog post on how to [deploy DistilBERT on Google Cloud](https://huggingface.co/blog/ho..."
          ],
          [
           "```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab..."
          ],
          [
           "```\n\n\n## DistilBertConfig\n\n[[autodoc]] DistilBertConfig\n\n## DistilBertTokenizer\n\n[[autodoc]] DistilB..."
          ],
          [
           "[[autodoc]] TFDistilBertForMultipleChoice\n    - call\n\n## TFDistilBertForTokenClassification\n\n[[autod..."
          ],
          [
           "!---\nCopyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Vers..."
          ],
          [
           "The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, e..."
          ],
          [
           "At the time of writing this guide, LLMs consist of at least a couple billion parameters. Each parame..."
          ],
          [
           "To give some examples of how much VRAM it roughly takes to load a model in bfloat16:\n\n-   **GPT3** r..."
          ],
          [
           "ðŸ¤— Transformers does not support tensor parallelism out of the box as it requires the model architect..."
          ],
          [
           "```\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretr..."
          ],
          [
           "```\n\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be q..."
          ],
          [
           "for every matrix multiplication. Dequantization and re-quantization is performed sequentially for al..."
          ],
          [
           "```\n\nOverall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32..."
          ],
          [
           "with \\\\( s^a_{ij} \\\\) and \\\\( s^b_{ij} \\\\) being some softmax normalization statistics that need to ..."
          ],
          [
           "```\nFor demonstration purposes, we duplicate the system prompt by ten so that the input length is lo..."
          ],
          [
           "```\n\nWe're getting the same output as before, however this time, the model repeats the answer multip..."
          ],
          [
           "```\n\nFor more information on how to use Flash Attention, please have a look at [this doc page](https..."
          ],
          [
           "![](/blog/assets/163_optimize_llm/self_attn_tokens.png)\n\nEach word token is given a probability mass..."
          ],
          [
           "The authors of the [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) paper introduced ..."
          ],
          [
           "Sinusoidal and learned position embeddings used to be the predominant methods to encode sentence ord..."
          ],
          [
           "```\n\n**Output**:\n```\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 20\nshape of inp..."
          ],
          [
           "```\n\nAs one can see, when using the key-value cache the text input tokens are *not* increased in len..."
          ],
          [
           "```\n\nIn this chat, the LLM runs auto-regressive decoding twice:\n  1. The first time, the key-value c..."
          ],
          [
           "```python\n# Generation as usual\nprompt = system_prompt + \"Question: Please write a function in Pytho..."
          ],
          [
           "```\n\n**Output**:\n```\n is a modified version of the function that returns Mega bytes instead.\n\ndef by..."
          ],
          [
           "```\n\nRoughly 8 billion float values! Storing 8 billion float values in `float16` precision requires ..."
          ],
          [
           "In addition to memory savings, MQA also leads to improved computational efficiency as explained in t..."
          ],
          [
           "Also, the checkpoint used in this notebook - `bigcode/octocoder` - makes use of MQA.\n\n#### 3.2.3 Gro..."
          ],
          [
           "## Conclusion\n\nThe research community is constantly coming up with new, nifty ways to speed up infer..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Transformer-based models are unable to process long ..."
          ],
          [
           "## Usage tips\n\n- [`LEDForConditionalGeneration`] is an extension of\n  [`BartForConditionalGeneration..."
          ],
          [
           "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n\n## Resou..."
          ],
          [
           "[[autodoc]] models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput\n\n[[autodoc]] models.led.m..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper states the following:\n\n*Visual language such as charts and plots is ubiqui..."
          ],
          [
           "## Usage example\n\nCurrently one checkpoint is available for DePlot:\n\n- `google/deplot`: DePlot fine-..."
          ],
          [
           "```\n\n## Fine-tuning\n\nTo fine-tune DePlot, refer to the pix2struct [fine-tuning notebook](https://git..."
          ],
          [
           "*NOTE**: This example is outdated and is not longer actively maintained. Please \nfollow the new inst..."
          ],
          [
           "```bash\n#!/usr/bin/env bash\npython run_asr.py \\\n--output_dir=\"./wav2vec2-base-timit-asr\" \\\n--num_tra..."
          ],
          [
           "```\n\nThe resulting model and inference examples can be found [here](https://huggingface.co/elgeish/w..."
          ],
          [
           "```\n\nThe instance above is used as follows:\n* creates a tokenizer with `do_lower_case=True` (ignores..."
          ],
          [
           "```bash\n#!/usr/bin/env bash\npython run_asr.py \\\n--output_dir=\"./wav2vec2-large-xlsr-53-arabic-speech..."
          ],
          [
           "```\n\nFirst, let's understand how this dataset represents Arabic text; it uses a format called\n[Buckw..."
          ],
          [
           "```\n\nThe instance above is used as follows:\n* creates a tokenizer with Buckwalter vocabulary and `wo..."
          ],
          [
           "```\nPYTHONPATH=../../../src deepspeed --num_gpus 2 \\\nrun_asr.py \\\n--output_dir=output_dir --num_trai..."
          ],
          [
           "```\n    \"zero_optimization\": {\n        ...\n        \"find_unused_parameters\": true,\n        ...\n    }..."
          ],
          [
           "```\nPYTHONPATH=../../../src deepspeed --num_gpus 4 run_pretrain.py \\\n--output_dir=\"./wav2vec2-base-l..."
          ],
          [
           "```\n\n\n### Forced Alignment\n\nCharacter level forced alignment for audio and text pairs with wav2vec2 ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Multilingual pre-trained models are known to suffer ..."
          ],
          [
           "### Input language\n\nThere are two ways to specify the input language:\n1. By setting a default langua..."
          ],
          [
           "```\n\n2. By explicitly passing the index of the language adapter for each sample:\n\n```python\nimport t..."
          ],
          [
           "```\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token cla..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n\nAll the methods of this logging module are documented below, the main ones are\n[`logging.get_v..."
          ],
          [
           "See reference of the `captureWarnings` method below.\n\n[[autodoc]] logging.captureWarnings\n\n## Base s..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "- [`DefaultFlowCallback`] which handles the default behavior for logging, saving and evaluation.\n- [..."
          ],
          [
           "If a package is installed but you don't wish to use the accompanying integration, you can change `Tr..."
          ],
          [
           "## TrainerCallback\n\n[[autodoc]] TrainerCallback\n\nHere is an example of how to register a custom call..."
          ],
          [
           "```\n\nAnother way to register a callback is to call `trainer.add_callback()` as follows:\n\n```python\nt..."
          ],
          [
           "ow to add BigBird to ðŸ¤— Transformers?\n=====================================\n\nMentor: [Patrick](https:..."
          ],
          [
           "A good first starting point to better understand the library is to read\nthe [documentation of our ph..."
          ],
          [
           "7.  [ ] Successfully ran forward pass in Transformers that gives\n    identical output to original ch..."
          ],
          [
           "-   What type of model is *BigBird*? BERT-like encoder-only\n    model? GPT2-like decoder-only model?..."
          ],
          [
           "#### Make sure you've understood the fundamental aspects of BigBird\n\nAlright, now you should be read..."
          ],
          [
           "2.  Clone your `transformers` fork to your local disk, and add the base\n    repository as a remote:\n..."
          ],
          [
           "```\n\n3.  Set up a development environment, for instance by running the\n    following command:\n\n    `..."
          ],
          [
           "```\n\nNow you have set up a development environment to port *BigBird*\nto ðŸ¤— Transformers.\n\n### Run a p..."
          ],
          [
           "Jupyter notebooks have the advantage that they allow for cell-by-cell\nexecution which can be helpful..."
          ],
          [
           "However, if the original code-base is very complex or only allows\nintermediate components to be run ..."
          ],
          [
           "```\n\nWe expect that every model added to ðŸ¤— Transformers passes a couple of\nintegration tests, meanin..."
          ],
          [
           "#### (Important) More details on how to create a debugging environment for BigBird \n\n- BigBird has m..."
          ],
          [
           "Otherwise, let's start generating a new model with the amazing\nCookiecutter!\n\n**Use the Cookiecutter..."
          ],
          [
           "```\n    git checkout -b add_big_bird\n```\n\n2.  Commit the automatically generated code:\n\n```\n    git ..."
          ],
          [
           "```\n\n5.  Once you are satisfied, go to the webpage of your fork on GitHub.\n    Click on \"Pull reques..."
          ],
          [
           "Now you can finally start coding :). The generated code in\n`src/transformers/models/big_bird/modelin..."
          ],
          [
           "```\n\nThe above command will create a model according to the default\nparameters as defined in `BigBir..."
          ],
          [
           "-   A good starting point to convert the original TF BigBird implementation to the PT Hugging Face i..."
          ],
          [
           "```\n\nIf either the shape or the name doesn't match, you probably assigned\nthe wrong checkpoint weigh..."
          ],
          [
           "```\n\n**7. Implement the forward pass**\n\nHaving managed to correctly load the pretrained weights into..."
          ],
          [
           "```\n\nIt is very likely that the ðŸ¤— Transformers implementation and the\noriginal model implementation ..."
          ],
          [
           "The best way to fix the problem is usually to look at the forward pass\nof the original implementatio..."
          ],
          [
           "```\n\nHaving fixed all common tests, it is now crucial to ensure that all the\nnice work you have done..."
          ],
          [
           "```\n\n**Note**: In case you are using Windows, you should replace `RUN_SLOW=1` with\n`SET RUN_SLOW=1`\n..."
          ],
          [
           "```\n\nTo ensure that the tokenizer works correctly, it is recommended to first\ncreate a script in the..."
          ],
          [
           "```\n\nWhen both `input_ids` yield the same values, as a final step a tokenizer\ntest file should also ..."
          ],
          [
           "Next, make sure that the docstring added to\n`src/transformers/models/big_bird/modeling_big_bird.py` ..."
          ],
          [
           "```\n\nThere are a couple of other very strict design tests in ðŸ¤— Transformers\nthat might still be fail..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*The BigCode project is an open-scientific collaborat..."
          ],
          [
           "## Implementation details\n\nThe main differences compared to GPT2.\n- Added support for Multi-Query At..."
          ],
          [
           "```\n\nMake also sure that you have a hardware that is compatible with Flash-Attention 2. Read more ab..."
          ],
          [
           "```\n\n### Expected speedups\n\nBelow is a expected speedup diagram that compares pure inference time be..."
          ],
          [
           "Examples\nIn this folder we showcase some examples to use code models for downstream tasks.\n\n## Compl..."
          ],
          [
           "```\n\n## Code generation: text to python\nIn this task we want to train a model to generate code from ..."
          ],
          [
           "```\n\n## Code explanation: python to text\nIn this task we want to train a model to explain python cod..."
          ],
          [
           "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\nIn several question answering benchmarks, pretrained ..."
          ],
          [
           "## Usage tips\n\n- Splinter was trained to predict answers spans conditioned on a special [QUESTION] t..."
          ],
          [
           "## SplinterForQuestionAnswering\n\n[[autodoc]] SplinterForQuestionAnswering\n    - forward\n\n## Splinter..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "As shown on the following figure, Jukebox is made of 3 `priors` which are decoder only models. They ..."
          ],
          [
           "## Usage tips\n\n- This model only supports inference. This is for a few reasons, mostly because it re..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self atten..."
          ],
          [
           "## Usage tips\n\n- One can use the [`AutoImageProcessor`] API to prepare images for the model.\n- NAT c..."
          ],
          [
           "## NatConfig\n\n[[autodoc]] NatConfig\n\n## NatModel\n\n[[autodoc]] NatModel\n    - forward\n\n## NatForImage..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Large multimodal models (LMM) have recently shown en..."
          ],
          [
           "- For better results, we recommend users to prompt the model with the correct prompt format: \n\n```ba..."
          ],
          [
           "```\n\nFor multiple turns conversation:\n\n```bash\n\"USER: <image>\\n<prompt1>ASSISTANT: <answer1>USER: <p..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*General-purpose language models that can solve vario..."
          ],
          [
           "## Usage tips\n\nInstructBLIP uses the same architecture as [BLIP-2](blip2) with a tiny but important ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nYou will then be able to use the auto classes like you would usually do!\n\n<Tip warning={true}>\n..."
          ],
          [
           "### AutoModelForPreTraining\n\n[[autodoc]] AutoModelForPreTraining\n\n### TFAutoModelForPreTraining\n\n[[a..."
          ],
          [
           "### TFAutoModelForSeq2SeqLM\n\n[[autodoc]] TFAutoModelForSeq2SeqLM\n\n### FlaxAutoModelForSeq2SeqLM\n\n[[a..."
          ],
          [
           "### AutoModelForTokenClassification\n\n[[autodoc]] AutoModelForTokenClassification\n\n### TFAutoModelFor..."
          ],
          [
           "### AutoModelForVideoClassification\n\n[[autodoc]] AutoModelForVideoClassification\n\n### AutoModelForMa..."
          ],
          [
           "### AutoModelForZeroShotObjectDetection\n\n[[autodoc]] AutoModelForZeroShotObjectDetection\n\n## Audio\n\n..."
          ],
          [
           "### AutoModelForTableQuestionAnswering\n\n[[autodoc]] AutoModelForTableQuestionAnswering\n\n### TFAutoMo..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [camembert](https://huggingface.co/camembert). The original code can b..."
          ],
          [
           "## CamembertForSequenceClassification\n\n[[autodoc]] CamembertForSequenceClassification\n\n## CamembertF..."
          ],
          [
           "# Token classification\n\nBased on the scripts [`run_ner.py`](https://github.com/huggingface/transform..."
          ],
          [
           "```\n\nThe GermEval 2014 dataset contains some strange \"control character\" tokens like `'\\x96', '\\u200..."
          ],
          [
           "```\n\n#### Run the Pytorch version\n\nTo start training, just run:\n\n```bash\npython3 run_ner.py --data_d..."
          ],
          [
           "```\n\nIt must be saved with a `.json` extension and can be used by running `python3 run_ner.py config..."
          ],
          [
           "```\n\n#### Run the Tensorflow 2 version\n\nTo start training, just run:\n\n```bash\npython3 run_tf_ner.py ..."
          ],
          [
           "```\n\nOn the test dataset the following results could be achieved:\n```bash\n           precision    re..."
          ],
          [
           "```\n\n### Emerging and Rare Entities task: WNUTâ€™17 (English NER) dataset\n\nDescription of the WNUTâ€™17 ..."
          ],
          [
           "```\n\nLet's define some variables that we need for further pre-processing steps:\n\n```bash\nexport MAX_..."
          ],
          [
           "```\n\n#### Run the Pytorch version\n\nFine-tuning with the PyTorch version can be started using the `ru..."
          ],
          [
           "```\n\nIf your GPU supports half-precision training, please set `fp16` to `true`.\n\nSave this JSON-base..."
          ],
          [
           "```\n\nWNUTâ€™17 is a very difficult task. Current state-of-the-art results on this dataset can be found..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--..."
          ],
          [
           "```\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model w..."
          ],
          [
           "```\n\nTake a look at an example now:\n\n```py\n>>> minds[\"train\"][0]\n{'audio': {'array': array([ 0.     ..."
          ],
          [
           "```\n\nNow you can convert the label id to a label name:\n\n```py\n>>> id2label[str(2)]\n'app_error'\n```\n\n..."
          ],
          [
           "```\n\nNow create a preprocessing function that:\n\n1. Calls the `audio` column to load, and if necessar..."
          ],
          [
           "```\n\n## Evaluate\n\nIncluding a metric during training is often helpful for evaluating your model's pe..."
          ],
          [
           "```\n\nYour `compute_metrics` function is ready to go now, and you'll return to it when you setup your..."
          ],
          [
           "```\n\nAt this point, only three steps remain:\n\n1. Define your training hyperparameters in [`TrainingA..."
          ],
          [
           "```\n</pt>\n</frameworkcontent>\n\n<Tip>\n\nFor a more in-depth example of how to finetune a model for aud..."
          ],
          [
           "```\n\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ..."
          ],
          [
           "```\n\nGet the class with the highest probability, and use the model's `id2label` mapping to convert i..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage example\n\n```python\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n..."
          ],
          [
           "```\n\n<Tip> \n\nThis implementation is the same as BERT, except for tokenization method. Refer to [BERT..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The process of selecting output tokens to generate text is known as decoding, and you can customize ..."
          ],
          [
           "```\n\nPrinting out the `model.generation_config` reveals only the values that are different from the ..."
          ],
          [
           "```\n\nEven if the default decoding strategy mostly works for your task, you can still tweak a few thi..."
          ],
          [
           "```python\n>>> from transformers import AutoModelForCausalLM, GenerationConfig\n\n>>> model = AutoModel..."
          ],
          [
           "```\n\nYou can also store several generation configurations in a single directory, making use of the `..."
          ],
          [
           ">>> # You could then use the named generation config file to parameterize generation\n>>> generation_..."
          ],
          [
           "```\n\n## Streaming\n\nThe `generate()` supports streaming, through its `streamer` input. The `streamer`..."
          ],
          [
           "```\n\n## Decoding strategies\n\nCertain combinations of the `generate()` parameters, and ultimately `ge..."
          ],
          [
           "```\n\n### Contrastive search\n\nThe contrastive search decoding strategy was proposed in the 2022 paper..."
          ],
          [
           "```\n\n### Multinomial sampling\n\nAs opposed to greedy search that always chooses a token with the high..."
          ],
          [
           "```\n\n### Beam-search decoding\n\nUnlike greedy search, beam-search decoding keeps several hypotheses a..."
          ],
          [
           "```\n\n### Beam-search multinomial sampling\n\nAs the name implies, this decoding strategy combines beam..."
          ],
          [
           "```\n\n### Diverse beam search decoding\n\nThe diverse beam search decoding strategy is an extension of ..."
          ],
          [
           ">>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return_tens..."
          ],
          [
           "```\n\nThis guide illustrates the main parameters that enable various decoding strategies. More advanc..."
          ],
          [
           ">>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n>>> inputs = tokenizer(prompt, return_tens..."
          ],
          [
           "```\n\nWhen using assisted decoding with sampling methods, you can use the `temperature` argument to c..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nIf you want to use a specific model from the [hub](https://huggingface.co) you can ignore the t..."
          ],
          [
           "```\n\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don..."
          ],
          [
           "```\n\n[[autodoc]] pipeline\n\n## Pipeline batching\n\nAll pipelines can use batching. This will work\nwhen..."
          ],
          [
           "```\n\n```\n# On GTX 970\n------------------------------\nStreaming no batching\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ..."
          ],
          [
           "```\n------------------------------\nStreaming no batching\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ..."
          ],
          [
           "```\n\nThere are no good (general) solutions for this problem, and your mileage may vary depending on ..."
          ],
          [
           "```\n\nThis should be very transparent to your code because the pipelines are used in\nthe same way.\n\nT..."
          ],
          [
           "```\n\nThat should enable you to do all the custom code you want.\n\n\n## Implementing a pipeline\n\n[Imple..."
          ],
          [
           "### VideoClassificationPipeline\n\n[[autodoc]] VideoClassificationPipeline\n    - __call__\n    - all\n\n#..."
          ],
          [
           "### TokenClassificationPipeline\n\n[[autodoc]] TokenClassificationPipeline\n    - __call__\n    - all\n\n#..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*Image segmentation is usually addressed by training ..."
          ],
          [
           "## Usage tips\n\n- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latt..."
          ],
          [
           "## CLIPSegTextModel\n\n[[autodoc]] CLIPSegTextModel\n    - forward\n\n## CLIPSegVisionModel\n\n[[autodoc]] ..."
          ],
          [
           "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is the following:\n\n*We introduce a new language representation model cal..."
          ],
          [
           "* a special mask token with probability 0.8\n    * a random token different from the one masked with ..."
          ],
          [
           "- A blog post on [BERT Text Classification in a different language](https://www.philschmid.de/bert-t..."
          ],
          [
           "- [`FlaxBertForSequenceClassification`] is supported by this [example script](https://github.com/hug..."
          ],
          [
           "<PipelineTag pipeline=\"token-classification\"/>..."
          ],
          [
           "- A blog post on how to use [Hugging Face Transformers with Keras: Fine-tune a non-English BERT for ..."
          ],
          [
           "- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ðŸ¤— Hugging Fa..."
          ],
          [
           "<PipelineTag pipeline=\"fill-mask\"/>\n\n- [`BertForMaskedLM`] is supported by this [example script](htt..."
          ],
          [
           "<PipelineTag pipeline=\"question-answering\"/>\n\n- [`BertForQuestionAnswering`] is supported by this [e..."
          ],
          [
           "**Multiple choice**\n- [`BertForMultipleChoice`] is supported by this [example script](https://github..."
          ],
          [
           "ðŸš€ **Deploy**\n- A blog post on how to [Convert Transformers to ONNX with Hugging Face Optimum](https:..."
          ],
          [
           "[[autodoc]] TFBertTokenizer\n\n</tf>\n</frameworkcontent>\n\n## Bert specific outputs\n\n[[autodoc]] models..."
          ],
          [
           "## TFBertForPreTraining\n\n[[autodoc]] TFBertForPreTraining\n    - call\n\n## TFBertModelLMHeadModel\n\n[[a..."
          ],
          [
           "[[autodoc]] FlaxBertForNextSentencePrediction\n    - __call__\n\n## FlaxBertForSequenceClassification\n\n..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Exporting a model requires two things:\n\n- model instantiation with the `torchscript` flag\n- a forwar..."
          ],
          [
           "```\n`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension..."
          ],
          [
           "```\n\nWe recommended you trace the model with a dummy input size at least as large as the\nlargest inp..."
          ],
          [
           "# Initializing the model with the torchscript flag\n# Flag set to True even though it is not necessar..."
          ],
          [
           "```\n\n### Loading a model\n\nNow you can load the previously saved `BertModel`, `traced_bert.pt`, from ..."
          ],
          [
           "```\n\n## Deploy Hugging Face TorchScript models to AWS with the Neuron SDK\n\nAWS introduced the [Amazo..."
          ],
          [
           "### Implications\n\nTransformers models based on the [BERT (Bidirectional Encoder Representations from..."
          ],
          [
           "### Converting a model for AWS Neuron\n\nConvert a model for AWS NEURON using the same code from [Usin..."
          ],
          [
           "```\n\nYou only need to modify the following line:\n\n```diff\n- torch.jit.trace(model, [tokens_tensor, s..."
          ]
         ],
         "hovertemplate": "source=transformers<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "transformers, circle",
         "marker": {
          "color": "#FFA15A",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "transformers, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -3.5204015,
          1.7851868,
          4.398573,
          -1.7365477,
          -1.5664274,
          -1.0944971,
          -1.3880491,
          -2.259895,
          -3.630998,
          -3.5828469,
          -4.571323,
          1.0102422,
          -8.976927,
          -11.99184,
          -9.669576,
          -9.542361,
          1.0229106,
          1.459369,
          -0.45514408,
          1.0534307,
          1.1300329,
          0.5628536,
          1.1530982,
          0.579516,
          1.189864,
          0.8502866,
          -8.818843,
          6.219388,
          7.1977744,
          8.043642,
          7.690721,
          7.8062143,
          7.868891,
          7.0498447,
          6.964169,
          7.983872,
          7.2366524,
          15.164083,
          15.208315,
          15.207917,
          15.178652,
          15.15408,
          14.620151,
          15.137444,
          15.149288,
          15.147747,
          15.158633,
          15.201793,
          15.174605,
          15.1702385,
          15.183297,
          6.5772095,
          7.8909187,
          7.66862,
          7.823362,
          7.712511,
          7.5283,
          -3.4510956,
          7.8033214,
          -0.32702458,
          7.7890277,
          -2.774237,
          -2.4839013,
          7.8832417,
          -2.0627642,
          7.77874,
          7.7227173,
          6.621525,
          7.863119,
          7.569357,
          7.778793,
          2.7624133,
          2.781457,
          7.037785,
          7.5882273,
          7.501518,
          -5.093933,
          -5.073057,
          -5.676403,
          -5.081092,
          -4.9846945,
          -5.10507,
          -4.9921203,
          -4.9614034,
          -5.1562295,
          -4.9800625,
          -5.0939875,
          -5.340591,
          -5.043194,
          -5.0671988,
          -5.553982,
          -5.111509,
          -4.825581,
          -5.3215985,
          -4.9174414,
          -4.9061155,
          -5.0720453,
          -5.0577016,
          -4.997106,
          -4.9767985,
          -5.0042257,
          -5.136648,
          -5.013315,
          -4.860595,
          -4.9709134,
          -5.496124,
          -5.0958366,
          -4.9456234,
          -5.0154223,
          -5.0272603,
          -4.9822326,
          -5.206058,
          -5.0048594,
          -4.79754,
          -5.009794,
          -5.4575644,
          -4.9423547,
          -4.7546906,
          -5.125537,
          -5.1143613,
          -5.7003756,
          -4.9572773,
          -4.9113326,
          -5.0472164,
          -4.8814816,
          -5.2278156,
          -5.1145744,
          -4.9786696,
          -4.9391284,
          -5.057751,
          -5.037208,
          -4.9318414,
          -4.9066215,
          -4.9401426,
          -4.988787,
          -4.9825625,
          -4.912394,
          7.8995657,
          7.838103,
          7.545456,
          -6.7705173,
          -2.0878248,
          3.9726403,
          4.106512,
          -1.2791318,
          1.7934276,
          1.9092823,
          2.1833487,
          3.9048686,
          3.7912188,
          -9.267206,
          -9.577388,
          -7.837771,
          -9.078,
          -8.749469,
          -6.5627117,
          -5.954649,
          -11.742119,
          -12.358776,
          -9.243016,
          -9.332956,
          -9.235874,
          -8.610286,
          -8.79832,
          -8.002497,
          -12.026158,
          -4.33188,
          0.18892862,
          6.274872,
          -6.359477,
          -6.442736,
          -6.204473,
          -5.1887746,
          -6.210372,
          -6.825091,
          -6.6708965,
          -6.954833,
          -6.6021233,
          -6.1329255,
          -6.217895,
          -11.039214,
          6.706393,
          8.570285,
          -8.2296715,
          -8.294066,
          -7.2718787,
          -8.62366,
          7.2367573,
          -6.5814424,
          -7.2702327,
          5.9529686,
          -6.54953,
          -6.306498,
          -6.2076015,
          -9.110759,
          -3.1384265,
          -6.2022567,
          -6.760194,
          -6.6199856,
          -6.7988772,
          -6.470855,
          -10.060524,
          6.586961,
          -8.16604,
          -5.910483,
          -3.3218246,
          -5.3981147,
          -5.1410594,
          -6.5368853,
          -6.598593,
          6.571964,
          -1.7481618,
          -1.5611156,
          -7.3718233,
          -7.5731244,
          -5.607646,
          -12.389063,
          -6.31911,
          -2.959641,
          -0.98634595,
          -3.4599125,
          -3.49495,
          -3.4754026,
          -2.5507104,
          -1.1462718,
          -3.2347515,
          -3.3825233,
          -5.4437222,
          4.0651364,
          2.4617734,
          -3.1577675,
          -1.1298482,
          -0.81607485,
          -2.9012253,
          -1.6048181,
          -0.1327559,
          -3.0471342,
          -1.3309028,
          -1.686202,
          -2.464811,
          4.6593485,
          4.9142704,
          -2.6557605,
          -1.3262734,
          -2.1081595,
          -0.803569,
          -0.7203931,
          -1.0499301,
          -0.43654105,
          14.230969,
          -0.7184523,
          -1.3038658,
          -0.98854095,
          -1.6623063,
          -1.670272,
          -1.1151408,
          -2.212247,
          -2.8733165,
          -2.0677886,
          -0.7436489,
          2.522343,
          -3.241372,
          -5.966481,
          -5.155616,
          -4.105721,
          0.65463275,
          -6.2476916,
          -5.7953897,
          -5.019035,
          -4.3760757,
          2.8418207,
          -5.588234,
          -2.4316704,
          6.6366444,
          -3.8786979,
          -5.5067186,
          -7.7186255,
          -7.640191,
          -11.305498,
          -6.1416173,
          -4.369189,
          -3.6885502,
          -1.2992148,
          -1.2701578,
          -3.3022275,
          -9.207267,
          1.7321237,
          3.113063,
          2.335127,
          1.1331753,
          -0.62563753,
          -1.6996793,
          -1.2817299,
          0.6554884,
          0.4400953,
          1.0291537,
          1.4816966,
          2.5272756,
          -2.0458238,
          -0.2711229,
          -0.12795803,
          0.25474197,
          -8.831918,
          -0.110605754,
          -6.010542,
          -5.902474,
          -3.1718836,
          -4.686274,
          -6.066472,
          -5.227998,
          -4.543382,
          -12.128053,
          7.6968307,
          7.2420487,
          6.691744,
          8.022501,
          8.018999,
          7.117594,
          6.989516,
          7.0559525,
          7.425126,
          15.1686125,
          15.209248,
          15.2100115,
          15.166646,
          15.1483755,
          14.601599,
          15.140897,
          15.148222,
          15.148139,
          15.189131,
          15.19017,
          15.173764,
          15.171204,
          14.948366,
          6.811042,
          7.9526,
          8.067779,
          -2.4827888,
          8.121287,
          8.185952,
          2.7016797,
          6.9238453,
          8.023594,
          -5.0689135,
          -5.0976586,
          -5.576233,
          -5.099374,
          -5.0028853,
          -5.202551,
          -5.1030164,
          -5.0526814,
          -5.078485,
          -5.3848124,
          -4.9974575,
          -5.3337984,
          -5.1602073,
          -5.0625887,
          -5.376166,
          -4.958206,
          -5.0046115,
          -5.113829,
          -5.247239,
          -4.8889637,
          -5.0367603,
          -5.032808,
          -5.0055375,
          -4.9927835,
          -4.9400053,
          -5.0718365,
          -5.073281,
          -4.786602,
          -4.845178,
          -4.9637,
          -5.352499,
          -4.986849,
          -4.969182,
          -5.044641,
          -5.0610504,
          -4.9608264,
          -5.002906,
          -5.082026,
          -5.038712,
          -4.9013376,
          -5.0233865,
          -4.999575,
          -5.128922,
          -5.6329427,
          -4.941103,
          -4.8389993,
          -5.0767803,
          -5.137634,
          -5.6993885,
          -5.0076327,
          -4.937402,
          -4.936234,
          -4.916757,
          -5.094444,
          -5.175965,
          -5.107626,
          -4.9432425,
          -5.0247307,
          -5.0605297,
          -5.0467615,
          -4.9316964,
          -4.911765,
          -5.0462775,
          -4.9529095,
          -4.9879827,
          -4.9645176,
          8.012617,
          -6.867348,
          -3.869256,
          -3.7664492,
          -2.794512,
          -3.2840393,
          -2.1005075,
          -1.9424151,
          -1.6817644,
          -3.5824077,
          -2.548769,
          -2.4785671,
          -2.3292134,
          -2.8667898,
          -2.7812984,
          -2.4565973,
          -2.7663417,
          -2.4899063,
          -2.6649325,
          -2.5418963,
          -2.9624233,
          -2.4420233,
          -3.2852566,
          -3.5959156,
          -0.4612395,
          -3.834531,
          -2.5184145,
          -2.8378463,
          -2.1487875,
          -7.571268,
          -7.6336007,
          -2.2071495,
          -6.823347,
          -2.7280757,
          -11.704269,
          6.122253,
          -6.01966,
          -5.8134975,
          -4.785505,
          -4.592849,
          -5.119102,
          -4.199809,
          -3.4796288,
          -12.172008,
          -12.445009,
          -12.423546,
          -9.052525,
          -0.46226013,
          -9.726342,
          -9.307809,
          -8.351319,
          -8.737741,
          0.8878642,
          -7.7263627,
          -7.0861983,
          -6.9714265,
          -6.850238,
          -7.0377784,
          -2.820606,
          -11.717467,
          -9.615718,
          -9.52986,
          -10.90039,
          -5.343707,
          -3.162909,
          -3.3000793,
          -3.3492706,
          -0.08544894,
          -9.259654,
          -9.491853,
          -8.027753,
          -7.2370496,
          -7.9454203,
          -7.901165,
          -7.8598685,
          -8.220812,
          -8.029617,
          -8.098562,
          -5.645182,
          -5.551197,
          -1.7127085,
          -11.586423,
          -11.676783,
          -11.861212,
          -11.930494,
          -5.965876,
          -6.0203447,
          -5.2164454,
          -9.687858,
          -9.796041,
          -8.244909,
          -7.380569,
          -6.8857164,
          -6.8505526,
          -6.7797155,
          -6.8403025,
          -1.8670086,
          -11.626985,
          -0.803084,
          -0.012849104,
          -0.042481236,
          0.67084116,
          -9.775702,
          -9.59817,
          -10.606345,
          -3.692763,
          -0.44154736,
          -7.421686,
          -7.572548,
          -3.4540868,
          -3.7396877,
          2.8644714,
          -3.0074065,
          -3.2607386,
          -3.4423554,
          -2.7049036,
          -3.4128134,
          -0.98440737,
          -0.9865788,
          -1.4294659,
          -1.0036913,
          -0.45684066,
          -0.70715165,
          -1.6245174,
          -1.2425712,
          -0.69889086,
          -6.1828732,
          -3.195841,
          -6.237652,
          -3.986649,
          -1.1711416,
          -3.3449106,
          -1.9682912,
          -1.2621362,
          -9.803968,
          -9.905323,
          6.093457,
          -5.313955,
          -5.481025,
          -1.0890771,
          -2.4619007,
          -2.6017404,
          -4.714293,
          -4.570128,
          -4.8039494,
          -11.7899475,
          -8.319346,
          -9.573905,
          -8.673295,
          -7.763598,
          -7.635191,
          -8.397553,
          -4.086622,
          -11.330658,
          -10.054782,
          -8.69934,
          -4.868667,
          -0.19828811,
          -4.402068,
          -3.8736584,
          -3.750573,
          -2.8003368,
          -2.6078975,
          -2.6255438,
          -5.609285,
          -7.8204384,
          -7.85837,
          -9.884651,
          0.9199817,
          1.023364,
          1.2060349,
          -7.617179,
          -7.6313505,
          -7.1780944,
          -7.140211,
          -1.6193655,
          -8.088283,
          -8.365739,
          -0.65239936,
          -0.83807486,
          -0.233335,
          -1.0059584,
          -0.38759604,
          -1.2083687,
          -1.2162184,
          -5.780182,
          -4.219085,
          -11.941747,
          -9.965027,
          -10.039049,
          -9.624869,
          -9.789598,
          -8.657855,
          -8.841292,
          -4.294557,
          -11.615311,
          -7.6859064,
          -7.4038033,
          -6.749219,
          -6.9457903,
          -2.1598527,
          -2.352129,
          -2.2745337,
          -2.278799,
          -2.4210863,
          -4.0555615,
          -2.434069,
          -3.6696734,
          -3.5118978,
          -3.6663413,
          -1.6062784,
          -2.4638934,
          -3.1683269,
          0.13285373,
          -3.0094228,
          -9.842246,
          -10.047166,
          -11.010454,
          -8.14985,
          -8.605296,
          2.5148346,
          0.39648688,
          -0.6060014,
          -0.45563194,
          -0.3538976,
          -0.4016106,
          -0.2569895,
          -1.3060714,
          -8.702301,
          -0.8511057,
          -4.031237,
          -0.35241726,
          -0.62248486,
          -2.793948,
          -2.3471534,
          -1.9690074,
          -2.0512419,
          -9.229683,
          -9.219109,
          -11.668724,
          6.047137,
          8.080763,
          7.9150195,
          7.9729047,
          7.0441647,
          7.0058274,
          7.1261187,
          7.3165994,
          6.7488933,
          7.483731,
          -3.9600327,
          7.209532,
          7.988651,
          7.032789,
          -2.1661785,
          7.476812,
          7.858901,
          7.8537874,
          2.6864429,
          2.7900538,
          7.160721,
          7.877995,
          -5.0905304,
          -5.0812078,
          -4.9590325,
          -5.1117277,
          -5.0543847,
          -4.9928417,
          -4.980354,
          -5.23139,
          -5.0427957,
          -4.9972754,
          -5.396905,
          -5.078591,
          -5.0182786,
          -5.542198,
          -5.0699635,
          -5.00648,
          -5.2851057,
          -5.0649667,
          -5.0024905,
          -4.948487,
          -4.9858527,
          -5.1783047,
          -5.058292,
          -4.842431,
          -4.812541,
          -4.9848995,
          -5.3022203,
          -5.0063434,
          -4.923282,
          -5.0370994,
          -4.969235,
          -5.067023,
          -5.0136003,
          -4.9728465,
          -5.289574,
          -5.0000305,
          -4.9038963,
          -5.041518,
          -4.8665133,
          -5.1433306,
          -5.2704964,
          -4.9337897,
          -4.756204,
          -5.1745286,
          -5.0909615,
          -5.315309,
          -5.5436153,
          -5.036565,
          -4.9229345,
          -4.87558,
          -4.770322,
          -5.171965,
          -5.170195,
          -5.092289,
          -4.9261026,
          -4.9216,
          -5.060745,
          -5.077725,
          -5.042295,
          -4.929452,
          -4.905039,
          -5.0725636,
          -5.010499,
          -5.0207825,
          -4.898611,
          7.2553515,
          8.017268,
          8.003506,
          -5.034747,
          -6.802319,
          -2.1170423,
          0.9037565,
          0.8002518,
          -0.98741645,
          -0.087125465,
          1.8217397,
          1.6945963,
          2.280647,
          0.8160105,
          -0.8724353,
          4.884083,
          2.9514449,
          1.7346363,
          -7.9756155,
          -8.57508,
          -2.4121284,
          -1.568141,
          -2.2637045,
          -5.9727955,
          -11.682246,
          -9.792932,
          -8.9587555,
          -0.05153992,
          -8.641276,
          -8.286213,
          -8.211581,
          2.483845,
          -1.6428409,
          -2.0431123,
          1.2522166,
          -1.3723896,
          -0.3694675,
          -1.25334,
          1.0610769,
          6.9178123,
          -1.3555064,
          -8.090051,
          -1.5530596,
          0.79779786,
          -0.9211405,
          -1.6343712,
          -3.6278346,
          1.3909682,
          -1.4139506,
          -3.1694367,
          -2.0368435,
          6.2987742,
          8.1069565,
          -6.2904096,
          4.3852534,
          -5.5563617,
          6.9685655,
          6.618253,
          6.700136,
          -9.53327,
          -6.943219,
          -3.7473035,
          -3.2849324,
          0.4004547,
          0.5350726,
          -2.0085242,
          -2.6031353,
          2.4215624,
          6.6306415,
          -3.303945,
          -5.042915,
          -5.6143336,
          -4.898616,
          -5.092343,
          -5.2914076,
          -4.9614353,
          -5.1984453,
          -5.025055,
          -5.036665,
          -5.4522505,
          -5.054899,
          -4.951319,
          -5.1922545,
          -5.1690636,
          -4.990263,
          -5.038754,
          -4.996284,
          -4.9846997,
          -4.908678,
          -5.0402284,
          -5.117829,
          -4.730677,
          -4.81061,
          -4.9868593,
          -5.337579,
          -5.066424,
          -4.9628186,
          -5.031872,
          -5.028021,
          -4.982839,
          -4.8546247,
          -5.0212026,
          -5.020466,
          -4.8144593,
          -5.0373874,
          -4.956452,
          -5.0629134,
          -5.24408,
          -4.917346,
          -4.830138,
          -5.09108,
          -5.1461997,
          -5.6884017,
          -4.9963903,
          -4.8933077,
          -4.9357147,
          -4.8612423,
          -5.1600447,
          -5.1002574,
          -5.0382557,
          -4.9282026,
          -5.031859,
          -5.055541,
          -4.9729977,
          -4.9002,
          -1.6832328,
          -6.756928,
          -5.023355,
          3.5032961,
          1.8722763,
          -1.9121232,
          -3.2142162,
          1.0133767,
          -0.22950768,
          -4.6463637,
          -1.6401242,
          -3.2985156,
          2.1845973,
          -0.4122734,
          -0.65105814,
          -1.4458314,
          -2.082304,
          -0.30218834,
          -1.3404487,
          -1.5114204,
          -0.8484746,
          -1.6656821,
          -0.70123935,
          -0.82864314,
          -1.2625717,
          -2.1603634,
          -2.2336118,
          -1.9059647,
          -4.9903693,
          -7.137589,
          -7.0229044,
          -7.0508385,
          -7.0684843,
          2.4667852,
          3.371474,
          -1.8448938,
          -1.7541108,
          -0.19383097,
          0.83665425,
          -0.9751295,
          -0.25867137,
          0.1417914,
          -2.4859643,
          -1.6825804,
          -7.683859,
          -7.743297,
          -11.852267,
          6.08516,
          -6.663904,
          -5.67023,
          -9.518972,
          -9.686346,
          -7.9161143,
          -12.064762,
          -9.406564,
          0.6454319,
          -3.5207112,
          -0.80532646,
          -1.6391128,
          -1.2809796,
          -1.8411839,
          -1.3063818,
          -9.867968,
          -9.989733,
          -8.906497,
          -11.393619,
          -8.834529,
          -7.57753,
          2.266305,
          -0.95161587,
          1.9435024,
          0.2975864,
          -9.385262,
          -9.040272,
          -10.152229,
          -11.584792,
          -9.211089,
          -8.866464,
          0.81592107,
          1.0324125,
          -0.65187883,
          -9.1951475,
          -9.259662,
          -9.322914,
          -8.696592,
          -11.20413,
          -11.306771,
          -11.430166,
          -5.9995236,
          -4.2615867,
          6.2876143,
          -5.532041,
          6.791346,
          6.372036,
          6.751271,
          -4.8710837,
          1.4734136,
          -2.2355013,
          0.9647924,
          0.8706165,
          -0.7241371,
          0.004997094,
          -9.248962,
          -9.176801,
          -9.198889,
          -8.822446,
          2.6880462,
          -0.7101882,
          -1.7789221,
          -2.0646126,
          -1.9108272,
          -1.8182317,
          -1.8135473,
          -0.7697604,
          -1.5806217,
          -1.3986542,
          -1.1710132,
          -1.6804736,
          -1.539955,
          -1.9248898,
          -0.959969,
          -0.55434734,
          -0.58334684,
          -1.6861696,
          -1.2327696,
          -0.8083615,
          -1.355883,
          -1.4793264,
          0.009560321,
          -0.8903864,
          -1.2597477,
          -3.7572277,
          -1.857083,
          -2.239312,
          5.9368143,
          -0.29056758,
          -2.5171583,
          6.0893617,
          4.9983497,
          3.300301,
          1.9516807,
          1.8342122,
          2.876997,
          5.176968,
          4.9240766,
          5.7421427,
          5.5464296,
          6.3124785,
          -1.348963,
          -6.761275,
          -6.6693144,
          -4.891235,
          -11.525568,
          -12.121405,
          4.5511413,
          2.4072108,
          3.601695,
          3.7780995,
          -1.9298629,
          -2.4461632,
          -2.3193815,
          -2.4514642,
          -4.8152795,
          -4.591663,
          -5.089763,
          -4.636729,
          -4.468426,
          -4.759214,
          -4.249004,
          -3.6177068,
          -4.5040007,
          -4.667635,
          -7.964263,
          -10.054322,
          -9.748062,
          -9.160621,
          -6.0418305,
          -6.0584836,
          -6.2835183,
          -7.679031,
          -8.925189,
          -8.885544,
          -8.703164,
          -1.3814485,
          0.59851587,
          0.8077777,
          1.0843555,
          0.9542183,
          0.9393845,
          0.67377836,
          -9.859102,
          -10.25634,
          -5.653177,
          -2.065035,
          -1.158363,
          -1.0078869,
          -3.905436,
          -7.1128726,
          -7.619156,
          -7.265622,
          -9.654432,
          -0.16454794,
          -9.829075,
          -9.77837,
          -9.920552,
          -3.8142908,
          -3.972971,
          -0.6245531,
          0.14297554,
          -2.5428228,
          -6.3610635,
          -5.7064605,
          -4.329926,
          -8.072712,
          -6.2136188,
          -4.1160808,
          -5.856696,
          -1.1355712,
          -1.1805794,
          -4.1770077,
          -1.3042619,
          -5.210929,
          -3.8250127,
          -4.851724,
          -7.152961,
          -11.845419,
          -6.0440006,
          -8.549763,
          -2.4191422,
          -2.5531657,
          -2.4090095,
          -2.4928384,
          -2.7047038,
          -2.4056315,
          -2.6231995,
          -5.3289895,
          -6.0720124,
          -5.757114,
          -12.20118,
          -12.459839,
          5.713946,
          -5.3248844,
          -5.0966563,
          -2.104174,
          -0.8041266,
          -3.890801,
          -4.6121764,
          -3.0192592,
          0.4269757,
          -3.0993958,
          0.46390048,
          3.9898958,
          -8.899786,
          -8.871837,
          -8.726737,
          -8.021565,
          -8.57591,
          -11.529956,
          -12.18608,
          5.7210135,
          5.012014,
          2.2813668,
          2.7590115,
          -3.8891573,
          4.7118835,
          3.8641107,
          3.9946108,
          4.979167,
          5.0100327,
          3.8728282,
          1.8829942,
          1.920476,
          4.30498,
          -8.824431,
          -8.9762535,
          -8.532794,
          -0.67996114,
          -0.032821253,
          -0.553873,
          -8.629696,
          -7.8843336,
          -3.755795,
          -2.8628414,
          -2.8780484,
          -3.0347424,
          -2.7915716,
          -7.7741623,
          -7.819495,
          -11.357601,
          -6.001118,
          -5.4601684,
          -2.0583837,
          -5.5429244,
          -4.2131667,
          -3.9124806,
          -4.0080137,
          -4.077234,
          -3.5393825,
          -4.5477967,
          -4.901593,
          -2.2223704,
          -1.7018275,
          -2.2106254,
          -2.710831,
          -3.2554636,
          -2.2150211,
          -2.971736,
          -2.921587,
          -3.1648598,
          1.2646599,
          0.27460298,
          -5.222433,
          -5.0057263,
          -2.371498,
          2.1415625,
          -2.805978,
          -2.7249079,
          -4.282913,
          -3.8825326,
          -3.1998506,
          -8.953014,
          0.8198682,
          -0.19512923,
          -0.87318456,
          -6.2749615,
          -5.750258,
          -5.208075,
          -4.3167653,
          -1.3685458,
          -3.7843225,
          -12.297964,
          -6.423205,
          -6.0996466,
          -1.6729769,
          -3.9998145,
          -5.6317286,
          -2.6700854,
          -5.443709,
          6.044309,
          6.1822805,
          -5.681563,
          -5.162221,
          -4.733544,
          -4.6268635,
          -5.32513,
          -4.194825,
          -2.8731384,
          -12.068972,
          -12.401379,
          -3.632174,
          4.6768928,
          2.6793523,
          7.5271044,
          7.247472,
          7.2539587,
          7.215313,
          7.334764,
          7.3128614,
          3.940496,
          1.5772135,
          -7.992631,
          -8.601087,
          -11.429798,
          -4.3350964,
          -3.91303,
          0.30388424,
          -0.74450576,
          -0.5411238,
          -0.97720045,
          -0.4821634,
          -0.8300272,
          -0.65929836,
          -0.8450618,
          8.123478,
          -6.7362747,
          -5.5285463,
          7.1318555,
          7.032975,
          6.937186,
          6.7789507,
          -8.386041,
          7.569254,
          7.775967,
          0.29378495,
          7.746284,
          -2.551262,
          -2.2412002,
          -6.505911,
          -6.3465514,
          2.6638792,
          6.921718,
          7.7904277,
          -4.9976907,
          -4.9868503,
          -5.048078,
          -5.367337,
          -4.9258604,
          -4.9025702,
          -5.027049,
          -5.106703,
          -5.2643504,
          -4.8325224,
          -5.6044383,
          -5.1568947,
          -4.917351,
          7.9203706,
          7.694696,
          -6.7690787,
          -5.9750624,
          -0.031036492,
          -3.0627763,
          -2.355519,
          -3.992312,
          -1.7557031,
          -1.936523,
          -1.8032924,
          -1.6098704,
          -1.7549824,
          -0.7450616,
          -0.69822925,
          -1.9554847,
          -1.7021059,
          -1.5619518,
          -1.7529454,
          -0.964797,
          -1.8457371,
          -1.772038,
          -1.5804807,
          -0.34672755,
          -1.745713,
          -1.8277949,
          -1.8179623,
          -2.381182,
          -1.88258,
          -1.8650918,
          -1.8246292,
          -1.9288669,
          -1.7299714,
          -0.377019,
          -0.34873736,
          -0.45514125,
          -4.002291,
          -1.9006503,
          0.4346561,
          -8.785213,
          -8.784386,
          4.850177,
          -6.053881,
          -4.062839,
          -2.10809,
          -3.515568,
          -6.346952,
          -6.0560513,
          -7.115356,
          -9.86206,
          -10.146891,
          -11.791918,
          -11.808878,
          -3.7895632,
          -3.3544724,
          -4.798294,
          6.0828934,
          -7.563761,
          0.490616,
          -7.4368186,
          1.5163065,
          -3.2383218,
          -4.013016,
          -1.8717294,
          -7.5065017,
          -7.3141136,
          -7.296143,
          -7.380329,
          -6.9597974,
          -7.132525,
          -6.7533407,
          -6.8448424,
          -6.600896,
          -6.019645,
          -6.7583675,
          -7.2028437,
          -6.5308313,
          -7.797271,
          -7.6073174,
          -7.5418854,
          -6.966891,
          -6.8824797,
          -6.2031527,
          -11.504539,
          -12.214558,
          -2.671569,
          -3.0755894,
          -2.8088074,
          -1.6998129,
          0.1633368,
          -1.4403127,
          -5.0955076,
          -4.6719627,
          -5.2774243,
          -0.011072855,
          -0.05453717,
          3.9056215,
          3.6905406,
          7.6949162,
          7.7047505,
          8.070905,
          8.153209,
          7.9195333,
          7.0791593,
          7.0117655,
          8.35225,
          7.1636696,
          15.176385,
          15.262243,
          15.201982,
          15.155297,
          15.153964,
          14.649949,
          15.141219,
          15.135124,
          15.139538,
          15.1964655,
          15.201652,
          15.17316,
          15.233461,
          15.132907,
          6.8725133,
          7.754651,
          -3.655891,
          7.4026237,
          -2.5224867,
          7.9048376,
          8.388136,
          8.418722,
          8.084955,
          2.763849,
          3.2521198,
          7.5421534,
          7.748973,
          -4.960129,
          -5.009712,
          -4.8898773,
          -4.6234837,
          -4.593696,
          -4.9060144,
          -4.774832,
          -4.7076073,
          -4.4591355,
          -4.7912607,
          -4.7685847,
          -4.8074408,
          -4.4207773,
          -4.886731,
          -4.4047403,
          -4.472447,
          -4.6286545,
          -4.5735207,
          -5.1569667,
          -4.404791,
          -4.480845,
          -4.945695,
          -4.725696,
          -5.092268,
          -4.493118,
          8.419225,
          8.400269,
          -4.9595246,
          -4.4524674,
          -4.6587396,
          -4.430002,
          -4.423845,
          -4.893933,
          -5.024774,
          -4.78802,
          -4.7695174,
          -4.9888306,
          -4.8990426,
          -4.768149,
          -4.709745,
          -5.3002143,
          -4.7598515,
          -4.6970253,
          -4.722255,
          -4.8153663,
          -4.5526385,
          -4.7277746,
          -4.929247,
          -4.8444357,
          -4.9288144,
          -4.711557,
          -4.805258,
          -4.725804,
          -4.7641745,
          -4.8242073,
          -5.075643,
          -4.6659813,
          -4.821677,
          -4.650462,
          -4.829318,
          -4.442183,
          -4.6918173,
          -4.7266808,
          -5.402572,
          -4.9150796,
          -4.6998587,
          -4.5708632,
          -4.7128935,
          -4.800324,
          -4.529355,
          -4.6066966,
          -4.936799,
          -4.959333,
          -4.3890705,
          -4.8675594,
          -4.6687584,
          -4.721648,
          -4.83646,
          -4.758999,
          -4.7639365,
          -4.451265,
          -4.7093725,
          -4.839563,
          -4.7545524,
          -4.384485,
          -4.2985945,
          8.419828,
          7.91303,
          7.8702383,
          -6.910064,
          -5.9912696,
          -5.8720384,
          -12.074523,
          -12.217441,
          -1.3203112,
          -1.3532777,
          -0.48712122,
          -0.43203753,
          -0.39106664,
          -0.32458714,
          -0.6183116,
          -0.9005623,
          -0.8564611,
          -0.19114971,
          -0.3701865,
          0.08206064,
          -0.01238331,
          -0.43472344,
          -0.81869483,
          0.25990516,
          -0.54364127,
          -0.22146165,
          -0.48500323,
          -0.78223616,
          -1.7330942,
          -1.0904981,
          -1.388127,
          -1.065106,
          -1.2218032,
          -0.7357143,
          -1.2523766,
          -0.99537003,
          -0.85149103,
          -0.7777968,
          -0.6030871,
          -0.60526127,
          -0.43217692,
          -0.62447035,
          -1.3862988,
          -1.6859256,
          -1.5773168,
          -0.95149225,
          -0.49049187,
          -0.5530512,
          -0.084381744,
          -0.30352575,
          -0.66162515,
          -1.2585422,
          -0.5841061,
          -1.2442416,
          -0.5384871,
          -0.3660867,
          -0.46014345,
          -0.28647843,
          -0.5576288,
          -0.66881657,
          -0.77959657,
          -0.93479675,
          -0.6621088,
          -0.88608795,
          -1.614022,
          -1.7837566,
          -1.528109,
          -1.4369265,
          -1.0868499,
          -0.36871275,
          -1.2750646,
          -0.6394664,
          -1.0173409,
          -0.7739276,
          -0.82905465,
          -1.0579917,
          -1.0028762,
          -0.990999,
          -0.74876255,
          -0.9726504,
          -0.86440665,
          -1.3872217,
          -4.8445606,
          -6.618862,
          -5.612769,
          -9.587557,
          -9.822444,
          -10.329518,
          -5.210948,
          0.49650756,
          -5.828083,
          -5.8040967,
          -5.6256356,
          -12.205418,
          -6.262349,
          -6.0959306,
          -5.7466755,
          -12.23951,
          -6.2454853,
          -6.297179,
          -5.406027,
          -12.187635,
          -4.0322247,
          -3.3103523,
          -4.6525717,
          -6.2542787,
          -4.014896,
          -3.9533412,
          -5.134759,
          -2.770132,
          -4.9245143,
          -4.9697156,
          -4.689867,
          -9.608296,
          -9.647787,
          -9.61841,
          -8.26835,
          -5.267336,
          -5.5000978,
          -5.592628,
          -5.7383323,
          -12.190784,
          -9.9744215,
          -10.35986,
          -4.841873,
          -9.881011,
          -8.585747,
          -9.120495,
          -9.129698,
          -8.625491,
          -7.6216354,
          -7.4153194,
          -9.117826,
          1.9435041,
          3.4939687,
          2.0552862,
          -0.33101714,
          -8.2868185,
          -0.42465085,
          0.7474416,
          0.8447,
          4.5860977,
          -7.147582,
          -6.8864303,
          -7.5682697,
          -11.678398,
          -3.7040966,
          -1.7699081,
          -3.5914717,
          -1.3780226,
          7.252991,
          7.3663874,
          7.2871075,
          7.4461174,
          7.31766,
          4.0009775,
          -1.5311533,
          -1.753817,
          -0.91373473,
          -0.9510364,
          -1.2193942,
          -0.5657732,
          -0.3170689,
          -0.3581067,
          3.071798,
          -6.743305,
          -0.6980488,
          -0.755381,
          -0.7989565,
          -1.2525175,
          -1.4079294,
          -2.2702227,
          -3.0158813,
          -0.38167757,
          0.09995912,
          -7.222549,
          -7.280079,
          -6.8652554,
          -6.401587,
          -4.7090726,
          -5.853959,
          -5.9199533,
          -5.8540773,
          -11.763558,
          -12.169751,
          -8.270751,
          -8.458353,
          -1.7275691,
          -9.979044,
          -10.05873,
          -9.42828,
          5.9008207,
          -5.431802,
          2.3677418,
          0.18549156,
          -0.5399303,
          -3.8270667,
          -1.1323427,
          -0.6595846,
          -1.0145835,
          -0.33501706,
          0.17759314,
          -0.2666301,
          -0.102695584,
          -1.8260264,
          -6.3410583,
          -6.3922863,
          -4.8352327,
          -3.2976556,
          -4.076238,
          -6.1717434,
          -6.820712,
          -11.750672,
          -7.7845664,
          -7.7581053,
          -7.3083487,
          -7.186065,
          -7.253112,
          -7.1562157,
          -2.4079034,
          -7.047896,
          -7.455129,
          -3.5013504,
          -3.0784402,
          -2.24602,
          -3.406458,
          -6.7017984,
          -1.0301144,
          -0.9270803,
          -1.1781131,
          -5.9477115,
          -7.0954595,
          -7.1650085,
          2.2554731,
          2.9333913,
          6.357779,
          -3.7522156,
          0.5668896,
          6.3350534,
          -1.52322,
          -1.8910987,
          -2.0672357,
          0.9148443,
          -1.4775459,
          -0.6062793,
          -2.662555,
          -2.487109,
          -1.6823004,
          -1.864298,
          -6.274569,
          -5.141898,
          -7.137166,
          4.664576,
          3.3377905,
          2.0982568,
          1.8740735,
          1.220334,
          2.3934042,
          2.1464992,
          2.1938381,
          2.2811346,
          2.2063127,
          0.32221547,
          -0.41693044,
          1.2789664,
          1.1805869,
          1.9015067,
          2.2420528,
          1.9916651,
          1.8257221,
          2.1110067,
          2.1729805,
          2.4649754,
          2.0497944,
          1.911136,
          1.903134,
          2.2919116,
          1.6713728,
          1.8896774,
          1.5618975,
          2.008469,
          2.1556458,
          4.421433,
          2.6154776,
          2.8667126,
          -9.954309,
          -10.021693,
          -0.46930844,
          -8.200154,
          -5.359702,
          -6.374647,
          -6.2957063,
          -6.354901,
          -7.2381077,
          -6.232431,
          -6.2666984,
          -6.2782464,
          -6.473511,
          -1.0282108,
          -5.967415,
          -6.5056534,
          -6.6806498,
          -5.753816,
          -6.307356,
          -5.4402432,
          -6.3652506,
          -5.871198,
          -6.2271137,
          -6.1866374,
          -6.057977,
          -6.369511,
          -6.3231707,
          -6.380196,
          -5.4604197,
          -4.4830403,
          8.047575,
          -6.2171135,
          -5.917009,
          7.12464,
          6.953672,
          7.0529847,
          6.759482,
          7.769746,
          7.4875097,
          7.628629,
          7.809466,
          7.671981,
          -2.5675328,
          -2.1339674,
          -6.3857713,
          -6.338253,
          2.6002905,
          6.8489614,
          7.6100316,
          -5.0737185,
          -5.0485506,
          -4.9920087,
          -4.724353,
          -4.779171,
          -5.2080355,
          -4.9829698,
          -4.9010115,
          -4.9332385,
          -5.1623554,
          -5.057748,
          -4.9848557,
          -4.918662,
          -5.0213366,
          -4.969956,
          -5.1794014,
          -5.0925355,
          -4.919324,
          -5.081251,
          -5.001459,
          -4.9176373,
          -4.912115,
          -4.9811525,
          -4.983303,
          -4.9386225,
          7.03452,
          7.717098,
          7.7943316,
          -6.721784,
          -9.090157,
          -9.700007,
          -9.476272,
          -11.601439,
          -10.009735,
          -10.180549,
          -10.489354,
          -5.7476816,
          7.631925,
          -7.0862684,
          -7.171035,
          -7.035223,
          2.9506998,
          -2.970814,
          0.51120466,
          -1.941223,
          -1.6791397,
          0.7748803,
          -1.4080756,
          -0.5858639,
          -2.691549,
          -3.1353946,
          -3.0911222,
          -7.627718,
          -7.475944,
          -5.217747,
          -3.5162153,
          2.4663656,
          -0.6831059,
          -8.704183,
          -7.822644,
          -7.90411,
          -11.975507,
          -7.559402,
          -7.036392,
          -6.961198,
          -11.592771,
          -8.830352,
          -8.898532,
          8.134923,
          -8.222688,
          -6.3452005,
          -6.1589274,
          -2.9887831,
          -11.721881,
          4.151618,
          -5.867082,
          -7.0193067,
          -11.775057,
          -6.407084,
          -6.382662,
          -3.6748471,
          -0.7351603,
          -4.6639977,
          -7.6841235,
          -7.666562,
          -7.8190713,
          -7.8453407,
          -8.174456,
          -8.056101,
          -11.622488,
          -11.482433,
          -4.350051,
          0.24092941,
          -5.2888203,
          -5.529627,
          -12.154461,
          -5.7227683,
          6.833115,
          2.5676057,
          -4.5959573,
          -1.7790858,
          -0.27626574,
          -0.23798507,
          -0.6489548,
          0.82271224,
          -0.32272795,
          -0.016070722,
          -0.6411524,
          -2.9759881,
          -2.5942183,
          1.9482946,
          5.7967076,
          7.3058844,
          3.071112,
          4.354216,
          1.3700013,
          1.6449783,
          2.3875518,
          1.7725327,
          -7.093946,
          2.0564582,
          0.82975405,
          -8.935804,
          -8.944346,
          -11.602577,
          -6.4119916,
          -4.8891716,
          -5.6044335,
          -4.0332527,
          -4.59544,
          5.9155397,
          -7.808054,
          -5.3697715,
          -2.0262513,
          -7.572385,
          -8.978127,
          -6.957164,
          -7.8953,
          -9.645523,
          -2.7972703,
          -3.254811,
          -1.9092088,
          -4.3316054,
          -8.990193,
          -4.5858297,
          -1.3380517,
          -6.063235,
          -4.802834,
          -3.49858,
          -1.201472,
          -3.4040194,
          -3.1682549,
          -8.887538,
          -8.850142,
          2.2632499,
          2.6489763,
          2.2530026,
          2.9999692,
          0.20615669,
          -6.998572,
          -7.1439075,
          -7.3116393,
          -3.8943534,
          -5.764393,
          -3.411468,
          -7.0218334,
          1.0040194,
          -7.01676,
          -7.052431,
          -7.192735,
          -7.198318,
          0.90040696,
          -1.2593278,
          -1.3356401,
          -6.4419823,
          -0.14801468,
          -7.0906787,
          -7.0255876,
          -7.1542687,
          -7.7808785,
          -7.751519,
          -8.124723,
          -5.784253,
          -3.9805572,
          -4.9901342,
          -4.41826,
          -4.6933236,
          -5.612864,
          -3.8011103,
          -4.128618,
          -1.1538262,
          -2.7466326,
          -1.0739043,
          -2.9434443,
          1.9379586,
          -3.6481752,
          -3.0041807,
          -0.8472515,
          -0.8019423,
          -1.2981045,
          0.82249343,
          -3.2100108,
          -0.84574485,
          -0.72957486,
          -0.2839644,
          0.28769937,
          -3.043077,
          -1.1812419,
          -0.8765643,
          -2.1870034,
          -1.2829498,
          -2.5801947,
          -0.26379138,
          -0.29545358,
          -2.8913708,
          -0.34005985,
          -3.0081353,
          -12.19434,
          -5.3652854,
          5.3673625,
          -5.4627113,
          -7.1680136,
          -7.0373015,
          3.3818328,
          -3.1673198,
          -1.4425154,
          -1.6173828,
          -1.1412413,
          -1.2054584,
          -0.23280914,
          0.8299065,
          -2.0725725,
          -0.33128473,
          0.29615986,
          -1.6327107,
          -1.0474035,
          -4.3752136,
          -3.8337655,
          -3.5332227,
          -2.4346476,
          0.072890095,
          -3.117077,
          -0.38549533,
          -0.2850722,
          -6.174204,
          -6.0524364,
          -11.893201,
          -9.055322,
          -9.569724,
          -8.69939,
          0.8407971,
          -8.225614,
          -8.137052,
          -8.045877,
          -9.633106,
          -9.6444,
          -0.09342966,
          -9.921308,
          -10.011933,
          0.9801119,
          -9.301998,
          1.6642656,
          1.0460049,
          2.0180764,
          1.7447917,
          3.4827025,
          1.1675744,
          0.51626426,
          0.75033206,
          0.6487956,
          -0.23315483,
          -0.09740889,
          -0.15515837,
          -0.8244338,
          0.5509683,
          -0.18395045,
          -0.93675536,
          0.025487622,
          0.81209797,
          0.55266446,
          1.0856038,
          -8.754586,
          -8.814232,
          -7.6661797,
          -0.112859145,
          -11.652893,
          -3.5370715,
          -1.2914138,
          -2.0011704,
          -0.9189818,
          -2.2544496,
          -1.6785963,
          -3.1279967,
          -4.2688484,
          -5.241525,
          -0.99697125,
          -0.7938852,
          -0.9372628,
          -4.2620034,
          2.3646998,
          -0.6564345,
          0.22049461,
          -0.032923643,
          0.45004222,
          -1.4914627,
          0.9831607,
          0.7575734,
          0.83150727,
          1.0622731,
          -3.4359624,
          1.7013952,
          1.2751265,
          1.5314012,
          1.3710833,
          -2.402451,
          2.1735668,
          2.53133,
          11.541534,
          1.2342489,
          -3.3625524,
          -2.3723445,
          -1.9418526,
          -1.3912874,
          -7.772673,
          -0.87219584,
          -0.16119398,
          -1.5515374,
          -7.8708663,
          -1.4664412,
          -2.0218987,
          -1.6482767,
          -8.093316,
          -0.87879103,
          -2.5833583,
          -1.7631246,
          -7.9968367,
          -1.4818081,
          -2.3477767,
          -1.6755112,
          -9.198903,
          -9.247413,
          -0.9229565,
          -9.062709,
          6.083934,
          -5.9523907,
          -5.3855796,
          -5.4331274,
          -11.634676,
          -12.155943,
          -5.7696834,
          -11.866348,
          -3.1468422,
          -3.4575696,
          2.3214746,
          3.248205,
          2.3633773,
          1.9527901,
          4.2828526,
          4.640849,
          -5.2875752,
          -2.1369238,
          -1.3112463,
          -4.0048847,
          -12.324203,
          -5.1578727,
          -5.6671486,
          -9.730125,
          -1.6149732,
          2.322323,
          1.217418,
          1.4366595,
          1.3332509,
          1.5634085,
          0.19471276,
          0.7325112,
          1.7788609,
          0.856958,
          0.45631337,
          14.213884,
          0.50477535,
          0.41106293,
          -0.6234242,
          0.54787815,
          0.24473003,
          1.9624753,
          2.795482,
          0.35279104,
          -0.3943437,
          -0.105977274,
          -0.73914367,
          0.14457753,
          1.2101101,
          -7.5105667,
          -7.439046,
          -7.435898,
          -7.5292306,
          -7.3462195,
          -7.2771115,
          -7.1527457,
          1.8941845,
          1.8568193,
          -6.9172997,
          -7.2268825,
          2.7178462,
          -7.0714583,
          -0.32956192,
          0.634046,
          2.0450335,
          3.6160176,
          -7.3856173,
          -6.9546437,
          -7.2420397,
          -6.8958874,
          -7.045827,
          -0.7009383,
          -7.197893,
          -6.88898,
          -7.4857597,
          -7.5717545,
          -7.479189,
          -6.144012,
          -4.4171333,
          -9.653758,
          -9.32399,
          -9.730816,
          0.20631544,
          -9.793748,
          6.421599,
          6.5099435,
          6.4262733,
          6.6786695,
          6.136883,
          6.486552,
          6.288582,
          6.2929626,
          6.2993145,
          -1.8983806,
          -3.785697,
          2.6974874,
          2.66309,
          3.4999268,
          2.9706283,
          2.8413866,
          2.8146772,
          2.905517,
          2.6108406,
          2.7863717,
          2.7304251,
          -1.1018721,
          -6.889884,
          1.482221,
          -4.7751436,
          -1.0841268,
          -0.6383509,
          -0.65633523,
          -1.7393351,
          -0.95541686,
          -1.2297101,
          -1.0312907,
          -0.6658396,
          -1.1441087,
          -1.039989,
          -1.1535891,
          -0.77619344,
          -1.1075466,
          -1.4301069,
          -0.54226136,
          -0.8665866,
          -5.818462,
          6.690779,
          -6.0044317,
          -5.131556,
          6.638514,
          -5.9877605,
          -5.5779977,
          -5.933576,
          -6.0653644,
          -6.0328193,
          -6.0049376,
          -6.1074786,
          -6.0876575,
          -5.677866,
          4.8167276,
          4.189949,
          4.011502,
          -2.7917469,
          -2.482288,
          2.2554684,
          4.2967806,
          4.844579,
          4.3350353,
          2.7925763,
          -5.7800426,
          -7.4677854,
          4.2394457,
          -6.6330924,
          -6.8209114,
          -8.937229,
          -8.982743,
          -8.157044,
          -11.45606,
          -5.9511075,
          1.3265318,
          -9.619376,
          -9.988056,
          -9.481299,
          6.050668,
          -2.993828,
          -11.769009,
          -9.561373,
          -8.965737,
          -1.0082799,
          -9.072921,
          9.333319,
          9.487296,
          9.595098,
          -9.057798,
          -4.104985,
          -6.9751244,
          -1.2596588,
          5.6405144,
          -2.5085127,
          -4.487069,
          0.10833098,
          1.2014465,
          -1.2141485,
          -1.0723596,
          -3.8603387,
          -7.590989,
          -2.9777362,
          -1.6747406,
          1.4629148,
          -2.0694463,
          -1.734615,
          -2.4466395,
          -1.7392457,
          -2.5435977,
          -2.4896405,
          -2.3719764,
          -3.9313297,
          -3.7383091,
          -3.9743655,
          -4.0267096,
          -3.926126,
          -3.8028638,
          -4.007806,
          -3.6899242,
          -3.530046,
          -3.7520347,
          -3.7240992,
          -3.8854363,
          -3.21982,
          -3.8424067,
          -4.470096,
          -6.1170154,
          -6.0378723,
          -11.804252,
          -12.29341,
          -8.482161,
          -2.130106,
          -6.749797,
          0.5273011,
          -1.2149812,
          -2.996981,
          -1.129235,
          -4.91124,
          -4.1270266,
          -5.1059465,
          6.770951,
          -5.228672,
          14.051973,
          -3.6290143,
          -4.733708,
          -4.978366,
          -4.866601,
          -4.1651917,
          -3.894101,
          -4.775811,
          0.03732743,
          -3.937017,
          -5.1537914,
          6.2946057,
          -4.8393154,
          -4.609278,
          6.5516253,
          6.2583327,
          -6.0721955,
          -3.3140194,
          -2.22821,
          -2.7417557,
          -1.2910581,
          -5.64583,
          -7.1705694,
          -7.563301,
          -7.3840976,
          -11.868706,
          -7.550279,
          -7.3803787,
          2.7980206,
          -7.006924,
          -7.2106075,
          -7.060672,
          -1.531382,
          -0.34494066,
          -0.62761086,
          0.7565048,
          -0.32557976,
          -7.0040636,
          -6.9631786,
          -7.879616,
          -5.9623766,
          -12.172546,
          -12.365646,
          -7.7445636,
          -7.65976,
          -5.646713,
          -5.56649,
          -2.1155157,
          -5.769445,
          -3.6784089,
          -2.2774918,
          -2.3643472,
          -2.685552,
          2.1830792,
          -3.480825,
          -2.4579935,
          -1.5907656,
          -1.4011759,
          -2.3943563,
          -2.8318143,
          -2.3562624,
          -1.7201558,
          -6.109249,
          -2.5546443,
          -2.4736984,
          -8.150442,
          -1.3279424,
          0.5916955,
          -9.372835,
          -1.3975536,
          -0.6519702,
          0.5445752,
          -9.47279,
          0.39112562,
          -1.0096561,
          2.802838,
          -6.102292,
          -5.7442656,
          -5.03211,
          -5.5736117,
          -5.824113,
          1.4909002,
          2.2486644,
          -7.0288305,
          -7.3469644,
          -7.631322,
          -1.2169545,
          -5.7294207,
          -8.7292795,
          -10.579068,
          -1.1787616,
          -1.9954304,
          -0.05549453,
          -0.54752505,
          -0.64154136,
          -10.673712,
          -11.02328,
          -9.945157,
          -11.560916,
          -5.5310893,
          -5.779953,
          -12.299797,
          -3.836607,
          -5.85503,
          -5.8712444,
          -11.785824,
          8.010685,
          7.69738,
          7.0307436,
          6.9956155,
          7.383532,
          7.2547235,
          15.171631,
          15.208909,
          15.208245,
          15.170219,
          15.153049,
          14.620068,
          15.135943,
          15.157563,
          15.146305,
          15.170151,
          15.180499,
          15.178127,
          15.172653,
          14.950458,
          6.6823845,
          7.902223,
          7.974461,
          -3.5255504,
          7.93892,
          7.9560375,
          -1.6130699,
          7.821149,
          7.158764,
          7.9106407,
          -5.066313,
          -5.012283,
          -5.135823,
          -5.0417137,
          -5.00134,
          -4.8770146,
          8.033186,
          -6.9174833,
          -6.246331,
          -6.302127,
          -2.3966289,
          -2.1639469,
          -2.983999,
          -1.0057305,
          -2.0248609,
          4.596235,
          -6.245358,
          -2.1416874,
          -4.2465796,
          7.3253264,
          7.6004105,
          8.095752,
          8.01466,
          7.148512,
          6.954335,
          7.6003323,
          6.7192283,
          7.753416,
          8.046866,
          -2.5314257,
          8.025793,
          8.108701,
          2.6426814,
          7.008307,
          8.005935,
          -5.380938,
          -5.038473,
          -4.988192,
          -4.893527,
          -5.070067,
          -5.0267987,
          -5.409038,
          -5.1511497,
          -5.716256,
          -4.9515553,
          -5.263447,
          -5.0640225,
          -4.9711447,
          -4.976625,
          -4.890131,
          8.078261,
          -6.907762,
          -8.353369,
          -8.506216,
          0.75542736,
          0.66250026,
          -6.6313977,
          -4.613991,
          -12.18091,
          -0.70547444,
          0.70917684,
          -7.18319,
          -7.236996,
          -9.555235,
          -9.741489,
          -8.290575,
          -5.477607,
          -5.45561,
          -6.3177686,
          -6.043363,
          -5.615016,
          -5.2666626,
          -11.523763,
          4.0414643,
          11.828476,
          -5.393709,
          -2.4486988,
          -0.048734423,
          -1.0812515,
          -0.33004126,
          2.4864395,
          -0.23139511,
          -2.463675,
          -7.680247,
          -7.3787723,
          -6.6386642,
          -7.254156,
          -7.3489275,
          -7.481228,
          -6.9351206,
          -7.3846574,
          -7.448608,
          -1.7307266,
          -1.6468464,
          5.5537777,
          -7.432257,
          -1.8490332,
          -1.8497903,
          -1.7994275,
          -7.377749,
          -7.43599,
          -7.5831704,
          -7.4881434,
          -7.600737,
          -7.5898285,
          -6.159837,
          -3.892624,
          -1.9512404,
          -7.404885,
          -7.082474,
          2.2539854,
          -6.9400907,
          -6.967294,
          -0.73814327,
          -7.186989,
          -7.3220215,
          -7.10197,
          -7.410137,
          -7.3352246,
          3.9243743,
          -6.842135,
          -7.6238723,
          -7.3252187,
          -7.4695344,
          -6.8911076,
          -7.390839,
          -1.2469145,
          -2.1842318,
          10.18573,
          10.043525,
          10.708967,
          10.632777,
          10.732317,
          9.856548,
          0.9999648,
          -1.8826624,
          -9.74785,
          -9.909049,
          -10.086649,
          2.6420019,
          -5.3217807,
          -3.2150328,
          0.68743247,
          -4.6629186,
          -3.438043,
          -0.062445913,
          -5.064216,
          -2.7867084,
          -3.277819,
          -0.20106111,
          -5.099915,
          -3.3038118,
          0.49508393,
          -2.0778718,
          -1.1682211,
          0.5997993,
          -0.6095837,
          0.22983482,
          -5.8224306,
          -5.6501865,
          -11.884496,
          -7.506418,
          -7.3015137,
          -7.239408,
          -7.3669996,
          -7.0946817,
          1.9910697,
          -7.294212,
          -7.1121945,
          -3.777291,
          -7.198761,
          2.380892,
          2.0350409,
          2.9067752,
          -7.346907,
          3.7797217,
          3.9605587,
          -7.1537704,
          0.58254015,
          -0.6606176,
          4.12242,
          -6.7303934,
          -3.4874325,
          1.9887971,
          -7.10953,
          -7.3318825,
          0.09454347,
          -0.5066109,
          -4.335794,
          -6.8693953,
          -7.214971,
          -7.2125425,
          -7.318336,
          -6.572133,
          -6.754811,
          -7.5391884,
          -6.30646,
          -7.539324,
          -6.025843,
          -2.1576457,
          -5.073138,
          1.2212596,
          -0.942845,
          -0.07238886,
          1.6147361,
          2.3591728,
          -9.755255,
          -9.907439,
          -8.872765,
          -6.279154,
          -6.0759344,
          -4.7534127,
          -2.5284553,
          -3.2893403,
          -10.089741,
          -10.024688,
          -10.122113,
          -10.312835,
          -4.4226036,
          -2.8771243,
          1.4972731,
          -5.437296,
          -4.8232226,
          -0.83230996,
          -4.858608,
          0.5972361,
          -4.2595882,
          -1.0284526,
          -8.589072,
          -8.577836,
          5.2010703,
          5.7488093,
          5.505574,
          -1.7578427,
          2.994656,
          0.95475537,
          0.32629883,
          -1.8738403,
          4.6651754,
          -6.969109,
          1.3656095,
          2.1231313,
          -4.2041187,
          -7.5386868,
          -4.496272,
          -2.6443849,
          -1.7861009,
          -0.99490714,
          -0.8160532,
          -5.971213,
          -4.856144,
          -8.497038,
          0.773782,
          -2.036075,
          -11.792736,
          -6.6395044,
          -6.5418954,
          -7.6832943,
          -1.1168865,
          -2.3273363,
          -8.13342,
          -7.8623304,
          -7.0178375,
          -7.2449465,
          -7.3367934,
          -9.945244,
          -9.807624,
          -9.940019,
          -7.5995746,
          -2.468505,
          0.11585449,
          0.12599596,
          -3.830353,
          -3.2530885,
          -3.4558322,
          -3.4007878,
          -3.1721215,
          -3.3391376,
          -3.322571,
          -3.4167552,
          -3.6356578,
          -3.2488523,
          -3.3652253,
          -3.4337785,
          -3.5864758,
          -3.8307989,
          -3.461458,
          -3.575982,
          -3.4198675,
          -3.3283813,
          7.4983654,
          7.988513,
          7.2448535,
          7.706673,
          7.1044264,
          7.058486,
          6.9784117,
          7.2367115,
          15.177457,
          15.211066,
          15.210268,
          15.1739025,
          15.1546135,
          14.648289,
          15.003429,
          15.136676,
          15.153993,
          15.167885,
          15.210789,
          15.160003,
          15.1723585,
          15.133927,
          6.7756157,
          7.667094,
          7.637915,
          7.625003,
          7.834949,
          -2.756362,
          -2.0484772,
          7.598935,
          7.695551,
          2.746128,
          6.9241657,
          7.758233,
          -5.115929,
          -5.029137,
          -5.6714683,
          -5.0978155,
          -5.078346,
          -5.0577946,
          -5.180591,
          -5.0084605,
          -4.974745,
          -5.1125445,
          -4.999518,
          -5.0393534,
          -5.3504004,
          -5.082549,
          -5.0598545,
          -5.59851,
          -5.0474005,
          -5.0372686,
          -5.1755037,
          -5.1809306,
          -4.9249067,
          -5.0114307,
          -5.0577354,
          -5.004528,
          -4.9824085,
          -4.9677234,
          -5.1097536,
          -5.380657,
          -4.7172065,
          -4.837596,
          -4.9809656,
          -5.397426,
          -5.044303,
          -4.933089,
          -5.0274253,
          -5.034333,
          -4.996323,
          -4.9538054,
          -5.0556545,
          -5.0169353,
          -4.949907,
          -5.0420384,
          -5.029738,
          -5.0550632,
          -5.14394,
          -5.194243,
          -4.9080443,
          -4.7688174,
          -5.0963616,
          -5.137389,
          -5.6054554,
          -5.000821,
          -4.9662304,
          -4.904041,
          -5.0219297,
          -4.9414825,
          -5.2456155,
          -5.0946255,
          -5.1045895,
          -4.914055,
          -5.0314593,
          -5.060407,
          -5.2233324,
          -4.9954147,
          -4.9231114,
          -4.912655,
          -5.0156393,
          -5.036105,
          -5.000328,
          -4.9273076,
          6.8835144,
          7.9167337,
          7.9476867,
          -6.7998304,
          4.888185,
          -7.401714,
          -7.1229324,
          -1.750156,
          -2.4159029,
          -1.8689889,
          -6.9995236,
          -7.016341,
          -11.6488285,
          -6.095732,
          -5.9465437,
          -11.76125,
          -12.244839,
          -8.707126,
          -8.879914,
          -11.522185,
          -11.993788,
          -6.015709,
          -6.1108284,
          -2.602065,
          -5.415455,
          -5.472867,
          -2.357421,
          -3.0514436,
          -12.262427,
          -3.537962,
          -3.5387707,
          -2.3165448,
          -2.2910166,
          -1.2920002,
          -3.639694,
          -3.387638,
          -4.846048,
          2.13563,
          -3.502552,
          -0.22549057,
          -0.77009994,
          -3.0111246,
          -1.6310351,
          0.15044074,
          -2.967026,
          -1.4216096,
          -0.77308565,
          3.0740874,
          -2.817329,
          4.8137636,
          -4.0848145,
          -2.6213088,
          -1.9405018,
          -0.5047462,
          -2.2410202,
          -0.83964545,
          -0.97616756,
          -0.53728753,
          -0.7922465,
          -1.4497586,
          -0.8813873,
          -1.641604,
          -2.1467931,
          -2.4091632,
          -2.763511,
          -2.9506094,
          -2.709485,
          -2.4105265,
          -2.7177947,
          -3.6161127,
          -2.814209,
          -3.1243014,
          -0.41275972,
          -0.55787665,
          0.07510647,
          0.09715974,
          -0.9460646,
          -7.231925,
          -7.257328,
          -7.302372,
          -0.8527608,
          -0.14999,
          -6.5704536,
          0.7737591,
          0.9206094,
          -3.1645913,
          0.6147252,
          -1.4569345,
          -6.161422,
          -6.131903,
          -9.787457,
          -9.634011,
          -9.822347,
          0.268963,
          -9.805889,
          5.6761727,
          -9.829518,
          -10.627034,
          -10.259824,
          -9.790118,
          -11.489298,
          -5.916343,
          -5.217227,
          -3.5725336,
          -3.7058623,
          -5.5574393,
          -9.669064,
          -9.640201,
          -9.723719,
          -9.753412,
          -9.138927,
          -11.404798,
          -8.620038,
          -8.674347,
          -7.8036804,
          -7.566891,
          -6.016052,
          -1.9614797,
          -3.7728572,
          -8.508808,
          -8.415876,
          -7.069007,
          -4.901966,
          -7.662822,
          -1.0138177,
          -1.2074672,
          -8.917897,
          -0.8811376,
          0.38296548,
          -10.03272,
          -1.4015231,
          -1.572842,
          -11.396685,
          -11.444642,
          -11.286799,
          -11.409243,
          -11.287836,
          -11.390559,
          -11.371049,
          -11.348989,
          -11.367311,
          -11.375242,
          -11.502096,
          -1.8379067,
          -1.3405185,
          -0.42963234,
          -8.346818,
          -6.2026215,
          1.1714872,
          -1.6559644,
          -1.2684623,
          -7.7701445,
          -7.729069,
          -11.990943,
          -9.221675,
          -9.947342,
          -9.070359,
          -8.747542,
          -7.393888,
          -7.0163946,
          -0.7010553,
          -6.79527,
          -6.5073853,
          -6.495314,
          -4.3702965,
          4.296836,
          5.9083977,
          4.9594674,
          -0.7258426,
          -1.0809785,
          -0.18924442,
          -1.6244199,
          -1.4744884,
          -1.4951255,
          -8.407475,
          -8.720638,
          -5.60891,
          -3.4531095,
          -5.2319293,
          -4.5447297,
          0.35316378,
          -2.7737057,
          -4.9944153,
          -4.7071114,
          -9.892047,
          -9.957851,
          -4.530379,
          -10.142312,
          -8.570869,
          -11.6522,
          -9.929092,
          -10.152589,
          -10.265395,
          -1.3927236,
          -9.37539,
          -11.639055,
          -5.175812,
          -7.1355333,
          -7.176787,
          -7.1011124,
          2.9387524,
          -2.458375,
          -2.1181512,
          -1.636457,
          -0.49932918,
          0.8454037,
          -1.2570332,
          -0.2607739,
          -3.5191631,
          -2.5822165,
          -2.2410524,
          -7.592521,
          -7.408617,
          -7.2038507,
          -1.6611189,
          -6.901069,
          -6.940705,
          -6.737067,
          -6.8014946,
          -6.452919,
          -7.11203,
          -6.9128838,
          -6.7542267,
          -2.3836565,
          -2.2522113,
          -0.8386791,
          -1.010503,
          -2.5533767,
          -6.4159656,
          -6.1218147,
          -11.663048,
          -9.232245,
          -8.923226,
          -7.97762,
          -8.901627,
          0.0055722455,
          -1.3566995,
          4.2042503,
          -0.34747773,
          -3.8839772,
          -3.539868,
          -3.5515015,
          2.0661876,
          4.270654,
          1.969396,
          -2.3316355,
          -0.91887677,
          -7.5956106,
          -6.7988343,
          -6.789519,
          -4.2792397,
          -7.14255,
          -9.214685,
          -9.270206,
          -8.639413,
          -7.996017,
          -8.34647,
          -9.398969,
          -5.4911075,
          2.8743258,
          -0.44528884,
          -3.3341253,
          2.6340485,
          -1.984022,
          -1.0057726,
          0.37167275,
          -0.4629892,
          -2.8484392,
          -3.761669,
          0.7759765,
          -0.32375064,
          -2.3733304,
          -0.2856674,
          1.7915797,
          -1.6974031,
          1.6191275,
          -5.824138,
          -7.8604646,
          -1.3221254,
          -0.94383,
          -1.1598797,
          -6.6547885,
          -1.6599661,
          -0.8899906,
          -0.040050503,
          -0.19741383,
          0.1508032,
          -0.071756825,
          -0.23613584,
          0.9603686,
          1.2485694,
          0.77133816,
          -0.56756175,
          -0.5775483,
          -0.284462,
          0.467802,
          0.10582097,
          -7.5882664,
          -7.3730636,
          0.19506776,
          -7.3093514,
          0.24101834,
          -2.145221,
          0.36124507,
          -2.049066,
          -5.3140125,
          -1.1041416,
          4.6844826,
          7.3027945,
          -5.399096,
          -5.5695014,
          -5.6960125,
          -5.6750865,
          -6.827952,
          -5.4000306,
          -5.472409,
          -5.7556586,
          -5.9370112,
          -6.341923,
          -5.6309013,
          -5.5410256,
          -5.304841,
          -5.8869953,
          -4.2423,
          -5.8480062,
          -5.084555,
          -5.318961,
          -7.1495876,
          6.413634,
          -9.550151,
          -8.914239,
          5.232788,
          -5.452172,
          -9.504557,
          -5.519061,
          -7.456249,
          -3.922236,
          -1.269594,
          -1.8280027,
          -7.132089,
          -7.4793634,
          -7.3556867,
          -7.319561,
          -7.1767893,
          -7.3914595,
          -7.173368,
          -7.314511,
          -7.4566474,
          -7.2020845,
          -7.3857346,
          -7.263356,
          -7.3143907,
          -7.421514,
          9.446749,
          -2.8334482,
          -2.6228502,
          -2.2958202,
          -2.1444001,
          -1.8213834,
          -2.286703,
          -1.3800601,
          -6.9618654,
          -7.5758595,
          -7.85035,
          -7.8913913,
          -9.726553,
          -9.31565,
          -9.949599,
          -9.774845,
          -9.845706,
          -10.031877,
          -10.085874,
          -6.075352,
          -5.309449,
          -5.784244,
          -5.634232,
          -6.246415,
          -5.472341,
          -5.88845,
          -5.764843,
          -11.984814,
          -2.8823261,
          -1.6508499,
          -9.781721,
          -9.887232,
          -9.461984,
          -9.046216,
          -9.824118,
          -9.336083,
          -0.27163205,
          0.07972781,
          -0.8977465,
          -1.2884675,
          1.653415,
          -9.981102,
          0.006621126,
          -9.730729,
          -10.538434,
          -3.2270286,
          -0.46231028,
          -0.58396435,
          0.31445855,
          0.34964254,
          -0.7009102,
          -0.07478409,
          -0.15688853,
          -0.111935765,
          1.9799942,
          1.9526796,
          -5.7039423,
          -5.472557,
          -11.850707,
          -5.504371,
          2.1823692,
          3.1163826,
          5.326856,
          7.559865,
          7.625131,
          7.620012,
          7.627398,
          7.589596,
          7.6116624,
          7.651389,
          7.604422,
          7.636196,
          7.610701,
          7.643718,
          7.5728493,
          7.497393,
          -1.9932832,
          -1.5737263,
          -0.33794877,
          0.69479793,
          -0.22432889,
          -0.28807288,
          0.022829654,
          -2.7159634,
          -2.6976154,
          -2.4134655,
          -2.051203,
          -0.32163796,
          -5.3294926,
          -6.1376266,
          -2.002597,
          -1.6364053,
          -5.66624,
          -4.4682684,
          -3.0348022,
          -3.4420176,
          -3.102129,
          -3.1435506,
          -3.138842,
          0.13039339,
          -8.806223,
          -9.03455,
          2.3295512,
          2.5861745,
          3.2247877,
          -1.4107181,
          -6.290666,
          -6.049983,
          -2.6472132,
          -4.01862,
          -5.842232,
          -2.40097,
          -4.2489295,
          -4.0689197,
          -5.6915226,
          -12.299854,
          -4.710652,
          -9.257241,
          -7.2104187,
          -7.419826,
          -7.315019,
          -6.929496,
          -7.393915,
          -7.43227,
          0.12988307,
          -1.8007457,
          -4.7871523,
          -2.711114,
          -3.5919998,
          -8.329712,
          2.3903344,
          2.5188322,
          -5.3265457,
          -2.149899,
          -2.8451293,
          -2.306607,
          -0.8631402,
          -2.2946799,
          -0.17581972,
          -1.2556347,
          -1.5552508,
          -0.9662043,
          -0.42966458,
          -0.726364,
          0.008418709,
          -0.49202135,
          -0.31546426,
          -6.435392,
          -5.9551144,
          -3.9457421,
          -7.37052,
          -5.13253,
          -5.1141243,
          -5.147788,
          -5.0177746,
          -12.11336,
          -12.276149,
          -4.6757903,
          -3.6541572,
          0.26513734,
          -1.245319,
          6.346277,
          -0.12216111,
          -0.30908838,
          0.86434,
          -9.902084,
          -1.5690861,
          -0.43002623,
          0.055580262,
          1.8063096,
          -0.25582063,
          -0.04990925,
          -0.73519474,
          -0.44219643,
          -0.5776184,
          -0.6269467,
          -0.53497756,
          -0.41968486,
          -0.17966375,
          0.04071391,
          -0.504934,
          -4.5263643,
          -3.6327925,
          -3.330598,
          0.19231848,
          -2.678423,
          6.1066785,
          -6.1924205,
          -5.737677,
          -5.769745,
          -5.058143,
          -1.8780026,
          -2.8850498,
          -2.1477032,
          -2.1580112,
          -1.6472406,
          -3.4876401,
          -2.59084,
          -3.0308623,
          -2.7899542,
          -4.617923,
          -3.5894644,
          -4.999168,
          6.2590537,
          -5.2803035,
          -4.346166,
          -12.291671,
          -7.445607,
          -7.2744923,
          -2.261138,
          -2.8944955,
          -2.1309423,
          -2.5175016,
          -1.3792431,
          -11.491437,
          3.173799,
          2.4064314,
          2.858313,
          -2.426558,
          -6.423052,
          -5.9807863,
          -5.281115,
          -11.821896,
          -12.171247,
          -7.6608195,
          -7.2713804,
          2.5445135,
          0.38958597,
          -1.4861146,
          1.1091379,
          -0.41193712,
          -2.2429023,
          5.929753,
          -0.9587521,
          6.477491,
          6.25676,
          6.3426166,
          6.1585584,
          6.5515857,
          6.512097,
          6.605666,
          -2.9605072,
          -2.0428514,
          -2.0311174,
          1.6293381,
          -0.37322077,
          -4.4686213,
          0.8223959,
          1.214715,
          -0.66854954,
          -5.7748985,
          -5.1390605,
          -5.851427,
          -5.517225,
          -5.5732107,
          -4.598964,
          -5.3963757,
          -4.360758,
          -3.2235754,
          -12.110648,
          -12.348207,
          -4.9607043,
          -7.1273937,
          -7.096627,
          -7.204997,
          2.514893,
          3.4931502,
          -2.226888,
          -3.030042,
          -2.3240798,
          -1.5916066,
          -0.70151913,
          -0.21700914,
          -0.3363026,
          -0.87354434,
          0.91879797,
          -1.5296054,
          -0.3224923,
          0.25566158,
          -2.4204538,
          -1.0465031,
          -1.0213517,
          -2.2753558,
          -5.3406563,
          -4.630074,
          3.3368418,
          -0.40700406,
          5.8521843,
          -1.0588416,
          -7.8811545,
          -7.7611027,
          -7.5547566,
          5.6831527,
          -3.150622,
          -0.002436518,
          -1.9136857,
          -0.30390406,
          -0.91080576,
          -8.299547,
          -8.462339,
          -0.7072875,
          -8.405006,
          -11.258445,
          -9.7869835,
          -9.971612,
          -9.350138,
          -9.075333,
          -1.1615565,
          -8.721644,
          -9.000518,
          -10.25403,
          -7.5778265,
          -7.3032994,
          -11.645284,
          1.0734603,
          1.1148489,
          0.4425916,
          2.7167387,
          -3.9131906,
          -5.5365386,
          -5.866045,
          -5.152004,
          -4.2023935,
          -4.7646227,
          -1.9852682,
          -5.4895115,
          -9.216504,
          -10.525403,
          -6.919065,
          2.6220748,
          0.57584506,
          0.22287004,
          -0.72516114,
          0.66779745,
          -0.31982064,
          -0.71251065,
          0.19481297,
          -0.03578199,
          -0.016545583,
          -0.09267763,
          0.019337403,
          -0.86317265,
          -8.673877,
          0.70878994,
          -0.540301,
          -7.548796,
          -6.736251,
          -6.8248005,
          3.1705372,
          -11.742163,
          -12.273662,
          -1.6796434,
          -0.5443126,
          -2.4455128,
          -2.3027053,
          -7.0964265,
          -5.580676,
          -5.5026484,
          -5.6607847,
          -3.981216,
          -1.5498289,
          -1.3490742,
          -11.630436,
          -11.70525,
          -11.820134,
          -11.846764,
          -11.907686,
          -5.8480754,
          -6.040138,
          -4.6151853,
          -4.577267,
          -5.3188114,
          -4.4000807,
          -11.99952,
          -7.7154646,
          -7.5321403,
          -7.236994,
          -11.738971,
          -0.7776567,
          -0.73561704,
          0.41857177,
          2.460822,
          -8.859316,
          -8.712787,
          -5.1162744,
          -1.1672108,
          -11.760033,
          -8.702778,
          -7.5774946,
          -11.944288,
          -9.793243,
          -9.852942,
          2.4932463,
          -6.7252965,
          -9.299305,
          -9.315884,
          -9.149997,
          -8.009488,
          -9.8902645,
          -8.462195,
          -8.710617,
          -4.52317,
          0.26301742,
          -0.7760023,
          -0.57165724,
          -0.6794558,
          -0.10753795,
          -0.17053643,
          -0.2501638,
          -4.0548825,
          1.3536814,
          1.4668458,
          2.2707853,
          1.7379844,
          1.7103035,
          2.064415,
          -2.811864,
          -2.7303426,
          -7.780367,
          -7.5339513,
          4.944655,
          -5.1978493,
          -1.7472076,
          -9.6139555,
          -9.927345,
          -7.807617,
          -8.836137,
          -8.874375,
          -0.28118622,
          -7.8989005,
          -6.222276,
          -3.0961306,
          -2.3409555,
          -1.4978682,
          1.249734,
          -7.1221533,
          -3.4184263,
          -3.0113242,
          -2.586582,
          -2.1689053,
          -2.666254,
          -2.4382854,
          -1.8045702,
          -1.4852033,
          -1.8318404,
          -0.37650833,
          -7.278497,
          -7.166431,
          -7.1872983,
          -7.23104,
          0.66382724,
          1.3484645,
          0.8802307,
          0.64814943,
          1.0118701,
          -7.1781044,
          -7.183353,
          -7.224404,
          -4.9924965,
          -4.6953816,
          -3.4137304,
          -4.8259406,
          -1.8059484,
          -3.5341146,
          -2.7115128,
          -3.516689,
          -3.2879653,
          -2.047382,
          -2.525506,
          -4.496429,
          -2.9605677,
          -4.2886605,
          -4.4363127,
          -1.8376011,
          -4.4749484,
          -1.548821,
          -2.237064,
          -1.8361039,
          -2.2717757,
          -1.8032429,
          -4.0798545,
          -4.245308,
          -5.568263,
          -5.1947246,
          -5.7456775,
          -7.501386,
          -8.859299,
          1.2305527,
          0.2812389,
          0.7497112,
          2.4012685,
          -3.576887,
          -0.70281297,
          -3.585123,
          1.007366,
          0.57148945,
          0.69195014,
          1.1739889,
          1.7415348,
          -2.30841,
          -1.2514918,
          -1.2743617,
          -1.2007247,
          -1.2745373,
          -1.6480722,
          -0.53801924,
          -1.0402164,
          -9.967513,
          -10.026092,
          -10.2122,
          -10.540746,
          -9.9217005,
          -10.0225115,
          -11.446047,
          -8.245646,
          -8.615145,
          -7.6567903,
          -4.457049,
          -4.57954,
          -4.4565988,
          -12.257944,
          -6.619805,
          -6.017461,
          -11.714575,
          -6.148261,
          -2.6141913,
          -1.4711084,
          1.1396562,
          -0.63977253,
          -0.7014827,
          -0.37357795,
          0.5275102,
          2.3649423,
          -0.41868138,
          -1.0734288,
          -0.8278033,
          -4.0055885,
          -2.8998885,
          1.2871926,
          0.22589922,
          2.167322,
          4.2402954,
          -7.6461625,
          1.7141436,
          1.2972993,
          0.40030327,
          -0.35669005,
          0.2459485,
          1.8058202,
          -7.4821568,
          -7.24621,
          -9.80385,
          -9.848468,
          -10.373388,
          6.1117954,
          -5.947834,
          -5.4809556,
          -5.61717,
          -12.234286,
          -12.38022,
          -8.783901,
          -8.884875,
          -0.23792572,
          -11.336206,
          -8.87117,
          -8.758764,
          -3.8573828,
          -3.251072,
          -2.7428408,
          -0.6309667,
          -1.5719182,
          -1.4728341,
          -1.8356594,
          -1.2295278,
          -2.5733442,
          -0.37407577,
          -7.77246,
          -8.710136,
          0.08902385,
          -0.6067013,
          5.2154613,
          -8.363027,
          -8.252018,
          -8.47059,
          1.982828,
          6.136505,
          -8.96183,
          0.22097778,
          0.6924523,
          -9.404543,
          -6.691645,
          -3.9334576,
          -0.67074233,
          -0.34517586,
          -0.3603799,
          -1.3363969,
          -0.42787087,
          -0.5940374,
          -11.558097,
          -4.6897535,
          -1.2509375,
          -0.07376384,
          -6.003572,
          -6.0738587,
          4.6304455,
          -6.008235,
          -5.8596964,
          -0.9825261,
          -3.4578907,
          -7.6032043,
          -7.705172,
          -5.350323,
          -4.692103,
          -6.436731,
          -7.585543,
          -7.9590564,
          -4.6415496,
          -5.695787,
          -11.967316,
          -12.193021,
          -2.9323416,
          -3.7799644,
          -3.2120895,
          -8.724063,
          -8.777443,
          -10.230816,
          6.1240664,
          -5.2196493,
          -5.212111,
          -5.5960855,
          -4.7467413,
          -4.6065392,
          -5.1225033,
          -4.225713,
          -3.6322062,
          -3.7190204,
          -0.9680996,
          -12.002969,
          -12.385546,
          -3.3876536,
          -5.142781,
          -7.0613885,
          -3.1352844,
          -2.5976663,
          -1.9800456,
          -1.9140971,
          -3.65055,
          -3.641635,
          -3.5630045,
          -8.016993,
          -1.5336305,
          -1.7338035,
          -7.401084,
          -7.5768175,
          -7.6787844,
          -7.6750455,
          -0.2166015,
          -7.2866964,
          -4.71553,
          -1.72927,
          -7.3437505,
          -7.47918,
          -7.444896,
          -6.453895,
          -4.4136295,
          -7.1580634,
          -7.5308523,
          -7.2729497,
          -5.288421,
          -11.926615,
          -8.198067,
          -8.416171,
          0.68543,
          -0.46792725,
          -7.4361205,
          0.4006266,
          -4.2357397,
          -4.1325917,
          0.2911588,
          -4.909087,
          -6.2843437,
          -0.38895696,
          -0.5212677,
          0.06285614,
          -6.87567,
          -6.022954,
          -6.2406015,
          -3.1470788,
          -2.7499127,
          -5.6460013,
          3.5080454,
          1.8122149,
          1.9438652,
          -0.20519342,
          0.7464565,
          0.59963214,
          0.08196594,
          0.45160043,
          -4.0743175,
          -3.0405722,
          -3.9923499,
          -5.5944653,
          -6.873704,
          3.636363,
          2.3552508,
          -3.6914003,
          -1.2928747,
          -1.6867126,
          -3.1424215,
          -2.8525643,
          -3.4154284,
          4.7535095,
          4.848542,
          -2.9330096,
          -1.2509806,
          -2.8193207,
          -1.3851742,
          -0.8652221,
          -1.7930722,
          -1.507332,
          -1.5274799,
          -2.8250937,
          -3.0719652,
          -2.5671084,
          -1.3630954,
          -3.3437004,
          -6.132904,
          -5.502709,
          -7.710938,
          -1.049586,
          -3.9531405,
          -3.9643438,
          -3.0022006,
          -2.4044523,
          -5.832732,
          -4.833754,
          -3.6692832,
          5.560824,
          -7.8119164,
          -7.6643534,
          -4.634379,
          -9.399813,
          -9.705979,
          -9.576567,
          -11.831786,
          -5.5894475,
          -8.984362,
          1.0853338,
          -4.660864,
          -8.994408,
          -8.962764,
          -11.139746,
          -2.6069438,
          -0.91956824,
          -5.4287467,
          -12.282499,
          -11.565981,
          -11.380741,
          -10.731032,
          -12.037915,
          -5.962834,
          -5.404915,
          -12.291952,
          -4.170223,
          -3.009657,
          -1.743233,
          0.07467531,
          -2.627886,
          -0.24458867,
          -4.3149996,
          -2.889055,
          -0.33372372,
          -0.71558386,
          -4.9254804,
          -7.7472234,
          -7.3325763,
          2.9647079,
          0.77233243,
          -7.2113085,
          -7.2634654,
          -0.18638897,
          -0.88027704,
          0.8830323,
          -7.163699,
          -7.107752,
          -0.88900733,
          -5.633585,
          -2.0246975,
          -3.4865372,
          -5.008284,
          -3.210646,
          -3.140183,
          -3.7163327,
          -1.3259295,
          -1.563353,
          -3.3122673,
          -2.375634,
          -3.7992246,
          -3.960911,
          -3.239985,
          -3.643454,
          -3.892241,
          -4.0117154,
          -2.3679433,
          -4.233935,
          -2.1309242,
          -3.6393769,
          -4.5345516,
          0.4282127,
          0.4711668,
          -0.21049805,
          -1.5304878,
          -1.0208318,
          -1.0738748,
          -0.42706472,
          2.5951297,
          -5.605222,
          -5.5296907,
          -9.355282,
          -9.361149,
          -9.085271,
          -11.512456,
          -5.9493213,
          -6.100241,
          -5.259883,
          -5.408012,
          -4.655353,
          -4.154988,
          -5.0688515,
          -6.6630173,
          -5.1834683,
          -4.3779206,
          -4.152849,
          -4.4242077,
          -11.71073,
          -12.23591,
          -12.389582,
          -1.0396867,
          -1.2706561,
          -0.24646495,
          -3.0352967,
          -2.282526,
          -2.4019463,
          -3.8638854,
          -4.5508227,
          -2.3728151,
          -0.725935
         ],
         "xaxis": "x",
         "y": [
          0.39487237,
          -0.38900703,
          -1.8497145,
          -0.79850227,
          -1.3330253,
          -3.039825,
          -2.9811046,
          -3.2542024,
          -0.1966548,
          0.49021247,
          -0.5476542,
          -0.021680042,
          -0.76461124,
          -3.231417,
          -0.56085277,
          -0.6458406,
          -2.0193653,
          -2.3330946,
          -6.085419,
          -2.173889,
          -2.2248564,
          -2.6024396,
          -2.5052364,
          -2.6611903,
          -2.7423627,
          -2.2930696,
          0.0965502,
          0.88433295,
          -0.048268124,
          -0.11237366,
          -0.40280664,
          -0.5408484,
          -0.43686485,
          -4.336092,
          -4.362078,
          -1.7879244,
          -3.793187,
          8.96906,
          9.000809,
          9.003564,
          8.981214,
          8.9573965,
          8.467198,
          8.940674,
          8.94938,
          8.946959,
          8.954641,
          8.998833,
          8.977117,
          8.975116,
          8.98265,
          -3.7503538,
          -0.7062003,
          -2.0368307,
          -0.8296254,
          -0.5435231,
          -0.56474674,
          -3.063976,
          -0.68324244,
          -4.313165,
          -0.434356,
          -2.6915092,
          -1.9875988,
          -1.644223,
          -0.63816994,
          -0.7404743,
          -1.9499413,
          -2.7810173,
          -0.8476458,
          -0.93193096,
          -0.74308866,
          1.1453428,
          1.2522085,
          0.07238404,
          -0.5067345,
          -0.43301773,
          21.300243,
          21.16895,
          20.994053,
          20.591616,
          20.716597,
          21.107994,
          20.806324,
          21.016014,
          20.156502,
          20.571882,
          21.120405,
          21.015802,
          20.804674,
          20.881378,
          20.943895,
          21.001076,
          20.902588,
          20.483084,
          20.2382,
          20.429947,
          20.92979,
          20.948303,
          20.400911,
          20.385468,
          20.617619,
          21.288946,
          20.962551,
          20.66985,
          20.73696,
          19.853619,
          20.840298,
          21.21668,
          20.937656,
          20.861591,
          20.827583,
          20.94511,
          21.023575,
          20.715708,
          20.991821,
          20.955795,
          20.611757,
          20.71615,
          21.089184,
          21.03631,
          21.029871,
          21.17193,
          21.82076,
          20.997606,
          20.958035,
          20.516695,
          20.955795,
          21.456978,
          21.574305,
          21.029072,
          20.997137,
          21.6061,
          21.626379,
          21.143152,
          21.14839,
          21.38309,
          21.74189,
          -0.28071335,
          -0.34556288,
          -0.41150534,
          0.11967204,
          0.4029385,
          -0.61993134,
          -0.561751,
          -0.70812935,
          0.18310034,
          0.10511088,
          -0.034393594,
          -0.77467656,
          -1.7552476,
          -1.0050997,
          -0.75969464,
          -0.6389605,
          -0.794883,
          0.40855417,
          -1.1153755,
          -2.0541542,
          -3.4840531,
          -3.5085204,
          -3.2262793,
          -3.2885125,
          -3.1952481,
          -1.0424919,
          -0.90529597,
          -1.9665418,
          -3.5843918,
          -1.3631963,
          -4.1930814,
          0.48031828,
          -0.100627445,
          -0.33412683,
          -0.5180148,
          -1.7210696,
          -0.49125654,
          -0.40221646,
          -0.4205352,
          -0.3790679,
          -0.38078222,
          -0.54121834,
          -0.73993355,
          0.14801101,
          -3.5866528,
          -1.803757,
          -0.0046783704,
          -0.21018353,
          -0.151735,
          0.21445477,
          -3.943977,
          -0.7917886,
          -5.579406,
          -3.812296,
          -0.41749033,
          -0.29951218,
          -0.026573557,
          -3.0049987,
          0.96679807,
          -0.55396193,
          -0.48331124,
          -0.54150826,
          -0.28065464,
          -0.41604626,
          -0.44062778,
          -3.4725177,
          0.004114841,
          -0.4281123,
          0.76697344,
          -0.23764612,
          -0.46843946,
          -0.49015144,
          -0.38068926,
          -3.1347616,
          0.62391406,
          -0.9225686,
          -2.432817,
          -2.5736454,
          -2.980876,
          -3.61799,
          -1.3498,
          2.3875911,
          0.49969053,
          -2.2264462,
          -0.8153425,
          -0.25439546,
          -0.51978874,
          -0.5538562,
          -0.33262792,
          -0.42974022,
          -2.4337316,
          -0.16443877,
          0.39816746,
          -0.5628982,
          -0.7970247,
          -1.5267653,
          -0.68453544,
          -1.0088127,
          -4.050626,
          -0.43858582,
          -1.3813146,
          -1.9256464,
          -0.5718399,
          -0.2272822,
          -0.5805596,
          -1.3707572,
          -1.7184691,
          -0.92735314,
          -0.5641305,
          -0.9325309,
          -2.319808,
          -3.7390337,
          8.105033,
          -0.3501598,
          -0.54655975,
          -0.19340558,
          -0.5756345,
          -0.94917923,
          -0.89782876,
          -2.629526,
          -3.5051417,
          -1.793398,
          -1.4732261,
          -1.7291353,
          -0.4044857,
          0.8622554,
          -1.827375,
          -3.8846111,
          0.8148531,
          -0.9726051,
          -0.9446549,
          -1.5115141,
          -3.9620974,
          -0.95227504,
          -1.334397,
          -5.377267,
          -3.9814534,
          -3.2975292,
          -2.0572412,
          -5.9774055,
          -5.9862857,
          -3.4082794,
          -1.5022782,
          -2.445691,
          -2.7571225,
          -4.0013514,
          -4.0790386,
          -3.0568354,
          -0.45262653,
          6.103942,
          -1.2517564,
          -4.1282897,
          -3.7802188,
          -1.3200147,
          -0.73613435,
          -0.932242,
          -3.042027,
          -3.9501138,
          -3.7529016,
          -3.8556743,
          5.7027307,
          0.4917504,
          0.089485705,
          -4.99849,
          0.32916966,
          -0.06677626,
          -1.9303972,
          -1.5209024,
          -1.9710892,
          -4.1112413,
          -1.888166,
          -0.16813177,
          -1.5946374,
          -2.6645255,
          -3.481481,
          -0.8566955,
          -0.10298477,
          -1.6559906,
          -0.06540251,
          -0.6134376,
          -4.226986,
          -4.367967,
          -3.828667,
          -3.7366235,
          8.971648,
          9.005856,
          9.00565,
          8.96909,
          8.951731,
          8.44987,
          8.9440975,
          8.945193,
          8.94711,
          8.984122,
          8.989107,
          8.977488,
          8.97502,
          8.770201,
          -3.846359,
          -0.685287,
          -0.8002976,
          -2.3071637,
          -0.4918882,
          -1.1452844,
          1.14679,
          0.10230654,
          -0.45510256,
          21.1382,
          21.202414,
          20.950438,
          20.574097,
          20.652058,
          21.114985,
          20.853662,
          21.139965,
          20.542908,
          20.87594,
          20.673222,
          21.031517,
          21.087622,
          21.026737,
          20.894903,
          20.80971,
          20.964048,
          20.884304,
          20.285622,
          20.08093,
          20.692842,
          20.955215,
          20.583538,
          20.526571,
          20.23836,
          21.067951,
          21.25432,
          20.690067,
          20.714167,
          20.620928,
          19.858912,
          20.640448,
          21.402954,
          20.979795,
          21.031143,
          20.850311,
          20.543026,
          21.08489,
          21.02952,
          20.758833,
          20.880455,
          20.987118,
          21.193748,
          20.80161,
          20.632452,
          20.66241,
          21.075678,
          20.983114,
          20.995108,
          21.079075,
          21.573032,
          21.627142,
          20.982485,
          20.698967,
          20.825367,
          21.038353,
          21.6362,
          21.11155,
          21.04049,
          21.049826,
          21.604801,
          21.689909,
          21.014395,
          21.292706,
          21.54158,
          21.502195,
          -0.31845912,
          0.34381875,
          3.146584,
          2.843789,
          1.9224935,
          2.3359163,
          1.6339986,
          0.9993397,
          -0.5151766,
          2.6507034,
          1.7735629,
          2.0773063,
          1.7329372,
          1.9980727,
          2.0571294,
          1.8478445,
          1.6704197,
          1.8684791,
          2.269138,
          1.8204217,
          2.313643,
          1.3427671,
          2.376867,
          2.5993028,
          5.959225,
          2.3709006,
          3.3710501,
          3.388154,
          3.6911638,
          -5.5831923,
          -5.68724,
          -2.7944155,
          -6.7436743,
          -3.8423312,
          -3.5016725,
          1.3438953,
          -2.0595503,
          -2.0379844,
          -2.3553417,
          -2.5605187,
          -2.312691,
          -1.7360946,
          -0.9055974,
          -3.4825852,
          -3.6340592,
          -3.5912125,
          -0.96676147,
          6.230267,
          -0.6373473,
          -0.7337066,
          0.3848,
          0.6075786,
          5.923394,
          -6.4378476,
          -6.969392,
          -7.1759367,
          -7.116421,
          -7.1298623,
          -4.7760563,
          -3.3367002,
          -0.54250747,
          -0.49103624,
          -1.9516231,
          -1.4409224,
          -0.15534927,
          -0.17945069,
          -0.049579166,
          -4.9982,
          -0.66205245,
          -0.5228325,
          0.67475307,
          -0.71457803,
          -2.7837083,
          -3.0530174,
          -3.2008672,
          -2.8812063,
          -2.6643975,
          -2.7326648,
          -3.01595,
          -0.3998758,
          -1.2301456,
          -3.3412735,
          -3.665534,
          -3.5835075,
          -3.563844,
          -1.8581672,
          -2.118597,
          -2.0342495,
          -0.73023206,
          -0.8564948,
          0.5597419,
          -5.563572,
          -5.380461,
          -4.9352427,
          -5.9419885,
          -5.940097,
          -3.461711,
          -3.5463593,
          2.7790647,
          0.71651787,
          1.5970595,
          2.678166,
          -1.2354228,
          -1.1129954,
          -1.4643012,
          0.10499526,
          -1.0856827,
          -4.8530636,
          -4.7652535,
          -3.0047596,
          -3.087488,
          -4.511336,
          -3.1992812,
          -3.1662047,
          -3.1043699,
          -4.2125363,
          -3.070443,
          -3.1238608,
          -2.5635412,
          -1.288588,
          -0.68460155,
          0.66253495,
          -3.8176005,
          -0.74313694,
          -1.057672,
          -3.1574242,
          -2.9716854,
          -1.7076771,
          -3.62174,
          -2.6865156,
          -0.5537383,
          -2.3769143,
          -3.2774873,
          -1.135411,
          -1.2011173,
          -1.0722973,
          1.356287,
          1.2425653,
          0.76658744,
          0.23186806,
          0.84316194,
          -2.7269533,
          0.8261509,
          1.4938202,
          1.5967506,
          -3.340526,
          0.019854417,
          -0.26834038,
          0.6317759,
          -5.7414107,
          -6.056133,
          -5.27567,
          -0.82260454,
          0.032429498,
          -0.3025634,
          0.6486605,
          -1.2145271,
          0.17930295,
          -1.6389568,
          2.770579,
          2.4588652,
          1.7346812,
          1.5722582,
          1.4220402,
          -3.065003,
          -5.9666963,
          -5.782585,
          -0.41946682,
          -1.7865312,
          -1.8215495,
          -2.626009,
          -5.949541,
          -6.125147,
          -6.9929514,
          -7.413438,
          0.43540365,
          -0.83293295,
          -0.7653431,
          -1.6223999,
          -2.6843252,
          -2.3871844,
          -3.1141891,
          -2.9764297,
          -3.1692588,
          0.3451549,
          -1.6241069,
          1.8377626,
          -3.381909,
          -0.84477675,
          -0.85654163,
          -0.8113936,
          -0.58808595,
          -0.941269,
          0.13616398,
          1.6830087,
          -2.7741942,
          -5.796357,
          -5.9834304,
          -6.3211937,
          -6.035967,
          -4.2274275,
          -4.5201945,
          -4.425029,
          -4.388549,
          -4.266419,
          -0.5229761,
          2.0642846,
          -0.48302376,
          -1.1637125,
          -0.49136773,
          1.8924818,
          1.9210478,
          0.04647873,
          1.9881487,
          -0.053584076,
          -0.89527774,
          -0.97171444,
          -2.1137784,
          -1.0883721,
          -0.944623,
          -1.3199998,
          -4.1266303,
          -3.8475757,
          -3.8731363,
          -3.1812146,
          -3.8578258,
          0.28898814,
          -2.1542518,
          -0.86885434,
          -2.721875,
          -0.6276682,
          0.029812373,
          1.9566201,
          2.123076,
          1.4415385,
          2.4531007,
          2.9058685,
          -3.138804,
          -3.221916,
          -3.1867714,
          1.4830027,
          -0.08613462,
          -0.45784032,
          -0.45700034,
          -4.326282,
          -4.3814344,
          -3.6870313,
          -3.7219813,
          -3.9413369,
          -0.29662725,
          -2.7192113,
          -1.1983322,
          -0.49421936,
          -3.602792,
          -0.13266705,
          -2.5538895,
          -1.1309905,
          -0.5290601,
          1.219218,
          0.58889186,
          0.088568956,
          -0.331548,
          21.202766,
          21.240536,
          20.616405,
          20.97227,
          20.619583,
          21.026049,
          21.160574,
          20.354277,
          20.626595,
          20.661001,
          20.870783,
          20.907614,
          21.07348,
          20.86931,
          20.982224,
          20.979591,
          20.382652,
          20.921455,
          20.50407,
          20.316105,
          20.565193,
          21.111908,
          21.077435,
          20.696154,
          20.680416,
          20.646011,
          19.9293,
          20.743162,
          21.49685,
          20.944206,
          20.940659,
          20.676485,
          20.792625,
          20.6188,
          21.041681,
          20.922712,
          20.78943,
          20.765175,
          20.922117,
          21.096365,
          21.024258,
          20.53477,
          20.586056,
          21.114197,
          20.850014,
          20.896547,
          20.86581,
          21.367165,
          21.62077,
          21.691654,
          20.957954,
          20.650723,
          20.864363,
          21.100388,
          21.611671,
          21.667904,
          21.04844,
          21.042852,
          21.058283,
          21.612339,
          21.542519,
          21.097895,
          21.2208,
          21.291988,
          21.726223,
          -2.4789152,
          -0.26376337,
          -0.2688282,
          -0.6676003,
          0.21759714,
          2.7805312,
          1.0670166,
          1.3104619,
          2.1660454,
          2.0268574,
          0.2173845,
          0.4919902,
          -0.28518552,
          -1.0024122,
          2.2583985,
          -2.9287157,
          -1.1526389,
          -0.45114753,
          -0.5651485,
          -0.74357015,
          -0.99569523,
          -3.1700535,
          2.5382323,
          -0.6508912,
          -3.3744366,
          -0.70329803,
          -0.9458107,
          -1.7554126,
          0.58882445,
          -0.4639748,
          -0.6209757,
          0.6221958,
          1.3316759,
          1.588856,
          -1.909487,
          -3.2693417,
          -2.4315872,
          -3.1221666,
          -2.0333917,
          -4.076707,
          -3.1569686,
          -1.0272493,
          -3.3310943,
          -2.498596,
          -3.1096196,
          -3.410693,
          -4.2866793,
          5.016152,
          -3.1651115,
          -1.6793581,
          -3.5915127,
          0.77293146,
          0.010917821,
          -0.8186901,
          0.36002985,
          -2.390881,
          -4.4467845,
          -4.4577312,
          -3.911607,
          -0.4476138,
          0.111549884,
          -2.991777,
          -2.7606432,
          -3.1356757,
          -1.8607063,
          -2.5279622,
          -0.5121266,
          0.8136869,
          0.1821667,
          0.5243592,
          21.346342,
          21.032217,
          21.51782,
          20.342619,
          20.956974,
          20.68343,
          21.123373,
          20.947033,
          20.956335,
          20.92234,
          20.966137,
          20.965675,
          20.721684,
          20.295229,
          20.678545,
          21.008184,
          20.446602,
          20.4456,
          20.398712,
          21.147734,
          21.10424,
          20.631912,
          20.59184,
          20.596947,
          19.783392,
          20.663916,
          21.580637,
          20.904802,
          21.018047,
          20.772442,
          20.476002,
          21.117792,
          20.918583,
          20.72449,
          20.785063,
          20.945984,
          21.202732,
          20.78392,
          20.619587,
          20.730356,
          21.058863,
          21.051476,
          20.99922,
          21.079071,
          21.78832,
          21.63567,
          20.914667,
          20.72301,
          20.856815,
          21.07541,
          21.722088,
          21.064566,
          20.887394,
          20.961456,
          21.78424,
          -1.3599632,
          0.31468612,
          -0.44204476,
          -1.7838252,
          -4.077225,
          -2.7763584,
          -1.1781309,
          -0.29993683,
          -4.822725,
          2.091027,
          2.2139368,
          -4.9763975,
          -1.3726072,
          2.5770988,
          0.32215145,
          0.83599544,
          2.6129067,
          2.0213754,
          1.4805154,
          2.4713726,
          1.6817245,
          2.1547244,
          2.5226083,
          2.3332303,
          1.8693966,
          2.9870024,
          2.4312358,
          2.9116793,
          -2.3396082,
          -0.9755324,
          -1.075432,
          -0.93780607,
          -0.94227463,
          0.7534333,
          -1.1201006,
          -4.7806354,
          -4.3195,
          -6.1297097,
          0.8518957,
          -0.80773866,
          -0.94064593,
          -0.16907343,
          -2.730339,
          -3.3252184,
          -2.5180185,
          -2.6750417,
          -3.3438845,
          1.1626534,
          -2.3947344,
          -2.115053,
          -0.34827602,
          -0.35913876,
          0.56229174,
          -3.4606164,
          -0.29359737,
          2.1751554,
          -0.015162039,
          0.071428604,
          1.5518473,
          0.49717525,
          1.2290158,
          -0.22909935,
          -0.859438,
          -0.7468949,
          0.24637674,
          -2.6215942,
          -0.706189,
          -0.7641027,
          -0.069995366,
          -0.3755401,
          -4.0889864,
          2.2713,
          -0.730813,
          -0.8520167,
          -0.21345246,
          -3.05334,
          -0.33134726,
          -0.20389938,
          -1.935836,
          -2.174083,
          -2.692755,
          -1.7633188,
          -1.8390566,
          -1.962437,
          -1.6970621,
          -2.7783368,
          -2.767827,
          -2.9043403,
          -1.3118919,
          -2.0252264,
          -3.579881,
          -2.0184221,
          -1.9480205,
          -1.7255695,
          -1.927718,
          -1.2254515,
          -4.299641,
          -1.3251895,
          -4.58362,
          -4.8542156,
          0.985249,
          1.892006,
          -3.2682612,
          -3.3085654,
          -3.1462908,
          -0.6074477,
          -4.120941,
          -0.93285036,
          3.0286872,
          2.7514436,
          2.8372905,
          2.4600608,
          2.6121397,
          0.9941276,
          2.2266757,
          2.2302446,
          1.9901006,
          2.205884,
          1.7575002,
          2.124492,
          1.7696179,
          -0.7555089,
          0.24636406,
          2.708178,
          2.9340682,
          2.0761607,
          2.162644,
          1.8791691,
          1.3231394,
          2.2277038,
          2.167384,
          2.1224027,
          3.0711906,
          3.0281749,
          0.6791285,
          -4.570283,
          -4.5087748,
          0.99674517,
          -2.2515628,
          -0.37511954,
          -1.7738543,
          -1.6271241,
          -0.05460679,
          -2.917819,
          -2.0705943,
          -2.2285414,
          -2.0860665,
          -2.4824378,
          2.9039407,
          -1.556073,
          -2.607602,
          -1.7428286,
          -3.4936893,
          -3.4759831,
          0.251512,
          -0.3866291,
          -0.4649974,
          -0.5069703,
          -1.0297308,
          -1.281204,
          -2.4753194,
          -1.739507,
          -0.2168815,
          -0.12917365,
          -1.2398088,
          -0.3300159,
          -1.3740014,
          -0.6878112,
          -0.44246045,
          -0.3124056,
          -0.6876365,
          0.20025112,
          -1.9119595,
          -0.8709876,
          -0.7470341,
          -0.99225813,
          -2.2119646,
          -2.5510454,
          -2.6044598,
          -5.894276,
          -0.832901,
          -0.7446072,
          5.219408,
          2.9549026,
          2.1747787,
          2.072916,
          2.6012516,
          2.5197105,
          2.476427,
          2.5441048,
          -0.80552936,
          -0.81012887,
          0.38145554,
          -0.071297996,
          0.12121766,
          -0.016639186,
          2.4181488,
          -1.3334284,
          -6.284437,
          -5.890915,
          -0.28159943,
          -3.6880605,
          -0.2569119,
          -0.15420018,
          -0.13024502,
          -3.0034907,
          -3.3026175,
          -3.6974545,
          -5.4482665,
          -2.6253872,
          -2.3977797,
          -2.1172996,
          -2.7033684,
          -1.0200708,
          -1.5826036,
          0.61677706,
          -1.6821827,
          2.324987,
          1.1784325,
          -1.6347461,
          0.74485606,
          -1.6857457,
          -2.4542894,
          -2.5491292,
          -2.6407702,
          -3.3609293,
          -1.5619215,
          -0.60362357,
          -2.7328699,
          -3.0039382,
          -2.9379687,
          -3.0669224,
          -3.441798,
          -3.0111122,
          -2.9021761,
          -2.9102502,
          -1.9367454,
          -2.4248414,
          -3.5215442,
          -3.6168408,
          1.3228945,
          -0.23845856,
          -0.44129348,
          -2.6469767,
          0.25249615,
          2.4577844,
          -2.3708699,
          -1.7160218,
          -0.0029827023,
          -0.58297634,
          -0.21291575,
          0.2576065,
          -0.62076175,
          -0.757645,
          -0.6171505,
          -0.17461498,
          -0.34182334,
          -3.1972294,
          -3.508892,
          0.29507098,
          0.0076084593,
          0.51968735,
          -0.06669635,
          -0.033775892,
          -0.50371206,
          -0.26342678,
          -0.31682244,
          -0.80155736,
          -2.2885315,
          -0.28026354,
          -0.6372158,
          -0.590977,
          -0.42419603,
          -1.1915203,
          -0.95319366,
          -0.8917223,
          -2.7201648,
          -2.7333112,
          -1.5868756,
          -0.9220667,
          -6.15163,
          2.5694804,
          1.4674015,
          1.8002038,
          0.19251029,
          1.3090869,
          -6.1497874,
          -6.125724,
          -3.6366653,
          -1.9195923,
          -2.229134,
          -3.2613754,
          -2.1847227,
          -3.5061324,
          -3.6141405,
          -3.006768,
          -3.690071,
          -0.011148865,
          -2.049635,
          -2.463183,
          -1.9672999,
          -1.6643882,
          -2.336266,
          -3.3820019,
          -3.3894453,
          -3.2330422,
          -2.4278655,
          -2.7111993,
          2.414135,
          -3.9492512,
          -4.2431855,
          1.2923716,
          0.34425056,
          1.1536998,
          -1.0957972,
          -3.5373285,
          -3.6211557,
          -3.2867231,
          -0.6068608,
          -2.1816406,
          -0.13233116,
          -2.1414697,
          -1.8487241,
          -0.7343194,
          -1.0556852,
          -0.65091115,
          -1.9440874,
          -1.6206464,
          -0.2295558,
          2.3600152,
          -3.5630977,
          -1.6534507,
          -2.3811603,
          -1.4687765,
          -3.5027232,
          -2.216499,
          -1.8680736,
          0.42998657,
          1.4251448,
          1.364908,
          -1.5735211,
          -1.8944317,
          -2.1101909,
          -2.525616,
          -2.1700463,
          -1.6793704,
          -1.0668771,
          -3.4639902,
          -3.605248,
          0.31898695,
          -0.6760397,
          -0.053513836,
          -0.8666507,
          -0.4091476,
          -0.31541252,
          -0.2459589,
          -0.33441672,
          -0.32501838,
          -0.19386703,
          0.6590453,
          -0.55072117,
          -0.77523345,
          -2.7975228,
          -2.450785,
          -2.6554267,
          1.1766902,
          1.6237487,
          1.6707226,
          2.0662587,
          1.418188,
          1.9086171,
          1.8838811,
          1.6441288,
          -0.06273501,
          -0.6315857,
          -0.3675033,
          -4.2619905,
          -4.4487066,
          -3.3415,
          -3.8686657,
          0.3240545,
          -0.378653,
          -0.38941598,
          -3.013,
          -0.13413125,
          -1.9834312,
          -1.4766098,
          -0.35839394,
          -0.3494121,
          1.1526787,
          0.13920277,
          -0.23170225,
          20.451622,
          20.398645,
          21.187687,
          19.872555,
          20.81081,
          20.438974,
          20.91772,
          21.19285,
          20.72893,
          20.783102,
          20.962708,
          20.839523,
          21.72816,
          -0.10363273,
          -0.14964677,
          0.18522608,
          -1.1379638,
          -0.14008626,
          -3.1925857,
          3.4052842,
          2.331149,
          2.9320703,
          3.0165734,
          2.8244772,
          2.8319347,
          2.6811912,
          1.8181252,
          1.5722569,
          3.047436,
          2.8618865,
          2.3339546,
          3.045342,
          2.7153301,
          2.8751185,
          2.9879591,
          2.954761,
          2.6554446,
          1.9855056,
          2.7028377,
          3.0271413,
          2.5729773,
          2.8966331,
          2.7983086,
          2.9774008,
          2.8174527,
          2.5779905,
          2.1988626,
          2.3139963,
          2.1359725,
          -1.1463665,
          0.37553662,
          0.37918296,
          -0.69282943,
          -0.8619425,
          -0.9345424,
          -2.8068933,
          -2.4142811,
          -1.4753416,
          -2.7420285,
          -1.2456733,
          -1.8706439,
          -0.5827137,
          -0.5971584,
          -0.91945845,
          -0.15571883,
          -0.18312335,
          -5.268597,
          -4.942103,
          -1.3714018,
          -3.1752493,
          -6.3515735,
          1.934735,
          -6.609749,
          0.77546984,
          -4.568254,
          1.6307154,
          4.4369593,
          -5.0271544,
          -4.8185673,
          -5.011972,
          -5.107776,
          -3.826177,
          -5.3475575,
          -6.1508045,
          -6.3746653,
          -6.0549793,
          -5.627026,
          -5.737936,
          -5.1595225,
          0.26749125,
          -5.9435825,
          -6.1502566,
          -6.0349116,
          -6.7810726,
          -6.8570223,
          -6.6351438,
          -3.4947712,
          -3.5460453,
          -5.437502,
          -5.3266582,
          -4.8672633,
          -1.2318299,
          -4.042992,
          -3.6566734,
          1.0517985,
          -0.5687508,
          -1.4288709,
          4.583826,
          3.0538292,
          -0.30231562,
          -1.6107256,
          -0.3653601,
          -0.58104956,
          -0.05740702,
          -0.4268045,
          -0.41139302,
          -4.2441416,
          -4.4124756,
          -0.874572,
          -3.9219735,
          8.977206,
          9.051237,
          8.998185,
          8.9592285,
          8.956474,
          8.498357,
          8.943424,
          8.938886,
          8.939722,
          8.9889555,
          8.9958315,
          8.977104,
          9.02913,
          8.940449,
          -3.9222813,
          -0.50508755,
          -3.0957978,
          -0.8212637,
          -2.0510511,
          -0.54290634,
          -0.88905644,
          -0.80758464,
          -0.6181326,
          1.1118283,
          0.69985306,
          -0.28021428,
          -0.43202782,
          20.785404,
          21.072128,
          20.325483,
          20.127161,
          20.186462,
          20.44384,
          20.427671,
          20.288937,
          20.07039,
          20.903616,
          20.746193,
          20.15953,
          20.131487,
          20.384687,
          20.02713,
          20.052286,
          20.224644,
          20.157219,
          20.9379,
          20.01226,
          20.090376,
          20.761826,
          20.509205,
          20.611027,
          20.09558,
          -0.8044397,
          -0.8063785,
          20.298948,
          20.086369,
          20.371387,
          20.045696,
          20.00856,
          20.292894,
          20.19499,
          20.224258,
          20.32524,
          21.003418,
          20.693035,
          20.515806,
          20.33529,
          19.8467,
          20.319244,
          20.842169,
          20.218056,
          20.658028,
          20.199228,
          20.409576,
          20.232183,
          20.698008,
          20.692423,
          20.293032,
          20.691824,
          20.413605,
          20.603867,
          20.530224,
          21.113092,
          20.21323,
          20.242268,
          20.405313,
          20.603224,
          20.077787,
          20.289387,
          20.186045,
          20.730545,
          21.084185,
          20.457521,
          20.270168,
          20.566767,
          20.666656,
          20.186317,
          20.29371,
          20.739603,
          21.340643,
          20.039375,
          20.850393,
          20.366396,
          20.464,
          20.703417,
          20.528204,
          20.50177,
          20.108597,
          20.398314,
          20.69553,
          20.314955,
          20.123148,
          19.950996,
          -0.9090005,
          -0.4030807,
          -0.34590667,
          0.23111537,
          -1.7821263,
          -2.0351586,
          -3.457781,
          -3.4890316,
          2.9487536,
          2.7640707,
          1.8209709,
          2.1628125,
          2.312519,
          2.3801203,
          2.6287897,
          2.4344308,
          2.6381197,
          2.24771,
          2.3559597,
          2.1597073,
          2.286986,
          2.4574192,
          2.6981916,
          2.2966886,
          2.7465081,
          2.658047,
          2.6213784,
          2.8370702,
          3.1631446,
          2.9512658,
          2.9156048,
          2.675277,
          3.1327803,
          2.6663609,
          2.97248,
          2.896535,
          2.8697958,
          2.6426947,
          2.803268,
          2.6278093,
          1.0297899,
          2.624658,
          2.858961,
          2.9707406,
          2.7330897,
          2.4761815,
          2.6212687,
          2.6021912,
          2.589915,
          2.410354,
          2.6099365,
          1.9401335,
          2.5967393,
          2.4125085,
          2.5668516,
          2.2981257,
          1.6609477,
          0.41642562,
          1.2659503,
          2.178069,
          1.1004003,
          -0.26039392,
          2.288105,
          2.6540534,
          2.893012,
          2.392052,
          2.8488424,
          2.6782045,
          2.7412765,
          1.9874262,
          2.8611698,
          -0.7627532,
          2.444894,
          2.5429232,
          -0.18002152,
          2.738913,
          2.5493948,
          2.2730367,
          2.3782797,
          2.1299658,
          1.8892831,
          3.0418603,
          -0.8745979,
          -0.38994545,
          -3.0626862,
          -0.76831734,
          -0.7438481,
          -0.7179172,
          -1.7077537,
          -0.11669502,
          -1.7292603,
          -1.8046123,
          -3.1287112,
          -3.464921,
          -1.2989926,
          -2.3851361,
          -2.055093,
          -3.5558515,
          -1.8320577,
          -2.4701753,
          -2.1652098,
          -3.490656,
          -4.9762664,
          -3.867057,
          20.42756,
          -2.2399404,
          -3.5801075,
          -3.5566976,
          1.11913,
          0.31106243,
          1.077311,
          0.95621663,
          1.1379894,
          -0.7793822,
          -0.80581486,
          -0.6592304,
          0.47314093,
          20.202164,
          -0.5085863,
          -0.28266224,
          -1.2408552,
          -3.4826415,
          -0.77100855,
          -0.84246796,
          0.22714698,
          -0.35060298,
          0.6612824,
          -3.0584443,
          -3.2429485,
          -3.340743,
          -3.5089734,
          0.7362037,
          0.1108225,
          -3.3829026,
          -2.8873982,
          -3.9851062,
          0.7186368,
          0.29185706,
          2.2401226,
          2.7389407,
          -2.5888014,
          0.24159594,
          -2.0107217,
          -2.6282554,
          -2.6413295,
          -3.5321765,
          0.10994828,
          -0.17557321,
          0.25772363,
          -0.84204346,
          -3.0922885,
          -0.48717397,
          -0.28290153,
          -0.16241409,
          -0.32083282,
          -0.13688341,
          -0.48019898,
          -1.2570351,
          0.77494615,
          1.9540014,
          1.5040236,
          0.2076637,
          -0.005618675,
          0.35600105,
          -0.06153286,
          -1.5066572,
          -1.7651991,
          -1.252736,
          -0.7294381,
          -0.7068843,
          -1.3208156,
          -2.348573,
          -3.8739746,
          -2.4175234,
          -2.2388918,
          -6.93038,
          -6.589075,
          -6.139715,
          -2.0168326,
          -3.0089703,
          -1.8720212,
          -1.9863963,
          -2.389387,
          -3.4487987,
          -3.4916718,
          -0.6744197,
          -0.6726029,
          -2.4319856,
          -0.75516444,
          -0.8042757,
          -0.10480892,
          -2.6406653,
          -1.3496342,
          -3.2273695,
          0.3263687,
          1.9052156,
          0.0073477775,
          0.031308442,
          1.8374825,
          -5.4093986,
          1.9974169,
          -3.246436,
          -3.594453,
          -0.03064417,
          2.0168843,
          -1.9111667,
          -2.3317473,
          -1.1448371,
          -3.654962,
          -3.41146,
          -1.7579912,
          -2.1660423,
          -3.5071015,
          -6.3882813,
          -6.384219,
          -7.017308,
          -7.2715645,
          -7.0944996,
          -7.277442,
          -2.145151,
          -6.3443637,
          -6.421055,
          -1.7387921,
          -2.2942848,
          -0.77636105,
          -3.9232948,
          -6.6854844,
          -0.8892006,
          -0.21608396,
          -0.70804894,
          -1.9210225,
          -0.86891204,
          -0.9103131,
          0.66584224,
          -1.6684382,
          -4.4369354,
          -3.0742772,
          -4.295596,
          -4.4434514,
          -4.5133443,
          -4.2487745,
          -3.9001064,
          0.88340175,
          -0.46059152,
          -0.6353391,
          -1.3695925,
          -2.726043,
          -3.2273996,
          -3.1405988,
          -1.7491583,
          -0.92743367,
          -3.9914558,
          0.63624233,
          -0.45322758,
          -0.70421046,
          -0.83106595,
          -0.5302843,
          -0.7474089,
          -0.74769074,
          -0.7642279,
          -0.78709453,
          -0.8869694,
          1.5163125,
          1.9679619,
          -0.15186225,
          0.13947207,
          -0.5622332,
          -1.0357682,
          -0.93942046,
          -0.83710456,
          -0.78598773,
          -0.97475755,
          -0.8871177,
          -0.9417938,
          -0.7289285,
          -0.84009916,
          -0.53685063,
          -0.73679835,
          -1.5539026,
          -1.5667439,
          -0.94856113,
          -0.8088888,
          -0.50559527,
          -0.65757537,
          -1.1660818,
          -0.6871717,
          -0.7054381,
          -1.0372273,
          0.5889712,
          0.27630562,
          0.31001878,
          0.042403758,
          0.11551466,
          -1.3234726,
          0.20785016,
          0.124725565,
          0.22667082,
          0.09812706,
          7.06098,
          0.15149365,
          0.12147435,
          0.37000126,
          -0.18531887,
          0.20258099,
          -0.57880676,
          0.10789006,
          1.553433,
          0.14345492,
          0.029008051,
          0.18429773,
          -0.6312348,
          0.11334664,
          -0.012132786,
          -0.19663753,
          1.6080966,
          -0.09811033,
          -0.541937,
          -0.32276633,
          -4.2856593,
          -4.3943,
          -3.3239691,
          -3.8235943,
          -0.41581294,
          -0.27217075,
          -0.3427033,
          -0.49019602,
          -0.26959357,
          -2.150278,
          -1.9085721,
          -0.2541381,
          -0.37004018,
          1.2792727,
          0.13136303,
          -0.33577397,
          21.015839,
          20.888727,
          20.451275,
          20.616287,
          20.653181,
          20.16092,
          20.74429,
          20.703968,
          20.816473,
          20.963085,
          21.0009,
          21.13471,
          21.705782,
          20.944992,
          20.91296,
          20.699991,
          20.931492,
          21.690132,
          21.042934,
          21.021296,
          21.648348,
          21.773138,
          21.066393,
          21.212242,
          21.669298,
          -4.0552297,
          -0.16385591,
          -0.21456593,
          0.13366722,
          -0.6081495,
          -0.7064085,
          -0.34196892,
          -3.0680306,
          -0.86908424,
          -0.902036,
          -0.5491524,
          -1.6795343,
          -1.4729407,
          -0.98787403,
          -0.8938072,
          -0.9032161,
          -1.5981466,
          -3.6448665,
          -4.272971,
          -4.3641,
          -4.1404324,
          0.896805,
          -0.54923546,
          -0.7192541,
          -1.6727713,
          -3.077985,
          -3.4742079,
          -5.7048774,
          -5.514303,
          -1.6941029,
          -1.2353846,
          -0.31319493,
          0.7615955,
          -2.057527,
          -2.6745172,
          -2.559892,
          -3.4394217,
          -5.3767037,
          -6.498653,
          -7.1854615,
          -3.5858781,
          -0.5996791,
          -0.78322464,
          -2.921696,
          0.09600402,
          -1.8064184,
          -2.1272554,
          -1.6887276,
          -3.3825436,
          0.083755225,
          -2.0021074,
          -2.5514047,
          -3.5062563,
          -1.0776289,
          -2.5850978,
          -0.46814728,
          -1.7013228,
          -1.765417,
          -5.810609,
          -5.768788,
          -5.8887844,
          -5.513346,
          -0.02447869,
          -5.4932566,
          -3.0308135,
          -2.7448895,
          -2.270655,
          2.3785396,
          0.32030177,
          0.61009324,
          -3.470949,
          -2.285185,
          -2.952777,
          0.82852745,
          -1.1230824,
          -4.375969,
          -5.91157,
          -6.502616,
          -0.8668871,
          0.7802025,
          -0.22420502,
          -0.42807725,
          -0.71063787,
          -2.5073512,
          -3.243499,
          -1.2783506,
          -3.0826614,
          -3.5118017,
          -0.46712282,
          0.17573294,
          -1.8008604,
          -1.7139188,
          -1.2280505,
          -1.6175317,
          -1.3952143,
          -1.221668,
          -2.0602193,
          -0.9039997,
          -0.8120578,
          -3.0277724,
          -0.9502869,
          -2.1900003,
          -2.939748,
          -0.75512177,
          0.53301334,
          -1.9205076,
          -5.8099284,
          -2.379803,
          -3.6036363,
          -4.948243,
          -0.7510783,
          -2.7714145,
          -2.6307464,
          -0.92731726,
          -3.2747085,
          -5.070289,
          -2.0689101,
          -2.5866654,
          -2.0470245,
          -2.4981666,
          1.8062497,
          -2.9760122,
          -1.9473894,
          -1.2410403,
          -4.5683513,
          -4.6466947,
          -2.4222548,
          0.05217374,
          0.1690483,
          -3.0064027,
          -3.5101318,
          -4.3834853,
          -2.3630905,
          2.7734873,
          -5.747209,
          -6.515875,
          -6.1726155,
          -5.3544216,
          -6.2210536,
          -5.2376246,
          -6.447544,
          -4.689083,
          -6.8479133,
          -6.929001,
          -6.7731333,
          -6.588646,
          -3.718035,
          -3.7408993,
          -3.5037909,
          -6.224006,
          0.01171803,
          -6.482794,
          -6.828157,
          -6.5298986,
          -5.774997,
          -5.973195,
          -5.5097427,
          -1.8554037,
          -2.4554133,
          -2.022435,
          -2.276605,
          -2.109988,
          -2.2651815,
          -2.3845649,
          -1.8960886,
          -0.81488985,
          -2.0891955,
          -0.9318549,
          -2.3761315,
          -3.1363797,
          -2.6380186,
          -2.9813933,
          -4.2676535,
          -3.8392272,
          -4.071539,
          -4.4587345,
          -3.0761333,
          -3.90258,
          -3.9815469,
          -4.1732197,
          -4.088454,
          -2.761986,
          -2.0339298,
          -1.2487466,
          -1.4952871,
          -1.2222291,
          -2.5860093,
          -4.5605145,
          -4.9437613,
          -2.6756277,
          -4.634717,
          -2.8801935,
          -3.4559722,
          -0.62465775,
          -2.8997715,
          -1.3054959,
          -0.9437949,
          -0.773195,
          -1.0706722,
          -4.4599767,
          -4.565245,
          -4.1723423,
          -3.6990552,
          -3.863817,
          -6.097235,
          0.82532567,
          -0.9054828,
          -0.950878,
          -0.15594573,
          -0.72075444,
          -3.3407044,
          -0.86694574,
          -0.5410346,
          -0.16406621,
          0.19979653,
          1.0329844,
          -0.85705674,
          1.9247911,
          2.0089066,
          -1.8515531,
          -2.0974598,
          -3.373264,
          -0.8057742,
          -0.78566504,
          0.6846514,
          -2.0454261,
          -2.4714644,
          -2.196738,
          -2.9894078,
          -0.07018879,
          -0.03048418,
          -3.46665,
          -0.2640655,
          -0.28325233,
          -3.3984942,
          0.047388233,
          -3.3803976,
          -3.9444644,
          -3.1032038,
          -3.3170996,
          -2.8571064,
          -3.0331995,
          -2.8567026,
          -2.5261734,
          -2.8567045,
          -6.050545,
          -6.195089,
          -6.3394322,
          -0.99388546,
          0.85884374,
          -0.11660092,
          -2.0970585,
          -0.5026459,
          -2.0567234,
          -2.369572,
          -2.597387,
          -0.7651898,
          -0.78885406,
          -1.0638673,
          -2.1737056,
          -3.4884589,
          -1.9740183,
          -3.3072891,
          -0.7914314,
          -3.8876135,
          -2.0338545,
          -3.271522,
          -1.548228,
          2.1397572,
          0.89379144,
          0.022478718,
          0.6121787,
          -0.37986398,
          2.085266,
          -0.6118032,
          -2.8866975,
          -2.8243115,
          -2.6205041,
          -2.4823046,
          -2.8788798,
          -2.1541998,
          -2.4225237,
          -2.338553,
          -2.1559906,
          -2.3799732,
          -1.5171007,
          -1.8603207,
          -1.7143208,
          -1.8269334,
          -2.3110456,
          0.24660715,
          -1.2572217,
          -4.5830765,
          -1.6959225,
          2.1236951,
          -0.47020686,
          -0.45382476,
          1.5985807,
          -2.4032557,
          1.665534,
          -2.2521198,
          1.5918027,
          -2.3878176,
          2.07315,
          1.0704031,
          1.6274529,
          -2.4171221,
          1.8985752,
          -1.3131632,
          1.4291328,
          -2.405497,
          2.009697,
          1.7307833,
          1.426378,
          -3.177992,
          -3.1731968,
          -0.24212168,
          -3.2266085,
          1.2762315,
          -1.8454787,
          -2.365576,
          -1.9150927,
          -3.4570277,
          -3.5293372,
          -1.6722364,
          -3.438779,
          -0.2699666,
          0.15923962,
          0.8508364,
          0.11954807,
          0.33689684,
          -0.108009495,
          -0.93840253,
          -1.0688324,
          -1.5846287,
          -3.4006603,
          0.5291713,
          2.232319,
          -3.5496435,
          -1.300814,
          -1.668711,
          -0.3920115,
          -0.085506104,
          -1.1407048,
          -3.543681,
          -3.1549203,
          -3.1812308,
          -3.1872654,
          -1.9506372,
          -2.466598,
          -3.5848029,
          -3.094051,
          -3.3278255,
          8.096946,
          -2.6813204,
          -2.8419058,
          -0.9091412,
          0.7446547,
          -3.2037942,
          -3.9459462,
          -3.0716984,
          -2.4863963,
          -4.812039,
          -7.073592,
          -1.1454684,
          -2.3698878,
          -2.9570997,
          -6.1323967,
          -5.966214,
          -5.8348193,
          -6.148748,
          -6.512191,
          -5.979642,
          -5.74248,
          -1.3448225,
          -2.080957,
          -6.547196,
          -6.6204233,
          -0.015073177,
          -6.704394,
          1.7448106,
          1.187305,
          -1.014784,
          -1.7264444,
          -6.1418986,
          -6.3756247,
          -6.338206,
          -6.834216,
          -6.568956,
          -4.0662622,
          -6.3872094,
          -6.117353,
          -5.783636,
          -6.057494,
          -6.161063,
          -1.1883456,
          0.69053096,
          -0.5506914,
          -0.6873216,
          -0.44437405,
          -2.7898498,
          -0.50109214,
          -3.02943,
          -2.9384015,
          -3.136336,
          -2.9035378,
          -2.8864121,
          -3.024323,
          -2.922874,
          -2.9939332,
          -2.751515,
          -1.1334722,
          0.58777326,
          -2.617379,
          0.435062,
          -0.070458785,
          -0.24185121,
          -0.38413024,
          0.34779564,
          0.3631593,
          -0.09109203,
          0.16468488,
          -0.657367,
          -0.7379717,
          -0.6719092,
          -0.8067061,
          -1.6225554,
          -0.60173696,
          -0.61308897,
          -0.7565837,
          -1.946609,
          -0.8638041,
          -1.0886451,
          -0.6228517,
          -1.2061279,
          -0.86780655,
          -0.7327451,
          -0.99465585,
          -0.76993555,
          -0.63661486,
          -1.2226762,
          -0.04057085,
          -0.07268534,
          2.0635016,
          -3.7917795,
          1.9369931,
          2.223772,
          -3.5516908,
          1.7953926,
          -0.73455906,
          1.4922037,
          1.5521411,
          1.1147212,
          1.5980046,
          1.6436527,
          1.5368007,
          0.67813826,
          -0.7682796,
          -0.5428657,
          -0.52948606,
          -3.1342115,
          -2.908097,
          0.03375627,
          -0.94175625,
          -0.7228571,
          -1.5855608,
          0.33000976,
          -1.214022,
          -5.382207,
          -0.9210772,
          1.7255013,
          2.993226,
          -0.89427304,
          -0.86778075,
          -0.8053751,
          -3.1319969,
          -2.2812555,
          -3.7315526,
          -0.8640543,
          -0.90071297,
          -0.5105979,
          -2.5997698,
          0.5354832,
          -3.460321,
          -0.9781833,
          -0.9823008,
          6.89471,
          -1.0824968,
          -4.1068435,
          -4.116433,
          -4.341316,
          -0.8484409,
          -1.6527946,
          -4.0627775,
          0.3234571,
          -3.19822,
          2.7146292,
          0.7749785,
          7.5386624,
          0.9005051,
          1.4101875,
          -0.07760859,
          2.3218443,
          -2.3375115,
          1.3215001,
          -2.8194945,
          0.37871194,
          3.1166985,
          1.6818913,
          1.8301915,
          2.1102643,
          2.4228504,
          0.79608935,
          1.0378445,
          -5.434816,
          -5.382131,
          -5.1073766,
          -5.321737,
          -5.7207055,
          -5.1933365,
          -5.3805223,
          -6.082215,
          -6.065305,
          -5.6602273,
          -5.866645,
          -5.6894755,
          -6.116904,
          -5.825528,
          -4.258357,
          -1.4837465,
          -2.0610807,
          -3.4392211,
          -3.504302,
          -1.0088757,
          -1.3494462,
          -3.4111,
          4.1092377,
          -0.59320265,
          -1.6205208,
          -1.1093227,
          -1.7264906,
          1.1691709,
          -1.8136699,
          -3.6389298,
          1.6656682,
          7.961871,
          -1.508529,
          -1.8703357,
          -1.5917408,
          -1.8668649,
          -3.4073832,
          -2.12631,
          -2.25937,
          1.8587254,
          -1.2995886,
          -0.8481535,
          -2.8326435,
          -0.68227595,
          -1.4078219,
          -2.74496,
          -2.9698136,
          -1.4374969,
          -0.9637592,
          -3.071281,
          -2.960208,
          0.12427084,
          -3.0901837,
          -2.3826447,
          -2.7091393,
          -2.5036802,
          -3.3876014,
          -5.6271224,
          -6.1243978,
          -1.4683915,
          -7.3217864,
          -7.5080795,
          -7.065439,
          -3.8420982,
          -5.9706354,
          -0.87457883,
          0.8029938,
          -0.05589171,
          -6.4990206,
          -6.695636,
          -2.4579108,
          -2.2584884,
          -3.4840555,
          -3.5919201,
          -5.8697248,
          -5.9706154,
          -0.92796147,
          0.15165077,
          -0.55904627,
          -2.1641374,
          -0.06695348,
          -0.62427175,
          -0.47218102,
          -0.0886962,
          0.40782905,
          -0.44996312,
          -1.0714816,
          -0.19981486,
          -0.5477641,
          -0.9246296,
          -0.6837677,
          -1.0164212,
          -1.023442,
          1.7042683,
          -0.20385268,
          -0.5641134,
          -0.35803083,
          -0.9519521,
          2.8801005,
          -0.52599144,
          3.8378792,
          2.3309972,
          2.4441628,
          -0.63046104,
          1.0414917,
          4.4521046,
          -2.7339687,
          -2.0346777,
          -2.3338022,
          1.082065,
          -0.7905872,
          -1.9378988,
          6.1220303,
          5.8838573,
          -1.9021841,
          -2.5785913,
          -2.2825534,
          -3.6273746,
          -2.2204454,
          -2.00262,
          -2.7739737,
          0.28863543,
          -0.40135634,
          0.20766193,
          0.27958497,
          0.14945877,
          -0.518291,
          0.011506723,
          0.0013892468,
          -2.951051,
          1.1766764,
          0.14008732,
          -3.5558264,
          2.9716094,
          -1.3771086,
          -2.0477958,
          -3.4153228,
          -0.024256213,
          -0.727342,
          -4.3017173,
          -4.3835025,
          -3.7676392,
          -3.8487575,
          8.972576,
          9.003151,
          9.005248,
          8.971704,
          8.956149,
          8.468415,
          8.941125,
          8.956832,
          8.945266,
          8.964809,
          8.980796,
          8.9790325,
          8.972043,
          8.773785,
          -3.874779,
          -0.83025753,
          -0.6383781,
          -3.0801377,
          -0.73347145,
          -0.4296079,
          -0.18187046,
          -0.41062763,
          -0.08876927,
          -0.32670498,
          20.948008,
          20.428057,
          20.935024,
          20.77369,
          21.09197,
          21.794075,
          -0.28385276,
          0.104898736,
          -0.84117144,
          -2.6473973,
          -3.1834922,
          -3.5490847,
          -3.336785,
          -3.1832283,
          -4.191205,
          -3.0735524,
          -2.0211985,
          -2.4125893,
          -4.638037,
          -0.54561913,
          -0.9907815,
          0.0148561355,
          -0.5478255,
          -4.2369866,
          -4.4731355,
          -3.7461653,
          -3.9599679,
          -0.5286109,
          -0.815218,
          -2.312699,
          -0.5842907,
          -1.0855547,
          1.184378,
          0.11141896,
          -0.46544543,
          20.944166,
          20.898787,
          20.451538,
          20.139103,
          21.027533,
          20.917648,
          20.790249,
          20.98325,
          20.965006,
          21.53469,
          20.734823,
          21.075974,
          21.013954,
          21.177086,
          21.825588,
          -0.28964344,
          0.18951839,
          -0.7202875,
          -0.6690608,
          -3.8451564,
          -3.430076,
          -0.4198816,
          -2.80104,
          -3.4830818,
          -0.5170201,
          -0.5971155,
          -6.395765,
          -6.4499426,
          -0.6431565,
          -0.8759204,
          0.5406816,
          0.02680964,
          0.48902863,
          -1.7552782,
          -1.9158218,
          -2.959573,
          1.0244644,
          -3.142319,
          0.023285255,
          -4.767451,
          -0.24320756,
          -0.23514214,
          -2.6101665,
          -1.1674767,
          -1.7054197,
          0.19046351,
          -6.113387,
          0.275294,
          -5.7076006,
          -6.3155785,
          -6.752011,
          -6.7137337,
          -6.670243,
          -6.6621537,
          -6.7817373,
          -6.5409303,
          -6.3470564,
          3.9478343,
          3.5580997,
          -2.619293,
          -6.4063106,
          4.3335876,
          4.242816,
          4.205854,
          -6.3137045,
          -6.3484573,
          -6.154014,
          -6.213096,
          -6.1507936,
          -6.3421187,
          -1.8077973,
          1.0271844,
          4.2468395,
          -6.2398024,
          -6.267692,
          0.17200032,
          -6.111948,
          -6.811427,
          -1.2098676,
          -6.2663827,
          -6.5808306,
          -5.843814,
          -6.653304,
          -5.7241983,
          -0.76566786,
          -6.7519164,
          -6.000964,
          -6.1395164,
          -6.7614856,
          -6.666603,
          -6.716028,
          2.4565887,
          1.3308245,
          0.72917205,
          0.550277,
          1.3589206,
          1.2711887,
          1.3969206,
          0.26867235,
          0.12647049,
          3.999347,
          -1.0360496,
          -0.97219473,
          -0.4388483,
          0.22128403,
          -1.7051234,
          -4.3074646,
          1.4710451,
          -1.3193175,
          -4.812252,
          0.01805302,
          -1.310515,
          -4.530555,
          -4.8969045,
          -0.12005906,
          -1.4689838,
          -4.333405,
          1.3902125,
          1.7922574,
          0.451352,
          1.6197543,
          1.6427389,
          1.4548614,
          -1.4255702,
          -1.6150013,
          -3.508596,
          -6.0273757,
          -6.160668,
          -6.5883894,
          -6.1752157,
          -6.48052,
          -4.5653048,
          -6.3186765,
          -6.323997,
          -5.4741797,
          -6.580654,
          0.3490191,
          0.5182689,
          -0.9569517,
          -6.3364387,
          -0.8671026,
          -0.16639642,
          -6.593063,
          1.3400458,
          2.2445602,
          -0.3660678,
          -6.6567135,
          2.2019742,
          0.7215987,
          -5.756977,
          -6.2920184,
          -5.0086923,
          -2.0117478,
          -5.360567,
          -6.6471763,
          -6.3607526,
          -6.2977166,
          -6.2306986,
          1.9679134,
          1.8432261,
          -6.1210775,
          1.1752207,
          -5.9182377,
          -1.6545259,
          -2.1436102,
          -2.4399722,
          0.62703115,
          1.7264694,
          1.6342244,
          0.7374689,
          -0.08166179,
          -0.8231587,
          -0.7155197,
          0.6241302,
          -1.5594215,
          -2.5043216,
          -2.6127043,
          -2.9761841,
          -3.8915548,
          -0.32831803,
          -0.383074,
          -0.5111907,
          -1.0641373,
          -2.1034756,
          -3.89236,
          -2.1085649,
          -1.6943933,
          -2.4674702,
          2.4643962,
          -1.8267323,
          -3.602049,
          -2.8749228,
          -3.823338,
          5.1182117,
          5.3790507,
          0.13849129,
          -1.3796847,
          -1.4347172,
          -0.79556483,
          -1.8650982,
          -1.6707625,
          -2.589307,
          -3.901367,
          -0.63927937,
          -6.869067,
          -1.4488866,
          -1.3256992,
          -1.6072129,
          -1.2162278,
          -0.43009835,
          -5.6555095,
          -4.4096594,
          -3.2688813,
          -2.7270129,
          -1.9309219,
          -2.2824087,
          -0.77231133,
          -3.9898849,
          -3.5305247,
          -3.4361475,
          -2.213372,
          -2.3424885,
          -1.6488231,
          -2.2591405,
          -3.6571276,
          -2.8379066,
          -2.9759889,
          -3.3754587,
          -3.3655894,
          -3.4427779,
          -0.11476877,
          -0.25589097,
          -0.37838385,
          0.5935314,
          1.0577147,
          2.022365,
          2.255783,
          -2.1791728,
          -2.8299658,
          -2.3583193,
          -2.4496295,
          -2.5762641,
          -2.3730917,
          -2.371834,
          -2.401091,
          -2.2265751,
          -2.5822687,
          -2.3695447,
          -2.448284,
          -2.3037877,
          -2.1059327,
          -2.456366,
          -2.351125,
          -2.723377,
          -2.4527917,
          -0.4274799,
          -0.06463944,
          -0.8445766,
          -0.5527449,
          -4.403361,
          -4.418242,
          -3.8663106,
          -3.8448243,
          8.981638,
          9.004445,
          9.003851,
          8.976628,
          8.957411,
          8.494405,
          8.8166685,
          8.939451,
          8.949521,
          8.964526,
          9.007423,
          8.961945,
          8.975549,
          8.940893,
          -3.9382873,
          -0.8950093,
          -0.45248696,
          -0.70332766,
          -0.35379606,
          -2.3536427,
          -0.46939138,
          -2.2994585,
          -0.72739995,
          1.246478,
          0.1716594,
          -0.4483462,
          21.070013,
          21.240639,
          20.895899,
          20.620829,
          20.665823,
          20.793642,
          21.019424,
          20.928265,
          21.08104,
          20.274145,
          20.596886,
          20.795177,
          20.98153,
          20.879606,
          21.02404,
          20.954613,
          20.87917,
          20.98157,
          20.688744,
          20.301802,
          20.28131,
          20.579393,
          20.881552,
          20.538378,
          20.477072,
          20.149836,
          20.80674,
          21.077969,
          20.736519,
          20.603687,
          20.658993,
          19.975965,
          20.624142,
          21.511211,
          20.799662,
          20.920584,
          20.748695,
          20.384712,
          20.870314,
          20.950792,
          20.67043,
          20.9388,
          20.922861,
          20.71306,
          21.357716,
          20.734035,
          20.576376,
          20.669367,
          21.013512,
          20.877213,
          20.893394,
          21.063913,
          21.314169,
          21.66018,
          20.94355,
          20.880768,
          20.656967,
          20.792728,
          21.049484,
          21.68133,
          21.104313,
          20.934952,
          20.978872,
          21.07461,
          21.614115,
          21.714191,
          20.820457,
          21.129848,
          21.265278,
          21.629257,
          -2.5167978,
          -0.3412408,
          -0.25070512,
          0.043945234,
          0.6316503,
          -5.726723,
          -6.543115,
          2.7008588,
          3.2444646,
          3.4997833,
          -6.558423,
          -6.1591253,
          -3.278353,
          -1.7783519,
          -2.118397,
          -3.5490482,
          -3.514355,
          -0.85379195,
          -0.8061645,
          -2.9716377,
          -3.451314,
          -1.8619733,
          -2.266881,
          -3.078794,
          0.53450567,
          0.7796245,
          -2.8698895,
          -0.3569873,
          -3.5353522,
          0.014166312,
          -0.25860074,
          -0.36748725,
          -1.3702303,
          -0.5152456,
          0.07406228,
          -0.35057452,
          -1.5959473,
          0.42211452,
          -1.0731562,
          -0.6195429,
          -0.10143128,
          -0.66812944,
          -1.2991333,
          -4.3673806,
          -0.82952833,
          -1.3989221,
          -1.6923164,
          0.0022186227,
          -0.6429182,
          -0.43431267,
          -0.8932259,
          -1.3904393,
          -0.9670052,
          -1.2370894,
          -1.1340926,
          -0.55649185,
          -0.86829937,
          -2.9436371,
          -0.48215383,
          -0.5581438,
          0.07312031,
          -0.68458676,
          -1.0807414,
          -1.2028836,
          -1.7999202,
          -3.7399273,
          -1.7050425,
          -1.4678717,
          -1.0724633,
          -0.7754995,
          -0.088099495,
          1.3141526,
          0.66245234,
          0.8279855,
          0.20935172,
          2.5478876,
          0.43668374,
          -5.1709743,
          -6.2423053,
          -6.4624267,
          1.8481822,
          2.1967325,
          -6.2992873,
          -4.2204022,
          -1.8598589,
          -3.330233,
          -2.1965582,
          1.2110273,
          -1.9426382,
          -2.1886103,
          -0.6262696,
          -0.6183125,
          -0.5078621,
          -2.6884384,
          -0.55551326,
          -1.4660679,
          -0.21536341,
          -0.17017663,
          -0.18957894,
          -0.12645891,
          -2.6929176,
          -1.0595955,
          -0.75442165,
          -4.860946,
          -3.6230023,
          -2.4493167,
          -0.8029879,
          -0.7667254,
          -0.5034785,
          -0.60485077,
          0.17922063,
          -2.5272171,
          5.3058763,
          5.0731254,
          -5.6260467,
          -5.8263183,
          -1.9445648,
          -2.45013,
          -4.322312,
          -0.7490088,
          -0.8270618,
          -0.23446912,
          -1.8106741,
          -0.8723107,
          -3.1046655,
          0.6741196,
          0.42899263,
          3.8551214,
          -2.4238658,
          -0.08342029,
          4.1744676,
          3.675216,
          0.4471938,
          0.46706766,
          0.38940486,
          0.45318347,
          0.5663655,
          0.40329033,
          0.42662397,
          0.42162055,
          0.44147342,
          0.430227,
          0.500589,
          2.7547138,
          2.3896937,
          1.611557,
          -0.70635414,
          0.20390178,
          -0.8849455,
          -4.3542285,
          1.1796832,
          -5.8738704,
          -5.9008594,
          -3.4504929,
          -1.5578384,
          -0.9091575,
          -0.8106928,
          0.5642607,
          -4.7895284,
          -4.334549,
          -0.41916072,
          -6.3455567,
          -5.9043107,
          -5.480087,
          -0.7385909,
          -2.7172077,
          0.26766625,
          0.110388845,
          2.200107,
          1.6399267,
          -0.32283494,
          -2.9010644,
          -2.9055965,
          -1.2301126,
          -0.7586167,
          -0.7518682,
          -1.7937262,
          0.02601154,
          -1.4426203,
          -0.7930842,
          2.1395266,
          -4.9025407,
          -0.4403175,
          -1.3982372,
          -0.8241083,
          -0.81298846,
          0.14616822,
          -0.44958606,
          0.6117031,
          -2.864407,
          -0.7398121,
          -0.7858537,
          -0.294264,
          -1.3294773,
          -0.06729264,
          -2.8933697,
          -1.3028741,
          -0.91095155,
          -0.9437763,
          -0.87765664,
          -1.3682263,
          -3.1105125,
          -4.4558735,
          -4.821613,
          -3.9334621,
          0.85825247,
          -0.552533,
          -0.2880984,
          -1.316452,
          -2.4294188,
          -3.3146796,
          -5.2950892,
          -5.5758524,
          -6.150198,
          -0.82273126,
          -6.658791,
          -6.5862446,
          -5.997535,
          -6.589942,
          -6.0695744,
          -6.3784156,
          -6.56122,
          -6.125675,
          2.770533,
          2.7159123,
          1.7145473,
          1.9239001,
          1.2355052,
          -1.7039323,
          -2.0695984,
          -3.5792177,
          -0.5504243,
          -0.84713143,
          -0.09477232,
          -0.5947532,
          -1.388798,
          -1.1402094,
          0.16809204,
          -0.4481068,
          -5.466665,
          -5.0861897,
          -5.047026,
          -1.0175213,
          -2.5259943,
          -0.5216894,
          2.8828547,
          2.3509567,
          -5.677874,
          -6.1536646,
          -6.7132382,
          -3.9199526,
          -6.011863,
          -1.3178545,
          -1.3152925,
          0.5958705,
          -0.9917192,
          -0.7382979,
          -0.67421514,
          -1.5813655,
          -3.346611,
          1.7554286,
          -0.041765887,
          -0.58230597,
          0.07759534,
          2.230251,
          -0.15143923,
          1.2460842,
          2.092143,
          1.5384935,
          0.6865425,
          2.0998218,
          1.8189511,
          1.130608,
          -0.7865469,
          1.7984475,
          -3.885304,
          -1.5030377,
          -0.9351373,
          0.3083835,
          0.30577067,
          -1.4497579,
          -1.5378336,
          -0.065704785,
          0.7489106,
          0.40930638,
          0.08330693,
          0.6502453,
          0.37960887,
          -0.22592019,
          -0.07354496,
          -0.6906921,
          0.053543705,
          2.0333734,
          2.3007083,
          2.7246087,
          2.6569824,
          2.5712953,
          -6.1407495,
          -6.5441747,
          2.284341,
          -6.455528,
          2.3580678,
          2.7590163,
          2.2148607,
          2.8177724,
          -2.104545,
          -0.991225,
          -3.3569157,
          -0.34828377,
          -0.9401435,
          -0.9509528,
          -0.10284499,
          -1.1020368,
          0.7985945,
          -1.3108813,
          -1.1599731,
          -1.1991936,
          0.6354301,
          -1.0495083,
          -1.23264,
          -1.0308356,
          -1.3785306,
          -2.3650298,
          -1.9640069,
          -1.1541864,
          -1.2445645,
          -1.2623091,
          -5.2443123,
          -3.9876578,
          -0.09977077,
          -0.20328902,
          -3.5108218,
          -1.7443082,
          -0.58334535,
          -1.4844135,
          -1.6651957,
          0.20110309,
          -0.7759371,
          -0.79168826,
          -0.7717329,
          -0.44717655,
          -0.47997102,
          -0.68907076,
          -0.8020026,
          -0.51956505,
          -0.65175354,
          -0.52653414,
          -0.5943814,
          -0.82792836,
          -0.50569713,
          -0.4551628,
          -0.46457407,
          -0.5480755,
          -0.2703788,
          1.0130711,
          0.9068926,
          0.7571773,
          0.4614269,
          0.28800333,
          0.5677681,
          -0.7759693,
          -1.3568264,
          -5.4080553,
          -5.8896656,
          -5.462251,
          -0.5455163,
          -0.6942165,
          -0.47131217,
          -0.8488061,
          -0.5139922,
          -0.5895355,
          -0.31688082,
          -2.2150211,
          -2.5143,
          -2.4544368,
          -2.653821,
          -3.1098182,
          -2.0049093,
          -1.9238365,
          -2.161148,
          -3.441314,
          0.6877951,
          0.08963121,
          -0.4656002,
          -0.55625737,
          -0.9799418,
          -1.5996823,
          -0.3498243,
          -0.8204215,
          -1.720716,
          5.0878425,
          -0.60753226,
          -0.4815197,
          -4.7109227,
          -0.3757095,
          -1.7746586,
          -0.40706933,
          -1.4056493,
          -0.17203332,
          -0.92835784,
          -0.52983224,
          -0.51929283,
          -2.5317075,
          -0.46525997,
          -0.5624516,
          -0.51885164,
          -0.6006926,
          -0.052243534,
          -0.40200844,
          -1.4160416,
          -1.7483795,
          -3.514133,
          -2.2823875,
          0.5956798,
          -1.3677632,
          -3.7204401,
          -2.323906,
          -2.428655,
          -2.386894,
          -2.4414914,
          -2.4708781,
          -2.4230947,
          -2.4442887,
          -2.421277,
          -2.3694367,
          -2.4202435,
          -2.3425877,
          -2.3948603,
          -2.298268,
          -4.3485765,
          -4.0950327,
          -5.8861923,
          0.81692123,
          -0.18564165,
          -0.49159604,
          -0.3636553,
          -2.3396764,
          -3.337848,
          2.2010314,
          2.734202,
          -6.1330986,
          -0.43545508,
          -1.6722964,
          2.2147334,
          -0.13408734,
          -3.0545487,
          -0.93959177,
          -2.4058495,
          -2.1202583,
          -1.8882902,
          -2.7613907,
          -1.5856926,
          2.0429282,
          0.049254764,
          0.0128354505,
          -2.970617,
          -3.429846,
          -2.2117538,
          0.9064666,
          -1.5720586,
          -2.5306413,
          -2.7595334,
          -3.8203602,
          -2.3657925,
          -1.7928596,
          -3.5758898,
          -3.8711479,
          -2.968933,
          -3.5682178,
          -0.99413794,
          -3.1058214,
          -5.875073,
          -5.71573,
          -6.2944036,
          -4.9698586,
          -6.3826337,
          -6.617977,
          1.1287634,
          2.62307,
          -1.554172,
          -2.6851609,
          -3.7583663,
          -0.72554445,
          -0.72254634,
          -0.54445875,
          -1.0257536,
          -0.1729477,
          -0.2685448,
          -4.3437424,
          -0.37651634,
          -1.0555568,
          -6.095903,
          -1.1682515,
          -1.2319616,
          -2.2631404,
          -2.622036,
          -0.018801171,
          0.9295567,
          1.483854,
          -5.167286,
          -1.2770798,
          -2.5153065,
          -3.1351035,
          0.6838235,
          -1.2571045,
          -0.39848313,
          -2.3363593,
          -1.9730741,
          -3.4748008,
          -3.635847,
          -1.4605961,
          -1.3178468,
          -0.24275091,
          1.6655167,
          -3.7985713,
          -4.589865,
          -5.643534,
          -0.85913354,
          -0.6401493,
          0.7900281,
          2.1212158,
          1.6727802,
          0.5279674,
          2.0422654,
          -0.4764622,
          -2.075177,
          -1.9618527,
          -1.4248024,
          -1.886298,
          -1.6324669,
          -1.8286679,
          -1.3740782,
          -1.1050485,
          -1.472727,
          0.13041086,
          1.0559155,
          -0.830324,
          0.8364951,
          -6.5479193,
          1.3962605,
          -2.387839,
          -2.148127,
          -1.7083119,
          -2.6712873,
          -3.0829585,
          -3.1890433,
          -3.4302225,
          -3.6489599,
          -3.0929887,
          -2.0135329,
          -2.9659202,
          -3.547459,
          -3.3752048,
          -0.59957516,
          -2.916432,
          -1.1319969,
          -2.7901027,
          -2.250535,
          -1.8511465,
          -3.5570533,
          -5.820641,
          -6.286276,
          1.6507956,
          -2.8465288,
          -2.482086,
          -3.0039222,
          -3.4280603,
          -3.3511508,
          -2.1453357,
          -1.4645556,
          -1.8108896,
          -3.3004248,
          -1.6481078,
          -2.1386416,
          -2.4947681,
          -3.4768038,
          -3.4952493,
          -0.98613393,
          -1.0486604,
          -1.2768512,
          -3.9191754,
          -3.8733647,
          -2.583402,
          -3.7195582,
          -3.9132137,
          -4.044997,
          -4.027682,
          -4.2189593,
          -4.052325,
          -4.098143,
          -4.20101,
          -4.133729,
          -4.026712,
          -4.4747224,
          -4.9029164,
          -4.4521346,
          -4.6822214,
          -3.3146682,
          -3.7041543,
          -1.3209857,
          0.7543623,
          0.24639511,
          -2.6390316,
          -1.6261909,
          -0.7026609,
          -1.7565743,
          -1.6775392,
          -1.6976323,
          -2.2149985,
          -2.356337,
          -1.6685759,
          -1.2239294,
          -3.4709566,
          -3.590942,
          -2.0626035,
          -0.9091525,
          -1.0068961,
          -0.88965046,
          0.5072345,
          -1.0691886,
          -4.3213553,
          -5.0881114,
          -4.790906,
          -4.660084,
          -4.3638406,
          -6.547682,
          -5.619394,
          -0.49399662,
          0.84922224,
          -0.7341885,
          -1.0349203,
          -0.2462089,
          -2.3767674,
          -2.9539034,
          -3.54265,
          2.7504048,
          1.7981358,
          1.7804928,
          -1.3827028,
          0.85084504,
          1.2153558,
          -5.47917,
          -5.8718414,
          -6.04308,
          -6.3349185,
          -2.3319967,
          0.6692248,
          -3.4612517,
          0.7804596,
          0.14865372,
          0.2950423,
          -0.64603746,
          -0.66756296,
          -0.79951197,
          -0.19823553,
          -2.8274984,
          -0.2059036,
          -0.4614109,
          0.21933095,
          0.030750534,
          -2.528775,
          0.26643518,
          0.30689046,
          -1.1290178,
          -5.640533,
          -5.272697,
          -3.5960832,
          -1.8759317,
          -1.94487,
          -1.8325374,
          5.6707244,
          -5.44439,
          -1.9866424,
          -2.0409431,
          -2.4582267,
          -2.6484344,
          -2.680338,
          -3.1090004,
          -2.5293508,
          -0.1303467,
          -0.43274802,
          -0.24766088,
          -1.8651462,
          -2.6931963,
          -3.8822742,
          -1.433596,
          -2.9569342,
          -6.1643753,
          -1.1726868,
          0.90862226,
          0.22547215,
          -0.11546235,
          -2.6455414,
          -0.7887924,
          -0.6418317,
          0.03341251,
          -1.8873138,
          -2.887585,
          -5.7432547,
          -6.049081,
          -6.433024,
          0.09083584,
          -3.5369737,
          -3.5624008,
          1.3753679,
          -0.8839569,
          1.9085485,
          -3.1245854,
          0.2711671,
          0.8234369,
          -1.7484423,
          -3.2915783,
          -1.0662469,
          -2.5205472,
          -2.0911925,
          -3.2553022,
          -3.2775426,
          -3.3996248,
          -3.4445739,
          -3.4802501,
          -1.7536469,
          -2.0137799,
          -1.4971362,
          -2.745038,
          -2.3014863,
          -1.8297858,
          -3.4189444,
          -5.928265,
          -6.3595977,
          -7.172285,
          -3.5525472,
          -0.034249917,
          -0.8365376,
          1.1407365,
          0.3932604,
          -0.8082535,
          -0.878487,
          -2.243108,
          -2.6923683,
          -3.3552907,
          -2.1789362,
          -2.7242079,
          -3.353433,
          -0.16503781,
          -0.21698408,
          6.3176146,
          0.56046546,
          -0.47747815,
          -0.025446888,
          0.10149765,
          -0.53014964,
          -0.9268767,
          0.5375892,
          0.58486503,
          -1.3296494,
          -0.38901415,
          2.7877016,
          2.511649,
          1.9784243,
          0.99075407,
          2.3586416,
          2.6364706,
          1.444322,
          -0.27835825,
          -0.16168912,
          -0.20140107,
          -0.15880279,
          -0.48990747,
          -1.7111057,
          2.0946171,
          -1.1064037,
          -5.826827,
          -6.131417,
          0.13022457,
          0.99805707,
          0.14898613,
          -0.19088475,
          -0.39682323,
          0.7302422,
          -0.7486559,
          -0.7230364,
          -3.0402126,
          0.4738838,
          -0.6787247,
          0.013391281,
          0.23322825,
          -0.548492,
          -0.44012457,
          -5.825431,
          -3.3980854,
          -4.730639,
          -4.4517612,
          -4.0234494,
          -4.6303177,
          -4.3250313,
          -2.8109257,
          -2.903388,
          -3.5465105,
          -2.814899,
          -7.2649307,
          -7.568603,
          -7.2184076,
          -7.315423,
          -2.4941564,
          -2.4065182,
          -2.8184159,
          -2.6778488,
          -2.690272,
          -6.361923,
          -7.401668,
          -7.4087152,
          -0.37533647,
          -1.5696303,
          -2.7322147,
          -1.9155717,
          -1.7131351,
          -3.1996322,
          -3.7462313,
          -3.385136,
          -3.291043,
          -2.8937156,
          -3.215861,
          -0.58468354,
          -3.4488828,
          -0.91304624,
          0.18766631,
          -1.0155314,
          -0.9829209,
          -1.5261799,
          -2.1797895,
          -2.2157903,
          -2.9909682,
          -2.5787885,
          -1.9093118,
          0.33929718,
          -1.1180952,
          -1.5579839,
          -0.89148235,
          -5.9926524,
          0.12543033,
          -4.5963182,
          0.19514419,
          0.32502174,
          0.7461028,
          -0.34449166,
          1.7663388,
          -0.27451158,
          1.1421964,
          0.3631323,
          0.12250403,
          0.30632082,
          -0.44752702,
          2.7639952,
          2.7695053,
          2.7086062,
          2.6428566,
          2.6020284,
          2.38246,
          1.8392683,
          2.5656478,
          -0.31935993,
          -0.26593456,
          -0.6607427,
          -1.3161671,
          -0.8588072,
          -0.83684206,
          -2.819461,
          -0.64733857,
          -0.7200192,
          -0.8288464,
          -2.659832,
          -2.5884821,
          -1.9884979,
          -3.4336264,
          -1.3818837,
          -1.6524776,
          -3.566421,
          -1.9038855,
          -2.5119238,
          0.93732584,
          -1.4924858,
          -0.71610767,
          0.7787811,
          -3.6133063,
          -1.7638446,
          -1.4259882,
          -2.0345986,
          -0.7254686,
          -0.6309025,
          -0.40792987,
          -0.0623696,
          -0.8561709,
          0.24804877,
          0.029619224,
          0.76734644,
          -0.21458769,
          -3.4357069,
          -3.1945863,
          -2.396952,
          -5.885056,
          0.40248397,
          5.616471,
          -1.3796306,
          -1.2653553,
          -0.66463256,
          -0.6136242,
          -1.1231971,
          1.3608396,
          -2.3302252,
          -1.8959167,
          -3.086113,
          -3.5218875,
          -3.5939271,
          -0.77284765,
          -0.87555254,
          -2.8873928,
          -3.099242,
          -0.8498817,
          -0.76971406,
          -1.2971866,
          0.12614875,
          -1.0677079,
          -0.94176674,
          -0.75423354,
          -0.43516332,
          -0.9046846,
          -1.0013998,
          2.3070807,
          7.9586234,
          -1.0117283,
          -0.7357242,
          -1.95256,
          -2.6927066,
          -3.582941,
          -0.6705504,
          -0.75891185,
          -0.73413664,
          0.45275387,
          -2.6895452,
          -1.2627591,
          -3.3705676,
          -3.4442334,
          -0.8320837,
          -0.12380683,
          -2.7734892,
          -3.280709,
          -2.9818842,
          -3.1789339,
          -3.5102208,
          -3.0235896,
          -3.2261732,
          -3.1040435,
          -1.8206197,
          0.022688624,
          1.9742547,
          -0.98827374,
          -2.3551998,
          -2.9760835,
          -1.8155078,
          -2.2626975,
          -1.399043,
          -3.8948405,
          -5.951423,
          -5.7861614,
          -1.6260304,
          -1.163747,
          -0.5756391,
          -2.7004173,
          -2.5283537,
          -2.3274035,
          -3.1680872,
          -3.4815109,
          -3.486468,
          0.57509243,
          -0.6132638,
          -0.5494039,
          -1.0959154,
          -0.9437574,
          -0.99501634,
          1.3595463,
          -1.8416233,
          -1.6701865,
          -1.0365696,
          -2.3798304,
          -2.605896,
          -2.258579,
          -1.8577329,
          -0.28546154,
          1.3055555,
          -0.17079586,
          -3.449133,
          -3.6085246,
          -0.7144329,
          0.93785787,
          -1.6990178,
          2.4351964,
          2.8431394,
          1.9409323,
          1.822769,
          2.9290333,
          2.9934392,
          2.9125752,
          -2.4686692,
          2.2948036,
          2.5784743,
          -2.0745704,
          -3.0437033,
          -3.062105,
          -2.9404662,
          -3.4816158,
          -2.3360445,
          -1.7819624,
          -3.0981,
          -2.3802857,
          -2.1165433,
          -2.2154338,
          -0.82288975,
          1.8604409,
          -0.6451181,
          -2.544417,
          -2.2140918,
          -1.5027965,
          -3.451587,
          -0.574506,
          -0.72663134,
          4.713828,
          5.725788,
          -6.312067,
          1.5467532,
          -5.371711,
          -5.335367,
          1.9164636,
          -5.662731,
          -5.8496246,
          2.2630663,
          2.5109353,
          1.8158269,
          -6.630038,
          -1.7901906,
          -2.285202,
          -2.910526,
          -1.9927343,
          -3.0234423,
          0.07680168,
          -1.0183663,
          -1.1893083,
          0.0054619443,
          -0.713874,
          -0.0377854,
          0.14496723,
          -0.10446001,
          -0.027490493,
          -0.4410907,
          -0.4186002,
          -2.150875,
          -2.0086794,
          -0.044469204,
          0.5822918,
          -0.25483224,
          -1.323884,
          -0.9846388,
          -0.40841278,
          -0.6746854,
          -0.07123962,
          -0.3056698,
          -0.55330884,
          -1.5760974,
          -0.41919577,
          -0.29031697,
          -0.43657377,
          0.14665927,
          -0.63033223,
          -0.94291997,
          -0.8666332,
          -1.8874686,
          -3.773589,
          -2.279716,
          -1.3780872,
          -0.4551904,
          -0.56716514,
          0.0872823,
          -2.1968179,
          -0.3616257,
          2.3466568,
          -0.8443163,
          -1.5634029,
          -1.9076973,
          -1.4996651,
          -1.5307692,
          -2.921958,
          -2.9989946,
          -6.361873,
          -6.1751394,
          1.8589054,
          -1.3391359,
          -1.192267,
          -0.46864045,
          -3.3867712,
          0.9662649,
          -0.7913417,
          -1.230967,
          0.630339,
          -1.0674274,
          -0.90565175,
          -2.8769665,
          -0.59448355,
          -0.7459676,
          -2.3870888,
          -3.5831883,
          -2.8602145,
          -2.9156113,
          -3.559613,
          -3.4737964,
          -1.9655969,
          -2.3560662,
          -3.5217645,
          -2.7676013,
          -3.582077,
          0.38159865,
          -3.900188,
          -0.061919756,
          -5.637614,
          -2.5455942,
          -3.652441,
          1.1277891,
          1.1163528,
          -2.4200385,
          -5.7966866,
          -6.42348,
          -1.5298634,
          -4.5725884,
          -7.3567696,
          -7.3479166,
          -6.178685,
          -1.0507581,
          0.87020075,
          -6.825557,
          -6.8331456,
          -3.3805046,
          -1.6502354,
          -2.4634047,
          -3.9739401,
          -2.012429,
          -3.2519975,
          -3.3215,
          -3.5351362,
          -1.2163281,
          -1.678573,
          -3.1187296,
          -3.0692883,
          -3.4491804,
          -3.452008,
          -3.6486132,
          -3.6201344,
          -3.8028176,
          -3.1686184,
          -2.279876,
          -3.6333764,
          -3.1761632,
          -3.5090015,
          -2.418517,
          -2.088285,
          -4.4104247,
          -3.6077464,
          2.6650062,
          2.6286128,
          2.4347072,
          -1.9079874,
          -1.9474682,
          -2.8089705,
          -3.0601764,
          -0.21022356,
          -0.18609281,
          -0.16962999,
          -3.1286426,
          -1.9489712,
          -2.082603,
          -1.5667943,
          -1.2637184,
          -1.9388012,
          -3.0986235,
          -1.4595114,
          -0.17699446,
          -2.2269485,
          -1.6916268,
          -0.88568664,
          -0.6746838,
          -3.393852,
          -3.5255642,
          -3.6054487,
          0.75890166,
          -0.14505643,
          -2.9486191,
          -2.5822203,
          -1.1867354,
          -0.98565024,
          1.8151611,
          -1.205597,
          -1.3904167,
          -0.9474311
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Git over SSH\n\nYou can access and write data in repositories on huggingface.co using SSH (Secure Shel..."
          ],
          [
           "```\n$ ssh-add ~/.ssh/id_ed25519\n```\n\nIf you chose a different location than the default to store you..."
          ],
          [
           "Using Flair at Hugging Face\n\n[Flair](https://github.com/flairNLP/flair) is a very simple framework f..."
          ],
          [
           "```\n\nIt outputs the following:\n\n```text\nSentence[6]: \"George Washington ging nach Washington.\" â†’ [\"G..."
          ],
          [
           "Widget Examples\n\nNote that each widget example can also optionally describe the corresponding model ..."
          ],
          [
           "```\n\n### Summarization\n\n```yaml\nwidget:\n- text: \"The tower is 324 metres (1,063 ft) tall, about the ..."
          ],
          [
           "```\n\n### Text Generation\n\n```yaml\nwidget:\n- text: \"My name is Julien and I like to\"\n  example_title:..."
          ],
          [
           "```\n\n### Feature Extraction\n\n```yaml\nwidget:\n- text: \"My name is Sylvain and I live in Paris\"\n  exam..."
          ],
          [
           "```\n\n### Voice Activity Detection\n\n```yaml\nwidget:\n- src: https://cdn-media.huggingface.co/speech_sa..."
          ],
          [
           "```\n\n### Text-to-Image\n\n```yaml\nwidget:\n- text: \"A cat playing with a ball\"\n  example_title: \"Cat\"\n-..."
          ],
          [
           "```\n\n## Other\n\n### Structured Data Classification\n\n```yaml\nwidget:\n- structured_data:\n    fixed_acid..."
          ],
          [
           "Configure the Dataset Viewer\n\nThe Dataset Viewer supports many [data files formats](./datasets-addin..."
          ],
          [
           "Adding a Sign-In with HF button to your Space\n\nYou can enable a built-in sign-in flow in your Space ..."
          ],
          [
           "```\n\nYou can check out the [configuration reference docs](./spaces-config-reference) for more inform..."
          ],
          [
           "Those scopes are optional and can be added by setting `hf_oauth_scopes` in your Space's metadata:\n\n-..."
          ],
          [
           "Basically, you need to:\n\n- Redirect the user to `https://huggingface.co/oauth/authorize?redirect_uri..."
          ],
          [
           "User access tokens\n\n## What are User Access Tokens?\n\nUser Access Tokens are the preferred way to aut..."
          ],
          [
           "If you are a member of an organization with read/write/admin role, then your User Access Tokens will..."
          ],
          [
           "## How to use User Access Tokens?\n\nThere are plenty of ways to use a User Access Token to access the..."
          ],
          [
           "```\n\n<Tip warning={true}>\nTry not to leak your token! Though you can always rotate it, anyone will b..."
          ],
          [
           "ZenML on Spaces\n\n[ZenML](https://github.com/zenml-io/zenml) is an extensible, open-source MLOps fram..."
          ],
          [
           "Visit [the ZenML documentation](https://docs.zenml.io/) to learn more about its\nfeatures and how to ..."
          ],
          [
           "![Choose the ZenML Docker template](https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "Once you have your ZenML server up and running, you can connect to it from your\nlocal machine. To do..."
          ],
          [
           "```\n\nYou can also use the Direct URL in your browser to use the ZenML dashboard as a\nfullscreen appl..."
          ],
          [
           "<Tip warning={true}>\nIf you wish to use a cloud secrets backend together with ZenML for secrets\nmana..."
          ],
          [
           "## ðŸ¤— Feedback and support\n\nIf you are having trouble with your ZenML server on HuggingFace Spaces, y..."
          ],
          [
           "Using Spaces for Organization Cards\n\nOrganization cards are a way to describe your organization to o..."
          ],
          [
           "Repository Settings \n\n## Private repositories\n\nYou can choose a repository's visibility when you cre..."
          ],
          [
           "If these are use cases you need help with, please send us an email at **website at huggingface.co**...."
          ],
          [
           "--\n# Example metadata to be added to a dataset card.  \n# Full dataset card template at https://githu..."
          ],
          [
           "- {bcp47_lang_1}  # Example: en-US\npretty_name: {pretty_name}  # Example: SQuAD\nsize_categories:\n- {..."
          ],
          [
           "# Optional. This part can be used to store the feature types and size of the dataset to be used in p..."
          ],
          [
           "```\n\n# Optional. If you want your dataset to be protected behind a gate that users have to accept to..."
          ],
          [
           "Valid license identifiers can be found in [our docs](https://huggingface.co/docs/hub/repositories-li..."
          ],
          [
           "Repository limitations and recommendations\n\nThere are some limitations to be aware of when dealing w..."
          ],
          [
           "Under the hood, the Hub uses Git to version the data, which has structural implications on what you ..."
          ],
          [
           "- **Repository size**: The total size of the data you're planning to upload. There is no hard limit ..."
          ],
          [
           "- **Number of commits**: There is no hard limit for the total number of commits on your repo history..."
          ],
          [
           "Dask\n\n[Dask](https://github.com/dask/dask) is a parallel and distributed computing library that scal..."
          ],
          [
           "```\n\nThis creates a dataset repository `username/my_dataset` containing your Dask dataset in Parquet..."
          ],
          [
           "Access control in organizations\n\n<Tip>\n\nYou can set up [Single Sign-On (SSO)](./security-sso) to be ..."
          ],
          [
           "Billing\n\nAt Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), an..."
          ],
          [
           "Any feedback or support request related to billing is welcome at billing@huggingface.co.\n\n## Invoici..."
          ],
          [
           "Streamlit Spaces\n\n**Streamlit** gives users freedom to build a full-featured web app with Python in ..."
          ],
          [
           "```\n\nYou can edit the `sdk_version`, but note that issues may occur when you use an unsupported Stre..."
          ],
          [
           "## Add the dependencies\n\nFor the **Hot Dog Classifier** we'll be using a [ðŸ¤— Transformers pipeline](h..."
          ],
          [
           "```\ntransformers\ntorch\n```\n\nThe Spaces runtime will handle installing the dependencies!\n\n## Create t..."
          ],
          [
           "```\n\nThis Python script uses a [ðŸ¤— Transformers pipeline](https://huggingface.co/docs/transformers/pi..."
          ],
          [
           "```\n\n<!-- The height of this iframe has been calculated as 236 + 64 * 2. 236 is the inner content he..."
          ],
          [
           "- `id` is set to `<iframe />` that is used to specify the auto-resize target.\n- The `iFrame Resizer`..."
          ],
          [
           "```\n\nAdditionally, you can checkout [our documentation](./spaces-embed)...."
          ],
          [
           "Next Steps\n\nThese next sections highlight features and additional information that you may find usef..."
          ],
          [
           "To learn about Git branching, you can try out the [Learn Git Branching interactive tutorial](https:/..."
          ],
          [
           "**Note that you will need to [install Git LFS](https://git-lfs.github.com/) and the [`huggingface_hu..."
          ],
          [
           "```\ngit clone git@hf.co:me/myfork\n```\n\n3. Fetch non-LFS files:\n\n```\ncd myfork\ngit lfs install --skip..."
          ],
          [
           "Run with Docker\n\nYou can use Docker to run most Spaces locally.\nTo view instructions to download and..."
          ],
          [
           "Advanced Topics\n\n## Contents\n\n- [Using OpenCV in Spaces](./spaces-using-opencv)\n- [More ways to crea..."
          ],
          [
           "Using spaCy at Hugging Face\n\n`spaCy` is a popular library for advanced Natural Language Processing u..."
          ],
          [
           "```\n\nTo find the link of interest, you can go to a repository with a `spaCy` model. When you open th..."
          ],
          [
           "```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy hug..."
          ],
          [
           "```\n\n| Argument             | Type         | Description                                            ..."
          ],
          [
           "```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./o..."
          ],
          [
           "```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browse..."
          ],
          [
           "Audit Logs\n\n<Tip warning={true}>\nThis feature is part of the <a href=\"https://huggingface.co/enterpr..."
          ],
          [
           "Spaces Overview\n\nHugging Face Spaces make it easy for you to create and deploy ML-powered demos in m..."
          ],
          [
           "Under the hood, Spaces stores your code inside a git repository, just like the model and dataset rep..."
          ],
          [
           "| **Hardware**        \t| **GPU Memory** \t| **CPU** \t| **Memory** \t| **Disk** \t| **Hourly Price** \t|\n..."
          ],
          [
           "| **Storage tier**     \t| **Size**             \t| **Persistent** \t| **Monthly price** \t|\n|----------..."
          ],
          [
           "Note: Find more detailed and comprehensive pricing information on [our pricing page](https://hugging..."
          ],
          [
           "Variables are publicly accessible and viewable and will be automatically added to Spaces duplicated ..."
          ],
          [
           "Some Spaces might have environment variables that you may need to set up. In these cases, the duplic..."
          ],
          [
           "In case [OAuth](./spaces-oauth) is enabled for your Space, the following variables will also be avai..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "```\ntitle: My lovely space\nemoji: ðŸ¤—\ncolorFrom: blue\ncolorTo: green\nsdk: docker\npinned: false\nmodels:..."
          ],
          [
           "Search\n\nYou can now easily search anything on the Hub with **Full-text search**. We index model card..."
          ],
          [
           "## Filter with ease\n\nBy default, models, datasets, & spaces are being searched when a user enters a ..."
          ],
          [
           "[paddlenlp-banner](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub..."
          ],
          [
           "4. Easily deploy your model as a Gradio app on Spaces.\n\n<div class=\"flex justify-center\">\n<img src=\"..."
          ],
          [
           "```\npip install -U paddlenlp\n```\n\n## Using existing models\n\nSimilar to `transformer` models, the `pa..."
          ],
          [
           "```\n\nIf you want to see how to load a specific model, you can click `Use in paddlenlp` and you will ..."
          ],
          [
           "Reference\n\n## Deep Learning Container\n\nBelow you can find a version table of currently available Hug..."
          ],
          [
           "**Example 1: PyTorch Training:**\n`763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-t..."
          ],
          [
           "| ðŸ¤— Transformers version | ðŸ¤— Datasets version | PyTorch/TensorFlow version | type     | device | Pyt..."
          ],
          [
           "| 4.10.2                  | 1.11.0              | TensorFlow 2.4.1           | training | GPU    | 3..."
          ],
          [
           "## Inference DLC Overview\n\nThe Inference DLC overview includes all released and available Hugging Fa..."
          ],
          [
           "| ðŸ¤— Transformers version | PyTorch/TensorFlow version | type      | device | Python Version |\n| ----..."
          ],
          [
           "| 4.11.0                  | PyTorch 1.9.0              | inference | GPU    | 3.8            |\n| 4.1..."
          ],
          [
           "## Hugging Face Transformers Amazon SageMaker Examples\n\nExample Jupyter notebooks that demonstrate h..."
          ],
          [
           "| Notebook                                                                                          ..."
          ],
          [
           "| [02 getting started with TensorFlow](https://github.com/huggingface/notebooks/blob/main/sagemaker/..."
          ],
          [
           "| [07 Distributed Training: Data Parallelism](https://github.com/huggingface/notebooks/blob/main/sag..."
          ],
          [
           "| [12 Batch Processing with Amazon SageMaker Batch Transform](https://github.com/huggingface/noteboo..."
          ],
          [
           "| [18 AWS Inferentia](https://github.com/huggingface/notebooks/blob/main/sagemaker/18_inferentia_inf..."
          ],
          [
           "## Inference Toolkit API\n\nThe Inference Toolkit accepts inputs in the `inputs` key, and supports add..."
          ],
          [
           "```\n\n**`sentiment-analysis`**\n\n```json\n{\n  \"inputs\": \"Don't waste your time.  We had two different p..."
          ],
          [
           "```\n\n**`parameterized-request`**\n\n```json\n{\n  \"inputs\": \"Hugging Face, the winner of VentureBeatâ€™s I..."
          ],
          [
           "```\n\n**`HF_API_TOKEN`**\n\n`HF_API_TOKEN` defines your Hugging Face authorization token. The `HF_API_T..."
          ],
          [
           "Pandas\n\n[Pandas](https://github.com/pandas-dev/pandas) is a widely used Python data analysis toolkit..."
          ],
          [
           "```\n\nThis creates a dataset repository `username/my_dataset` containing your Pandas dataset in Parqu..."
          ],
          [
           "Datasets without language challenge\n\nRelated to https://github.com/huggingface/hub-docs/issues/986.\n..."
          ],
          [
           "```\n\nHaving this field filled in is essential for users to find datasets in their language and give ..."
          ],
          [
           "1. Find a dataset that doesn't have the `language` field filled in. You can find a list of datasets ..."
          ],
          [
           "4. Once you've identified the language(s) of the dataset, you can add the language tag(s) to the dat..."
          ],
          [
           "## F.A.Q.\n\n### Does it make sense to add language metadata to all datasets?\n\nNo! This is why we have..."
          ],
          [
           "```\n\n## Datasets without language field filled in..."
          ],
          [
           "| status | pr_url                                                                                   ..."
          ],
          [
           "|        |                                                                                          ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/unaidedelf87777/openapi-function-invocations-25k/d..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text/discussions..."
          ],
          [
           "|        |                                                                                          ..."
          ],
          [
           "|        |  [here](https://huggingface.co/datasets/vivym/midjourney-prompts/discussions/1)          ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca/discussions..."
          ],
          [
           "|        |                                                                                          ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/ajaykarthick/imdb-movie-reviews/discussions/1)    ..."
          ],
          [
           "|        |                                                                                          ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/vhtran/id-en/discussions/1#651ababdc4fdc1c93efb0f2..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dandrade/es-en/discussions/1#651ac2720047dc5f7aae8..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/joelniklaus/german_rental_agreements/discussions/1..."
          ],
          [
           "|        |                                                                                          ..."
          ],
          [
           "|        |                                                                                          ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/Iskaj/dutch_corpora_parliament_processed/discussio..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/manu/french_librispeech_text_only/discussions/1)  ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/stas/wmt16-en-ro-pre-processed/discussions/1#651ab..."
          ],
          [
           "|        |                                                                                          ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/W4nkel/turkish-sentiment-dataset/discussions/1#651..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/vhtran/de-en/discussions/1#651abad1b61121b12838a02..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/yongsun-yoon/open-ner-english/discussions/1#651abb..."
          ],
          [
           "| Merged | [here](https://huggingface.co/datasets/indiejoseph/wikipedia-en-filtered/discussions/1#65..."
          ],
          [
           "| Merged | [here](https://huggingface.co/datasets/thesistranslation/distilled-ccmatrix-de-en/discuss..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/erebos/germanZickleinLLAMA2Dataset/discussions/1) ..."
          ],
          [
           "| Merged | [here](https://huggingface.co/datasets/thesistranslation/distilled-ccmatrix-en-es/discuss..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/thisserand/health_care_german/discussions/1)      ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/neil-code/subset-data-en-zh/discussions/1#651ac98a..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/t5-qe-2023-enmr-da-sys-test/discuss..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/t5-qe-2023-ente-da-test/discussions..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/t5-qe-2023-enhi-da-test/discussions..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/llama-2-qe-2023-enta-da-sys-test/di..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/llama-2-qe-2023-engu-da-sys-test/di..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/llama-2-qe-2023-enmr-da-test/discus..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/llama-2-qe-2023-enta-sys-test/discu..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/llama-2-qe-2023-enhi-sys-test/discu..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/llama-2-qe-2023-enta-test/discussio..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/dipteshkanojia/llama-2-qe-2023-engu-test/discussio..."
          ],
          [
           "| Merged | [here](https://huggingface.co/datasets/ChanceFocus/flare-multifin-en/discussions/1#651ac6..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/VFiona/covid-19-synthetic-it-en-10000/discussions/..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/yezhengli9/wmt20-cs-en/discussions/1#651ac588394b6..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/yezhengli9/wmt20-en-ps/discussions/1#651ac54adeec0..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/yezhengli9/wmt20-ps-en/discussions/1#651ac4fcf0354..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/yezhengli9/wmt20-ja-en/discussions/1#651ac48fd03e9..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/yezhengli9/wmt20-de-en/discussions/1#651ac41a1c53e..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/shreevigneshs/iwslt-2023-en-pt-train-val-split-0.2..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/shreevigneshs/iwslt-2023-en-es-train-val-split-0.1..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/cahya/instructions-en/discussions/1#651ac25fbf3fb2..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/NadiaHassan/ar-en/discussions/1#651ac1936a6b822b88..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/vocab-transformers/wiki-en-passages-20210101/discu..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/Makxxx/french_CEFR/discussions/1)                 ..."
          ],
          [
           "| Merged | [here](https://huggingface.co/datasets/gollumeo/french-litterature/discussions/1)        ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/fathyshalab/germanquad_qg_qg_dataset/discussions/1..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/germank/hh-rlhf_with_features/discussions/1)      ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/philschmid/prompted-germanquad/discussions/1)     ..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/cansen88/turkishReviews_5_topic/discussions/1#651a..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/orhanxakarsu/turkishPoe-generation/discussions/1#6..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/Harsit/xnli2.0_train_turkish/discussions/1#651aeb1..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/erytrn/turkishReviews-ds-mini/discussions/1#651aeb..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/sebinbusra/turkishReviews-ds-mini/discussions/1#65..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/bosnakdev/turkishReviews-ds-mini/discussions/1#651..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/Veyselbyte/turkishReviews-ds-mini/discussions/1#65..."
          ],
          [
           "|        | [here](https://huggingface.co/datasets/Memis/turkishReviews-ds-mini/discussions/1#651aec9..."
          ],
          [
           "| Merged | [here](https://huggingface.co/datasets/AnanthZeke/tamil_sentences_sample/discussions/1#65..."
          ],
          [
           "Advanced Topics\n\n## Contents\n\n- [Integrate your library with the Hub](./models-adding-libraries)\n- [..."
          ],
          [
           "Storage Regions on the Hub\n\nRegions let you decide where your org's models and datasets will be stor..."
          ],
          [
           "For companies in the EU, that means you can use the Hub to build ML in a GDPR compliant way: with da..."
          ],
          [
           "Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with ..."
          ],
          [
           "```\n\nFinally, create an Action that automatically checks the file size of any new pull request:\n\n\n``..."
          ],
          [
           "Webhook guide: Setup an automatic metadata quality review for models and datasets \n\n<Tip>\n\nWebhooks ..."
          ],
          [
           "```\n\nThis metadata contains essential information about your model or dataset for potential users. T..."
          ],
          [
           "```python\nfrom huggingface_hub import DatasetCard, ModelCard\nfrom huggingface_hub.utils import Entry..."
          ],
          [
           "```\n\nThis function will return a Python dictionary containing the metadata associated with the repos..."
          ],
          [
           "```\n\nOnce we have this dictionary, we can create our metadata report. In the interest of brevity, we..."
          ],
          [
           "```\n\n<Tip>\n    `:=` is the Python Syntax for an assignment expression operator added to the Python l..."
          ],
          [
           "## Create a new Bot user profile\n\nThis guide creates a separate user account that will post the meta..."
          ],
          [
           "KEY = os.environ.get(\"WEBHOOK_SECRET\")\n\napp = FastAPI()\n\n@app.post(\"/webhook\")\nasync def webhook(req..."
          ],
          [
           "```\n\nThe above function will receive Webhook events and creates or updates the metadata review repor..."
          ],
          [
           "## Conclusion and next steps\n\nWe now have an automatic metadata review bot! Here are some ideas for ..."
          ],
          [
           "ðŸŸ§ Label Studio on Spaces\n\n[Label Studio](https://labelstud.io) is an [open-source data labeling\nplat..."
          ],
          [
           "By default, Label Studio is installed in Spaces with a configuration that uses\nlocal storage for the..."
          ],
          [
           "* `LABEL_STUDIO_DISABLE_SIGNUP_WITHOUT_LINK`: Setting this value to `true` will\n  disable unrestrict..."
          ],
          [
           "* `STORAGE_PERSISTENCE`: Set this to `1` to remove the warning about ephemeral\n  storage.\n\nRestart t..."
          ],
          [
           "* `STORAGE_TYPE`: Set this to `azure`.\n\n* `STORAGE_AZURE_ACCOUNT_NAME`: `<YOUR_STORAGE_ACCOUNT>`\n\n* ..."
          ],
          [
           "Webhooks\n\n<Tip>\n\nWebhooks are now publicly available!\n\n</Tip>\n\nWebhooks are a foundation for MLOps-r..."
          ],
          [
           "You can view the history of payloads sent in the activity tab of the webhook settings page, it's als..."
          ],
          [
           "```json\n{\n  \"event\": {\n    \"action\": \"create\",\n    \"scope\": \"discussion\"\n  },\n  \"repo\": {\n    \"type\"..."
          ],
          [
           "```\n\n### Event\n\nThe top-level properties `event` is always specified and used to determine the natur..."
          ],
          [
           "In the current version of webhooks, the top-level property `repo` is always specified, as events can..."
          ],
          [
           "```\n\n`repo.headSha` is the sha of the latest commit on the repo's `main` branch. It is only sent whe..."
          ],
          [
           "```\n\n## Webhook secret\n\nSetting a Webhook secret is useful to make sure payloads sent to your Webhoo..."
          ],
          [
           "## Debugging Webhooks\n\nYou can easily find recently generated events for your webhooks. Open the act..."
          ],
          [
           "Dataset viewer\n\nThe dataset page includes a table with the contents of the dataset, arranged by page..."
          ],
          [
           "## Access the parquet files\n\nTo power the dataset viewer, every dataset is auto-converted to the Par..."
          ],
          [
           "## Configure the Dataset Viewer\n\nTo have a properly working Dataset Viewer for your dataset, make su..."
          ],
          [
           "The Model Hub\n\n## What is the Model Hub?\n\nThe Model Hub is where the members of the Hugging Face com..."
          ],
          [
           "Signing commits with GPG\n\n`git` has an authentication layer to control who can push commits to a rep..."
          ],
          [
           "For a more in-depth explanation of how git and GPG interact, please visit the the [git documentation..."
          ],
          [
           "```\n\nGPG will then guide you through the process of creating a GPG key pair.\n\nMake sure you specify ..."
          ],
          [
           "Spaces Changelog\n\n## [2023-07-28] - Upstream Streamlit frontend for `>=1.23.0`\n\n- Streamlit SDK uses..."
          ],
          [
           "- Read more doc about: [Docker Spaces](./spaces-sdks-docker)\n\n## [2022-12-14] - Ability to set a cus..."
          ],
          [
           "- All `1.x.0` versions are now supported (up to `1.9.0`).\n\n## [2022-05-16] - Gradio 3 is out!\n\n- Thi..."
          ],
          [
           "## [2021-08-10] - Upgrade Streamlit to `0.83.0`\n\n- [Streamlit changelog](https://github.com/streamli..."
          ],
          [
           "Licenses\n\nYou are able to add a license to any repo that you create on the Hugging Face Hub to let o..."
          ],
          [
           "<!-- region licenses -->\nFullname | License identifier (to use in model card)\n--- | ---\nApache licen..."
          ],
          [
           "Creative Commons Attribution Share Alike 3.0 | `cc-by-sa-3.0`\nCreative Commons Attribution Share Ali..."
          ],
          [
           "Etalab Open License 2.0 | `etalab-2.0`\nEuropean Union Public License 1.1 | `eupl-1.1`\nGNU Affero Gen..."
          ],
          [
           "Llama 2 Community License Agreement | `llama2`\nUnknown | `unknown`\nOther | `other`\n<!-- endregion --..."
          ],
          [
           "In case of `license: other` please add the license's text to a `LICENSE` file inside your repo (or c..."
          ],
          [
           "Hugging Face Hub documentation\n\nThe Hugging Face Hub is a platform with over 350k models, 75k datase..."
          ],
          [
           "<div class=\"group flex flex-col space-y-2 rounded-xl border border-orange-100 bg-gradient-to-br from..."
          ],
          [
           "<div class=\"flex items-center py-0.5 text-lg font-semibold text-orange-600 dark:text-gray-400 mb-1\">..."
          ],
          [
           "<a class=\"transform !no-underline transition-colors hover:translate-x-px hover:text-gray-700\" href=\"..."
          ],
          [
           "<div class=\"group flex flex-col space-y-2 rounded-xl border border-indigo-100 bg-gradient-to-br from..."
          ],
          [
           "<div class=\"flex items-center py-0.5 text-lg font-semibold text-indigo-600 dark:text-gray-400 mb-1\">..."
          ],
          [
           "<a class=\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\" href=\"./m..."
          ],
          [
           "<a class=\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\" href=\"./m..."
          ],
          [
           "<div class=\"group flex flex-col space-y-2 rounded-xl border border-red-100 bg-gradient-to-br from-re..."
          ],
          [
           "<svg class=\"shrink-0 mr-1.5 text-red-400\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www..."
          ],
          [
           "7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5..."
          ],
          [
           "<a class=\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\" href=\"./d..."
          ],
          [
           "<a class=\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\" href=\"./d..."
          ],
          [
           "<div class=\"group flex flex-col space-y-2 rounded-xl border border-blue-100 bg-gradient-to-br from-b..."
          ],
          [
           "<svg class=\"shrink-0 mr-1.5 text-blue-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://ww..."
          ],
          [
           "7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.08..."
          ],
          [
           "<a class=\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\" href=\"./s..."
          ],
          [
           "<a class=\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\" href=\"./s..."
          ],
          [
           "<div class=\"group flex flex-col space-y-2 rounded-xl border border-green-100 bg-gradient-to-br from-..."
          ],
          [
           "<div class=\"flex items-center py-0.5 text-lg font-semibold text-green-600 dark:text-gray-400 mb-1\">\n..."
          ],
          [
           "<a class=\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\" href=\"./o..."
          ],
          [
           "<a class=\"!no-underline hover:opacity-60 transform transition-colors hover:translate-x-px\" href=\"./a..."
          ],
          [
           "</div>\n\n## What's the Hugging Face Hub?\n\nWe are helping the community work together towards the goal..."
          ],
          [
           "## Models\n\nYou can discover and use dozens of thousands of open-source ML models shared by the commu..."
          ],
          [
           "The [ðŸ¤— `datasets`](https://huggingface.co/docs/datasets/index) library allows you to programmaticall..."
          ],
          [
           "## Organizations\n\nCompanies, universities and non-profits are an essential part of the Hugging Face ..."
          ],
          [
           "Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including aut..."
          ],
          [
           "Here is an inference example:\n\n```py\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Spe..."
          ],
          [
           "```\n\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be ..."
          ],
          [
           "Model Card components\n\n**Model Card Components** are special elements that you can inject directly i..."
          ],
          [
           "Annotated Model Card Template\n\n\n## Template\n\n[modelcard_template.md file](https://github.com/hugging..."
          ],
          [
           "* The **project organizer** is necessary for filling out [Model Details](#model-details) and [Uses](..."
          ],
          [
           "* **Shared by [optional]:** `shared_by`\n\n_List (and ideally link to) the people/organization making ..."
          ],
          [
           "_Explain how the model can be used without fine-tuning, post-processing, or plugging into a pipeline..."
          ],
          [
           "## Training Procedure [optional]\n\n\n### Preprocessing\n\n\n`preprocessing`\n\n_Detail tokenization, resizi..."
          ],
          [
           "**Section Overview:** This is an experimental section some developers are beginning to add, where wo..."
          ],
          [
           "**Section Overview:** This section defines common terms and how metrics are calculated.\n\n\n`glossary`..."
          ],
          [
           "Organizations\n\nThe Hugging Face Hub offers **Organizations**, which can be used to group accounts an..."
          ],
          [
           "Using ðŸ¤— Datasets\n\nOnce you've found an interesting dataset on the Hugging Face Hub, you can load the..."
          ],
          [
           "Appendix\n\n## Appendix A: User Study\n_Full text responses to key questions_\n\n### How would you define..."
          ],
          [
           "### What do you like about model cards?\n\n* They are interesting to teach people about new models\n* A..."
          ],
          [
           "### Other key new insights\n\n* Model cards are best filled out when done by people with different rol..."
          ],
          [
           "* Google Cloud: [Face Detection](https://modelcards.withgoogle.com/face-detection), [Object Detectio..."
          ],
          [
           "* [Duke PULSE Model Card](https://arxiv.org/pdf/2003.03808.pdf)\n* [Stanford Dynasent](https://github..."
          ],
          [
           "### MODEL CARDS FOR LARGE LANGUAGE MODELS\nLarge language models are often released with associated d..."
          ],
          [
           "### MODEL CARD GENERATION TOOLS\nTools for programmatically or interactively generating model cards i..."
          ],
          [
           "Notifications\n\nNotifications allow you to know when new activities (Pull Requests or discussions) ha..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "![Notifications settings page](https://huggingface.co/datasets/huggingface/documentation-images/reso..."
          ],
          [
           "How to configure SAML SSO with Azure\n\nIn this guide, we will use Azure as the SSO provider and with ..."
          ],
          [
           "### Step 2: Configure your application on Azure\n\nOpen a new tab/window in your browser and navigate ..."
          ],
          [
           "Then under \"SAML Certificates\", verify that \"Signin Option\" is set to \"Sign SAML response and assert..."
          ],
          [
           "```\n-----BEGIN CERTIFICATE-----\n{certificate}\n-----END CERTIFICATE-----\n```\n\n<div class=\"flex justif..."
          ],
          [
           "Gradio Spaces\n\n**Gradio** provides an easy and intuitive interface for running a model from a list o..."
          ],
          [
           "## Create a new Gradio Space\n\nWe'll start by [creating a brand new Space](https://huggingface.co/new..."
          ],
          [
           "```\ntransformers\ntorch\n```\n\nThe Spaces runtime will handle installing the dependencies!\n\n## Create t..."
          ],
          [
           "```\n\nThis Python script uses a [ðŸ¤— Transformers pipeline](https://huggingface.co/docs/transformers/pi..."
          ],
          [
           "Cookie limitations in Spaces\n\nIn Hugging Face Spaces, applications have certain limitations when usi..."
          ],
          [
           "Argilla on Spaces\n\n**Argilla** is an open-source, data labelling tool, for highly efficient human-in..."
          ],
          [
           "<Tip>\n**IMPORTANT NOTE ABOUT DATA PERSISTENCE:**\nYou can use the Argilla Quickstart Space as is for ..."
          ],
          [
           "<Tip>\nFor quick experimentation, you can jump directly into the next section. If you want to secure ..."
          ],
          [
           "The usernames, passwords, and API keys to upload, read, update, and delete datasets can be configure..."
          ],
          [
           "The combination of these secret variables gives you the following setup options:\n\n1. *I want to avoi..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentatio..."
          ],
          [
           "```\n\nThird, you need to read the dataset using the `datasets` library. For reading other file types,..."
          ],
          [
           "```\n\nTo train a SetFit model with this dataset:\n\n```python\nfrom sentence_transformers.losses import ..."
          ],
          [
           "Using Stable-Baselines3 at Hugging Face\n\n`stable-baselines3` is a set of reliable implementations of..."
          ],
          [
           "```\npackage_to_hub(model=model, \n               model_name=\"ppo-LunarLander-v2\",\n               mode..."
          ],
          [
           "File names and splits\n\nTo host and share your dataset, create a dataset repository on the Hugging Fa..."
          ],
          [
           "```\nmy_dataset_repository/\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ train.csv\nâ”œâ”€â”€ test.csv\nâ””â”€â”€ validation.csv\n```\n\nIf you d..."
          ],
          [
           "```\n\n### Multiple files per split\n\nSplits can span several files, for example:\n\n```\nmy_dataset_repos..."
          ],
          [
           "Integrate your library with the Hub\n\nThe Hugging Face Hub aims to facilitate sharing machine learnin..."
          ],
          [
           "```\n\n2. Once you have successfully installed the `huggingface_hub` library, log in to your Hugging F..."
          ],
          [
           "```\n\n   `notebook_login` will launch a widget in your notebook from which you can enter your Hugging..."
          ],
          [
           "```\n\nUse the `cache_dir` parameter to change where a file is stored:\n\n```python\n>>> from huggingface..."
          ],
          [
           "```\n\nDoing so will also add a tag to your model so users can quickly identify models from your libra..."
          ],
          [
           "```\n\nWhen you check your Hugging Face account, you should now see a `test-model` repository under yo..."
          ],
          [
           "```\n\nIf you need to upload more than one file, look at the [utilities offered by the `Repository` cl..."
          ],
          [
           "```\n\n    * For each task your library supports, modify the `app/pipelines/task_name.py` files accord..."
          ],
          [
           "```\n\n### Register your libraries supported tasks on the hub\n\nTo register the tasks supported by your..."
          ],
          [
           "Aim on Spaces\n\n**Aim** is an easy-to-use & supercharged open-source experiment tracker. Aim logs you..."
          ],
          [
           "Now, when you navigate to your Space's **App** section, you can access the Aim UI.\n\n## Compare your ..."
          ],
          [
           "```\n\nThe experiments tracked by Aim are stored in the `.aim` folder. **To display the logs with the ..."
          ],
          [
           "```\n\nThatâ€™s it! Now open the App section of your Space and the Aim UI is available with your logs.\nH..."
          ],
          [
           "# Model `license:other` challenge\n\nRelated to https://github.com/huggingface/hub-docs/issues/985.\n\n#..."
          ],
          [
           "```\n\nwhich display on the Hub as\n\n![](https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "```\n\n## How to contribute?\n\nHow to do it in practice? That's simple! We have listed models below tha..."
          ],
          [
           "For each model, the workflow looks like this:\n1. Choose a model in the list below. We suggest focusi..."
          ],
          [
           "## F.A.Q.\n\n### What if the model has 2 licenses?\n\nThis use case can happen when a model is finetuned..."
          ],
          [
           "|status|pr_url                                                                      |model_id|nb_dow..."
          ],
          [
           "|------|----------------------------------------------------------------------------|--------|------..."
          ],
          [
           "-|------------|-------------------------|-----------------------------------------------------------..."
          ],
          [
           "-------------------------------------------|--------------------------------------------------------..."
          ],
          [
           "----------------------------------------------|-----------------------------------------------------..."
          ],
          [
           "------------------------------------------------------|---------------------------------------------..."
          ],
          [
           "--------------------------------------------------------------------------------|-------------------..."
          ],
          [
           "------------------------------------------------------------------------|..."
          ],
          [
           "|Opened|[Hub PR](https://huggingface.co/decapoda-research/llama-7b-hf/discussions/130)|[decapoda-res..."
          ],
          [
           "|Opened|[hub_pr](https://huggingface.co/elinas/chronos-13b-v2/discussions/3) |[elinas/chronos-13b-v2..."
          ],
          [
           "|Opened|[HUB_PR](https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ/discu..."
          ],
          [
           "|      |                                                                            |[aipicasso/pica..."
          ],
          [
           "|      |                                                                            |[sambanovasyste..."
          ],
          [
           "|Opened|[HUB_PR](https://huggingface.co/huggyllama/llama-30b/discussions/1)                         ..."
          ],
          [
           "|      |                                                                            |[TheBloke/Llama..."
          ],
          [
           "|      |                                                                            |[georgesung/lla..."
          ],
          [
           "|      |                                                                            |[TheBloke/orca_..."
          ],
          [
           "|      |                                                                            |[jondurbin/airo..."
          ],
          [
           "|      |                                                                            |[h2oai/h2ogpt-r..."
          ],
          [
           "|      |                                                                            |[TheBloke/tulu-..."
          ],
          [
           "|      |                                                                            |[decapoda-resea..."
          ],
          [
           "|      |                                                                            |[luodian/llama-..."
          ],
          [
           "|      |                                                                            |[learnanything/..."
          ],
          [
           "|      |                                                                            |[waylandy/phosf..."
          ],
          [
           "|      |                                                                            |[aipicasso/cool..."
          ],
          [
           "|      |                                                                            |[TheBloke/airob..."
          ],
          [
           "|      |                                                                            |[TheBloke/airob..."
          ],
          [
           "|      |                                                                            |[aipicasso/cool..."
          ],
          [
           "|      |                                                                            |[TheBloke/orca_..."
          ],
          [
           "|      |                                                                            |[valurank/finet..."
          ],
          [
           "|      |                                                                            |[TheBloke/OpenC..."
          ],
          [
           "|      |                                                                            |[valurank/disti..."
          ],
          [
           "|      |                                                                            |[TheBloke/Zaraf..."
          ],
          [
           "|      |                                                                            |[Mitsua/vroid-d..."
          ],
          [
           "|      |                                                                            |[TheBloke/LLaMA..."
          ],
          [
           "|      |                                                                            |[TheBloke/Mytho..."
          ],
          [
           "|      |                                                                            |[TheBloke/llama..."
          ],
          [
           "|      |                                                                            |[TheBloke/Zarab..."
          ],
          [
           "|      |                                                                            |[coqui/XTTS-v1]..."
          ],
          [
           "|      |                                                                            |[TheBloke/LLaMA..."
          ],
          [
           "|      |                                                                            |[TheBloke/OpenA..."
          ],
          [
           "|      |                                                                            |[TheBloke/airob..."
          ],
          [
           "|      |                                                                            |[dfurman/llama-..."
          ],
          [
           "|      |                                                                            |[Neko-Institute..."
          ],
          [
           "|      |                                                                            |[tawfikgh/llama..."
          ],
          [
           "|      |                                                                            |[TheBloke/CodeF..."
          ],
          [
           "|      |                                                                            |[deerslab/llama..."
          ],
          [
           "|      |                                                                            |[TheBloke/airob..."
          ],
          [
           "|      |                                                                            |[TheBloke/llama..."
          ],
          [
           "|      |                                                                            |[TheBloke/Llama..."
          ],
          [
           "|      |                                                                            |[TheBloke/airob..."
          ],
          [
           "|      |                                                                            |[ibm/roberta-la..."
          ],
          [
           "|      |                                                                            |[TheBloke/llama..."
          ],
          [
           "|      |                                                                            |[TheBloke/OpenO..."
          ],
          [
           "|      |                                                                            |[nenkoru/llama-..."
          ],
          [
           "|      |                                                                            |[TheBloke/llama..."
          ],
          [
           "|      |                                                                            |[TheBloke/Mytho..."
          ],
          [
           "|      |                                                                            |[zohaib99k/Nous..."
          ],
          [
           "|      |                                                                            |[TheBloke/Airob..."
          ],
          [
           "|      |                                                                            |[TheBloke/Mytho..."
          ],
          [
           "|      |                                                                            |[unoooo/llama-7..."
          ],
          [
           "|      |                                                                            |[TheBloke/Mytho..."
          ],
          [
           "|      |                                                                            |[TheBloke/orca_..."
          ],
          [
           "|      |                                                                            |[Jsevisal/balan..."
          ],
          [
           "|      |                                                                            |[Jsevisal/rober..."
          ],
          [
           "|      |                                                                            |[nenkoru/alpaca..."
          ],
          [
           "|      |                                                                            |[TheBloke/airob..."
          ],
          [
           "|      |                                                                            |[TheBloke/OpenC..."
          ],
          [
           "|      |                                                                            |[TheBloke/Zarab..."
          ],
          [
           "|      |                                                                            |[Cartinoe5930/o..."
          ],
          [
           "|      |                                                                            |[Neko-Institute..."
          ],
          [
           "|      |                                                                            |[TheBloke/airob..."
          ],
          [
           "|      |                                                                            |[TheBloke/Chron..."
          ],
          [
           "|      |                                                                            |[TheBloke/qCamm..."
          ],
          [
           "|      |                                                                            |[4bit/Redmond-P..."
          ],
          [
           "|      |                                                                            |[Agtian/llama-3..."
          ],
          [
           "|      |                                                                            |[amayprro552/js..."
          ],
          [
           "|      |                                                                            |[aoyoo/llama-7b..."
          ],
          [
           "|      |                                                                            |[baffo32/llama-..."
          ],
          [
           "|      |                                                                            |[camelids/alpac..."
          ],
          [
           "|      |                                                                            |[camelids/llama..."
          ],
          [
           "|      |                                                                            |[camelids/llama..."
          ],
          [
           "|      |                                                                            |[camelids/llama..."
          ],
          [
           "|      |                                                                            |[camelids/llama..."
          ],
          [
           "|      |                                                                            |[camelids/llama..."
          ],
          [
           "|      |                                                                            |[coyotte508/ber..."
          ],
          [
           "|      |                                                                            |[decapoda-resea..."
          ],
          [
           "|      |                                                                            |[deepsbn/llama-..."
          ],
          [
           "|      |                                                                            |[fragro/llama-7..."
          ],
          [
           "|      |                                                                            |[Jsevisal/balan..."
          ],
          [
           "|      |                                                                            |[Jsevisal/balan..."
          ],
          [
           "|      |                                                                            |[Jsevisal/balan..."
          ],
          [
           "|      |                                                                            |[Jsevisal/disti..."
          ],
          [
           "|      |                                                                            |[khachdallak/ll..."
          ],
          [
           "|      |                                                                            |[Mithilss/llama..."
          ],
          [
           "|      |                                                                            |[nonlinearshima..."
          ],
          [
           "|      |                                                                            |[prodm93/llama_..."
          ],
          [
           "|      |                                                                            |[ruibin-wang/ll..."
          ],
          [
           "|      |                                                                            |[shekharchatter..."
          ],
          [
           "|      |                                                                            |[TheBloke/airob..."
          ],
          [
           "|      |                                                                            |[TheBloke/Airob..."
          ],
          [
           "|      |                                                                            |[TheBloke/Herme..."
          ],
          [
           "|      |                                                                            |[TheBloke/LLaMA..."
          ],
          [
           "|      |                                                                            |[TheBloke/Mytho..."
          ],
          [
           "|      |                                                                            |[TheBloke/Puddl..."
          ],
          [
           "|      |                                                                            |[TheBloke/tulu-..."
          ],
          [
           "|      |                                                                            |[TheBloke/Zarab..."
          ],
          [
           "|      |                                                                            |[Thireus/Vicuna..."
          ],
          [
           "|      |                                                                            |[usamakenway/ll..."
          ],
          [
           "|      |                                                                            |[Zhejian/llama-..."
          ],
          [
           "ChatUI on Spaces\n\n**Hugging Chat** is an open-source interface enabling everyone to try open-source ..."
          ],
          [
           "<a href=\"Parameters\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "Model Cards\n\n<Tip>\n\n[New! Try our experimental Model Card Creator App](https://huggingface.co/spaces..."
          ],
          [
           "Dataset, metric, and language identifiers are those listed on the [Datasets](https://huggingface.co/..."
          ],
          [
           "This UI will allow you to add key metadata to your model card and many of the fields will autocomple..."
          ],
          [
           "```\n\nYou can find the detailed model card metadata specification <a href=\"https://github.com/hugging..."
          ],
          [
           "```\n\nThis metadata will be used to display the base model on the model page. Users can also use this..."
          ],
          [
           "```\n\n### Specifying a task (`pipeline_tag`)\n\nYou can specify the `pipeline_tag` in the model card me..."
          ],
          [
           "```\n\nIf the license is not available via a URL you can link to a LICENSE stored in the model repo.\n\n..."
          ],
          [
           "```yaml\n---\nmodel-index:\n  - name: Yi-34B\n    results:\n      - task:\n          type: text-generation..."
          ],
          [
           "```\n\nFor more details on how to format this data, check out the [Model Card specifications](https://..."
          ],
          [
           "### Can I add custom tags to my model?\n\nYes, you can add custom tags to your model by adding them to..."
          ],
          [
           "Uploading datasets\n\nThe [Hub](https://huggingface.co/datasets) is home to an extensive collection of..."
          ],
          [
           "<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/document..."
          ],
          [
           "You can also look at the [Dataset Card specifications](https://github.com/huggingface/hub-docs/blob/..."
          ],
          [
           "## Using other libraries\n\nSome libraries like [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/inde..."
          ],
          [
           "WebDataset\n\n[WebDataset](https://github.com/webdataset/webdataset) is a library to write I/O pipelin..."
          ],
          [
           "Pull requests and Discussions\n\nHub Pull requests and Discussions allow users to do community contrib..."
          ],
          [
           "## View\n\nThe Discussion page allows you to see the comments from different users. If it's a Pull Req..."
          ],
          [
           "## Pin a Discussion / Pull Request\n\nIf you have write access to a repository, you can pin discussion..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "Once the comment has been edited, a new link will appear above the comment. This link shows the edit..."
          ],
          [
           "Read also [moderation](./moderation) to see how to report an abusive comment.\n\n## Can I use Markdown..."
          ],
          [
           "```\n\n### Draft mode\n\nDraft mode is the default status when opening a new Pull request from scratch i..."
          ],
          [
           "Using fastai at Hugging Face\n\n`fastai` is an open-source Deep Learning library that leverages PyTorc..."
          ],
          [
           "```\n\n\nIf you want to see how to load a specific model, you can click `Use in fastai` and you will be..."
          ],
          [
           "Using SpeechBrain at Hugging Face\n\n`speechbrain` is an open-source and all-in-one conversational too..."
          ],
          [
           "```\n\nIf you want to see how to load a specific model, you can click `Use in speechbrain` and you wil..."
          ],
          [
           "Model Card Guidebook \n\nModel cards are an important documentation and transparency framework for mac..."
          ],
          [
           "Our work presents a view of where we think model cards stand right now and where they could go in th..."
          ],
          [
           "Models Frequently Asked Questions\n\n## How can I see what dataset was used to train the model?\n\nIt's ..."
          ],
          [
           "The Hugging Face Hub is also home to Spaces, which are interactive demos used to showcase models. If..."
          ],
          [
           "## What if I have a different checkpoint of the model trained on a different dataset?\n\nBy convention..."
          ],
          [
           "Using OpenCLIP at Hugging Face\n\n[OpenCLIP](https://github.com/mlfoundations/open_clip) is an open-so..."
          ],
          [
           "```\n\nOnce loaded, you can encode the image and text to do [zero-shot image classification](https://h..."
          ],
          [
           "```\n\nIt outputs the probability of each possible class:\n\n```text\nLabel probs: tensor([[0.0020, 0.003..."
          ],
          [
           "Libraries\n\nThe Datasets Hub has support for several libraries in the Open Source ecosystem.\nThanks t..."
          ],
          [
           "The table below summarizes the supported libraries and their level of integration.\n\n| Library       ..."
          ],
          [
           "Using Stanza at Hugging Face\n\n`stanza` is a collection of accurate and efficient tools for the lingu..."
          ],
          [
           "Panel on Spaces\n\n[Panel](https://panel.holoviz.org/) is an open-source Python library that lets you ..."
          ],
          [
           "Once you have created the Space, it will start out in â€œBuildingâ€ status, which will change to â€œRunni..."
          ],
          [
           "### 3. requirements.txt\n\nThis file defines the required packages for our Panel app. When using Space..."
          ],
          [
           "User Studies\n## Model Card Audiences and Use Cases\n\nDuring our investigation into the landscape of m..."
          ],
          [
           "* **Stakeholder Perspectives**\n\nAs different people, of varying technical backgrounds, could be coll..."
          ],
          [
           "1) For those who have not created model cards before or who do not usually make a model card or any ..."
          ],
          [
           "**Insights:**\n\n* While a few respondents really liked this format, most found it overwhelming or as ..."
          ],
          [
           "[Checkout the Appendix](./model-card-appendix)\n\n </Tip>\n\n\nAcknowledgements\n================\n\nWe want..."
          ],
          [
           "Docker Spaces Examples\n\nWe gathered some example demos in the [Spaces Examples](https://huggingface...."
          ],
          [
           "Datasets\n\nThe Hugging Face Hub is home to a growing collection of datasets that span a variety of do..."
          ],
          [
           "Using GPU Spaces\n\nYou can upgrade your Space to use a GPU accelerator using the _Settings_ button in..."
          ],
          [
           "## Hardware Specs\n\nIn the following table, you can see the Specs for the different upgrade options.\n..."
          ],
          [
           "### PyTorch\n\nYou'll need to install a version of PyTorch compatible with the built-in CUDA drivers. ..."
          ],
          [
           "```\n--extra-index-url https://download.pytorch.org/whl/cu113\ntorch\n```\n\nYou can verify whether the i..."
          ],
          [
           "```\n\n### Tensorflow\n\nThe default `tensorflow` installation should recognize the CUDA device. Just ad..."
          ],
          [
           "```\n\n## Billing\n\nBilling on Spaces is based on hardware usage and is computed by the minute: you get..."
          ],
          [
           "If you want your Space never to deactivate or if you want to set a custom sleep time, you need to up..."
          ],
          [
           "How to Add a Space to ArXiv\n\nDemos on Hugging Face Spaces allow a wide audience to try out state-of-..."
          ],
          [
           "And that's it! Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes ..."
          ],
          [
           "```py\n    from transformers import LayoutLMForTokenClassification\n    \n    layoutlm_dummy = LayoutLM..."
          ],
          [
           "```\n\n    *Note*: Here's an [overview on building demos on Hugging Face Spaces](./spaces-overview) an..."
          ],
          [
           "Datasets Overview\n\n## Datasets on the Hub\n\nThe Hugging Face Hub hosts a [large number of community-c..."
          ],
          [
           "## Privacy\n\nSince datasets are repositories, you can [toggle their visibility between private and pu..."
          ],
          [
           "Using AllenNLP at Hugging Face\n\n`allennlp` is a NLP library for developing state-of-the-art models o..."
          ],
          [
           "```\n\nTo get a snippet such as this, you can click `Use in AllenNLP` at the top right,\n\n<div class=\"f..."
          ],
          [
           "```\n\n| Argument                    \t| Type         \t| Description                                   ..."
          ],
          [
           "The `push_to_hf` function has the same parameters as the bash script.\n\n```py\nfrom allennlp.common.pu..."
          ],
          [
           "```\n\nIn just a minute, you can get your model in the Hub, try it out directly in the browser, and sh..."
          ],
          [
           "Webhook guide: Setup an automatic system to re-train a model when a dataset changes\n\n<Tip>\n\nWebhooks..."
          ],
          [
           "First, let's create a Webhook from your [settings]( https://huggingface.co/settings/webhooks).\n\n- Se..."
          ],
          [
           "1. It spawns a FastAPI app that will listen to HTTP `POST` requests on `/webhook`:\n\n```python\nfrom f..."
          ],
          [
           "```\n\n2.  2. This route checks that the `X-Webhook-Secret` header is present and that its value is th..."
          ],
          [
           "```\n\n3. The event's payload is encoded as JSON. Here, we'll be using pydantic models to parse the ev..."
          ],
          [
           "```\n\n4. If the payload is valid, the next step is to create a project on AutoTrain, schedule a fine-..."
          ],
          [
           "```\n\nVisit the link inside the comment to review the training cost estimate, and start fine-tuning t..."
          ],
          [
           "```\n\n## Configure your Webhook to send events to your Space\n\nLast but not least, you'll need to conf..."
          ],
          [
           "Downloading models\n\n## Integrated libraries\n\nIf a model on the Hub is tied to a [supported library](..."
          ],
          [
           "```\n\n## Using Git\n\nSince all models on the Model Hub are Git repositories, you can clone the models ..."
          ],
          [
           "Getting Started with Repositories\n\nThis beginner-friendly guide will help you get the basic skills y..."
          ],
          [
           "```\n\n**The content in the Getting Started section of this document is also available as a video!**\n\n..."
          ],
          [
           "3. Enter your modelâ€™s name. This will also be the name of the repository. \n\n4. Specify whether you w..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "### Uploading a file\n\nIf you choose _Upload file_ you'll be able to choose a local file to upload, a..."
          ],
          [
           "```\n\nYou can clone over SSH with the following command:\n```bash\ngit clone git@hf.co:<your-username>/..."
          ],
          [
           "```\n\nAnd you're done! You can check your repository on Hugging Face with all the recently added file..."
          ],
          [
           "You can click on an individual commit to see what changes that commit introduced:\n\n<div class=\"flex ..."
          ],
          [
           "Embed your Space in another website\n\nOnce your Space is up and running you might wish to embed it in..."
          ],
          [
           "```\n\nFor instance using the [NimaBoscarino/hotdog-gradio](https://huggingface.co/spaces/NimaBoscarin..."
          ],
          [
           "Tabby on Spaces\n\n[Tabby](https://tabby.tabbyml.com) is an open-source, self-hosted AI coding assista..."
          ],
          [
           "</Tip>\n\n### Your Tabby Space URL\n\nOnce Tabby is up and running, for a space link such as https://hug..."
          ],
          [
           "![Code Completion](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub..."
          ],
          [
           "Data files Configuration\n\nThere are no constraints on how to structure dataset repositories.\n\nHoweve..."
          ],
          [
           "Single Sign-On (SSO)\n\n<Tip warning={true}>\nThis feature is part of the <a href=\"https://huggingface...."
          ],
          [
           "Models\n\nThe Hugging Face Hub hosts many models for a [variety of machine learning tasks](https://hug..."
          ],
          [
           "Using Asteroid at Hugging Face\n\n`asteroid` is a Pytorch toolkit for audio source separation. It enab..."
          ],
          [
           "```\n\nIf you want to see how to load a specific model, you can click `Use in Adapter Transformers` an..."
          ],
          [
           "Models Download Stats\n\n## How are download stats generated for models?\n\nCounting the number of downl..."
          ],
          [
           "```json\n{\n    \"adapter-transformers\": {\n        filter: [\n            {\n                term: { path..."
          ],
          [
           "{\n                term: { path: \"cfg.json\" },\n            },\n        ],\n    },\n    \"paddlenlp\": {\n  ..."
          ],
          [
           "],\n    },\n    \"diffusers\": {\n        /// Filter out nested safetensors and pickle weights to avoid d..."
          ],
          [
           "Using RL-Baselines3-Zoo at Hugging Face\n\n`rl-baselines3-zoo` is a training framework for Reinforceme..."
          ],
          [
           "```\n\nYou can define three parameters:\n- `--repo-name`: The name of the repo.\n- `-orga`: Your Hugging..."
          ],
          [
           "Using OpenCV in Spaces\n\nIn order to use OpenCV in your Gradio or Streamlit Spaces, you'll need to ma..."
          ],
          [
           "Uploading models\n\nTo upload models to the Hub, you'll need to create an account at [Hugging Face](ht..."
          ],
          [
           "1. In the \"Files and versions\" tab, select \"Add File\" and specify \"Upload File\":\n\n<div class=\"flex j..."
          ],
          [
           "The UI allows you to explore the model files and commits and to see the diff introduced by each comm..."
          ],
          [
           "Models trained with ðŸ¤— Transformers will generate [TensorBoard traces](https://huggingface.co/docs/tr..."
          ],
          [
           "Digital Object Identifier (DOI)\n\nThe Hugging Face Hub offers the possibility to generate DOI for you..."
          ],
          [
           "After you agree to those terms, your model or dataset will get a DOI assigned, and a new tag should ..."
          ],
          [
           "## Why is there a 'locked by DOI' message on delete, rename and change visibility action on my model..."
          ],
          [
           "Secrets Scanning\n\nIt is important to manage [your secrets (env variables) properly](./spaces-overvie..."
          ],
          [
           "Downloading datasets\n\n## Integrated libraries\n\nIf a dataset on the Hub is tied to a [supported libra..."
          ],
          [
           "```py\nfrom huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"YOUR_REPO_ID\"\nFILE..."
          ],
          [
           "```\n\n## Using Git\n\nSince all datasets on the Hub are Git repositories, you can clone the datasets lo..."
          ],
          [
           "Handling Spaces Dependencies\n\n## Default dependencies\n\nThe default Spaces environment comes with sev..."
          ],
          [
           "Debian dependencies are also supported. Add a **packages.txt** file at the root of your repository, ..."
          ],
          [
           "Inference API\n\nPlease refer to [Inference API Documentation](https://huggingface.co/docs/api-inferen..."
          ],
          [
           "Specify `inference: false` in your model card's metadata.\n\n## Why don't I see an inference widget or..."
          ],
          [
           "Enterprise Hub\n\nEnterprise Hub adds advanced capabilities to organizations, enabling safe, compliant..."
          ],
          [
           "How to configure SAML SSO with Okta\n\nIn this guide, we will use Okta as the SSO provider and with th..."
          ],
          [
           "### Step 2: Configure your application on Okta\n\nOpen a new tab/window in your browser and navigate t..."
          ],
          [
           "On Okta, set the following settings:\n\n* Set Audience URI (SP Entity Id) to match the \"SP Entity ID\" ..."
          ],
          [
           "```\n-----BEGIN CERTIFICATE-----\n{certificate}\n-----END CERTIFICATE-----\n```\n\n<div class=\"flex justif..."
          ],
          [
           "Using ðŸ§¨ `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained ..."
          ],
          [
           "## Using existing pipelines\n\nAll `diffusers` pipelines are a line away from being used! To run gener..."
          ],
          [
           "```\n\nIf you want to load a specific pipeline component such as the UNet, you can do so by:\n\n```py\nfr..."
          ],
          [
           "Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data ar..."
          ],
          [
           "Using SpanMarker at Hugging Face\n\n[SpanMarker](https://github.com/tomaarsen/SpanMarkerNER) is a fram..."
          ],
          [
           "```\n```json\n[\n    {\"span\": \"Amelia Earhart\", \"label\": \"person-other\", \"score\": 0.7629689574241638, \"..."
          ],
          [
           "```\n\nIf you want to load a specific SpanMarker model, you can click `Use in SpanMarker` and you will..."
          ],
          [
           "Disk usage on Spaces\n\nEvery Space comes with a small amount of disk storage. This disk space is ephe..."
          ],
          [
           "<Tip warning={true}>\n\tWARNING: all data stored in the storage is lost when you delete it.\n</Tip>\n\n##..."
          ],
          [
           "Visit the [`datasets` library](https://huggingface.co/docs/datasets/index) documentation and the [`h..."
          ],
          [
           "Hugging Face on Amazon SageMaker\n\n![cover](https://huggingface.co/datasets/huggingface/documentation..."
          ],
          [
           "- Cost-effective: Training instances are only live for the duration of your job. Once your job is co..."
          ],
          [
           "**One command is all you need**\n\nWith the new Hugging Face DLCs, train cutting-edge Transformers-bas..."
          ],
          [
           "Take a look at our published blog posts, videos, documentation, sample notebooks and scripts for add..."
          ],
          [
           "### Documentation\n\n- [Run training on Amazon SageMaker](/docs/sagemaker/train)\n- [Deploy models to A..."
          ],
          [
           "- [All notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n- [Getting Started..."
          ],
          [
           "- [Image Classification with Vision Transformer](https://github.com/huggingface/notebooks/blob/main/..."
          ],
          [
           "Hub API Endpoints\n\nWe have open endpoints that you can use to retrieve information from the Hub as w..."
          ],
          [
           "### GET /api/models\n\nGet information from all models in the Hub. The response is paginated, use the ..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.list_models()`.\n\n### GET /api/models/{repo_id} or /api/m..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.list_datasets()`.\n\n### GET /api/datasets/{repo_id} or /a..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.dataset_info(repo_id, revision)`.\n\n### GET /api/datasets..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.list_spaces()`.\n\n### GET /api/spaces/{repo_id} or /api/s..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.update_repo_visibility()`.\n\n### POST /api/repos/move\n\nMo..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.whoami()`.\n\n## Collections API\n\nUse Collections to group..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.create_collection()`.\n\n### GET /api/collections/{namespa..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.list_collections()`.\n\n### PATCH /api/collections/{namesp..."
          ],
          [
           "```\n\nThis is equivalent to `huggingface_hub.add_collection_item()`.\n\n### PATCH /api/collections/{nam..."
          ],
          [
           "Displaying carbon emissions for your model\n\n## Why is it beneficial to calculate the carbon emission..."
          ],
          [
           "```\n\n## How is the carbon footprint of my model calculated? ðŸŒŽ\n\nConsidering the computing hardware, l..."
          ],
          [
           "Train and deploy Hugging Face on Amazon SageMaker\n\nThe get started guide will show you how to quickl..."
          ],
          [
           "```\n\nIf you want to run this example in [SageMaker Studio](https://docs.aws.amazon.com/sagemaker/lat..."
          ],
          [
           "```\n\n## Preprocess\n\nThe ðŸ¤— Datasets library makes it easy to download and preprocess a dataset for tr..."
          ],
          [
           "```\n\n## Upload dataset to S3 bucket\n\nNext, upload the preprocessed dataset to your S3 session bucket..."
          ],
          [
           "```\n\n## Start a training job\n\nCreate a Hugging Face Estimator to handle end-to-end SageMaker trainin..."
          ],
          [
           "```\n\nBegin training with one line of code:\n\n```python\nhuggingface_estimator.fit({\"train\": training_i..."
          ],
          [
           "Dataset Cards\n\n## What are Dataset Cards?\n\nEach dataset may be documented by the `README.md` file in..."
          ],
          [
           "```\n\nThe metadata that you add to the dataset card enables certain interactions on the Hub. For exam..."
          ],
          [
           "* Visit the Paper page\n* Filter for other models on the Hub that cite the same paper.\n\n<div class=\"f..."
          ],
          [
           "Using sample-factory at Hugging Face\n\n[`sample-factory`](https://github.com/alex-petrenko/sample-fac..."
          ],
          [
           "```\ngit clone git@hf.co:<Name of HuggingFace Repo> # example: git clone git@hf.co:bigscience/bloom\n`..."
          ],
          [
           "```\npython -m sample_factory.huggingface.push_to_hub -r <hf_username>/<hf_repo_name> -d <experiment_..."
          ],
          [
           "```\npython -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<re..."
          ],
          [
           "hub-docs\n\nThis repository regroups documentation and information that is hosted on the Hugging Face ..."
          ],
          [
           "Datasets Download Stats\n\n## How are download stats generated for datasets?\n\nThe Hub provides downloa..."
          ],
          [
           "Using TensorBoard\n\nTensorBoard provides tooling for tracking and visualizing metrics as well as visu..."
          ],
          [
           "Organizations, Security, and the Hub API\n\n## Contents\n\n- [Organizations](./organizations)\n  - [Manag..."
          ],
          [
           "Using timm at Hugging Face\n\n`timm`, also known as [pytorch-image-models](https://github.com/rwightma..."
          ],
          [
           "```\n\nIf you want to see how to load a specific model, you can click **Use in timm** and you will be ..."
          ],
          [
           "# Create Transform\ntransform = create_transform(**resolve_data_config(model.pretrained_cfg, model=mo..."
          ],
          [
           "```\n\nThis should leave you with a list of predictions, like this:\n\n```py\n[\n    {'label': 'american_p..."
          ],
          [
           "```\n\nThen, push your model using the `push_to_hf_hub` method:\n\n```py\nimport timm\n\n# Build or load a ..."
          ],
          [
           "```\n\n## Inference Widget and API\n\nAll `timm` models on the Hub are automatically equipped with an [i..."
          ],
          [
           "```\n\n## Additional resources\n\n* timm (pytorch-image-models) [GitHub Repo](https://github.com/rwightm..."
          ],
          [
           "Image Dataset\n\nThis guide will show you how to configure your dataset repository with image files. Y..."
          ],
          [
           "```\nmy_dataset_repository/\nâ”œâ”€â”€ train\nâ”‚Â Â  â”œâ”€â”€ 1.jpg\nâ”‚Â Â  â””â”€â”€ 2.jpg\nâ””â”€â”€ test\n    â”œâ”€â”€ 3.jpg\n    â””â”€â”€ 4.jp..."
          ],
          [
           "```\n\n## Relative paths\n\nMetadata file must be located either in the same directory with the images i..."
          ],
          [
           "```\nmy_dataset_repository/\nâ”œâ”€â”€ test\nâ”‚Â Â  â”œâ”€â”€ green\nâ”‚Â Â  â”‚Â Â  â””â”€â”€ 2.jpg\nâ”‚Â Â  â””â”€â”€ red\nâ”‚Â Â      â””â”€â”€ 4.jpg\nâ””â”€..."
          ],
          [
           "Deploy models to Amazon SageMaker\n\nDeploying a ðŸ¤— Transformers models in SageMaker for inference is a..."
          ],
          [
           "```\n\nThis guide will show you how to deploy models with zero-code using the [Inference Toolkit](http..."
          ],
          [
           "```\n\n**SageMaker environment**\n\nSetup your SageMaker environment as shown below:\n\n```python\nimport s..."
          ],
          [
           "```\n\n## Deploy a ðŸ¤— Transformers model trained in SageMaker\n\n<iframe width=\"700\" height=\"394\" src=\"ht..."
          ],
          [
           "############ pseudo code end ############\n\n# deploy model to SageMaker Inference\npredictor = hf_esti..."
          ],
          [
           "```\n\nAfter you run your request you can delete the endpoint as shown:\n\n```python\n# delete endpoint\np..."
          ],
          [
           "```\n\nAfter you run our request, you can delete the endpoint again with:\n\n```python\n# delete endpoint..."
          ],
          [
           "```\n\nNow you can provide the S3 URI to the `model_data` argument to deploy your model later.\n\n## Dep..."
          ],
          [
           "# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n   initial_instance_coun..."
          ],
          [
           "```\n\nAfter you run our request, you can delete the endpoint again with:\n\n```python\n# delete endpoint..."
          ],
          [
           "```\n\nðŸ““ Open the [deploy_transformer_model_from_hf_hub.ipynb notebook](https://github.com/huggingface..."
          ],
          [
           "```python\nbatch_job = huggingface_estimator.transformer(\n    instance_count=1,\n    instance_type='ml..."
          ],
          [
           "```\n\nIf you want to run your batch transform job later or with a model from the ðŸ¤— Hub, create a `Hug..."
          ],
          [
           "```\n\nðŸ““ Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/ma..."
          ],
          [
           "```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file ..."
          ],
          [
           "Here is an example of a custom inference module with `model_fn`, `input_fn`, `predict_fn`, and `outp..."
          ],
          [
           "```\n\nCustomize your inference module with only `model_fn` and `transform_fn`:   \n\n```python\nfrom sag..."
          ],
          [
           "Single Sign-On (SSO)\n\nThe Hugging Face Hub gives you the ability to implement mandatory Single Sign-..."
          ],
          [
           "### Supported Identity Providers\n\nYou can easily integrate Hugging Face Hub with a variety of Identi..."
          ],
          [
           "This section allows you to define a mapping from your IdP's user profile data from your IdP to the a..."
          ],
          [
           "Spaces Configuration Reference\n\nSpaces are configured through the `YAML` block at the top of the **R..."
          ],
          [
           "**`suggested_hardware`** : _string_  \nSpecify the suggested [hardware](https://huggingface.co/docs/h..."
          ],
          [
           "**`models`** : _List[string]_  \nHF model IDs (like `gpt2` or `deepset/roberta-base-squad2`) used in ..."
          ],
          [
           "**`custom_headers`** : _Dict[string, string]_  \nSet custom HTTP headers that will be added to all HT..."
          ],
          [
           "```\n\n*Note:* all headers and values must be lowercase.\n\n**`preload_from_hub`**: _List[string]_\nSpeci..."
          ],
          [
           "Spaces Settings\n\nYou can configure your Space's appearance and other settings inside the `YAML` bloc..."
          ],
          [
           "Your First Docker Space: Text Generation with T5\n\nIn the following sections, you'll learn the basics..."
          ],
          [
           "```\n\n## Add the dependencies\n\nFor the **Text Generation** Space, we'll be building a FastAPI app tha..."
          ],
          [
           "```\n\n## Create the Dockerfile\n\nThe main step for a Docker Space is creating a Dockerfile. You can re..."
          ],
          [
           "```\n\nIf you have [Secrets](spaces-sdks-docker#secret-management) you can use `docker buildx` and pas..."
          ],
          [
           "```\n\nLet's go through all the steps to make this working. We'll skip some of the details of the CSS ..."
          ],
          [
           "```\n\n3. In the `app.py` file, mount the static files and show the html file in the root route\n\n```py..."
          ],
          [
           "```\n\n5. Grant permissions to the right directories\n\nAs discussed in the [Permissions Section](./spac..."
          ],
          [
           "```\n\nSuccess! Your app should be working now! Check out [DockerTemplates/fastapi_t5](https://hugging..."
          ],
          [
           "If everything went well, you will see `Pushing Image` and `Scheduling Space` on the **Build** tab\n\n<..."
          ],
          [
           "Using Sentence Transformers at Hugging Face\n\n`sentence-transformers` is a library that provides easy..."
          ],
          [
           "## Using existing models\n\nThe pre-trained models on the Hub can be loaded with a single line of code..."
          ],
          [
           "```\n\nHere is an example that encodes sentences and then computes the distance between them for doing..."
          ],
          [
           "```\n\nIf you want to see how to load a specific model, you can click `Use in sentence-transformers` a..."
          ],
          [
           "```\n\nThis command creates a repository with an automatically generated model card, an inference widg..."
          ],
          [
           "How to configure OIDC SSO with Okta\n\nIn this guide, we will use Okta as the SSO provider and with th..."
          ],
          [
           "### Step 2: Configure your application in Okta\n\nOpen a new tab/window in your browser and navigate t..."
          ],
          [
           "Save your new application.\n\n### Step 3: Finalize configuration on Hugging Face\n\nIn your Okta applica..."
          ],
          [
           "A green check mark near the OIDC selector will attest that the test was successful.\n\n\n<div class=\"fl..."
          ],
          [
           "Repositories\n\nModels, Spaces, and Datasets are hosted on the Hugging Face Hub as [Git repositories](..."
          ],
          [
           "Malware Scanning\n\nWe run every file of your repositories through a [malware scanner](https://www.cla..."
          ],
          [
           "If at least one file has a been scanned as unsafe, a message will warn the users:\n\n<div class=\"flex ..."
          ],
          [
           "Managing Spaces with CircleCI Workflows\n\nYou can keep your app in sync with your GitHub repository w..."
          ],
          [
           "```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```..."
          ],
          [
           "Docker Spaces\n\nSpaces accommodate custom [Docker containers](https://docs.docker.com/get-started/) f..."
          ],
          [
           "```\n\nInternally you could have as many open ports as you want. For instance, you can install Elastic..."
          ],
          [
           "```\n\n```Dockerfile\n# Expose the secret SECRET_EXAMPLE at buildtime and use its value as a Bearer tok..."
          ],
          [
           "```\n\n<Tip warning=\"{true}\">\nAlways specify the `--chown=user` with `ADD` and `COPY` to ensure the ne..."
          ],
          [
           "```\n(same goes for `ADD` command)\n\n\n## Data Persistence\n\nThe data written on disk is lost whenever y..."
          ],
          [
           "<Tip warning=\"{true}\">\nDuring Docker buildtime, you don't have access to a GPU hardware. Therefore, ..."
          ],
          [
           "Webhook guide: build a Discussion bot based on BLOOM\n\n<Tip>\n\nWebhooks are now publicly available!\n\n<..."
          ],
          [
           "## Create a Space that will react to your Webhook\n\nThe third step is actually to listen to the Webho..."
          ],
          [
           "```\n\nHere, we listen to POST requests made to `/`, and then we check that the `X-Webhook-Secret` hea..."
          ],
          [
           "```\n\nThis is the coolest part: we call the Inference API for the BLOOM model, prompting it with `PRO..."
          ],
          [
           "Sign in with Hugging Face\n\nYou can use the HF OAuth / OpenID connect flow to create a **\"Sign in wit..."
          ],
          [
           "</Tip>\n\n## Currently supported scopes\n\nThe currently supported scopes are:\n\n- `openid`: Get the ID t..."
          ],
          [
           "Check out [our badges](https://huggingface.co/datasets/huggingface/badges#sign-in-with-hugging-face)..."
          ],
          [
           "[![Sign in with Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/sign-i..."
          ],
          [
           "Using `Transformers.js` at Hugging Face\n\nTransformers.js is a JavaScript library for running ðŸ¤— Trans..."
          ],
          [
           "```\n\n</td>\n</tr>\n</table>\n\n\nYou can also use a different model by specifying the model id or path as..."
          ],
          [
           "Using SetFit with Hugging Face\n\nSetFit is an efficient and prompt-free framework for few-shot fine-t..."
          ],
          [
           "```\npip install -U setfit\n```\n\n## Using existing models\n\nAll `setfit` models can easily be loaded fr..."
          ],
          [
           "Spaces\n\n[Hugging Face Spaces](https://huggingface.co/spaces) offer a simple way to host ML demo apps..."
          ],
          [
           "You'll also be able to upgrade your Space to run [on a GPU or other accelerated hardware](./spaces-g..."
          ],
          [
           "Using Adapter Transformers at Hugging Face\n\n`adapter-transformers` is a library that extends ðŸ¤— `tran..."
          ],
          [
           "```\n\nYou can also use `list_adapters` to find all Adapter Models programmatically\n\n```py\nfrom transf..."
          ],
          [
           "```\n\nIf you want to see how to load a specific model, you can click `Use in Adapter Transformers` an..."
          ],
          [
           "```\n\nThis command creates a repository with an automatically generated model card and all necessary ..."
          ],
          [
           "Using PEFT at Hugging Face\n\nðŸ¤— [Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/p..."
          ],
          [
           "```\n\nOnce loaded, you can pass your inputs to the tokenizer to prepare them, and call `model.generat..."
          ],
          [
           "```\n\nIf you want to load a specific PEFT model, you can click `Use in PEFT` in the model card and yo..."
          ],
          [
           "Organization cards\n\nYou can create an organization card to help users learn more about what your org..."
          ],
          [
           "Libraries\n\nThe Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the `..."
          ],
          [
           "| Library                                                                     | Description         ..."
          ],
          [
           "| [Diffusers](https://github.com/huggingface/diffusers)                       | A modular toolbox fo..."
          ],
          [
           "| [NeMo](https://github.com/NVIDIA/NeMo)                                      | Conversational AI to..."
          ],
          [
           "| [Sentence Transformers](https://github.com/UKPLab/sentence-transformers)    | Compute dense vector..."
          ],
          [
           "| [Transformers](https://github.com/huggingface/transformers)                 | State-of-the-art Nat..."
          ],
          [
           "### How can I add a new library to the Inference API?\n\nIf you're interested in adding your library, ..."
          ],
          [
           "Gated models\n\nTo give more control over how models are used, the Hub allows model authors to enable ..."
          ],
          [
           "<div class=\"flex justify-center\">\n    <img class=\"block dark:hidden\" src=\"https://huggingface.co/dat..."
          ],
          [
           "#### From the UI\n\nYou can review who has access to your gated model from its settings page by clicki..."
          ],
          [
           "| Method | URI | Description | Headers | Payload\n| ------ | --- | ----------- | -------  | -------  ..."
          ],
          [
           "### Download access report\n\nYou can download a report of all access requests for a gated model with ..."
          ],
          [
           "If you want to collect more user information, you can configure additional fields. This information ..."
          ],
          [
           "```\n\n\nIn some cases, you might also want to modify the text in the gate heading and the text in the ..."
          ],
          [
           "```\n\n### Example use cases of programmatically managing access requests\n\nHere are a few interesting ..."
          ],
          [
           "Requesting access can only be done from your browser. Go to the model on the Hub and you will be pro..."
          ],
          [
           "```\n\nAlternatively, you can programmatically login using `login()` in a notebook or a script:\n\n```py..."
          ],
          [
           "Gated datasets\n\nTo give more control over how datasets are used, the Hub allows datasets authors to ..."
          ],
          [
           "<div class=\"flex justify-center\">\n    <img class=\"block dark:hidden\" src=\"https://huggingface.co/dat..."
          ],
          [
           "### From the UI\n\nYou can review who has access to your gated dataset from its settings page by click..."
          ],
          [
           "| Method | URI | Description | Headers | Payload\n| ------ | --- | ----------- | -------  | -------  ..."
          ],
          [
           "### Download access report\n\nYou can download a report of all access requests for a gated datasets wi..."
          ],
          [
           "If you want to collect more user information, you can configure additional fields. This information ..."
          ],
          [
           "```\n\n## Access gated datasets as a user\n\n\nAs a user, if you want to use a gated dataset, you will ne..."
          ],
          [
           "</Tip>\n\n### Download files\n\nTo download files from a gated dataset you'll need to be authenticated. ..."
          ],
          [
           "Custom Python Spaces\n\n<Tip>\n\nSpaces now support arbitrary Dockerfiles so you can host any Python app..."
          ],
          [
           "Paper Pages\n\nPaper pages allow people to find artifacts related to a paper such as models, datasets ..."
          ],
          [
           "## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based ..."
          ],
          [
           "### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URL..."
          ],
          [
           "DuckDB\n\n[DuckDB](https://github.com/duckdb/duckdb) is an in-process SQL [OLAP](https://en.wikipedia...."
          ],
          [
           "```\n\nThis creates a file `data.parquet` in the dataset repository `username/my_dataset` containing y..."
          ],
          [
           "More ways to create Spaces\n\n## Duplicating a Space\n\nYou can duplicate a Space by clicking the three ..."
          ],
          [
           "Shiny on Spaces\n\n[Shiny](https://shiny.posit.co/) is an open-source framework for building simple, b..."
          ],
          [
           "_app.py_\n\nThis file defines your app's logic. To learn more about how to modify this file, see [the ..."
          ],
          [
           "To deploy an R Shiny Space, click this button and fill out the space metadata. \nThis will populate t..."
          ],
          [
           "Using ML-Agents at Hugging Face\n\n`ml-agents` is an open-source toolkit that enables games and simula..."
          ],
          [
           "```\nmlagents-load-from-hf --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --local-dir=\"./downloads\"\n```..."
          ],
          [
           "--\n# Example metadata to be added to a model card.  \n# Full model card template at https://github.co..."
          ],
          [
           "# Optional. Add this if you want to encode your eval results in a structured way.\nmodel-index:\n- nam..."
          ],
          [
           "value: {metric_value}       # Required. Example: 20.90\n        name: {metric_name}         # Optiona..."
          ],
          [
           "This markdown file contains the spec for the modelcard metadata regarding evaluation parameters. Whe..."
          ],
          [
           "Using ðŸ¤— `transformers` at Hugging Face\n\nðŸ¤— `transformers` is a library maintained by Hugging Face and..."
          ],
          [
           "You can find models for many different tasks:\n\n* Extracting the answer from a context ([question-ans..."
          ],
          [
           "You can try out the models directly in the browser if you want to test them out without downloading ..."
          ],
          [
           "```\n\nYou can also load a model from a specific version (based on commit hash, tag name, or branch) a..."
          ],
          [
           "```\n\nThere is much more you can do, so we suggest to review the [Share a model](https://huggingface...."
          ],
          [
           "THE LANDSCAPE OF ML DOCUMENTATION TOOLS\nThe development of the model cards framework in 2018 was ins..."
          ],
          [
           "| **Stage of ML System Lifecycle** \t|  **Tool**                                                     ..."
          ],
          [
           "|:--------------------------------:\t|---------------------------------------------------------------..."
          ],
          [
           "-----------------\t|---------------------------------------------------------------------------------..."
          ],
          [
           "----------------------------------------\t|----------------------------------------------------------..."
          ],
          [
           "-----------------------------------\t|..."
          ],
          [
           "| DATA                             \t| ***Datasheets*** [(Gebru et al., 2018)](https://www.fatml.org/..."
          ],
          [
           "| DATA                             \t| ***Data Cards for NLP*** [(McMillan-Major et al., 2021)](https..."
          ],
          [
           "| DATA                             \t| ***CrowdWorkSheets***  [(DÃ­az et al., 2022)](https://huggingfa..."
          ],
          [
           "| MODELS AND METHODS               \t| ***Method Cards*** [Adkins et al. (2022)](https://dl.acm.org/d..."
          ],
          [
           "| SYSTEMS                          \t| ***System Cards***  [Procope et al. (2022)](https://ai.faceboo..."
          ],
          [
           "| SYSTEMS                          \t| ***ABOUT ML***  [Raji and Yang, (2019)](https://huggingface.co..."
          ],
          [
           "### DATA-FOCUSED DOCUMENTATION TOOLS\n\nSeveral proposed documentation tools focus on datasets used in..."
          ],
          [
           "* Extending the concept of datasheets in the electronics industry, [Gebru et al. (2018)](https://www..."
          ],
          [
           "* [Hutchinson et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445918) describe the need f..."
          ],
          [
           "### MODEL-AND-METHOD-FOCUSED DOCUMENTATION TOOLS\n\nAnother set of documentation tools can be thought ..."
          ],
          [
           "* They envision the relationship between model cards and method cards, in part, by stating: â€œThe sec..."
          ],
          [
           "Rather than focusing on particular models, datasets, or methods, system-focused documentation tools ..."
          ],
          [
           "## THE EVOLUTION OF MODEL CARDS\n\nSince the proposal for model cards by Mitchell et al. in 2018, mode..."
          ],
          [
           "From our analysis of all the models on the hub, we noticed that the most downloads come from top 200..."
          ],
          [
           "[^6]: See Appendix A.\n\n[^7]: See GSA / US Census Bureau Collaboration on Model Card Generator.\n\n[^8]..."
          ],
          [
           "Using Keras at Hugging Face\n\n`keras` is an open-source machine learning library that uses a consiste..."
          ],
          [
           "```\n\nIf you want to see how to load a specific model, you can click **Use in keras** and you will be..."
          ],
          [
           "```py\nfrom huggingface_hub import push_to_hub_keras\n\npush_to_hub_keras(model,\n    \"your-username/you..."
          ],
          [
           "```\nThe repository will host your TensorBoard traces like below.\n\n<div class=\"flex justify-center\">\n..."
          ],
          [
           "Widgets\n\n## What's a widget?\n\nMany model repos have a widget that allows anyone to run inferences di..."
          ],
          [
           "For some libraries, such as ðŸ¤—  `Transformers`, the model type should be inferred automatically based..."
          ],
          [
           "```\n\nYou can provide more than one example input. In the examples dropdown menu of the widget, they ..."
          ],
          [
           "```\n\nNote that you can also include example files in your model repository and use\nthem as:\n\n```yaml..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"450\" src=\"https://huggi..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"450\" src=\"https://huggi..."
          ],
          [
           "Here are some links to examples:\n\n- `text-classification`, for instance [`roberta-large-mnli`](https..."
          ],
          [
           "## How can I control my model's widget Inference API parameters?\n\nGenerally, the Inference API for a..."
          ],
          [
           "```\n\nOr if you'd like to change the temperature for a summarization task in the widget:\n\n```yaml\ninf..."
          ],
          [
           "No-license models challenge\n\n## Context\n\nThe Hugging Face Hub hosts hundreds of thousands of public ..."
          ],
          [
           "```\n\nOtherwise, the license is considered as `other`. In that case, we can set a custom name and a U..."
          ],
          [
           "```\n\nThis challenge aims to improve the completeness of this metadata on the Hub, which will ultimat..."
          ],
          [
           "For each model, the workflow looks like this:\n1. Choose a model in the list below. We suggest focusi..."
          ],
          [
           "6. Once done, open a PR on GitHub to update the table below. Once merged, this will count as a Hackt..."
          ],
          [
           "## F.A.Q.\n\n### What if the model has 2 licenses?\n\nThis use case can happen when a model is finetuned..."
          ],
          [
           "|status|pr_url|model_id                                                                             ..."
          ],
          [
           "|------|------|-------------------------------------------------------------------------------------..."
          ],
          [
           "----------------------------------------------------------------------|------------|--------|-------..."
          ],
          [
           "|-------------------------|-------------------------------------------------------------------------..."
          ],
          [
           "-----------------------------------------------|----------------------------------------------------..."
          ],
          [
           "--------------------------------------------------------|-------------|..."
          ],
          [
           "|opened|[here](https://huggingface.co/NousResearch/Llama-2-13b-hf/discussions/5)|[NousResearch/Llama..."
          ],
          [
           "|opened|[here](https://huggingface.co/NousResearch/Llama-2-7b-hf/discussions/4)|[NousResearch/Llama-..."
          ],
          [
           "|      |      |[meta-llama/Llama-2-13b-chat-hf](https://huggingface.co/meta-llama/Llama-2-13b-chat-h..."
          ],
          [
           "|      |      |[meta-llama/Llama-2-70b-chat-hf](https://huggingface.co/meta-llama/Llama-2-70b-chat-h..."
          ],
          [
           "|      |      |[vinai/phobert-base-v2](https://huggingface.co/vinai/phobert-base-v2)                ..."
          ],
          [
           "|      |      |[symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli](https://huggingface.co/symanto/sn-x..."
          ],
          [
           "|      |      |[aipicasso/picasso-diffusion-1-1](https://huggingface.co/aipicasso/picasso-diffusion-..."
          ],
          [
           "|      |      |[huggyllama/llama-13b](https://huggingface.co/huggyllama/llama-13b)                  ..."
          ],
          [
           "|      |      |[decapoda-research/llama-30b-hf](https://huggingface.co/decapoda-research/llama-30b-h..."
          ],
          [
           "|      |      |[TheBloke/Llama-2-7B-Chat-GGML](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)..."
          ],
          [
           "|      |      |[jphme/Llama-2-13b-chat-german](https://huggingface.co/jphme/Llama-2-13b-chat-german)..."
          ],
          [
           "|      |      |[TheBloke/orca_mini_v3_7B-GPTQ](https://huggingface.co/TheBloke/orca_mini_v3_7B-GPTQ)..."
          ],
          [
           "|      |      |[camembert/camembert-large](https://huggingface.co/camembert/camembert-large)        ..."
          ],
          [
           "|      |      |[stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese..."
          ],
          [
           "|      |      |[TheBloke/VicUnlocked-alpaca-65B-QLoRA-fp16](https://huggingface.co/TheBloke/VicUnloc..."
          ],
          [
           "|      |      |[TheBloke/tulu-7B-fp16](https://huggingface.co/TheBloke/tulu-7B-fp16)                ..."
          ],
          [
           "|      |      |[shibing624/chinese-llama-plus-13b-hf](https://huggingface.co/shibing624/chinese-llam..."
          ],
          [
           "|      |      |[kuelumbus/polyBERT](https://huggingface.co/kuelumbus/polyBERT)                      ..."
          ],
          [
           "|      |      |[decapoda-research/llama-65b-hf](https://huggingface.co/decapoda-research/llama-65b-h..."
          ],
          [
           "|      |      |[uklfr/gottbert-base](https://huggingface.co/uklfr/gottbert-base)                    ..."
          ],
          [
           "|      |      |[TheBloke/airoboros-l2-13B-gpt4-1.4.1-GPTQ](https://huggingface.co/TheBloke/airoboros..."
          ],
          [
           "|      |      |[THUDM/chatglm-6b-int4-qe](https://huggingface.co/THUDM/chatglm-6b-int4-qe)          ..."
          ],
          [
           "|      |      |[liuhaotian/llava-llama-2-7b-chat-lightning-lora-preview](https://huggingface.co/liuh..."
          ],
          [
           "|      |      |[aipicasso/manga-diffusion-poc](https://huggingface.co/aipicasso/manga-diffusion-poc)..."
          ],
          [
           "|      |      |[4bit/Llama-2-70b-chat-hf](https://huggingface.co/4bit/Llama-2-70b-chat-hf)          ..."
          ],
          [
           "|      |      |[aipicasso/cool-japan-diffusion-2-1-1-beta](https://huggingface.co/aipicasso/cool-jap..."
          ],
          [
           "|      |      |[allenai/open-instruct-stanford-alpaca-7b](https://huggingface.co/allenai/open-instru..."
          ],
          [
           "|      |      |[THUDM/WebGLM-2B](https://huggingface.co/THUDM/WebGLM-2B)                            ..."
          ],
          [
           "|      |      |[TheBloke/Llama-2-70B-Chat-GGML](https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGM..."
          ],
          [
           "|      |      |[TheBloke/Airoboros-L2-70B-GPT4-m2.0-GPTQ](https://huggingface.co/TheBloke/Airoboros-..."
          ],
          [
           "|      |      |[TheBloke/OpenChat_v3.2-GPTQ](https://huggingface.co/TheBloke/OpenChat_v3.2-GPTQ)    ..."
          ],
          [
           "|      |      |[valurank/distilroberta-bias](https://huggingface.co/valurank/distilroberta-bias)    ..."
          ],
          [
           "|      |      |[TheBloke/Kuchiki-L2-7B-GPTQ](https://huggingface.co/TheBloke/Kuchiki-L2-7B-GPTQ)    ..."
          ],
          [
           "|      |      |[sschet/bert-base-uncased_clinical-ner](https://huggingface.co/sschet/bert-base-uncas..."
          ],
          [
           "|      |      |[allenai/tulu-7b](https://huggingface.co/allenai/tulu-7b)                            ..."
          ],
          [
           "|      |      |[TheBloke/airoboros-l2-70B-gpt4-1.4.1-GPTQ](https://huggingface.co/TheBloke/airoboros..."
          ],
          [
           "|      |      |[TheBloke/airoboros-l2-70B-GPT4-2.0-GPTQ](https://huggingface.co/TheBloke/airoboros-l..."
          ],
          [
           "|      |      |[valurank/en_readability](https://huggingface.co/valurank/en_readability)            ..."
          ],
          [
           "|      |      |[Neko-Institute-of-Science/LLaMA-65B-HF](https://huggingface.co/Neko-Institute-of-Sci..."
          ],
          [
           "|      |      |[TheBloke/LLaMA-13b-GPTQ](https://huggingface.co/TheBloke/LLaMA-13b-GPTQ)            ..."
          ],
          [
           "|      |      |[TheBloke/HermesLimaRP-L2-7B-GGUF](https://huggingface.co/TheBloke/HermesLimaRP-L2-7B..."
          ],
          [
           "|      |      |[valurank/final_headline_generator](https://huggingface.co/valurank/final_headline_ge..."
          ],
          [
           "|      |      |[TheBloke/llama2_7b_chat_uncensored-GGUF](https://huggingface.co/TheBloke/llama2_7b_c..."
          ],
          [
           "|      |      |[bigcode/octogeex](https://huggingface.co/bigcode/octogeex)                          ..."
          ],
          [
           "|      |      |[TheBloke/orca_mini_v3_7B-GGUF](https://huggingface.co/TheBloke/orca_mini_v3_7B-GGUF)..."
          ],
          [
           "|      |      |[TheBloke/CodeFuse-CodeLlama-34B-GPTQ](https://huggingface.co/TheBloke/CodeFuse-CodeL..."
          ],
          [
           "|      |      |[TheBloke/Zarafusionex-1.1-L2-7B-GGUF](https://huggingface.co/TheBloke/Zarafusionex-1..."
          ],
          [
           "|      |      |[michaelfeil/ct2fast-Llama-2-7b-hf](https://huggingface.co/michaelfeil/ct2fast-Llama-..."
          ],
          [
           "|      |      |[valurank/distilroberta-mbfc-bias](https://huggingface.co/valurank/distilroberta-mbfc..."
          ],
          [
           "|      |      |[TheBloke/airoboros-l2-13b-gpt4-m2.0-GGUF](https://huggingface.co/TheBloke/airoboros-..."
          ],
          [
           "|      |      |[TheBloke/CodeFuse-CodeLlama-34B-GGUF](https://huggingface.co/TheBloke/CodeFuse-CodeL..."
          ],
          [
           "|      |      |[RicardoLee/Llama2-chat-Chinese-50W](https://huggingface.co/RicardoLee/Llama2-chat-Ch..."
          ],
          [
           "|      |      |[valurank/en_pos_counter](https://huggingface.co/valurank/en_pos_counter)            ..."
          ],
          [
           "|      |      |[turboderp/Llama2-70B-exl2](https://huggingface.co/turboderp/Llama2-70B-exl2)        ..."
          ],
          [
           "|      |      |[anonymous4chan/llama-2-70b](https://huggingface.co/anonymous4chan/llama-2-70b)      ..."
          ],
          [
           "|      |      |[localmodels/Llama-2-13B-GPTQ](https://huggingface.co/localmodels/Llama-2-13B-GPTQ)  ..."
          ],
          [
           "|      |      |[4bit/llama-13b-4bit-hf](https://huggingface.co/4bit/llama-13b-4bit-hf)              ..."
          ],
          [
           "|      |      |[TheBloke/Llama2-13B-MegaCode2-OASST-GGUF](https://huggingface.co/TheBloke/Llama2-13B..."
          ],
          [
           "|      |      |[gsaivinay/Llama-2-7b-Chat-GPTQ](https://huggingface.co/gsaivinay/Llama-2-7b-Chat-GPT..."
          ],
          [
           "|      |      |[valurank/xsum_headline_generator](https://huggingface.co/valurank/xsum_headline_gene..."
          ],
          [
           "|      |      |[TheBloke/Kimiko-7B-fp16](https://huggingface.co/TheBloke/Kimiko-7B-fp16)            ..."
          ],
          [
           "|      |      |[TheBloke/OpenChat_v3.2-GGML](https://huggingface.co/TheBloke/OpenChat_v3.2-GGML)    ..."
          ],
          [
           "|      |      |[nenkoru/alpaca-lora-7b-onnx-fp16-with-past](https://huggingface.co/nenkoru/alpaca-lo..."
          ],
          [
           "|      |      |[TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGUF](https://huggingface.co/TheBloke/airoboros..."
          ],
          [
           "|      |      |[TheBloke/llama2-22B-daydreamer-v2-GGUF](https://huggingface.co/TheBloke/llama2-22B-d..."
          ],
          [
           "|      |      |[TheBloke/Tulu-30B-SuperHOT-8K-GPTQ](https://huggingface.co/TheBloke/Tulu-30B-SuperHO..."
          ],
          [
           "|      |      |[AllanFrostin/analise-morfossintatica-ptbr](https://huggingface.co/AllanFrostin/anali..."
          ],
          [
           "|      |      |[TheBloke/Airoboros-L2-70B-GPT4-m2.0-GGUF](https://huggingface.co/TheBloke/Airoboros-..."
          ],
          [
           "|      |      |[usamakenway/llama2_7b_chat_uncensored-AutoGPTQ_Wizard_Vicuna](https://huggingface.co..."
          ],
          [
           "Manual Configuration\n\nThis guide will show you how to configure a custom structure for your dataset ..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nNote that `config_name` field is required even if you have a single confi..."
          ],
          [
           "Static HTML Spaces\n\nSpaces also accommodate custom HTML for your app instead of using Streamlit or G..."
          ],
          [
           "Tasks\n\n## What's a task?\n\nTasks, or pipeline types, describe the \"shape\" of each model's API (inputs..."
          ],
          [
           "* ðŸ¤— using a `transformers` model\n* ðŸ³ using a model from an [officially supported library](./models-l..."
          ],
          [
           "**Adding a new task is relatively straightforward and requires 2 PRs:**\n* PR 1: Add the new task to ..."
          ],
          [
           "* PR 2: Add the new task to a library docker image. You should also add a template to [`docker_image..."
          ],
          [
           "### Adding Community Inference API for a quick prototype\n\n**My model is not supported by any library..."
          ],
          [
           "### Widget\n\nOnce the task is in production, what could be more exciting than implementing some way f..."
          ],
          [
           "Collections\n\nUse Collections to group repositories from the Hub (Models, Datasets, Spaces and Papers..."
          ],
          [
           "![Add items to collections](https://huggingface.co/datasets/huggingface/documentation-images/resolve..."
          ],
          [
           "### Deleting items from a collection\n\nTo delete an item from a collection, click the trash icon in t..."
          ],
          [
           "![Collection image drop zone with images](https://huggingface.co/datasets/huggingface/documentation-..."
          ],
          [
           "Run training on Amazon SageMaker\n\n<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embe..."
          ],
          [
           "To start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/s..."
          ],
          [
           "```\n\n## Prepare a ðŸ¤— Transformers fine-tuning script\n\nOur training script is very similar to a traini..."
          ],
          [
           "# data, model, and output directories\n    parser.add_argument(\"--model-dir\", type=str, default=os.en..."
          ],
          [
           "```\n\n_Note that SageMaker doesnâ€™t support argparse actions. For example, if you want to use a boolea..."
          ],
          [
           "In addition to the options already mentioned above, there is another option to save the training art..."
          ],
          [
           "1. `entry_point` specifies which fine-tuning script to use.\n2. `instance_type` specifies an Amazon i..."
          ],
          [
           "```\n\nIf you are running a `TrainingJob` locally, define `instance_type='local'` or `instance_type='l..."
          ],
          [
           "```\n\n## Access trained model\n\nOnce training is complete, you can access your model through the [AWS ..."
          ],
          [
           "```\n\nðŸ““ Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/ma..."
          ],
          [
           "```\n\nðŸ““ Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/ma..."
          ],
          [
           "# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        ..."
          ],
          [
           "```\n\nðŸ““ Open the [sagemaker-notebook.ipynb notebook](https://github.com/huggingface/notebooks/blob/ma..."
          ],
          [
           "```\n\n## SageMaker metrics\n\n[SageMaker metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/train..."
          ],
          [
           "Managing organizations\n\n## Creating an organization\n\nVisit the [New Organization](https://hf.co/orga..."
          ],
          [
           "Pickle Scanning\n\nPickle is a widely used serialization format in ML. Most notably, it is the default..."
          ],
          [
           "```\n\nWhen you run this, it will create a pickle file and print the following instructions in your te..."
          ],
          [
           "```\n\nWhen we run this script we get the `payload.pkl` again. When we check the fileâ€™s contents:\n\n```..."
          ],
          [
           "```\n\nHere weâ€™re using the [fickling](https://github.com/trailofbits/fickling) library for simplicity..."
          ],
          [
           "```\n\nBasically, this is whatâ€™s happening when you unpickle:\n\n```python\n# ...\nopcodes_stack = [exec_f..."
          ],
          [
           "```\n\nThe instructions that pose a threat are `STACK_GLOBAL`, `GLOBAL` and `REDUCE`.\n\n`REDUCE` is wha..."
          ],
          [
           "```\n\n### Use your own serialization format\n\n- [MsgPack](https://msgpack.org/index.html)\n- [Protobuf]..."
          ],
          [
           "On the hub the list of imports will be displayed next to each file containing imports. If any import..."
          ],
          [
           "Thankfully, there is always a trace of the `eval` import, so reading the opcodes directly should all..."
          ],
          [
           "[CTFtime.org / Balsn CTF 2019 / pyshv1 / Writeup](https://ctftime.org/writeup/16723)\n\n[Rehabilitatin..."
          ],
          [
           "Moderation\n\n<Tip>\n\nCheck out the [Code of Conduct](https://huggingface.co/code-of-conduct) and the [..."
          ],
          [
           "Livebook on Spaces\n\n**Livebook** is an open-source tool for writing interactive code notebooks in [E..."
          ],
          [
           "Then:\n\n1. Give your Space a name\n2. Set the password of your Livebook\n3. Set its visibility to publi..."
          ],
          [
           "## Livebook integration with Hugging Face Models\n\nLivebook has an [official integration with Hugging..."
          ]
         ],
         "hovertemplate": "source=hub-docs<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "hub-docs, circle",
         "marker": {
          "color": "#19d3f3",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "hub-docs, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          4.8974996,
          4.914907,
          -5.58312,
          5.373185,
          -3.7038214,
          6.7158117,
          6.73102,
          -7.2024565,
          -7.4866776,
          6.743983,
          0.67129475,
          2.879665,
          6.455258,
          6.719456,
          6.2295403,
          5.731463,
          5.2020054,
          5.1392345,
          4.5515428,
          5.347099,
          7.490959,
          7.3448606,
          7.066546,
          6.5544424,
          7.0293617,
          6.1374106,
          6.6568837,
          5.9152746,
          5.2363133,
          5.6313677,
          3.6744556,
          4.0176883,
          2.034365,
          2.2950785,
          6.792788,
          4.646649,
          4.9359474,
          4.8224406,
          4.6273756,
          3.6322093,
          3.3195379,
          5.5167828,
          5.401254,
          5.651504,
          7.390609,
          7.76835,
          4.6946354,
          2.0355318,
          7.2854443,
          8.06331,
          8.21895,
          6.256595,
          4.9312572,
          4.877658,
          4.671058,
          4.614443,
          7.3933315,
          6.6809154,
          -5.874521,
          3.2324958,
          3.9918306,
          3.7884426,
          4.217571,
          3.4889135,
          5.7454166,
          8.192819,
          7.0022197,
          -1.7973696,
          6.0221763,
          7.124018,
          6.6452556,
          7.032474,
          6.7416267,
          3.9129767,
          6.450339,
          4.219007,
          5.4537416,
          4.295473,
          9.904964,
          2.1853807,
          2.792199,
          -3.288666,
          -3.233652,
          -2.181111,
          -1.9241115,
          -3.4142241,
          -1.8075986,
          -2.0342078,
          -2.6581888,
          -5.572548,
          -3.3008423,
          -2.7932534,
          -2.9744365,
          4.3268404,
          -7.2628217,
          -4.671667,
          -5.0924883,
          5.136451,
          3.4829564,
          3.3355453,
          -5.830975,
          -5.7242026,
          -6.045191,
          4.4979253,
          -5.737179,
          -6.0423627,
          5.8612003,
          11.135735,
          11.126625,
          -6.2699833,
          11.119209,
          11.11854,
          11.180725,
          11.119199,
          11.188389,
          11.119585,
          11.109863,
          11.114672,
          11.101869,
          11.143628,
          11.109811,
          11.109244,
          11.112418,
          11.116286,
          11.148856,
          11.112927,
          11.114344,
          11.110508,
          11.116684,
          11.114819,
          11.112191,
          11.111067,
          11.11282,
          11.1640215,
          11.165397,
          11.145456,
          11.198273,
          11.230278,
          11.234439,
          11.200463,
          11.238096,
          11.251895,
          11.19587,
          11.170338,
          11.125923,
          11.12156,
          11.090537,
          11.107532,
          11.118329,
          11.120992,
          11.113656,
          11.101512,
          11.099409,
          11.122309,
          11.1156225,
          11.113722,
          11.102579,
          11.103817,
          11.1159935,
          11.120188,
          11.115564,
          11.109131,
          11.102131,
          11.103106,
          11.109111,
          11.110642,
          11.102066,
          11.110135,
          11.107696,
          11.115363,
          4.1119795,
          4.819677,
          4.4383845,
          5.6493526,
          4.671601,
          3.788541,
          3.6801538,
          3.350441,
          3.1871161,
          4.0695486,
          5.443634,
          5.8460956,
          5.8509736,
          5.7459044,
          5.4186916,
          7.299096,
          6.6562896,
          6.2336316,
          6.125235,
          6.11908,
          5.8626695,
          5.8800435,
          4.994738,
          5.8543415,
          5.2501307,
          5.525013,
          6.0100927,
          5.9776554,
          3.371922,
          3.524081,
          2.9789975,
          3.6810386,
          4.8443747,
          4.7719574,
          4.757301,
          7.9238734,
          15.01364,
          15.363149,
          16.004301,
          6.4163303,
          7.0760307,
          7.2154546,
          7.0247245,
          12.771632,
          6.883764,
          4.3221726,
          7.922582,
          7.871235,
          5.864049,
          7.895335,
          7.94212,
          6.7166367,
          7.4654775,
          7.919008,
          7.938581,
          14.319759,
          3.637344,
          7.5208697,
          7.9995103,
          7.8589916,
          14.327514,
          8.627933,
          7.354045,
          7.8390403,
          7.838416,
          6.3184385,
          6.1749325,
          4.2644873,
          3.7819893,
          4.333232,
          5.107953,
          -7.4002476,
          -7.0064173,
          3.05896,
          3.7741842,
          3.433321,
          3.098234,
          -5.7811437,
          -3.2459347,
          -3.1522424,
          2.6722033,
          3.0811813,
          5.049892,
          3.2072258,
          3.441115,
          3.3519294,
          3.328333,
          -8.040279,
          3.3325016,
          -4.651831,
          3.4048774,
          5.835284,
          5.7194514,
          5.556502,
          5.797166,
          5.8651023,
          5.8615727,
          5.8172283,
          11.491596,
          9.876329,
          11.343851,
          11.278868,
          7.5818605,
          7.470635,
          6.7386637,
          6.2468667,
          4.515844,
          3.6791885,
          3.177246,
          0.8068756,
          -1.9458765,
          -7.692909,
          3.246204,
          3.2245395,
          2.7765937,
          2.6104927,
          4.1663504,
          4.099613,
          4.7589536,
          4.322534,
          3.327405,
          4.3699994,
          4.3267484,
          3.8723898,
          4.5109286,
          7.7034636,
          7.2942286,
          4.6030464,
          7.270542,
          6.8728304,
          6.6166024,
          6.959437,
          6.8506665,
          6.8478346,
          12.727368,
          10.254167,
          10.194732,
          10.68914,
          10.708596,
          10.679777,
          10.70852,
          9.817176,
          12.712678,
          12.711904,
          12.713991,
          12.354032,
          12.718063,
          12.7321825,
          12.713771,
          12.70807,
          12.711048,
          12.729819,
          12.747381,
          12.719897,
          12.722265,
          12.484867,
          12.342924,
          12.365048,
          12.323351,
          12.752661,
          12.713179,
          12.347095,
          12.739403,
          12.71798,
          12.71562,
          12.720241,
          12.710918,
          12.242121,
          12.724811,
          12.709826,
          12.730883,
          12.717737,
          12.709615,
          12.717255,
          12.695021,
          12.375244,
          12.713634,
          12.718711,
          12.725269,
          12.720257,
          12.711677,
          12.725814,
          12.724958,
          12.718122,
          12.732268,
          12.714933,
          12.72156,
          12.697292,
          12.725572,
          12.7139845,
          12.723036,
          12.717875,
          12.792846,
          12.723019,
          12.745604,
          12.719163,
          12.72487,
          12.715336,
          12.713037,
          12.698777,
          12.767735,
          12.711341,
          12.715829,
          12.72253,
          12.663621,
          12.773571,
          12.724287,
          12.730439,
          12.724261,
          12.698682,
          12.716697,
          12.719358,
          12.729092,
          12.735478,
          12.732879,
          12.735201,
          12.727919,
          12.72543,
          12.717365,
          12.717979,
          12.729557,
          12.721015,
          12.71862,
          12.701094,
          12.721794,
          12.70089,
          12.717526,
          12.724884,
          12.720375,
          12.722942,
          12.72171,
          12.716392,
          12.691603,
          12.766184,
          12.757238,
          12.721739,
          12.722652,
          12.719032,
          12.727558,
          12.723399,
          12.721895,
          12.718359,
          12.700157,
          12.704506,
          -4.8708925,
          7.40583,
          3.5359342,
          3.7897108,
          3.8629575,
          3.3720596,
          3.5509472,
          6.2300878,
          6.908858,
          -1.7694149,
          4.285106,
          4.3113313,
          3.705625,
          4.130097,
          3.6671898,
          3.4767213,
          3.1452694,
          5.377066,
          5.460858,
          5.7228174,
          5.6733418,
          5.9112463,
          5.4504275,
          4.8361855,
          2.179685,
          2.7030253,
          -7.4125667,
          2.9395702,
          3.5760093,
          3.56051,
          3.429342,
          3.949475,
          3.0418472,
          -6.4775753,
          0.015056479,
          -0.66700596,
          3.6951976,
          3.0464263,
          -5.9071555,
          7.529254,
          7.369062,
          6.4593267,
          3.461535,
          3.4241712,
          3.4021754,
          3.418408,
          3.7281268,
          7.7279387,
          3.441508,
          7.450168,
          -2.59349,
          0.8586548,
          0.39543056,
          0.039918363,
          7.2068768,
          7.112162,
          7.825654,
          5.953095,
          -0.52007085,
          9.86088,
          3.3464844,
          4.364692,
          -5.3494124,
          3.649377,
          3.775168,
          3.2210555,
          3.465347,
          -3.9604325,
          6.0787187,
          5.8848405,
          5.885404,
          5.767111,
          3.0993302,
          -3.8786178,
          6.0295324,
          2.8562188,
          4.2854304,
          4.4674854,
          3.8988597,
          3.9296222,
          4.8707166,
          4.9751496,
          4.489233,
          4.7476754,
          5.448629,
          8.1138115,
          9.823471,
          7.3556156,
          7.0458336,
          6.270522,
          2.942814,
          5.855718,
          3.7716377,
          3.5386648,
          2.8228426,
          1.1511427,
          4.9383774,
          4.082485,
          0.4013112,
          -7.7755995,
          3.965724,
          11.571465,
          3.800812,
          4.548884,
          3.121043,
          2.2737808,
          4.327007,
          4.7939677,
          4.739471,
          5.8937955,
          3.3583143,
          3.353889,
          4.148914,
          7.0240836,
          2.1070173,
          4.492925,
          4.7293196,
          5.2791204,
          5.926432,
          5.7961206,
          5.9083395,
          5.7226014,
          0.027146421,
          1.1413414,
          1.2187924,
          5.4968386,
          -5.4235353,
          4.1479726,
          5.3989234,
          6.5786138,
          5.8930902,
          3.6537938,
          -3.9151115,
          -4.1718163,
          -3.9828262,
          -3.800919,
          -3.4811165,
          -2.3760438,
          -3.051022,
          5.004892,
          4.490157,
          4.4289737,
          3.8850749,
          4.323369,
          4.992034,
          5.2353225,
          4.5384536,
          4.6383758,
          4.760501,
          4.8018713,
          2.2969494,
          1.6922022,
          -3.2441964,
          0.2503619,
          0.33878475,
          2.571115,
          -0.91875976,
          -1.0249999,
          3.835903,
          3.722235,
          5.079045,
          2.2712343,
          3.7640662,
          2.7566261,
          2.6627545,
          4.9921317,
          3.3462863,
          1.8895097,
          5.471735,
          1.8478394,
          1.7060411,
          0.090640284,
          1.7463497,
          1.7595344,
          2.5496242,
          1.3633033,
          3.0093863,
          2.538048,
          2.5281537,
          2.4903047,
          -1.5936353,
          -1.6824806,
          0.86886823,
          -1.8010827,
          -0.8991467,
          -0.30089042,
          2.8580189,
          -0.656121,
          -1.5426893,
          2.933246,
          -2.4017992,
          0.98340666,
          -2.0401812,
          -2.399966,
          -0.1052738,
          -1.1133205,
          -1.3151649,
          5.610202,
          5.500802,
          5.322414,
          8.160951,
          7.4527116,
          7.0303087,
          7.6366253,
          4.4411287,
          8.43002,
          7.4391427,
          3.0169601,
          6.990502,
          5.7968464,
          7.7547727,
          8.297199,
          6.183098,
          7.3733015,
          7.2068214,
          -5.5462494,
          -2.2980027,
          -4.792981,
          2.596369,
          -3.9990122,
          5.8614035,
          5.8084626,
          5.8822203,
          5.735553,
          4.9450755,
          5.6461453,
          5.7153115,
          5.309039,
          4.7765737,
          8.017994,
          6.005935,
          5.100235,
          4.8540096,
          6.569267,
          1.2263179,
          5.951611,
          6.2313166,
          5.943726,
          5.8326507,
          5.8394537,
          5.7320685,
          5.8283353,
          5.8868027,
          -5.5433064,
          -0.96565694,
          -5.351411,
          1.229384,
          7.6803083,
          7.361002,
          -3.7604775,
          -1.5352898,
          2.7058644,
          3.3262877,
          -0.47064754,
          -1.9395759,
          2.7063794,
          4.838992,
          4.3174925,
          -5.875225,
          -0.5024097,
          -6.0618563,
          -6.427153,
          -5.190362,
          4.3155985,
          4.718298,
          5.206755,
          5.2545094,
          5.4508805,
          5.0478287,
          4.65646,
          6.303113,
          5.2094603,
          4.2487993,
          4.3090568,
          4.748476,
          5.0806513,
          5.0057287,
          4.898806,
          4.8289537,
          4.4292307,
          4.803827,
          3.9081128,
          8.604794,
          4.760453,
          5.611719,
          5.4231014,
          3.643951,
          3.4720976,
          8.907522,
          7.52906,
          6.7576356,
          7.4278097,
          -7.5792875,
          3.0616834,
          3.4734051,
          -0.33348653,
          0.17843361,
          6.7871985,
          -5.051546,
          -5.4055786,
          2.6481419,
          2.3843174,
          -3.646123,
          3.3929877,
          2.5074,
          9.804881,
          10.721631,
          10.707952,
          9.804288,
          3.0661108,
          3.3270178,
          3.2293503,
          3.2948835,
          -6.9968576,
          -7.329671,
          2.9952767,
          3.3331175,
          3.2062871,
          3.2974117,
          3.3199713,
          3.3133044,
          3.429283,
          3.4773502,
          4.2028112,
          2.29455,
          2.6388807,
          2.6824927,
          2.038676,
          -4.6777477,
          -0.10261448,
          -3.4021115,
          2.574824,
          4.050124,
          4.0743012,
          -5.2362294,
          4.6034465,
          4.662415,
          6.8769336,
          6.752685,
          6.8053493,
          6.92861,
          4.96319,
          6.890747,
          6.938618,
          10.257639,
          10.711546,
          10.1595545,
          10.711909,
          9.894143,
          12.684273,
          12.714933,
          12.708178,
          12.715961,
          12.706446,
          12.720117,
          12.361425,
          12.722122,
          12.719608,
          12.71469,
          12.712014,
          12.720595,
          12.730189,
          12.725421,
          12.719145,
          12.726079,
          12.711655,
          12.749266,
          12.72262,
          12.720261,
          12.7320175,
          12.383634,
          12.593292,
          12.357006,
          12.308814,
          12.350116,
          12.7210045,
          12.348648,
          12.341866,
          12.753351,
          12.71196,
          12.716393,
          12.7159815,
          12.716675,
          12.721388,
          12.70924,
          12.735031,
          12.716238,
          12.716321,
          12.71382,
          12.719933,
          12.721031,
          12.689354,
          12.729427,
          12.712813,
          12.7142105,
          12.69166,
          12.719355,
          12.7279825,
          12.789058,
          12.715833,
          12.693345,
          12.715889,
          12.722925,
          12.728383,
          12.688058,
          12.7250185,
          12.720651,
          12.743462,
          12.707564,
          12.730535,
          12.713042,
          12.725187,
          12.765446,
          12.726222,
          12.724607,
          12.687421,
          12.788607,
          12.717144,
          3.09798,
          2.8398912,
          8.184655,
          4.4810266,
          4.0350094,
          4.498993,
          4.183089,
          4.2774806,
          4.877671,
          4.54399,
          4.692341,
          4.9823923,
          5.828914,
          -1.1726965,
          0.4413271,
          -0.5487383,
          1.1440245,
          -0.3882552,
          -1.0576257,
          -0.502415,
          -0.5121659,
          -1.2949415,
          -1.6092726,
          -1.1678258,
          -0.95427066,
          -1.2898138,
          0.35409135,
          5.224987,
          1.6438853,
          1.7117476,
          1.8907375,
          1.8979051,
          1.4811597,
          1.5191988,
          1.6107595,
          3.2040105,
          1.780133,
          1.2581693,
          5.754679,
          7.3777146,
          7.144617,
          -4.9008827
         ],
         "xaxis": "x",
         "y": [
          -0.57975066,
          -0.6708585,
          -1.1480384,
          -2.4094174,
          -3.0048544,
          -4.3846107,
          -3.9079654,
          -5.5241404,
          -5.3410673,
          -4.082205,
          -4.6144686,
          -4.1032248,
          -1.730521,
          -1.9499273,
          -1.3119594,
          -0.5535804,
          -0.74058044,
          -0.7241845,
          -0.47712207,
          -2.2416186,
          -2.4137115,
          -2.3388638,
          -2.2515924,
          -2.1869192,
          -2.2770457,
          -2.1006434,
          -1.9410647,
          -2.5138133,
          -0.98563814,
          -1.8001608,
          -2.9383655,
          -3.3371825,
          -4.3812513,
          -4.087911,
          -0.159725,
          -1.379596,
          -1.0176163,
          -1.3542236,
          -1.1693183,
          -3.4862523,
          -3.27291,
          -1.0484111,
          0.15856595,
          0.32661945,
          -2.379171,
          -2.5344694,
          0.13231082,
          0.7675103,
          -2.3323934,
          -3.1825135,
          -3.4363103,
          -3.29844,
          -0.7816142,
          -0.6866569,
          -0.607891,
          -0.59602726,
          -2.3023174,
          -2.7932227,
          -0.4636839,
          -0.60825765,
          -0.25874642,
          -1.483948,
          -0.15237708,
          -0.24434833,
          -1.4383821,
          -2.6296954,
          -2.0243828,
          3.1953444,
          -3.1707172,
          -2.1243622,
          -1.9618001,
          -2.0040164,
          -1.8896154,
          -1.6699376,
          -2.7526538,
          -1.6880132,
          -2.2805922,
          -0.6583806,
          -3.2524428,
          -0.11218239,
          -0.5125304,
          1.3617612,
          0.71996063,
          1.2379655,
          1.2958723,
          1.8106575,
          1.9667932,
          1.5699844,
          1.1582202,
          -0.061256874,
          1.1343601,
          1.2508429,
          1.1437608,
          0.9491145,
          -3.8347447,
          -2.4316394,
          1.5781891,
          -0.71693575,
          -3.921566,
          -3.340294,
          -5.082943,
          -5.0031643,
          -5.286194,
          -2.8149538,
          -4.9790473,
          -5.42318,
          -1.8387545,
          18.061596,
          18.08156,
          -0.27000377,
          18.090408,
          18.094587,
          18.006424,
          18.093483,
          17.991982,
          18.090904,
          18.105803,
          18.097576,
          18.119385,
          18.059692,
          18.103214,
          18.10507,
          18.10145,
          18.097715,
          18.040592,
          18.1029,
          18.096714,
          18.106916,
          18.09441,
          18.0963,
          18.102987,
          18.103964,
          18.101067,
          18.030909,
          18.032833,
          18.061716,
          17.988197,
          17.945204,
          17.938646,
          17.98702,
          17.934229,
          17.913008,
          17.991343,
          18.025652,
          18.08303,
          18.089867,
          18.134544,
          18.112644,
          18.096668,
          18.09401,
          18.10232,
          18.117266,
          18.11878,
          18.090685,
          18.101004,
          18.100534,
          18.115118,
          18.113894,
          18.09891,
          18.093393,
          18.098948,
          18.114555,
          18.120405,
          18.123028,
          18.11156,
          18.113173,
          18.123177,
          18.113922,
          18.111921,
          18.090794,
          -0.5785522,
          -1.3826511,
          -1.556819,
          -1.2649493,
          -1.1862358,
          -2.411486,
          -2.4501276,
          -2.0553162,
          -3.064242,
          -3.0002694,
          -2.4261694,
          -2.063641,
          -2.2824056,
          -2.1722875,
          -2.8169632,
          -2.399105,
          -2.358418,
          -2.4855385,
          -2.2237775,
          -2.5034416,
          -2.0872622,
          -1.9538589,
          -3.8779426,
          -1.9942588,
          -2.3138447,
          -1.6741527,
          -2.1348033,
          -2.091827,
          -3.5540621,
          -4.1038427,
          -4.1427937,
          -0.4661493,
          -0.60525006,
          -0.5312305,
          -0.49378565,
          -2.5473619,
          -3.3815649,
          -3.8179212,
          -2.0422413,
          -0.13150178,
          -0.8636665,
          -1.5810561,
          -1.3882576,
          15.704639,
          -0.60347784,
          -0.68301415,
          -4.10534,
          -4.4112687,
          -2.4788308,
          -4.0592437,
          -4.396238,
          -3.2316895,
          -3.7706978,
          -4.090578,
          -4.444853,
          8.180603,
          -3.8963497,
          -3.7205253,
          -4.0403013,
          -4.495879,
          8.200322,
          -2.780288,
          -3.3119717,
          -4.0779166,
          -4.32853,
          -2.5785744,
          -1.1344502,
          -0.7267606,
          -1.3368341,
          -3.237877,
          -0.9166869,
          -5.299958,
          -6.537547,
          -0.7150619,
          -1.8458055,
          -1.8433368,
          -2.2917764,
          0.56519324,
          -0.12290488,
          1.1603825,
          -2.5394793,
          -2.325016,
          -0.9258166,
          -2.9662876,
          -2.0751815,
          -2.039231,
          -2.0755284,
          1.1153518,
          -1.9787146,
          -0.21320236,
          -2.0003378,
          -1.6142173,
          -1.577193,
          -1.8356144,
          -0.14634533,
          -0.25583068,
          -0.15427616,
          -0.26362136,
          -4.409291,
          -3.3391235,
          -4.923193,
          -4.3386827,
          -2.4165037,
          -2.3893542,
          -2.1324866,
          -2.2762856,
          -2.784258,
          -3.2695053,
          -3.2648535,
          -3.939017,
          -1.6850191,
          5.7932386,
          0.14715916,
          -3.673215,
          -4.005265,
          -4.098734,
          -0.6859262,
          -0.38936943,
          -0.6420777,
          -1.0973259,
          -0.64776343,
          -0.5509715,
          -0.06559637,
          0.1843399,
          -0.09157987,
          -2.3420606,
          -2.3557332,
          -1.5156217,
          -2.2629497,
          0.10548735,
          0.31745964,
          -0.11990021,
          0.013758535,
          0.06789794,
          15.732205,
          0.8101288,
          0.7616556,
          1.3352864,
          1.3639275,
          1.3230395,
          1.3549088,
          0.26880276,
          15.806028,
          15.802831,
          15.800805,
          16.357523,
          15.794807,
          15.777965,
          15.802283,
          15.810665,
          15.805239,
          15.781674,
          15.750272,
          15.790418,
          15.789059,
          16.1543,
          16.374659,
          16.341206,
          16.406935,
          15.745199,
          15.805555,
          16.369696,
          15.765114,
          15.786953,
          15.798897,
          15.784048,
          15.80511,
          16.516546,
          15.784563,
          15.80532,
          15.774471,
          15.792824,
          15.802155,
          15.795144,
          15.82582,
          16.325478,
          15.803277,
          15.794819,
          15.783173,
          15.791485,
          15.808811,
          15.78127,
          15.7846985,
          15.793917,
          15.776211,
          15.787738,
          15.792017,
          15.826445,
          15.783204,
          15.801481,
          15.788119,
          15.794266,
          15.688536,
          15.787714,
          15.755996,
          15.794852,
          15.774214,
          15.780081,
          15.785007,
          15.820708,
          15.724214,
          15.803475,
          15.791984,
          15.77575,
          15.8744545,
          15.718096,
          15.786824,
          15.778588,
          15.7881,
          15.82179,
          15.79395,
          15.7933655,
          15.774083,
          15.7706,
          15.77395,
          15.772321,
          15.784133,
          15.787226,
          15.796389,
          15.78974,
          15.777941,
          15.789077,
          15.798244,
          15.799382,
          15.770712,
          15.798582,
          15.781524,
          15.785247,
          15.790623,
          15.788579,
          15.788872,
          15.797932,
          15.833616,
          15.72941,
          15.741313,
          15.788248,
          15.788317,
          15.793576,
          15.781226,
          15.781872,
          15.787233,
          15.794227,
          15.826585,
          15.811277,
          1.0064458,
          -2.252118,
          -1.7868806,
          -2.2298963,
          -2.0722215,
          -1.7371529,
          -1.5257862,
          -0.4092371,
          0.028902335,
          -4.2036114,
          -1.7379236,
          -2.1924403,
          -3.0092354,
          -2.677827,
          -3.0642622,
          -3.0691185,
          -3.7537186,
          -1.4963408,
          -1.5750923,
          -1.7320566,
          -1.628069,
          -1.839663,
          -1.5479486,
          -0.6393036,
          -0.45986155,
          -0.37070018,
          -5.9885435,
          -0.9285389,
          -1.9276644,
          -1.910968,
          -1.1389034,
          -0.9885187,
          -1.0719911,
          -0.09977729,
          -2.6556103,
          -2.623513,
          -2.8080013,
          -4.040144,
          -0.8988716,
          -2.2895575,
          -2.2801664,
          -2.4995573,
          -1.987066,
          -1.8472023,
          -1.9746225,
          -1.9241736,
          -2.0771444,
          -2.4075367,
          -3.340731,
          -2.2266977,
          2.8682718,
          1.007383,
          0.82885873,
          1.3196568,
          -2.159695,
          -2.230473,
          -2.6076858,
          -2.1263409,
          -1.7869273,
          -3.392725,
          -3.30342,
          -2.729641,
          -0.70692974,
          -1.0431043,
          -0.81160045,
          -0.938473,
          -0.90522593,
          1.0868956,
          -2.1127691,
          -2.1334908,
          -2.1786585,
          -2.3616512,
          -2.2726376,
          1.1494286,
          -2.0417006,
          -0.3874889,
          -0.5714678,
          -0.6427051,
          -0.63706124,
          -1.0962502,
          -1.1977814,
          -1.0780452,
          -0.6128831,
          -1.1919208,
          -1.6058539,
          -2.7504623,
          -3.3395998,
          -2.3228464,
          -2.3499541,
          -2.3952694,
          -4.0960445,
          -1.0594951,
          -0.81875664,
          -0.56130826,
          -0.51466817,
          -0.29503325,
          -3.974692,
          -4.3986263,
          8.450568,
          5.694955,
          -0.54688364,
          -4.886426,
          -0.56117946,
          -1.3788083,
          -1.3178419,
          -0.1542938,
          -1.986084,
          -2.0901504,
          -2.4171436,
          -2.023455,
          -3.424314,
          -3.406105,
          -1.7683122,
          -2.1541839,
          0.4509995,
          0.83237475,
          0.562184,
          -1.006822,
          0.004754807,
          -0.18476145,
          -0.14587225,
          -0.23334064,
          8.086829,
          7.6859145,
          8.048193,
          -0.8772141,
          -1.1282388,
          -4.7152996,
          -2.0926185,
          -2.1418927,
          -2.0541492,
          -3.2439811,
          1.4898258,
          1.7030351,
          1.3313842,
          1.2191144,
          1.2185487,
          0.9918288,
          1.1586882,
          -2.098452,
          -2.2348008,
          -2.5323427,
          -3.9020562,
          -3.0558197,
          -2.0669436,
          -1.4673508,
          -2.4390507,
          -2.797903,
          -2.4602165,
          -2.5853565,
          -2.252467,
          -2.7397113,
          1.1418647,
          0.78937906,
          -4.098796,
          -3.0514443,
          0.92302096,
          0.7028241,
          -2.7511992,
          -2.9590187,
          -1.6671025,
          1.0373777,
          -0.36892873,
          -0.16961169,
          -0.57442707,
          -0.49959278,
          -3.4081583,
          -0.47204617,
          -1.5501338,
          -0.6258241,
          -0.6630711,
          -2.8614218,
          -0.39634869,
          -0.08587888,
          -0.5526331,
          -1.0801528,
          -3.765039,
          -3.8881886,
          -4.0205426,
          -3.8268197,
          1.0122672,
          1.0101305,
          0.62037706,
          1.0473899,
          0.62579894,
          0.63656914,
          -0.15874127,
          0.92002785,
          0.6783415,
          -0.62805057,
          1.0107533,
          -3.9887362,
          0.75250477,
          0.7834976,
          -0.956813,
          -0.0307097,
          -0.27016643,
          -0.5181105,
          -0.3806911,
          -0.6021289,
          -2.7138448,
          -2.2492075,
          -2.2499392,
          -2.72315,
          -1.0447649,
          -2.8522363,
          -2.3754535,
          0.24377511,
          -2.184839,
          -2.062184,
          -3.4055998,
          -3.6127381,
          -2.1137395,
          -2.2719069,
          -2.0653393,
          -1.7464335,
          -1.5403364,
          -2.5945268,
          -0.60262114,
          -0.79190665,
          -0.16653699,
          -0.25903198,
          -0.20993316,
          -0.41165608,
          -1.0029395,
          -1.683962,
          -1.6296053,
          -1.269712,
          -0.6843654,
          -2.6089075,
          -2.045372,
          -1.9253421,
          -2.0288868,
          -2.1441672,
          1.1349275,
          -2.0035486,
          -2.1597273,
          -2.0616121,
          -1.7812102,
          -0.8997999,
          -0.97668034,
          -0.6338832,
          -1.1359267,
          0.16628672,
          -1.4874171,
          -0.8311872,
          0.09872813,
          -2.4774911,
          -2.133627,
          -0.24744079,
          -1.3095002,
          -0.5031673,
          -1.5532516,
          2.6825335,
          -2.6601303,
          -0.68009824,
          -1.7485317,
          -0.85265267,
          -0.8643593,
          7.62767,
          0.020090368,
          -0.44406447,
          -0.07378035,
          0.33879438,
          -1.8820903,
          -1.9330359,
          -2.0056424,
          -1.5646515,
          -2.025606,
          -2.43324,
          -2.1587858,
          -1.6976635,
          -0.7088979,
          -0.3604621,
          -2.416683,
          -2.1546395,
          -2.2549677,
          -2.3514938,
          -2.363392,
          -3.0889027,
          -1.9683651,
          -0.9022813,
          -2.8500032,
          -1.7169861,
          -1.5172515,
          -1.8225288,
          -3.805316,
          -3.8317509,
          -2.9450579,
          -2.2777598,
          -2.0995162,
          -2.3700662,
          5.272035,
          0.12397024,
          -1.889242,
          -4.1941648,
          -6.248788,
          -0.0007340482,
          -0.13325505,
          -1.9021244,
          -0.58766407,
          -0.43447727,
          0.06355952,
          -2.353768,
          -1.7172554,
          0.28117856,
          1.3761119,
          1.3597765,
          0.21414815,
          -3.5385764,
          -3.207655,
          -2.3641732,
          -2.1830895,
          2.4045248,
          2.4138556,
          -2.9995255,
          -3.4315045,
          -3.389181,
          -2.1767833,
          -2.0948238,
          -2.2560189,
          -1.8278756,
          -1.6439203,
          -1.8538262,
          -0.37133205,
          -0.3691667,
          0.06447448,
          -0.42084688,
          -1.5842828,
          -1.4071497,
          -3.689864,
          -1.1452949,
          -1.8893316,
          -1.4484619,
          -1.9700326,
          0.43193918,
          0.55360174,
          0.055840634,
          -0.59952295,
          0.16871595,
          0.04033432,
          -1.0832142,
          -0.008370458,
          0.028185826,
          0.81279045,
          1.3660575,
          0.709447,
          1.3638071,
          0.3659169,
          15.842715,
          15.801007,
          15.809339,
          15.797377,
          15.803695,
          15.785003,
          16.347706,
          15.78836,
          15.7939415,
          15.797868,
          15.80516,
          15.789446,
          15.7741375,
          15.770654,
          15.7936125,
          15.7801285,
          15.801829,
          15.739865,
          15.7870865,
          15.785441,
          15.769711,
          16.310087,
          15.988448,
          16.351719,
          16.42483,
          16.361467,
          15.788323,
          16.36659,
          16.376781,
          15.741996,
          15.804045,
          15.786429,
          15.796472,
          15.783246,
          15.788575,
          15.809192,
          15.76354,
          15.788234,
          15.795333,
          15.79975,
          15.788493,
          15.779239,
          15.838704,
          15.773973,
          15.801243,
          15.799627,
          15.836982,
          15.791632,
          15.77364,
          15.687282,
          15.796119,
          15.829203,
          15.783211,
          15.787651,
          15.778805,
          15.842118,
          15.780857,
          15.787027,
          15.757489,
          15.810104,
          15.778397,
          15.801996,
          15.780468,
          15.731098,
          15.78183,
          15.780698,
          15.825277,
          15.69126,
          15.797937,
          -4.0322275,
          -4.199883,
          -2.83751,
          0.3506165,
          0.6095233,
          0.56257075,
          0.37248048,
          -0.045055784,
          -0.25836802,
          -2.0490127,
          -2.16215,
          -2.5116198,
          -2.1195974,
          0.9800509,
          0.8814502,
          0.6359908,
          -0.5014819,
          0.9113617,
          1.1494111,
          0.72969747,
          0.88546115,
          1.0187817,
          0.9882734,
          0.9861636,
          0.94306797,
          0.86278296,
          -0.4070455,
          -1.0080577,
          -1.3546311,
          -1.8885015,
          -1.7890766,
          -1.8098762,
          -2.0630991,
          -1.2166368,
          -1.0912771,
          -1.4325315,
          -1.6199046,
          -2.5487936,
          -1.7058079,
          -2.2751675,
          -2.288653,
          1.4744567
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "SE-ResNet\n\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) th..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, fil..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the ..."
          ],
          [
           "```..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: SE ResNet\n  Paper:\n    Title: Squeeze-and-Excitation Net..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet15..."
          ],
          [
           "Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bi..."
          ],
          [
           "Res2Net\n\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Res2Net\n  Paper:\n    Title: 'Res2Net: A New Multi-scale ..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net101..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_..."
          ],
          [
           "# Ensemble Adversarial Inception ResNet v2\n\n**Inception-ResNet-v2** is a convolutional neural archit..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. Yo..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Ensemble Adversarial\n  Paper:\n    Title: Adversaria..."
          ],
          [
           "TResNet\n\nA **TResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) that aim..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: TResNet\n  Paper:\n    Title: 'TResNet: High Performance G..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_..."
          ],
          [
           "Momentum: 0.9\n    Image Size: '448'\n    Weight Decay: 0.0001\n    Interpolation: bilinear\n  Code: htt..."
          ],
          [
           "Training Resources: 8x NVIDIA 100 GPUs\n    Training Time: < 24 hours\n    ID: tresnet_m\n    LR: 0.01\n..."
          ],
          [
           "Tasks:\n    - Image Classification\n    Training Techniques:\n    - AutoAugment\n    - Cutout\n    - Labe..."
          ],
          [
           "- Convolution\n    - Global Average Pooling\n    - InPlace-ABN\n    - Leaky ReLU\n    - ReLU\n    - Resid..."
          ],
          [
           "FLOPs: 60641712730\n    Parameters: 75646610\n    File Size: 224440219\n    Architecture:\n    - 1x1 Con..."
          ],
          [
           "SE-ResNet\n\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) th..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the ..."
          ],
          [
           "CSP-ResNeXt\n\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `cspresnext50`. You can find the ..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: CSP ResNeXt\n  Paper:\n    Title: 'CSPNet: A New Back..."
          ],
          [
           "RexNet\n\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: RexNet\n  Paper:\n    Title: 'ReXNet: Diminishing Represen..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_10..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_13..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_15..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_20..."
          ],
          [
           "RegNetY\n\n**RegNetY** is a convolutional network design space with simple, regular models with parame..."
          ],
          [
           "```\n\nTo get the model predictions:\n```python\nimport torch\nwith torch.no_grad():\n    out = model(tens..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the I..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: RegNetY\n  Paper:\n    Title: Designing Network Design Spa..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 70...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 74...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 75...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 77...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 82...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80...."
          ],
          [
           "Feature Extraction\n\nAll of the models in `timm` have consistent mechanisms for obtaining various typ..."
          ],
          [
           "```\nOutput:\n```text\nUnpooled shape: torch.Size([2, 2048, 7, 7])\n```\n\n#### Remove it later\n```python ..."
          ],
          [
           "```\nOutput:\n```text\nOriginal shape: torch.Size([2, 1000])\nPooled shape: torch.Size([2, 1024])\n```\n\n\n..."
          ],
          [
           "```\n\n### Query the feature information\n\nAfter a feature backbone has been created, it can be queried..."
          ],
          [
           "```\n\n### Select specific feature levels or limit the stride\n\nThere are two additional creation argum..."
          ],
          [
           "Wide ResNet\n\n**Wide Residual Networks** are a variant on [ResNets](https://paperswithcode.com/method..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `wide_resnet101_2`. You can find ..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Wide ResNet\n  Paper:\n    Title: Wide Residual Networks\n ..."
          ],
          [
           "Top 5 Accuracy: 94.28%\n- Name: wide_resnet50_2\n  In Collection: Wide ResNet\n  Metadata:\n    FLOPs: 1..."
          ],
          [
           "Res2NeXt\n\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https://paperswithcod..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Res2NeXt\n  Paper:\n    Title: 'Res2Net: A New Multi-..."
          ],
          [
           "RegNetY\n\n**RegNetY** is a convolutional network design space with simple, regular models with parame..."
          ],
          [
           "```\n\nTo load and preprocess the image:\n\n```py\n>>> import urllib\n>>> from PIL import Image\n>>> from t..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n\n```py\n>>> # Get imagenet class mappings\n>>> url, fil..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the I..."
          ],
          [
           "Archived Changes\n\n### Nov 22, 2021\n* A number of updated weights anew new model defs\n  * `eca_halone..."
          ],
          [
           "### Oct 19, 2021\n* ResNet strikes back (https://arxiv.org/abs/2110.00476) weights added, plus any ex..."
          ],
          [
           "* ConvMixer (https://openreview.net/forum?id=TVHS5Y4dNvM), CrossVit (https://arxiv.org/abs/2103.1489..."
          ],
          [
           "### Aug 18, 2021\n* Optimizer bonanza!\n  * Add LAMB and LARS optimizers, incl trust ratio clipping op..."
          ],
          [
           "### July 5-9, 2021\n* Add `efficientnetv2_rw_t` weights, a custom 'tiny' 13.6M param variant that is ..."
          ],
          [
           "### June 20, 2021\n* Release Vision Transformer 'AugReg' weights from [How to train your ViT? Data, A..."
          ],
          [
           "* Add `eca_nfnet_l2` weights from my 'lightweight' series. 84.7 top-1 at 384x384.\n* Add distilled Bi..."
          ],
          [
           "### June 8, 2021\n* Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w/ my XLA branch. 24 b..."
          ],
          [
           "### May 5, 2021\n* Add MLP-Mixer models and port pretrained weights from [Google JAX impl](https://gi..."
          ],
          [
           "### April 13, 2021\n* Add Swin Transformer models and weights from https://github.com/microsoft/Swin-..."
          ],
          [
           "### April 1, 2021\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and..."
          ],
          [
           "### Feb 18, 2021\n* Add pretrained weights and model variants for NFNet-F* models from [DeepMind Haik..."
          ],
          [
           "### Feb 16, 2021\n* Add Adaptive Gradient Clipping (AGC) as per https://arxiv.org/abs/2102.06171. Int..."
          ],
          [
           "### Feb 8, 2021\n* Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320..."
          ],
          [
           "### Jan 25, 2021\n* Add ResNetV2 Big Transfer (BiT) models w/ ImageNet-1k and 21k weights from https:..."
          ],
          [
           "### Dec 18, 2020\n* Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256\n  * 256x2..."
          ],
          [
           "### Oct 21, 2020\n* Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79..."
          ],
          [
           "### Aug 12, 2020\n* New/updated weights from training experiments\n  * EfficientNet-B3 - 82.1 top-1 (v..."
          ],
          [
           "### Aug 5, 2020\nUniversal feature extraction, new models, new weights, new test sets.\n* All models s..."
          ],
          [
           "### June 11, 2020\nBunch of changes:\n* DenseNet models updated with memory efficient addition from to..."
          ],
          [
           "### May 1, 2020\n* Merged a number of execellent contributions in the ResNet model family over the pa..."
          ],
          [
           "### April 5, 2020\n* Add some newly trained MobileNet-V2 models trained with latest h-params, rand au..."
          ],
          [
           "### Feb 18, 2020\n* Big refactor of model layers and addition of several attention mechanisms. Severa..."
          ],
          [
           "### Feb 1/2, 2020\n* Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 ..."
          ],
          [
           "### Dec 28, 2019\n* Add new model weights and training hparams (see Training Hparams section)\n  * `ef..."
          ],
          [
           "### Nov 29, 2019\n* Brought EfficientNet and MobileNetV3 up to date with my https://github.com/rwight..."
          ],
          [
           "(Tensorflow) EfficientNet CondConv\n\n**EfficientNet** is a convolutional neural network architecture ..."
          ],
          [
           "```\n\nTo load and preprocess the image:\n```python \nimport urllib\nfrom PIL import Image\nfrom timm.data..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_cc_b0_4e`. You c..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: TF EfficientNet CondConv\n  Paper:\n    Title: 'CondConv: ..."
          ],
          [
           "Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n..."
          ],
          [
           "- Weight Decay\n    Training Data:\n    - ImageNet\n    ID: tf_efficientnet_cc_b0_8e\n    LR: 0.256\n    ..."
          ],
          [
           "- CondConv\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residual Block\n   ..."
          ],
          [
           "SWSL ResNeXt\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-blo..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `swsl_resnext101_32x16d`. You can..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: SWSL ResNext\n  Paper:\n    Title: Billion-scale semi-supe..."
          ],
          [
           "Weights: https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resne..."
          ],
          [
           "Batch Size: 1536\n    Image Size: '224'\n    Weight Decay: 0.0001\n    Interpolation: bilinear\n  Code: ..."
          ],
          [
           "ID: swsl_resnext101_32x8d\n    LR: 0.0015\n    Epochs: 30\n    Layers: 101\n    Crop Pct: '0.875'\n    Ba..."
          ],
          [
           "Training Techniques:\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - IG-1B-Targe..."
          ],
          [
           "Model Summaries\n\nThe model architectures included come from a wide variety of sources. Sources, incl..."
          ],
          [
           "## DenseNet [[densenet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models..."
          ],
          [
           "## HRNet [[hrnet.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/hrnet..."
          ],
          [
           "## Inception-ResNet-V2 [[inception_resnet_v2.py](https://github.com/rwightman/pytorch-image-models/b..."
          ],
          [
           "## EfficientNet [[efficientnet.py](https://github.com/rwightman/pytorch-image-models/blob/master/tim..."
          ],
          [
           "## MobileNet-V3 [[mobilenetv3.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm..."
          ],
          [
           "* ResNet (V1B)\n  * Paper: `Deep Residual Learning for Image Recognition` - https://arxiv.org/abs/151..."
          ],
          [
           "* Squeeze-and-Excitation Networks\n  * Paper: `Squeeze-and-Excitation Networks` - https://arxiv.org/a..."
          ],
          [
           "## Res2Net [[res2net.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/r..."
          ],
          [
           "## SelecSLS [[selecsls.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models..."
          ],
          [
           "## VGG [[vgg.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vgg.py)]\n..."
          ],
          [
           "## Xception (Modified Aligned, Gluon) [[gluon_xception.py](https://github.com/rwightman/pytorch-imag..."
          ],
          [
           "(Tensorflow) MobileNet v3\n\n**MobileNetV3** is a convolutional neural network that is designed for mo..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_mobilenetv3_large_075`. You c..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: TF MobileNet V3\n  Paper:\n    Title: Searching for Mobile..."
          ],
          [
           "Momentum: 0.9\n    Batch Size: 4096\n    Image Size: '224'\n    Weight Decay: 1.0e-05\n    Interpolation..."
          ],
          [
           "Training Techniques:\n    - RMSProp\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training..."
          ],
          [
           "- Depthwise Separable Convolution\n    - Dropout\n    - Global Average Pooling\n    - Hard Swish\n    - ..."
          ],
          [
           "In Collection: TF MobileNet V3\n  Metadata:\n    FLOPs: 48457664\n    Parameters: 2040000\n    File Size..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 65...."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilen..."
          ],
          [
           "Image Size: '224'\n    Weight Decay: 4.0e-05\n    Interpolation: bilinear\n    RMSProp Decay: 0.9\n  Cod..."
          ],
          [
           "Sharing and Loading Models From the Hugging Face Hub\n\nThe `timm` library has a built-in integration ..."
          ],
          [
           "```\n\nRunning the above would push the model to `<your-username>/resnet18-random` on the Hub. You can..."
          ],
          [
           "Big Transfer (BiT)\n\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `resnetv2_101x1_bitm`. You can fi..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Big Transfer\n  Paper:\n    Title: 'Big Transfer (BiT): Ge..."
          ],
          [
           "Momentum: 0.9\n    Batch Size: 4096\n    Image Size: '480'\n    Weight Decay: 0.0001\n    Interpolation:..."
          ],
          [
           "Training Resources: Cloud TPUv3-512\n    ID: resnetv2_101x3_bitm\n    LR: 0.03\n    Epochs: 90\n    Laye..."
          ],
          [
           "Tasks:\n    - Image Classification\n    Training Techniques:\n    - Mixup\n    - SGD with Momentum\n    -..."
          ],
          [
           "Tasks:\n    - Image Classification\n    Training Techniques:\n    - Mixup\n    - SGD with Momentum\n    -..."
          ],
          [
           "- Weight Standardization\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Mixup\n..."
          ],
          [
           "- Convolution\n    - Global Average Pooling\n    - Group Normalization\n    - Max Pooling\n    - ReLU\n  ..."
          ],
          [
           "MobileNet v2\n\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `mobilenetv2_100`. You can find t..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: MobileNet V2\n  Paper:\n    Title: 'MobileNetV2: Inverted ..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv..."
          ],
          [
           "EfficientNet (Knapsack Pruned)\n\n**EfficientNet** is a convolutional neural network architecture and ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `efficientnet_b1_pruned`. You can..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: EfficientNet Pruned\n  Paper:\n    Title: Knapsack Pruning..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78...."
          ],
          [
           "Top 1 Accuracy: 79.91%\n      Top 5 Accuracy: 94.86%\n- Name: efficientnet_b3_pruned\n  In Collection: ..."
          ],
          [
           "(Tensorflow) Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_inception_v3`. You can find t..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: TF Inception v3\n  Paper:\n    Title: Rethinking the ..."
          ],
          [
           "DenseNet\n\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet..."
          ],
          [
           "```\n\nTo get the model predictions:\n\n```py\n>>> import torch\n>>> with torch.no_grad():\n...     out = m..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `densenet121`. You can find the I..."
          ],
          [
           "```\n\n```\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: DenseNet\n  Paper:\n    Title: Densely Connected Convoluti..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/densenet12..."
          ],
          [
           "Weights: https://download.pytorch.org/models/densenet161-8d451a50.pth\n  Results:\n  - Task: Image Cla..."
          ],
          [
           "Weights: https://download.pytorch.org/models/densenet169-b2777c0a.pth\n  Results:\n  - Task: Image Cla..."
          ],
          [
           "Weights: https://download.pytorch.org/models/densenet201-c1103571.pth\n  Results:\n  - Task: Image Cla..."
          ],
          [
           "Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76.59%\n      Top 5 Accuracy: 93.2%\n- Name: tv_d..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 74...."
          ],
          [
           "Training Examples\n\n## EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5\nThese params are for..."
          ],
          [
           "## SE-ResNeXt-26-D and SE-ResNeXt-26-T\nThese hparams (or similar) work well for a wide range of ResN..."
          ],
          [
           "`./distributed_train.sh 2 /imagenet/ --model efficientnet_b0 -b 384 --sched step --epochs 450 --deca..."
          ],
          [
           "`./distributed_train.sh 8 /imagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay..."
          ],
          [
           "`./distributed_train.sh 8 /imagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 ..."
          ],
          [
           "MnasNet\n\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: MNASNet\n  Paper:\n    Title: 'MnasNet: Platform-Aware Neu..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mnasnet_b1..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 75...."
          ],
          [
           "SK-ResNet\n\n**SK ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) th..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: SKResNet\n  Paper:\n    Title: Selective Kernel Networks\n ..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 73...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76...."
          ],
          [
           "Inception v4\n\n**Inception-v4** is a convolutional neural network architecture that builds on previou..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `inception_v4`. You can find the ..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Inception v4\n  Paper:\n    Title: Inception-v4, Ince..."
          ],
          [
           "CSP-ResNeXt\n\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "MnasNet\n\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `mnasnet_100`. You can find the I..."
          ],
          [
           "NASNet\n\n**NASNet** is a type of convolutional neural network discovered through neural architecture ..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: NASNet\n  Paper:\n    Title: Learning Transferable Ar..."
          ],
          [
           "SelecSLS\n\n**SelecSLS** uses novel selective long and short range skip connections to improve the inf..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `selecsls42b`. You can find the I..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: SelecSLS\n  Paper:\n    Title: 'XNect: Real-time Multi-Per..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 77...."
          ],
          [
           "- Name: selecsls60b\n  In Collection: SelecSLS\n  Metadata:\n    FLOPs: 4657653144\n    Parameters: 3277..."
          ],
          [
           "MixNet\n\n**MixNet** is a type of convolutional neural network discovered via AutoML that utilises [Mi..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `mixnet_l`. You can find the IDs ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: MixNet\n  Paper:\n    Title: 'MixConv: Mixed Depthwise Con..."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78.98%\n      T..."
          ],
          [
           "In Collection: MixNet\n  Metadata:\n    FLOPs: 321264910\n    Parameters: 4130000\n    File Size: 167279..."
          ],
          [
           "- Batch Normalization\n    - Dense Connections\n    - Dropout\n    - Global Average Pooling\n    - Group..."
          ],
          [
           "DenseNet\n\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "SK-ResNeXt\n\n**SK ResNeXt** is a variant of a [ResNeXt](https://www.paperswithcode.com/method/resnext..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `skresnext50_32x4d`. You can find..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: SKResNeXt\n  Paper:\n    Title: Selective Kernel Netw..."
          ],
          [
           "Recent Changes\n\n### Aug 29, 2022\n* MaxVit window size scales with img_size by default. Add new RelPo..."
          ],
          [
           "### Aug 26, 2022\n* CoAtNet (https://arxiv.org/abs/2106.04803) and MaxVit (https://arxiv.org/abs/2204..."
          ],
          [
           "* GCVit (weights adapted from https://github.com/NVlabs/GCVit, code 100% `timm` re-write for license..."
          ],
          [
           "### Aug 15, 2022\n* ConvNeXt atto weights added\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\n  * `con..."
          ],
          [
           "### July 27, 2022\n* All runtime benchmark and validation result csv files are finally up-to-date!\n* ..."
          ],
          [
           "### July 8, 2022\nMore models, more fixes\n* Official research models (w/ weights) added:\n  * EdgeNeXt..."
          ],
          [
           "* `cs3darknet_l` - 80.4 @ 256, 80.9 @ 288\n  * `cs3darknet_focus_l` - 80.3 @ 256, 80.9 @ 288\n  * `vit..."
          ],
          [
           "* Numerous bug fixes\n* Currently testing for imminent PyPi 0.6.x release\n* LeViT pretraining of larg..."
          ],
          [
           "### May 13, 2022\n* Official Swin-V2 models and weights added from (https://github.com/microsoft/Swin..."
          ],
          [
           "### May 2, 2022\n* Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (`visi..."
          ],
          [
           "### March 23, 2022\n* Add `ParallelBlock` and `LayerScale` option to base vit models to support model..."
          ],
          [
           "### March 21, 2022\n* Merge `norm_norm_norm`. **IMPORTANT** this update for a coming 0.6.x release wi..."
          ],
          [
           "* `regnetz_c16_evos`  - 81.9 @ 256, 82.64 @ 320 (EvoNormS)\n  * `regnetz_d8_evos`  - 83.42 @ 256, 84...."
          ],
          [
           "* Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm,..."
          ],
          [
           "### Feb 2, 2022\n* [Chris Hughes](https://github.com/Chris-hughes10) posted an exhaustive run through..."
          ],
          [
           "### Jan 14, 2022\n* Version 0.5.4 w/ release to be pushed to pypi. It's been a while since last pypi ..."
          ],
          [
           "### Dec 23, 2022 ðŸŽ„â˜ƒ\n* Add FlexiViT models and weights from https://github.com/google-research/big_vi..."
          ],
          [
           "| model                                     | top1 | param_count |  gmac | macts | hub              ..."
          ],
          [
           "| model                                    |   top1 |   param_count |   gmac |   macts | hub        ..."
          ],
          [
           "### Dec 5, 2022\n\n* Pre-release (`0.8.0dev0`) of multi-weight support (`model_arch.pretrained_tag`). ..."
          ],
          [
           "| model                                            |   top1 |   param_count |   gmac |   macts | hub..."
          ],
          [
           "| vit_large_patch14_clip_336.laion2b_ft_in12k_in1k |   88.2 |         304.5 |  191.1 |   270.2 | [li..."
          ],
          [
           "| vit_huge_patch14_clip_224.laion2b_ft_in1k        |   87.6 |         632   |  167.4 |   139.4 | [li..."
          ],
          [
           "| vit_base_patch16_clip_384.openai_ft_in1k         |   86.2 |          86.9 |   55.5 |   101.6 | [li..."
          ],
          [
           "| vit_base_patch32_clip_384.laion2b_ft_in12k_in1k  |   85.4 |          88.3 |   13.1 |    16.5 | [li..."
          ],
          [
           "* Port of MaxViT Tensorflow Weights from official impl at https://github.com/google-research/maxvit\n..."
          ],
          [
           "| model                              |   top1 |   param_count |   gmac |   macts | hub              ..."
          ],
          [
           "| maxvit_large_tf_512.in21k_ft_in1k  |   88   |         212.3 |  244.8 |   942.2 | [link](https://hu..."
          ],
          [
           "| maxvit_small_tf_512.in1k           |   86.1 |          69.1 |   67.3 |   383.8 | [link](https://hu..."
          ],
          [
           "| maxvit_tiny_tf_224.in1k            |   83.4 |          30.9 |    5.6 |    35.8 | [link](https://hu..."
          ],
          [
           "### Oct 15, 2022\n* Train and validation script enhancements\n* Non-GPU (ie CPU) device support\n* SLUR..."
          ],
          [
           "### Oct 10, 2022\n* More weights in `maxxvit` series, incl first ConvNeXt block based `coatnext` and ..."
          ],
          [
           "### Sept 7, 2022\n* Hugging Face [`timm` docs](https://huggingface.co/docs/hub/timm) home now exists,..."
          ],
          [
           "### Aug 26, 2022\n* CoAtNet (https://arxiv.org/abs/2106.04803) and MaxVit (https://arxiv.org/abs/2204..."
          ],
          [
           "### Aug 15, 2022\n* ConvNeXt atto weights added\n  * `convnext_atto` - 75.7 @ 224, 77.0 @ 288\n  * `con..."
          ],
          [
           "### July 27, 2022\n* All runtime benchmark and validation result csv files are up-to-date!\n* A few mo..."
          ],
          [
           "* `cs3darknet_l` - 80.4 @ 256, 80.9 @ 288\n  * `cs3darknet_focus_l` - 80.3 @ 256, 80.9 @ 288\n  * `vit..."
          ],
          [
           "### Jan 14, 2022\n* Version 0.5.4 w/ release to be pushed to pypi. It's been a while since last pypi ..."
          ],
          [
           "Hugging Face Timm Docs\n\n## Getting Started\n\n```\npip install git+https://github.com/huggingface/doc-b..."
          ],
          [
           "ResNeSt\n\nA **ResNeSt** is a variant on a [ResNet](https://paperswithcode.com/method/resnet), which i..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `resnest101e`. You can find the I..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: ResNeSt\n  Paper:\n    Title: 'ResNeSt: Split-Attention Ne..."
          ],
          [
           "Momentum: 0.9\n    Batch Size: 4096\n    Image Size: '256'\n    Weight Decay: 0.0001\n    Interpolation:..."
          ],
          [
           "Training Resources: 64x NVIDIA V100 GPUs\n    ID: resnest14d\n    LR: 0.1\n    Epochs: 270\n    Layers: ..."
          ],
          [
           "Tasks:\n    - Image Classification\n    Training Techniques:\n    - AutoAugment\n    - DropBlock\n    - L..."
          ],
          [
           "Architecture:\n    - 1x1 Convolution\n    - Convolution\n    - Dense Connections\n    - Global Average P..."
          ],
          [
           "Top 1 Accuracy: 84.53%\n      Top 5 Accuracy: 96.99%\n- Name: resnest26d\n  In Collection: ResNeSt\n  Me..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78...."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50-..."
          ],
          [
           "Momentum: 0.9\n    Batch Size: 8192\n    Image Size: '224'\n    Weight Decay: 0.0001\n    Interpolation:..."
          ],
          [
           "- Mixup\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Re..."
          ],
          [
           "FBNet\n\n**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https://p..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `fbnetc_100`. You can find the ID..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: FBNet\n  Paper:\n    Title: 'FBNet: Hardware-Aware Ef..."
          ],
          [
           "HRNet\n\n**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for ..."
          ],
          [
           "```\n\nTo get the model predictions:\n\n```py\n>>> import torch\n>>> with torch.no_grad():\n...     out = m..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `hrnet_w18`. You can find the IDs..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: HRNet\n  Paper:\n    Title: Deep High-Resolution Represent..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76...."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 72.34%\n      T..."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 75.11%\n      T..."
          ],
          [
           "Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78.21%\n      Top 5 Accuracy: 94.22%\n- Name: hrn..."
          ],
          [
           "Metrics:\n      Top 1 Accuracy: 78.45%\n      Top 5 Accuracy: 94.19%\n- Name: hrnet_w40\n  In Collection..."
          ],
          [
           "Top 1 Accuracy: 78.93%\n      Top 5 Accuracy: 94.48%\n- Name: hrnet_w44\n  In Collection: HRNet\n  Metad..."
          ],
          [
           "Top 5 Accuracy: 94.37%\n- Name: hrnet_w48\n  In Collection: HRNet\n  Metadata:\n    FLOPs: 22285865760\n ..."
          ],
          [
           "Top 5 Accuracy: 94.51%\n- Name: hrnet_w64\n  In Collection: HRNet\n  Metadata:\n    FLOPs: 37239321984\n ..."
          ],
          [
           "Learning Rate Schedulers\n\nThis page contains the API reference documentation for learning rate sched..."
          ],
          [
           "MobileNet v3\n\n**MobileNetV3** is a convolutional neural network that is designed for mobile phone CP..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `mobilenetv3_large_100`. You can ..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: MobileNet V3\n  Paper:\n    Title: Searching for MobileNet..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv..."
          ],
          [
           "Momentum: 0.9\n    Batch Size: 4096\n    Image Size: '224'\n    Weight Decay: 1.0e-05\n    Interpolation..."
          ],
          [
           "ResNet\n\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer ..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: ResNet\n  Paper:\n    Title: Deep Residual Learning for Im..."
          ],
          [
           "Top 5 Accuracy: 89.09%\n- Name: resnet26\n  In Collection: ResNet\n  Metadata:\n    FLOPs: 3026804736\n  ..."
          ],
          [
           "File Size: 87290831\n    Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Bottlene..."
          ],
          [
           "- Global Average Pooling\n    - Max Pooling\n    - ReLU\n    - Residual Block\n    - Residual Connection..."
          ],
          [
           "- Residual Connection\n    - Softmax\n    Tasks:\n    - Image Classification\n    Training Data:\n    - I..."
          ],
          [
           "- Weight Decay\n    Training Data:\n    - ImageNet\n    ID: tv_resnet101\n    LR: 0.1\n    Epochs: 90\n   ..."
          ],
          [
           "Tasks:\n    - Image Classification\n    Training Techniques:\n    - SGD with Momentum\n    - Weight Deca..."
          ],
          [
           "- Residual Block\n    - Residual Connection\n    - Softmax\n    Tasks:\n    - Image Classification\n    T..."
          ],
          [
           "- Max Pooling\n    - ReLU\n    - Residual Block\n    - Residual Connection\n    - Softmax\n    Tasks:\n   ..."
          ],
          [
           "SK-ResNet\n\n**SK ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) th..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `skresnet18`. You can find the ID..."
          ],
          [
           "SelecSLS\n\n**SelecSLS** uses novel selective long and short range skip connections to improve the inf..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `selecsls42b`. You can find the I..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "PNASNet\n\n**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the struc..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `pnasnet5large`. You can find the..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: PNASNet\n  Paper:\n    Title: Progressive Neural Arch..."
          ],
          [
           "(Tensorflow) EfficientNet CondConv\n\n**EfficientNet** is a convolutional neural network architecture ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_cc_b0_4e`. You c..."
          ],
          [
           "ECA-ResNet\n\nAn **ECA ResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) t..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ecaresnet101d`. You can find the..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: ECAResNet\n  Paper:\n    Title: 'ECA-Net: Efficient Channe..."
          ],
          [
           "Crop Pct: '0.875'\n    Batch Size: 256\n    Image Size: '224'\n    Weight Decay: 0.0001\n    Interpolati..."
          ],
          [
           "Training Data:\n    - ImageNet\n    ID: ecaresnet101d_pruned\n    Layers: 101\n    Crop Pct: '0.875'\n   ..."
          ],
          [
           "Training Techniques:\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n  ..."
          ],
          [
           "- Max Pooling\n    - ReLU\n    - Residual Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-..."
          ],
          [
           "- Efficient Channel Attention\n    - Global Average Pooling\n    - Max Pooling\n    - ReLU\n    - Residu..."
          ],
          [
           "Validation and Benchmark Results\n\nThis folder contains validation and benchmark results for the mode..."
          ],
          [
           "### ImageNetV2 Matched Frequency - [`results-imagenetv2-matched-frequency.csv`](results-imagenetv2-m..."
          ],
          [
           "### ImageNet-Rendition - [`results-imagenet-r.csv`](results-imagenet-r.csv)\n\nRenditions of 200 Image..."
          ],
          [
           "Results\n\nCSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation resul..."
          ],
          [
           "|Model | Acc@1 (Err) | Acc@5 (Err) | Param # (M) | Interpolation | Image Size |\n|---|---|---|---|---..."
          ],
          [
           "| seresnet50 | 80.274 (19.726) | 95.070 (4.930) | 28.1 | bicubic | 224 |\n| skresnext50d_32x4d | 80.1..."
          ],
          [
           "| mixnet_l | 78.976 (21.024 | 94.184 (5.816) | 7.33 | bicubic | 224 |\n| efficientnet_b1 | 78.692 (21..."
          ],
          [
           "| seresnext26_32x4d | 77.104 (22.896) | 93.316 (6.684) | 16.8 | bicubic | 224 |\n| skresnet34 | 76.91..."
          ],
          [
           "| fbnetc_100 | 75.124 (24.876) | 92.386 (7.614) | 5.6 | bilinear | 224 |\n| resnet34 | 75.110 (24.890..."
          ],
          [
           "## Ported and Other Weights\n\nFor weights ported from other deep learning frameworks (Tensorflow, MXN..."
          ],
          [
           "Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the Inception fam..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Inception v3\n  Paper:\n    Title: Rethinking the Inc..."
          ],
          [
           "CSP-DarkNet\n\n**CSPDarknet53** is a convolutional neural network and backbone for object detection th..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `cspdarknet53`. You can find the ..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: CSP DarkNet\n  Paper:\n    Title: 'YOLOv4: Optimal Sp..."
          ],
          [
           "SPNASNet\n\n**Single-Path NAS** is a novel differentiable NAS method for designing hardware-efficient ..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: SPNASNet\n  Paper:\n    Title: 'Single-Path NAS: Desi..."
          ],
          [
           "Wide ResNet\n\n**Wide Residual Networks** are a variant on [ResNets](https://paperswithcode.com/method..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `wide_resnet101_2`. You can find ..."
          ],
          [
           "SSL ResNeXT\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-bloc..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ssl_resnext101_32x16d`. You can ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: SSL ResNext\n  Paper:\n    Title: Billion-scale semi-super..."
          ],
          [
           "Weights: https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_3..."
          ],
          [
           "Batch Size: 1536\n    Image Size: '224'\n    Weight Decay: 0.0001\n    Interpolation: bilinear\n  Code: ..."
          ],
          [
           "ID: ssl_resnext101_32x8d\n    LR: 0.0015\n    Epochs: 30\n    Layers: 101\n    Crop Pct: '0.875'\n    Bat..."
          ],
          [
           "Training Techniques:\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n  ..."
          ],
          [
           "RegNetX\n\n**RegNetX** is a convolutional network design space with simple, regular models with parame..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `regnetx_002`. You can find the I..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: RegNetX\n  Paper:\n    Title: Designing Network Design Spa..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 68...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 72...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 73...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 75...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80...."
          ],
          [
           "Installation\n\nBefore you start, you'll need to setup your environment and install the appropriate pa..."
          ],
          [
           "```\n\n## From Source\n\nBuilding `timm` from source lets you make changes to the code base. To install ..."
          ],
          [
           "(Tensorflow) MobileNet v3\n\n**MobileNetV3** is a convolutional neural network that is designed for mo..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_mobilenetv3_large_075`. You c..."
          ],
          [
           "Big Transfer (BiT)\n\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `resnetv2_101x1_bitm`. You can fi..."
          ],
          [
           "Instagram ResNeXt WSL\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/re..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ig_resnext101_32x16d`. You can f..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: IG ResNeXt\n  Paper:\n    Title: Exploring the Limits of W..."
          ],
          [
           "Momentum: 0.9\n    Batch Size: 8064\n    Image Size: '224'\n    Weight Decay: 0.001\n    Interpolation: ..."
          ],
          [
           "- ImageNet\n    Training Resources: 336x GPUs\n    ID: ig_resnext101_32x32d\n    Epochs: 100\n    Layers..."
          ],
          [
           "- Softmax\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Nesterov Accelerated ..."
          ],
          [
           "- Convolution\n    - Global Average Pooling\n    - Grouped Convolution\n    - Max Pooling\n    - ReLU\n  ..."
          ],
          [
           "(Gluon) Xception\n\n**Xception** is a convolutional neural network architecture that relies solely on ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_xception65`. You can find ..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Gloun Xception\n  Paper:\n    Title: 'Xception: Deep ..."
          ],
          [
           "This guideline is very much a work-in-progress.*\n\nContributions to `timm` for code, documentation, t..."
          ],
          [
           "```\nblack --skip-string-normalization --line-length 120 <path-to-file>\n```\n\nAvoid formatting code th..."
          ],
          [
           "```\npytest -k \"substring-to-match\" -n 4 tests/\n```\n\n## Building documentation\n\nPlease refer to [this..."
          ],
          [
           "Deep Layer Aggregation\n\nExtending  â€œshallowâ€ skip connections, **Dense Layer Aggregation (DLA)** inc..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: DLA\n  Paper:\n    Title: Deep Layer Aggregation\n    URL: ..."
          ],
          [
           "Weights: http://dl.yf.io/dla/models/imagenet/dla102-d94d9790.pth\n  Results:\n  - Task: Image Classifi..."
          ],
          [
           "Weights: http://dl.yf.io/dla/models/imagenet/dla102x-ad62be81.pth\n  Results:\n  - Task: Image Classif..."
          ],
          [
           "Weights: http://dl.yf.io/dla/models/imagenet/dla102x2-262837b6.pth\n  Results:\n  - Task: Image Classi..."
          ],
          [
           "Weights: http://dl.yf.io/dla/models/imagenet/dla169-0914e092.pth\n  Results:\n  - Task: Image Classifi..."
          ],
          [
           "Weights: http://dl.yf.io/dla/models/imagenet/dla34-ba72cf86.pth\n  Results:\n  - Task: Image Classific..."
          ],
          [
           "Weights: http://dl.yf.io/dla/models/imagenet/dla46_c-2bfd52c3.pth\n  Results:\n  - Task: Image Classif..."
          ],
          [
           "Weights: http://dl.yf.io/dla/models/imagenet/dla46x_c-d761bae7.pth\n  Results:\n  - Task: Image Classi..."
          ],
          [
           "Weights: http://dl.yf.io/dla/models/imagenet/dla60-24839fc4.pth\n  Results:\n  - Task: Image Classific..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 67...."
          ],
          [
           "Data\n\n[[autodoc]] timm.data.create_dataset\n\n[[autodoc]] timm.data.create_loader\n\n[[autodoc]] timm.da..."
          ],
          [
           "CSP-ResNet\n\n**CSPResNet** is a convolutional neural network where we apply the Cross Stage Partial N..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `cspresnet50`. You can find the I..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: CSP ResNet\n  Paper:\n    Title: 'CSPNet: A New Backb..."
          ],
          [
           "Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the Inception fam..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `inception_v3`. You can find the ..."
          ],
          [
           "Getting Started\n\n## Welcome\n\nWelcome to the `timm` documentation, a lean set of docs that covers the..."
          ],
          [
           "```\n\n## List Models with Pretrained Weights\n```python\nimport timm\nfrom pprint import pprint\nmodel_na..."
          ],
          [
           "Model Summaries\n\nThe model architectures included come from a wide variety of sources. Sources, incl..."
          ],
          [
           "## DenseNet\n\n* Implementation: [densenet.py](https://github.com/rwightman/pytorch-image-models/blob/..."
          ],
          [
           "## HRNet\n\n* Implementation: [hrnet.py](https://github.com/rwightman/pytorch-image-models/blob/master..."
          ],
          [
           "## Inception-ResNet-V2\n\n* Implementation: [inception_resnet_v2.py](https://github.com/rwightman/pyto..."
          ],
          [
           "## EfficientNet\n\n* Implementation: [efficientnet.py](https://github.com/rwightman/pytorch-image-mode..."
          ],
          [
           "## MobileNet-V3\n\n* Implementation: [mobilenetv3.py](https://github.com/rwightman/pytorch-image-model..."
          ],
          [
           "## Res2Net\n\n* Implementation: [res2net.py](https://github.com/rwightman/pytorch-image-models/blob/ma..."
          ],
          [
           "## SelecSLS\n\n* Implementation: [selecsls.py](https://github.com/rwightman/pytorch-image-models/blob/..."
          ],
          [
           "## VGG\n\n* Implementation: [vgg.py](https://github.com/rwightman/pytorch-image-models/blob/master/tim..."
          ],
          [
           "## Xception\n\n* Implementation: [xception.py](https://github.com/rwightman/pytorch-image-models/blob/..."
          ],
          [
           "Deep Layer Aggregation\n\nExtending  â€œshallowâ€ skip connections, **Dense Layer Aggregation (DLA)** inc..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `dla102`. You can find the IDs in..."
          ],
          [
           "Feature Extraction\n\nAll of the models in `timm` have consistent mechanisms for obtaining various typ..."
          ],
          [
           "```\n\nOutput:\n\n```text\nUnpooled shape: torch.Size([2, 2048, 7, 7])\n```\n\n#### Remove it later\n\n```py\n>..."
          ],
          [
           "```\n\nOutput:\n\n```text\nPooled shape: torch.Size([2, 2048])\n```\n\n#### Remove it later\n\n```py\n>>> impor..."
          ],
          [
           "```\n\nOutput:\n\n```text\ntorch.Size([2, 64, 112, 112])\ntorch.Size([2, 256, 56, 56])\ntorch.Size([2, 512,..."
          ],
          [
           "```\n\n### Select specific feature levels or limit the stride\n\nThere are two additional creation argum..."
          ],
          [
           "(Gluon) ResNet\n\n**Residual Networks**, or **ResNets**, learn residual functions with reference to th..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_resnet101_v1b`. You can fi..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Gloun ResNet\n  Paper:\n    Title: Deep Residual Learning ..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.53%\n      T..."
          ],
          [
           "Metrics:\n      Top 1 Accuracy: 80.4%\n      Top 5 Accuracy: 95.02%\n- Name: gluon_resnet101_v1s\n  In C..."
          ],
          [
           "Top 5 Accuracy: 95.16%\n- Name: gluon_resnet152_v1b\n  In Collection: Gloun ResNet\n  Metadata:\n    FLO..."
          ],
          [
           "In Collection: Gloun ResNet\n  Metadata:\n    FLOPs: 15165680128\n    Parameters: 60210000\n    File Siz..."
          ],
          [
           "Metadata:\n    FLOPs: 15166131712\n    Parameters: 60210000\n    File Size: 241613584\n    Architecture:..."
          ],
          [
           "Parameters: 60320000\n    File Size: 242032606\n    Architecture:\n    - 1x1 Convolution\n    - Batch No..."
          ],
          [
           "Architecture:\n    - 1x1 Convolution\n    - Batch Normalization\n    - Bottleneck Residual Block\n    - ..."
          ],
          [
           "- Bottleneck Residual Block\n    - Convolution\n    - Global Average Pooling\n    - Max Pooling\n    - R..."
          ],
          [
           "- Global Average Pooling\n    - Max Pooling\n    - ReLU\n    - Residual Block\n    - Residual Connection..."
          ],
          [
           "- Residual Block\n    - Residual Connection\n    - Softmax\n    Tasks:\n    - Image Classification\n    T..."
          ],
          [
           "- Softmax\n    Tasks:\n    - Image Classification\n    Training Data:\n    - ImageNet\n    ID: gluon_resn..."
          ],
          [
           "- Image Classification\n    Training Data:\n    - ImageNet\n    ID: gluon_resnet50_v1s\n    Crop Pct: '0..."
          ],
          [
           "Adversarial Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the I..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `adv_inception_v3`. You can find ..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Adversarial Inception v3\n  Paper:\n    Title: Advers..."
          ],
          [
           "ResNet-D\n\n**ResNet-D** is a modification on the [ResNet](https://paperswithcode.com/method/resnet) a..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `resnet101d`. You can find the ID..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: ResNet-D\n  Paper:\n    Title: Bag of Tricks for Image Cla..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 82...."
          ],
          [
           "Top 5 Accuracy: 96.35%\n- Name: resnet18d\n  In Collection: ResNet-D\n  Metadata:\n    FLOPs: 2645205760..."
          ],
          [
           "Parameters: 64690000\n    File Size: 259662933\n    Architecture:\n    - 1x1 Convolution\n    - Batch No..."
          ],
          [
           "- Bottleneck Residual Block\n    - Convolution\n    - Global Average Pooling\n    - Max Pooling\n    - R..."
          ],
          [
           "- Residual Connection\n    - Softmax\n    Tasks:\n    - Image Classification\n    Training Data:\n    - I..."
          ],
          [
           "- ImageNet\n    ID: resnet50d\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bicubic\n..."
          ],
          [
           "(Tensorflow) EfficientNet Lite\n\n**EfficientNet** is a convolutional neural network architecture and ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_lite0`. You can ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: TF EfficientNet Lite\n  Paper:\n    Title: 'EfficientNet: ..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 74...."
          ],
          [
           "- Name: tf_efficientnet_lite2\n  In Collection: TF EfficientNet Lite\n  Metadata:\n    FLOPs: 106849443..."
          ],
          [
           "File Size: 33161413\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normal..."
          ],
          [
           "- Dense Connections\n    - Dropout\n    - Inverted Residual Block\n    - RELU6\n    Tasks:\n    - Image C..."
          ],
          [
           "CSP-DarkNet\n\n**CSPDarknet53** is a convolutional neural network and backbone for object detection th..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "EfficientNet\n\n**EfficientNet** is a convolutional neural network architecture and scaling method tha..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `efficientnet_b0`. You can find t..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: EfficientNet\n  Paper:\n    Title: 'EfficientNet: Rethinki..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 77...."
          ],
          [
           "- Name: efficientnet_b2\n  In Collection: EfficientNet\n  Metadata:\n    FLOPs: 1265324514\n    Paramete..."
          ],
          [
           "Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolutio..."
          ],
          [
           "- Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n  ..."
          ],
          [
           "Training Data:\n    - ImageNet\n    ID: efficientnet_b3a\n    Crop Pct: '1.0'\n    Image Size: '320'\n   ..."
          ],
          [
           "- ImageNet\n    ID: efficientnet_em\n    Crop Pct: '0.882'\n    Image Size: '240'\n    Interpolation: bi..."
          ],
          [
           "- ImageNet\n    ID: efficientnet_es\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation: bi..."
          ],
          [
           "- ImageNet\n    ID: efficientnet_lite0\n    Crop Pct: '0.875'\n    Image Size: '224'\n    Interpolation:..."
          ],
          [
           "timm\n\n<img class=\"float-left !m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[150px]\" src=\"ht..."
          ],
          [
           "Read the [quick start guide](quickstart) to get up and running with the `timm` library. You will lea..."
          ],
          [
           "Scripts\nA train, validation, inference, and checkpoint cleaning script included in the github root f..."
          ],
          [
           "`python validate.py /imagenet/validation/ --model seresnext26_32x4d --pretrained`\n\nTo run inference ..."
          ],
          [
           "(Legacy) SE-ResNeXt\n\n**SE ResNeXt** is a variant of a [ResNeXt](https://www.paperswithcode.com/metho..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `legacy_seresnext101_32x4d`. You ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Legacy SE ResNeXt\n  Paper:\n    Title: Squeeze-and-Excita..."
          ],
          [
           "Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bi..."
          ],
          [
           "- ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: legacy_seresnext26_32x4d\n    LR: 0..."
          ],
          [
           "- ResNeXt Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks..."
          ],
          [
           "RexNet\n\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `rexnet_100`. You can find the ID..."
          ],
          [
           "MixNet\n\n**MixNet** is a type of convolutional neural network discovered via AutoML that utilises [Mi..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "(Gluon) SE-ResNeXt\n\n**SE ResNeXt** is a variant of a [ResNext](https://www.paperswithcode.com/method..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_seresnext101_32x4d`. You c..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Gloun SEResNeXt\n  Paper:\n    Title: Squeeze-and-Excitati..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80...."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79...."
          ],
          [
           "RegNetX\n\n**RegNetX** is a convolutional network design space with simple, regular models with parame..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `regnetx_002`. You can find the I..."
          ],
          [
           "(Tensorflow) EfficientNet\n\n**EfficientNet** is a convolutional neural network architecture and scali..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0`. You can fin..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: TF EfficientNet\n  Paper:\n    Title: 'EfficientNet: Rethi..."
          ],
          [
           "Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n..."
          ],
          [
           "LR: 0.256\n    Epochs: 350\n    Crop Pct: '0.882'\n    Momentum: 0.9\n    Batch Size: 2048\n    Image Siz..."
          ],
          [
           "Tasks:\n    - Image Classification\n    Training Techniques:\n    - AutoAugment\n    - Label Smoothing\n ..."
          ],
          [
           "Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolutio..."
          ],
          [
           "Top 1 Accuracy: 81.65%\n      Top 5 Accuracy: 95.72%\n- Name: tf_efficientnet_b4\n  In Collection: TF E..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficie..."
          ],
          [
           "Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n..."
          ],
          [
           "- ImageNet\n    ID: tf_efficientnet_b6\n    LR: 0.256\n    Epochs: 350\n    Crop Pct: '0.942'\n    Moment..."
          ],
          [
           "- Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Classification\n    Training Techni..."
          ],
          [
           "Parameters: 87410000\n    File Size: 351379853\n    Architecture:\n    - 1x1 Convolution\n    - Average ..."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 85.35%\n      T..."
          ],
          [
           "- Name: tf_efficientnet_em\n  In Collection: TF EfficientNet\n  Metadata:\n    FLOPs: 3636607040\n    Pa..."
          ],
          [
           "File Size: 22008479\n    Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normal..."
          ],
          [
           "- Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - Inverted Residua..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 88...."
          ],
          [
           "(Legacy) SE-ResNet\n\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/r..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `legacy_seresnet101`. You can fin..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Legacy SE ResNet\n  Paper:\n    Title: Squeeze-and-Excitat..."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet10..."
          ],
          [
           "Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bi..."
          ],
          [
           "- ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: legacy_seresnet18\n    LR: 0.6\n    ..."
          ],
          [
           "- Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - ..."
          ],
          [
           "- Bottleneck Residual Block\n    - Convolution\n    - Global Average Pooling\n    - Max Pooling\n    - R..."
          ],
          [
           "Inception ResNet v2\n\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on t..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `inception_resnet_v2`. You can fi..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Inception ResNet v2\n  Paper:\n    Title: Inception-v..."
          ],
          [
           "(Gluon) Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the Incep..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_inception_v3`. You can fin..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Gloun Inception v3\n  Paper:\n    Title: Rethinking t..."
          ],
          [
           "(Legacy) SENet\n\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `legacy_senet154`. You can find t..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Legacy SENet\n  Paper:\n    Title: Squeeze-and-Excita..."
          ],
          [
           "Quickstart\n\nThis quickstart is intended for developers who are ready to dive into the code and see a..."
          ],
          [
           "```\n\nYou can also list models with a specific pattern in their name.\n\n```py\n>>> import timm\n>>> from..."
          ],
          [
           "```\n\n## Image Augmentation\n\nTo transform images into valid inputs for a model, you can use [`timm.da..."
          ],
          [
           "```\n\nWe can then resolve only the data related configuration by using [`timm.data.resolve_data_confi..."
          ],
          [
           "```\n\n<Tip>\n    Note: Here, the pretrained model's config happens to be the same as the generic confi..."
          ],
          [
           "```\n\nNow we can pass that image to the model to get the predictions. We use `unsqueeze(0)` in this c..."
          ],
          [
           "```\n\nIf we check the imagenet labels for the top index, we can see what the model predicted...\n\n```p..."
          ],
          [
           "Optimization\n\nThis page contains the API reference documentation for learning rate optimizers includ..."
          ],
          [
           "SE-ResNeXt\n\n**SE ResNeXt** is a variant of a [ResNext](https://www.paperswithcode.com/method/resneXt..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `seresnext26d_32x4d`. You can fin..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: SEResNeXt\n  Paper:\n    Title: Squeeze-and-Excitation Net..."
          ],
          [
           "Crop Pct: '0.875'\n    Momentum: 0.9\n    Batch Size: 1024\n    Image Size: '224'\n    Interpolation: bi..."
          ],
          [
           "Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: seresnext26t_32..."
          ],
          [
           "- ReLU\n    - ResNeXt Block\n    - Residual Connection\n    - Softmax\n    - Squeeze-and-Excitation Bloc..."
          ],
          [
           "SWSL ResNet\n\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the l..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `swsl_resnet18`. You can find the..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: SWSL ResNet\n  Paper:\n    Title: Billion-scale semi-super..."
          ],
          [
           "Weights: https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resne..."
          ],
          [
           "Weights: https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resne..."
          ],
          [
           "FBNet\n\n**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https://p..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `fbnetc_100`. You can find the ID..."
          ],
          [
           "Dual Path Network (DPN)\n\nA **Dual Path Network (DPN)** is a convolutional neural network which prese..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: DPN\n  Paper:\n    Title: Dual Path Networks\n    URL: http..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80...."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.83%\n      T..."
          ],
          [
           "Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76.31%\n      Top 5 Accuracy: 92.97%\n- Name: dpn..."
          ],
          [
           "Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.21%\n      Top 5 Accuracy: 94.42%\n- Name: dpn..."
          ],
          [
           "Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.99%\n      Top 5 Accuracy: 94.84%\n- Name: dpn..."
          ],
          [
           "(Tensorflow) MixNet\n\n**MixNet** is a type of convolutional neural network discovered via AutoML that..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_mixnet_l`. You can find the I..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: TF MixNet\n  Paper:\n    Title: 'MixConv: Mixed Depthwise ..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 78...."
          ],
          [
           "Top 5 Accuracy: 93.16%\n- Name: tf_mixnet_s\n  In Collection: TF MixNet\n  Metadata:\n    FLOPs: 3025876..."
          ],
          [
           "Inception ResNet v2\n\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on t..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `inception_resnet_v2`. You can fi..."
          ],
          [
           "(Gluon) SE-ResNeXt\n\n**SE ResNeXt** is a variant of a [ResNext](https://www.paperswithcode.com/method..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_seresnext101_32x4d`. You c..."
          ],
          [
           "(Tensorflow) Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_inception_v3`. You can find t..."
          ],
          [
           "SK-ResNeXt\n\n**SK ResNeXt** is a variant of a [ResNeXt](https://www.paperswithcode.com/method/resnext..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `skresnext50_32x4d`. You can find..."
          ],
          [
           "HRNet\n\n**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for ..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "(Legacy) SE-ResNeXt\n\n**SE ResNeXt** is a variant of a [ResNeXt](https://www.paperswithcode.com/metho..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `legacy_seresnext101_32x4d`. You ..."
          ],
          [
           "SE-ResNeXt\n\n**SE ResNeXt** is a variant of a [ResNext](https://www.paperswithcode.com/method/resneXt..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `seresnext26d_32x4d`. You can fin..."
          ],
          [
           "(Legacy) SE-ResNet\n\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/r..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `legacy_seresnet101`. You can fin..."
          ],
          [
           "EfficientNet\n\n**EfficientNet** is a convolutional neural network architecture and scaling method tha..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "Res2NeXt\n\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https://paperswithcod..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `res2next50`. You can find the ID..."
          ],
          [
           "ECA-ResNet\n\nAn **ECA ResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) t..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ecaresnet101d`. You can find the..."
          ],
          [
           "Instagram ResNeXt WSL\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/re..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ig_resnext101_32x16d`. You can f..."
          ],
          [
           "Res2Net\n\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find..."
          ],
          [
           "Scripts\n\nA train, validation, inference, and checkpoint cleaning script included in the github root ..."
          ],
          [
           "```\n\nTo run inference from a checkpoint:\n\n```bash\npython inference.py /imagenet/validation/ --model ..."
          ],
          [
           "```\n\n### SE-ResNeXt-26-D and SE-ResNeXt-26-T\n\nThese hparams (or similar) work well for a wide range ..."
          ],
          [
           "```\n### EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5\n\nThe training of this model starte..."
          ],
          [
           "```\n### ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5\n\nTrai..."
          ],
          [
           "```\n### MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5\n\n```bash\n./distributed_train.sh 2 /imagen..."
          ],
          [
           "ResNeSt\n\nA **ResNeSt** is a variant on a [ResNet](https://paperswithcode.com/method/resnet), which i..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `resnest101e`. You can find the I..."
          ],
          [
           "ResNet\n\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `resnet18`. You can find the IDs ..."
          ],
          [
           "NASNet\n\n**NASNet** is a type of convolutional neural network discovered through neural architecture ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `nasnetalarge`. You can find the ..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: NASNet\n  Paper:\n    Title: Learning Transferable Ar..."
          ],
          [
           "(Gluon) Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the Incep..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_inception_v3`. You can fin..."
          ],
          [
           "(Gluon) SENet\n\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and-..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_senet154`. You can find th..."
          ],
          [
           "```\n\n<!--\nType: model-index\nCollections:\n- Name: Gloun SENet\n  Paper:\n    Title: Squeeze-and-Excitat..."
          ],
          [
           "Vision Transformer (ViT)\n\nThe **Vision Transformer** is a model for image classification that employ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `vit_base_patch16_224`. You can f..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Vision Transformer\n  Paper:\n    Title: 'An Image is Wort..."
          ],
          [
           "Batch Size: 4096\n    Image Size: '224'\n    Warmup Steps: 10000\n    Weight Decay: 0.03\n    Interpolat..."
          ],
          [
           "- JFT-300M\n    Training Resources: TPUv3\n    ID: vit_base_patch16_384\n    Crop Pct: '1.0'\n    Moment..."
          ],
          [
           "Training Techniques:\n    - Cosine Annealing\n    - Gradient Clipping\n    - SGD with Momentum\n    Trai..."
          ],
          [
           "- Layer Normalization\n    - Multi-Head Attention\n    - Scaled Dot-Product Attention\n    - Tanh Activ..."
          ],
          [
           "Architecture:\n    - Attention Dropout\n    - Convolution\n    - Dense Connections\n    - Dropout\n    - ..."
          ],
          [
           "Metadata:\n    FLOPs: 174702764032\n    Parameters: 304720000\n    File Size: 1218907013\n    Architectu..."
          ],
          [
           "Top 1 Accuracy: 85.17%\n      Top 5 Accuracy: 97.36%\n- Name: vit_small_patch16_224\n  In Collection: V..."
          ],
          [
           "ResNet-D\n\n**ResNet-D** is a modification on the [ResNet](https://paperswithcode.com/method/resnet) a..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "PNASNet\n\n**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the struc..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `pnasnet5large`. You can find the..."
          ],
          [
           "AdvProp (EfficientNet)\n\n**AdvProp** is an adversarial training scheme which treats adversarial examp..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ap`. You can ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: AdvProp\n  Paper:\n    Title: Adversarial Examples Improve..."
          ],
          [
           "Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n..."
          ],
          [
           "Training Data:\n    - ImageNet\n    ID: tf_efficientnet_b1_ap\n    LR: 0.256\n    Epochs: 350\n    Crop P..."
          ],
          [
           "- Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n    - Image Clas..."
          ],
          [
           "Metadata:\n    FLOPs: 2275247568\n    Parameters: 12230000\n    File Size: 49384538\n    Architecture:\n ..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 81...."
          ],
          [
           "Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficie..."
          ],
          [
           "Interpolation: bicubic\n    RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n..."
          ],
          [
           "Training Data:\n    - ImageNet\n    ID: tf_efficientnet_b6_ap\n    LR: 0.256\n    Epochs: 350\n    Crop P..."
          ],
          [
           "- Dropout\n    - Inverted Residual Block\n    - Squeeze-and-Excitation Block\n    - Swish\n    Tasks:\n  ..."
          ],
          [
           "In Collection: AdvProp\n  Metadata:\n    FLOPs: 80962956270\n    Parameters: 87410000\n    File Size: 35..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 85...."
          ],
          [
           "MobileNet v2\n\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `mobilenetv2_100`. You can find t..."
          ],
          [
           "TResNet\n\nA **TResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) that aim..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tresnet_l`. You can find the IDs..."
          ],
          [
           "ESE-VoVNet\n\n**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https://paper..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ese_vovnet19b_dw`. You can find ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: ESE VovNet\n  Paper:\n    Title: 'CenterMask : Real-Time A..."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 76.82%\n      T..."
          ],
          [
           "CSP-ResNet\n\n**CSPResNet** is a convolutional neural network where we apply the Cross Stage Partial N..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "AdvProp (EfficientNet)\n\n**AdvProp** is an adversarial training scheme which treats adversarial examp..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ap`. You can ..."
          ],
          [
           "(Legacy) SENet\n\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `legacy_senet154`. You can find t..."
          ],
          [
           "(Gluon) SENet\n\nA **SENet** is a convolutional neural network architecture that employs [squeeze-and-..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_senet154`. You can find th..."
          ],
          [
           "SWSL ResNeXt\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-blo..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `swsl_resnext101_32x16d`. You can..."
          ],
          [
           "ResNeXt\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-block) t..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `resnext101_32x8d`. You can find ..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: ResNeXt\n  Paper:\n    Title: Aggregated Residual Transfor..."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.3%\n      To..."
          ],
          [
           "Top 5 Accuracy: 94.61%\n- Name: resnext50d_32x4d\n  In Collection: ResNeXt\n  Metadata:\n    FLOPs: 5781..."
          ],
          [
           "In Collection: ResNeXt\n  Metadata:\n    FLOPs: 5472648192\n    Parameters: 25030000\n    File Size: 100..."
          ],
          [
           "PyTorch Image Models\n- [What's New](#whats-new)\n- [Introduction](#introduction)\n- [Models](#models)\n..."
          ],
          [
           "â—Updates after Oct 10, 2022 are available in version >= 0.9â—\n* Many changes since the last 0.6.x sta..."
          ],
          [
           "* The Hugging Face Hub (https://huggingface.co/timm) is now the primary source for `timm` weights. M..."
          ],
          [
           "### Nov 23, 2023\n* Added EfficientViT-Large models, thanks [SeeFun](https://github.com/seefun)\n* Fix..."
          ],
          [
           "### Oct 20, 2023\n* [SigLIP](https://huggingface.co/papers/2303.15343) image tower weights supported ..."
          ],
          [
           "### Aug 28, 2023\n* Add dynamic img size support to models in `vision_transformer.py`, `vision_transf..."
          ],
          [
           "### Aug 25, 2023\n* Many new models since last release\n  * FastViT - https://arxiv.org/abs/2303.14189..."
          ],
          [
           "### Aug 11, 2023\n* Swin, MaxViT, CoAtNet, and BEiT models support resizing of image/window size on c..."
          ],
          [
           "### May 11, 2023\n* `timm` 0.9 released, transition from 0.8.xdev releases\n\n### May 10, 2023\n* Huggin..."
          ],
          [
           "### April 21, 2023\n* Gradient accumulation support added to train script and tested (`--grad-accum-s..."
          ],
          [
           "### April 5, 2023\n* ALL ResNet models pushed to Hugging Face Hub with multi-weight support\n  * All p..."
          ],
          [
           "| model                                                                                             ..."
          ],
          [
           "* Add EVA-02 MIM pretrained and fine-tuned weights, push to HF hub and update model cards for all EV..."
          ],
          [
           "| model                                              |top1  |top5  |param_count|img_size|\n|---------..."
          ],
          [
           "| eva_giant_patch14_336.clip_ft_in1k                 |89.466|98.82 |1013.01    |336     |\n| eva_larg..."
          ],
          [
           "* Multi-weight and HF hub for DeiT and MLP-Mixer based models\n\n### March 22, 2023\n* More weights pus..."
          ],
          [
           "### Feb 26, 2023\n* Add ConvNeXt-XXLarge CLIP pretrained image tower weights for fine-tune & features..."
          ],
          [
           "### Feb 7, 2023\n* New inference benchmark numbers added in [results](results/) folder.\n* Add convnex..."
          ],
          [
           "* Move ImageNet meta-data (synsets, indices) from `/results` to [`timm/data/_info`](timm/data/_info/..."
          ],
          [
           "### Jan 20, 2023\n* Add two convnext 12k -> 1k fine-tunes at 384x384\n  * `convnext_tiny.in12k_ft_in1k..."
          ],
          [
           "|model                                                                                              ..."
          ],
          [
           "|[maxvit_large_tf_512.in21k_ft_in1k](https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k) ..."
          ],
          [
           "|[coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_384.sw_in12k..."
          ],
          [
           "|[maxvit_large_tf_512.in1k](https://huggingface.co/timm/maxvit_large_tf_512.in1k)                   ..."
          ],
          [
           "|[maxvit_small_tf_384.in1k](https://huggingface.co/timm/maxvit_small_tf_384.in1k)                   ..."
          ],
          [
           "|[coatnet_rmlp_2_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_rmlp_2_rw_224.sw_in1k)         ..."
          ],
          [
           "|[maxvit_tiny_tf_224.in1k](https://huggingface.co/timm/maxvit_tiny_tf_224.in1k)                     ..."
          ],
          [
           "|[coatnet_bn_0_rw_224.sw_in1k](https://huggingface.co/timm/coatnet_bn_0_rw_224.sw_in1k)             ..."
          ],
          [
           "### Jan 11, 2023\n* Update ConvNeXt ImageNet-12k pretrain series w/ two new fine-tuned weights (and p..."
          ],
          [
           "| model                                     | top1 | param_count |  gmac | macts | hub              ..."
          ],
          [
           "### Oct 15, 2022\n* Train and validation script enhancements\n* Non-GPU (ie CPU) device support\n* SLUR..."
          ],
          [
           "### Sept 23, 2022\n* LAION-2B CLIP image towers supported as pretrained backbones for fine-tune or fe..."
          ],
          [
           "## Models\n\nAll model architecture families include variants with pretrained weights. There are speci..."
          ],
          [
           "* Aggregating Nested Transformers - https://arxiv.org/abs/2105.12723\n* BEiT - https://arxiv.org/abs/..."
          ],
          [
           "* DPN (Dual-Path Network) - https://arxiv.org/abs/1707.01629\n* EdgeNeXt - https://arxiv.org/abs/2206..."
          ],
          [
           "* EfficientViT (MSRA) - https://arxiv.org/abs/2305.07027\n* EVA - https://arxiv.org/abs/2211.07636\n* ..."
          ],
          [
           "* LeViT (Vision Transformer in ConvNet's Clothing) - https://arxiv.org/abs/2104.01136\n* MaxViT (Mult..."
          ],
          [
           "* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00..."
          ],
          [
           "* Weakly-supervised (WSL) Instagram pretrained / ImageNet tuned ResNeXt101 - https://arxiv.org/abs/1..."
          ],
          [
           "* TResNet - https://arxiv.org/abs/2003.13630\n* Twins (Spatial Attention in Vision Transformers) - ht..."
          ],
          [
           "## Features\n\nSeveral (less common) features that I often utilize in my projects are included. Many o..."
          ],
          [
           "* All models have a common default configuration interface and API for\n    * accessing/changing the ..."
          ],
          [
           "* PyTorch DistributedDataParallel w/ multi-gpu, single process (AMP disabled as it crashes when enab..."
          ],
          [
           "* `novograd` by [Masashi Kimura](https://github.com/convergence-lab/novograd) (https://arxiv.org/abs..."
          ],
          [
           "* CutMix (https://arxiv.org/abs/1905.04899)\n* AutoAugment (https://arxiv.org/abs/1805.09501) and Ran..."
          ],
          [
           "* Bottleneck Transformer - https://arxiv.org/abs/2101.11605\n    * CBAM - https://arxiv.org/abs/1807...."
          ],
          [
           "## Results\n\nModel validation results can be found in the [results tables](results/README.md)\n\n## Get..."
          ],
          [
           "## Awesome PyTorch Resources\n\nOne of the greatest assets of PyTorch is the community and their contr..."
          ],
          [
           "### Pretrained Weights\nSo far all of the pretrained weights available here are pretrained on ImageNe..."
          ],
          [
           "```\n\n### Latest DOI\n\n[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/lates..."
          ],
          [
           "ResNeXt\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-block) t..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `resnext101_32x8d`. You can find ..."
          ],
          [
           "Xception\n\n**Xception** is a convolutional neural network architecture that relies solely on [depthwi..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `xception`. You can find the IDs ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Xception\n  Paper:\n    Title: 'Xception: Deep Learning wi..."
          ],
          [
           "Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 79.05%\n      Top 5 Accuracy: 94.4%\n- Name: xcep..."
          ],
          [
           "Metadata:\n    FLOPs: 17585702144\n    Parameters: 39920000\n    File Size: 160536780\n    Architecture:..."
          ],
          [
           "- Convolution\n    - Dense Connections\n    - Depthwise Separable Convolution\n    - Global Average Poo..."
          ],
          [
           "Dual Path Network (DPN)\n\nA **Dual Path Network (DPN)** is a convolutional neural network which prese..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `dpn107`. You can find the IDs in..."
          ],
          [
           "(Gluon) ResNeXt\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_resnext101_32x4d`. You can..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Gloun ResNeXt\n  Paper:\n    Title: Aggregated Residual Tr..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80...."
          ],
          [
           "- Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 80.63%\n      T..."
          ],
          [
           "# Ensemble Adversarial Inception ResNet v2\n\n**Inception-ResNet-v2** is a convolutional neural archit..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. Yo..."
          ],
          [
           "(Gluon) ResNeXt\n\nA **ResNeXt** repeats a [building block](https://paperswithcode.com/method/resnext-..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_resnext101_32x4d`. You can..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "SSL ResNet\n\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the la..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ssl_resnet18`. You can find the ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: SSL ResNet\n  Paper:\n    Title: Billion-scale semi-superv..."
          ],
          [
           "Weights: https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet18-d92..."
          ],
          [
           "Weights: https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-083..."
          ],
          [
           "Xception\n\n**Xception** is a convolutional neural network architecture that relies solely on [depthwi..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "(Gluon) ResNet\n\n**Residual Networks**, or **ResNets**, learn residual functions with reference to th..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_resnet101_v1b`. You can fi..."
          ],
          [
           "(Tensorflow) EfficientNet Lite\n\n**EfficientNet** is a convolutional neural network architecture and ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_lite0`. You can ..."
          ],
          [
           "Models\n\n[[autodoc]] timm.create_model\n\n[[autodoc]] timm.list_models..."
          ],
          [
           "SPNASNet\n\n**Single-Path NAS** is a novel differentiable NAS method for designing hardware-efficient ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `spnasnet_100`. You can find the ..."
          ],
          [
           "MobileNet v3\n\n**MobileNetV3** is a convolutional neural network that is designed for mobile phone CP..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `mobilenetv3_large_100`. You can ..."
          ],
          [
           "EfficientNet (Knapsack Pruned)\n\n**EfficientNet** is a convolutional neural network architecture and ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `efficientnet_b1_pruned`. You can..."
          ],
          [
           "```\n\n```\n@misc{aflalo2020knapsack,\n      title={Knapsack Pruning with Inner Distillation},\n      aut..."
          ],
          [
           "Inception v4\n\n**Inception-v4** is a convolutional neural network architecture that builds on previou..."
          ],
          [
           "```\n\nTo get the top-5 predictions class names:\n```python\n# Get imagenet class mappings\nurl, filename..."
          ],
          [
           "```\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\nscr..."
          ],
          [
           "Noisy Student (EfficientNet)\n\n**Noisy Student Training** is a semi-supervised learning approach. It ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ns`. You can ..."
          ],
          [
           "<!--\nType: model-index\nCollections:\n- Name: Noisy Student\n  Paper:\n    Title: Self-training with Noi..."
          ],
          [
           "RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n    Stochastic Depth Surviv..."
          ],
          [
           "Training Data:\n    - ImageNet\n    - JFT-300M\n    Training Resources: Cloud TPU v3 Pod\n    ID: tf_eff..."
          ],
          [
           "- Average Pooling\n    - Batch Normalization\n    - Convolution\n    - Dense Connections\n    - Dropout\n..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 82...."
          ],
          [
           "RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n    Stochastic Depth Surviv..."
          ],
          [
           "- RandAugment\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    - JFT-300M\n    Training Resou..."
          ],
          [
           "Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolutio..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 86...."
          ],
          [
           "RMSProp Decay: 0.9\n    Label Smoothing: 0.1\n    BatchNorm Momentum: 0.99\n    Stochastic Depth Surviv..."
          ],
          [
           "- RandAugment\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    - JFT-300M\n    Training Resou..."
          ],
          [
           "Architecture:\n    - 1x1 Convolution\n    - Average Pooling\n    - Batch Normalization\n    - Convolutio..."
          ],
          [
           "Results:\n  - Task: Image Classification\n    Dataset: ImageNet\n    Metrics:\n      Top 1 Accuracy: 88...."
          ],
          [
           "Noisy Student (EfficientNet)\n\n**Noisy Student Training** is a semi-supervised learning approach. It ..."
          ],
          [
           "```\n\nTo load and preprocess the image:\n\n```py \n>>> import urllib\n>>> from PIL import Image\n>>> from ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0_ns`. You can ..."
          ],
          [
           "(Tensorflow) MixNet\n\n**MixNet** is a type of convolutional neural network discovered via AutoML that..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_mixnet_l`. You can find the I..."
          ],
          [
           "SSL ResNet\n\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the la..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ssl_resnet18`. You can find the ..."
          ],
          [
           "Adversarial Inception v3\n\n**Inception v3** is a convolutional neural network architecture from the I..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `adv_inception_v3`. You can find ..."
          ],
          [
           "SWSL ResNet\n\n**Residual Networks**, or **ResNets**, learn residual functions with reference to the l..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `swsl_resnet18`. You can find the..."
          ],
          [
           "(Tensorflow) EfficientNet\n\n**EfficientNet** is a convolutional neural network architecture and scali..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_b0`. You can fin..."
          ],
          [
           "ESE-VoVNet\n\n**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https://paper..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `ese_vovnet19b_dw`. You can find ..."
          ],
          [
           "(Gluon) Xception\n\n**Xception** is a convolutional neural network architecture that relies solely on ..."
          ],
          [
           "```\n\nReplace the model name with the variant you want to use, e.g. `gluon_xception65`. You can find ..."
          ]
         ],
         "hovertemplate": "source=pytorch-image-models<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "pytorch-image-models, circle",
         "marker": {
          "color": "#FF6692",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "pytorch-image-models, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -14.336165,
          -0.40152264,
          -19.71239,
          5.7481,
          -20.355627,
          -21.3595,
          -21.307632,
          -14.160178,
          -19.708933,
          -20.361134,
          -21.38369,
          -21.334093,
          -21.416067,
          -21.307777,
          -21.435547,
          -21.185333,
          -13.636922,
          -0.45796824,
          -19.713894,
          -23.254599,
          -20.265306,
          -14.099132,
          3.0283356,
          -23.252401,
          -20.311325,
          -21.319784,
          -21.349974,
          -21.390417,
          -21.46913,
          -21.423618,
          -21.292273,
          -14.3485,
          -19.712759,
          -14.037483,
          -19.712984,
          -20.347782,
          -13.903743,
          3.0280802,
          -23.253574,
          -20.370077,
          -21.334513,
          -21.367067,
          -21.38746,
          -21.31152,
          -13.749557,
          -0.2225082,
          -19.712587,
          -20.354858,
          -20.204018,
          -20.222109,
          -20.206526,
          -20.233883,
          -20.222214,
          -20.224646,
          -20.222654,
          -20.21479,
          -20.237328,
          -20.209948,
          -20.210722,
          -20.601545,
          -0.04066867,
          0.0062385728,
          -0.034052964,
          -0.08108253,
          0.03742764,
          -14.245212,
          -19.715841,
          -23.255747,
          -20.407711,
          -20.804428,
          -14.243553,
          3.0279925,
          -23.252466,
          -20.314268,
          -13.146136,
          -13.92269,
          -0.41258207,
          -19.712404,
          -11.901021,
          -11.973214,
          5.642709,
          -11.857764,
          -11.759096,
          -11.762142,
          -11.8475065,
          -11.844793,
          -11.870313,
          -11.72357,
          -11.816191,
          -11.585546,
          -11.834296,
          -11.890655,
          -11.81289,
          -11.863922,
          -11.877694,
          -11.895956,
          -11.826716,
          -11.893335,
          -11.822949,
          -11.831048,
          -11.781524,
          -11.8871155,
          -11.927004,
          -12.004454,
          -12.3876095,
          -13.912986,
          -19.715307,
          -23.254633,
          -20.365007,
          -21.544807,
          -21.446646,
          -21.5046,
          -14.386847,
          -19.718008,
          -23.25465,
          -20.495802,
          -20.865034,
          -21.265755,
          -21.001877,
          -21.364515,
          -11.815108,
          -12.851898,
          -12.608195,
          -12.707232,
          -12.9215765,
          -12.7976675,
          -12.472933,
          -12.467586,
          -12.709565,
          -12.736199,
          -12.233528,
          -13.308989,
          -14.147922,
          -19.714672,
          -23.255264,
          -20.405115,
          -21.443293,
          -21.474964,
          -21.45838,
          -20.825392,
          -20.612402,
          -21.443285,
          -21.39316,
          2.8103132,
          1.9364631,
          -13.942754,
          -19.711048,
          -20.386976,
          -21.37177,
          -21.360685,
          -21.300646,
          -21.360657,
          -21.424494,
          -21.40059,
          -14.195079,
          -19.71483,
          -23.253881,
          -20.400244,
          -21.368204,
          -21.33875,
          -21.343287,
          -21.217335,
          -12.341973,
          -19.714558,
          -20.258125,
          -20.56326,
          -21.013119,
          -13.656672,
          -19.715794,
          -23.253231,
          -20.216223,
          -13.384056,
          -0.35600567,
          -19.71133,
          1.0901837,
          -20.334557,
          -21.25154,
          -21.11235,
          -21.130316,
          -21.107494,
          -20.98004,
          -20.547144,
          -1.8573909,
          -11.964591,
          -11.942103,
          -11.970904,
          -11.9841385,
          -14.193358,
          3.0278404,
          -23.254452,
          -20.30816,
          -21.261015,
          -20.556568,
          -14.218583,
          3.028031,
          -23.253057,
          -20.33724,
          -20.393301,
          -20.595104,
          -13.685561,
          -19.715141,
          -20.254215,
          -14.086158,
          3.028136,
          -23.251968,
          -14.210691,
          -19.71229,
          -13.914208,
          3.0283573,
          -23.254566,
          -20.323643,
          -14.011347,
          -19.713184,
          -20.276333,
          -20.466501,
          -20.553577,
          -14.018714,
          -19.712523,
          -20.392244,
          -20.68804,
          -20.82687,
          -21.36163,
          -13.728438,
          3.028346,
          -23.254623,
          -14.23763,
          -19.712505,
          -20.32737,
          7.081566,
          -11.791265,
          -11.071367,
          -11.927047,
          -11.880219,
          -11.931562,
          -11.877331,
          -11.833829,
          -11.823012,
          -11.673237,
          -11.851908,
          -11.880325,
          -11.750137,
          -11.494542,
          -11.737913,
          -11.877854,
          -11.825743,
          14.033718,
          13.97747,
          -11.770538,
          14.038467,
          14.123083,
          14.151503,
          14.128194,
          14.149954,
          -2.3132637,
          14.01268,
          14.148016,
          14.157334,
          14.157658,
          -11.835548,
          -11.876789,
          -7.9065585,
          -11.807373,
          -12.0821705,
          -11.860056,
          -11.862487,
          -11.945612,
          4.782858,
          -14.272202,
          -19.712688,
          -20.337822,
          -21.366953,
          -21.292776,
          -21.407228,
          -21.431562,
          -21.167217,
          -20.607176,
          -21.336369,
          -21.366179,
          -21.40409,
          -14.028529,
          -19.712185,
          -20.336496,
          -13.635664,
          -0.47673082,
          -19.713665,
          -20.610296,
          -20.506746,
          -20.869137,
          -20.810589,
          -21.02085,
          -21.159342,
          -20.982828,
          -21.077301,
          -21.084732,
          0.7568051,
          -14.172819,
          -19.714525,
          -23.254797,
          -20.367767,
          -21.328579,
          -21.415651,
          -14.264123,
          3.0283113,
          -23.255888,
          -20.381697,
          -20.984766,
          -20.949875,
          -21.079147,
          -21.15506,
          -21.309614,
          -21.498545,
          -21.251379,
          -21.37195,
          -14.1658125,
          -19.711678,
          -13.922636,
          -19.714554,
          -23.25119,
          -13.694131,
          -19.712315,
          -20.34687,
          -12.334821,
          -19.716276,
          -14.25209,
          -19.712746,
          -20.30135,
          -21.301216,
          -21.351316,
          -21.425323,
          -21.384493,
          -21.417107,
          -20.636135,
          -20.465458,
          -11.637664,
          -11.536914,
          14.118945,
          14.242813,
          14.196265,
          14.252602,
          14.184438,
          -11.851723,
          -13.526819,
          3.0282016,
          -20.089767,
          -13.917418,
          -19.710365,
          -20.345078,
          -13.770264,
          3.0280871,
          -23.254229,
          -20.241257,
          -14.223083,
          -19.711996,
          -14.302162,
          -19.716692,
          -20.50609,
          -20.931042,
          -21.29916,
          -21.167353,
          -21.416357,
          -13.977835,
          -19.711302,
          -20.325462,
          -20.18408,
          -20.200752,
          -20.182848,
          -20.214737,
          -20.200974,
          -20.184532,
          -20.18239,
          -20.206724,
          -20.177975,
          -20.194687,
          -20.213074,
          -20.601902,
          2.103334,
          1.7734376,
          -14.070789,
          -19.715385,
          -13.878537,
          -19.712988,
          -14.255638,
          -19.715057,
          -20.448957,
          -21.336937,
          -21.349632,
          -21.361528,
          -21.387964,
          -13.843739,
          -19.713547,
          -20.281096,
          1.934026,
          1.798797,
          1.2666502,
          -13.790935,
          3.0278718,
          -23.252996,
          -20.45657,
          -20.6886,
          -20.63893,
          -20.636679,
          -20.546904,
          -20.642046,
          -20.69104,
          -20.742523,
          -20.758732,
          -20.549221,
          -20.566933,
          -20.494692,
          -20.600023,
          2.4101064,
          -13.964417,
          -19.712576,
          -20.373545,
          -13.605492,
          -19.71478,
          0.7843914,
          -0.831074,
          -11.890925,
          -12.791109,
          -12.728029,
          -12.806043,
          -12.778883,
          -12.787576,
          -12.78255,
          -12.553147,
          -11.395085,
          -13.382342,
          -13.58544,
          -19.712614,
          0.18643801,
          -0.15106583,
          -0.12697645,
          -0.13809475,
          -0.11165346,
          -14.275464,
          -19.71184,
          -20.45641,
          -20.51025,
          -20.679033,
          -20.974653,
          -20.944197,
          -20.805254,
          -20.847057,
          -20.854465,
          -20.922016,
          -21.004284,
          -21.010014,
          -21.04819,
          -21.018532,
          -21.161783,
          -13.629344,
          -19.716282,
          -23.25486,
          -20.111504,
          -14.264312,
          -19.71225,
          -20.500523,
          -20.47616,
          -20.92674,
          -21.07334,
          -21.097542,
          -21.004059,
          -21.232698,
          -12.473063,
          -19.71625,
          -20.378979,
          -20.541536,
          -20.831297,
          -21.05133,
          -21.183924,
          -13.996706,
          3.0281417,
          -23.25541,
          -12.229574,
          -19.716335,
          -20.415733,
          -20.508907,
          -20.873674,
          -21.243597,
          -21.337223,
          -21.181213,
          -21.19442,
          -21.15559,
          -21.273424,
          -8.982092,
          7.5845485,
          0.056419607,
          0.1540055,
          -14.365239,
          -19.710274,
          -20.31798,
          -21.36456,
          -21.447683,
          -21.473475,
          -14.011012,
          -19.712175,
          -13.946298,
          3.028202,
          -23.253265,
          -14.372525,
          -19.71171,
          -20.37635,
          -20.469181,
          -20.449842,
          -20.516813,
          -13.599875,
          -19.713636,
          -12.398135,
          -19.716589,
          -20.406572,
          -21.609076,
          -21.468273,
          -21.529217,
          -21.477785,
          -20.953957,
          -21.464258,
          -21.567175,
          -21.418325,
          -21.529406,
          -21.492004,
          -20.731703,
          -20.805994,
          -21.001875,
          -21.569063,
          -20.594437,
          -14.338672,
          -19.711092,
          -20.307575,
          -21.354557,
          -21.397825,
          -21.404139,
          -21.506426,
          -21.44296,
          -13.819949,
          -19.713373,
          -20.256598,
          -13.555956,
          -19.712242,
          -20.223955,
          -14.041762,
          -19.710417,
          -20.334991,
          -0.7152028,
          -19.706852,
          0.4191174,
          -0.7842,
          -13.927441,
          -0.10270019,
          -0.5680337,
          -2.5655308,
          -14.375156,
          -19.712053,
          -20.361782,
          -21.358292,
          -21.469154,
          -21.454988,
          -14.2401285,
          -19.716375,
          -23.25447,
          -20.464735,
          -21.050636,
          -20.894241,
          -14.024197,
          -19.711723,
          -13.622451,
          3.0280888,
          -23.255022,
          -20.360641,
          -20.44846,
          -20.724333,
          -21.016169,
          -20.97774,
          -20.91339,
          -14.002428,
          -19.71254,
          -20.406528,
          -20.576916,
          -20.97505,
          -13.645374,
          -19.713717,
          -14.392077,
          -19.71299,
          -13.656063,
          -19.715076,
          -14.273113,
          -19.711708,
          -13.700547,
          3.0283554,
          -23.253607,
          -14.3528595,
          -19.71114,
          -14.356473,
          -19.710552,
          -14.323649,
          -19.710875,
          -12.516014,
          3.0283208,
          -23.251928,
          -14.274933,
          -19.71266,
          -14.2691965,
          -19.713074,
          -14.209318,
          -19.713717,
          -14.160768,
          -19.715214,
          0.08526358,
          0.21192811,
          -11.861434,
          -11.881988,
          -11.812846,
          -11.886595,
          -14.077917,
          -19.71045,
          -14.237682,
          -19.710695,
          -13.9351225,
          -19.712126,
          -20.320173,
          -13.647192,
          -19.713305,
          -13.993773,
          -19.712627,
          -20.328161,
          -13.895287,
          -19.713715,
          -10.081213,
          -21.432217,
          -21.616983,
          -21.471,
          -21.514437,
          -21.578716,
          -21.602806,
          -21.362751,
          -14.2435665,
          3.0282478,
          -23.25484,
          -13.784681,
          -19.711657,
          -13.966875,
          -19.71699,
          -20.410494,
          -21.687607,
          -21.426514,
          -21.608593,
          -21.440199,
          -20.480774,
          -21.367771,
          -21.547358,
          -21.545378,
          -21.501163,
          -20.830147,
          -20.55455,
          -14.194238,
          -19.712542,
          -14.075804,
          -19.712355,
          -14.049714,
          -19.712236,
          -20.380985,
          -20.722729,
          -13.970952,
          3.028454,
          -13.968293,
          -19.71345,
          -14.035477,
          -19.712233,
          -14.062496,
          -19.71274,
          -14.353101,
          -19.71839,
          -14.315113,
          -19.715637,
          -23.255724,
          -20.46554,
          -20.74556,
          -20.941046,
          -20.814713,
          -11.478557,
          -11.903113,
          1.151991,
          -11.812582,
          -11.717823,
          2.3663573,
          -11.8034725,
          -11.768748,
          -11.759688,
          -11.824572,
          -11.73927,
          13.988208,
          -2.6088111,
          13.999834,
          14.155155,
          -11.825251,
          -11.83723,
          -11.712463,
          -11.469668,
          -11.766378,
          13.993836,
          14.172128,
          14.15646,
          14.156646,
          14.157526,
          14.155596,
          14.157273,
          14.159661,
          -11.892553,
          14.0045595,
          -11.82757,
          -11.457568,
          -2.5493526,
          -10.429816,
          -12.040075,
          -11.19913,
          -10.811132,
          -11.283764,
          -11.563196,
          -10.009283,
          3.6381295,
          -9.000888,
          -1.5907178,
          -1.8315122,
          -11.282181,
          -4.317558,
          -10.8687105,
          -11.557418,
          -11.307158,
          5.956586,
          -14.357393,
          -19.713066,
          -13.846265,
          -19.712997,
          -20.401821,
          -20.737547,
          -20.815825,
          -21.058334,
          -13.5256605,
          -19.711805,
          -14.318573,
          -19.713512,
          -20.410522,
          -20.542995,
          -20.662132,
          -13.485962,
          -19.715462,
          -14.311633,
          -19.715708,
          -23.254587,
          -14.268207,
          -19.714949,
          -20.40481,
          -21.083267,
          -20.854982,
          -13.840974,
          3.0282798,
          -23.25485,
          -14.27929,
          -19.715084,
          -12.396593,
          -19.713902,
          -0.55757433,
          -13.771773,
          -19.710333,
          -14.311629,
          -19.714262,
          -12.454333,
          -19.7148,
          0.89341605,
          -13.608975,
          3.0282974,
          -23.254759,
          -4.792775,
          -19.718271,
          -20.313034,
          -21.546602,
          -21.432703,
          -21.548862,
          -20.646473,
          -21.53011,
          -21.47371,
          -21.516563,
          -20.587313,
          -21.641628,
          -21.507114,
          -21.511417,
          -20.534857,
          -3.9843378,
          -13.918651,
          -19.715853,
          -14.003776,
          -19.71449,
          -14.277759,
          -19.715982,
          -13.639442,
          -19.715199,
          -14.23845,
          -19.716307,
          -12.468415,
          -19.716208,
          -14.022276,
          -19.713139,
          -13.881913,
          -19.71253
         ],
         "xaxis": "x",
         "y": [
          0.6311586,
          -3.2256384,
          -13.143035,
          -3.5769157,
          6.345977,
          6.829152,
          6.728412,
          0.59799093,
          -13.142198,
          6.40124,
          6.839994,
          6.9191456,
          6.8657856,
          6.863995,
          6.8494897,
          6.8800564,
          0.54832596,
          -3.2134383,
          -13.1446705,
          -6.346707,
          6.355574,
          0.5720074,
          -22.725006,
          -6.346423,
          6.3612227,
          6.848007,
          6.760448,
          6.730681,
          6.7303376,
          6.729313,
          6.7636967,
          0.6309451,
          -13.1437,
          0.56110656,
          -13.142041,
          6.39811,
          0.5349218,
          -22.725698,
          -6.346328,
          6.391303,
          6.843843,
          6.870257,
          6.85608,
          6.832285,
          0.50871587,
          -2.9303105,
          -13.143028,
          6.3938775,
          8.166933,
          8.138024,
          8.168873,
          8.114242,
          8.14053,
          8.138637,
          8.139207,
          8.156778,
          8.105338,
          8.167405,
          8.1678295,
          7.326774,
          -2.0215409,
          -2.0697014,
          -2.1123958,
          -2.1223497,
          -2.1015313,
          0.617299,
          -13.145768,
          -6.346716,
          6.4447207,
          6.7529397,
          0.61215043,
          -22.726042,
          -6.3471704,
          6.375118,
          0.35359788,
          0.5696338,
          -3.148698,
          -13.143644,
          0.572941,
          0.53288853,
          -2.8839414,
          0.58962566,
          0.49793994,
          0.4547783,
          0.57829815,
          0.5345356,
          0.53389466,
          0.41953364,
          0.57019377,
          0.5775411,
          0.47220677,
          0.5667545,
          0.5235513,
          0.5466834,
          0.53139526,
          0.5913997,
          0.5361942,
          0.56824833,
          0.53640944,
          0.56586224,
          0.55077016,
          0.5962273,
          0.5849433,
          0.4522401,
          -0.17174375,
          0.57232237,
          -13.145885,
          -6.3461146,
          6.387911,
          6.7133055,
          6.7403045,
          6.723917,
          0.6385847,
          -13.148224,
          -6.345747,
          6.477123,
          6.7049313,
          6.750555,
          6.710839,
          6.727462,
          0.55856746,
          0.37332326,
          0.3812647,
          0.38062063,
          0.38160333,
          0.33226264,
          0.29966816,
          0.35483518,
          0.34236002,
          0.32880133,
          0.20712095,
          0.48642015,
          0.5682068,
          -13.144573,
          -6.345411,
          6.4142013,
          6.750961,
          6.723997,
          6.7532597,
          6.7202263,
          7.306139,
          6.832347,
          6.769449,
          -0.09095533,
          -0.060761925,
          0.56618303,
          -13.141989,
          6.40776,
          6.778006,
          6.7038455,
          6.766095,
          6.7570486,
          6.7050567,
          6.7249756,
          0.5750712,
          -13.143174,
          -6.3471403,
          6.3938413,
          6.8926907,
          6.8496976,
          6.8504233,
          6.871475,
          -0.11880416,
          -13.145403,
          6.3356943,
          7.334186,
          6.868824,
          0.5522191,
          -13.145154,
          -6.3465295,
          6.313922,
          0.35075304,
          -3.1669183,
          -13.143718,
          -2.8108501,
          6.378345,
          6.8804297,
          6.8895154,
          6.933182,
          6.871022,
          6.8856797,
          7.4142256,
          2.5060399,
          0.7282095,
          0.7702133,
          0.75264835,
          0.67753136,
          0.56588,
          -22.723516,
          -6.3464413,
          6.375727,
          6.8124347,
          7.361606,
          0.6030665,
          -22.725145,
          -6.345242,
          6.3856673,
          7.7652707,
          7.332622,
          0.5468112,
          -13.145143,
          6.3380823,
          0.5751037,
          -22.726738,
          -6.346355,
          0.5657293,
          -13.142999,
          0.5223336,
          -22.725668,
          -6.346541,
          6.342436,
          0.5749129,
          -13.142104,
          6.3790627,
          7.450755,
          6.6770473,
          0.5538379,
          -13.144179,
          6.4306617,
          6.933333,
          6.7588687,
          6.7378674,
          0.47914478,
          -22.726063,
          -6.3461723,
          0.60636765,
          -13.143412,
          6.3750944,
          -3.4917681,
          0.55899906,
          0.0775321,
          0.5650196,
          0.60457176,
          0.56372166,
          0.5863627,
          0.6181972,
          0.50576043,
          0.4137336,
          0.52441823,
          0.58860564,
          0.49427405,
          0.29576963,
          0.60120535,
          0.5359856,
          0.5489447,
          7.967466,
          7.9190607,
          0.6020506,
          7.969253,
          8.03553,
          8.061526,
          8.0381565,
          8.060056,
          1.5397111,
          7.9517403,
          8.060596,
          8.066987,
          8.066682,
          0.63682026,
          0.52985096,
          1.0258746,
          0.55763024,
          0.5685515,
          0.6197834,
          0.58569556,
          0.57465976,
          -0.10977387,
          0.60890424,
          -13.142499,
          6.376416,
          6.7548695,
          6.7145925,
          6.727313,
          6.708958,
          6.829616,
          7.4621944,
          6.811532,
          6.7558074,
          6.725222,
          0.5513014,
          -13.14317,
          6.3776913,
          0.46975583,
          -3.1138551,
          -13.143716,
          6.502691,
          7.475658,
          6.9736376,
          6.972187,
          6.853568,
          6.8246613,
          6.856875,
          6.8523536,
          6.85029,
          7.7635484,
          0.5714061,
          -13.1447525,
          -6.345836,
          6.3936954,
          6.8407936,
          6.743609,
          0.62060064,
          -22.725065,
          -6.346948,
          6.416458,
          6.733849,
          6.6953173,
          6.606199,
          6.6166735,
          6.7431207,
          6.6509733,
          6.7050276,
          6.722196,
          0.5929232,
          -13.143803,
          0.55193365,
          -13.144956,
          -6.3461776,
          0.48441708,
          -13.142158,
          6.3839855,
          -0.189856,
          -13.146161,
          0.61038655,
          -13.144293,
          6.355701,
          6.742373,
          6.745952,
          6.721491,
          6.7236943,
          6.7284875,
          7.1197853,
          7.109608,
          0.54062855,
          0.6121705,
          8.041089,
          8.131515,
          8.095769,
          8.145405,
          8.087581,
          0.56913674,
          0.5561866,
          -22.72732,
          6.2679152,
          0.52936405,
          -13.142196,
          6.3752394,
          0.51909757,
          -22.725237,
          -6.345989,
          6.3311453,
          0.6089515,
          -13.143574,
          0.6182823,
          -13.146553,
          6.5035,
          6.859061,
          6.7556787,
          6.810814,
          6.7160907,
          0.54959375,
          -13.142723,
          6.3624496,
          8.218313,
          8.186376,
          8.2316475,
          8.159776,
          8.18854,
          8.234457,
          8.214591,
          8.194903,
          8.220748,
          8.218963,
          8.1623125,
          7.3193493,
          0.30175748,
          -0.09719253,
          0.55564314,
          -13.14333,
          0.5575943,
          -13.14187,
          0.60935986,
          -13.144202,
          6.3897953,
          6.7525697,
          6.7487445,
          6.7281,
          6.726043,
          0.5681115,
          -13.145244,
          6.3314404,
          -2.8746448,
          -1.3712085,
          -3.120801,
          0.515176,
          -22.725328,
          -6.345923,
          6.5426106,
          7.144562,
          7.172324,
          7.1757765,
          7.2277145,
          7.239047,
          7.178875,
          7.136659,
          7.1359615,
          7.375128,
          7.411237,
          7.4596086,
          7.326053,
          -4.0610576,
          0.5459298,
          -13.143024,
          6.3963957,
          0.53865445,
          -13.144908,
          -0.45188543,
          -0.7571043,
          0.5208368,
          0.35374627,
          0.40573165,
          0.38347152,
          0.35887364,
          0.37688085,
          0.386401,
          0.29497063,
          0.17627183,
          0.53739816,
          0.46764937,
          -13.142345,
          -1.9162909,
          -1.9449812,
          -2.0086615,
          -2.2553825,
          -1.9639375,
          0.6168206,
          -13.144234,
          6.442387,
          7.404125,
          7.0064726,
          6.7544475,
          6.8034806,
          6.7644763,
          6.746065,
          6.652161,
          6.602269,
          6.6281133,
          6.6561484,
          6.585364,
          6.64553,
          6.7297997,
          0.5481339,
          -13.145071,
          -6.34652,
          6.288168,
          0.61160856,
          -13.142965,
          6.484817,
          7.4758,
          6.8161097,
          6.5829196,
          6.6577153,
          6.6312485,
          6.7307005,
          -0.13185306,
          -13.145579,
          6.3667593,
          7.3706827,
          6.6815267,
          6.6474853,
          6.659306,
          0.5450994,
          -22.722742,
          -6.3463316,
          -0.17377807,
          -13.146724,
          6.4129515,
          7.451411,
          6.727635,
          6.628267,
          6.6670136,
          6.7116704,
          6.706721,
          6.7005296,
          6.699649,
          0.68202776,
          -3.9002135,
          2.882409,
          3.4437544,
          0.63002,
          -13.143328,
          6.3522477,
          6.7392483,
          6.7080007,
          6.674252,
          0.56071085,
          -13.14352,
          0.5269365,
          -22.726639,
          -6.3456554,
          0.62859845,
          -13.143581,
          6.392007,
          7.4654737,
          7.4194913,
          7.3937736,
          0.48162183,
          -13.143982,
          -0.104470976,
          -13.146318,
          6.4172206,
          6.7147994,
          6.726567,
          6.7109194,
          6.7229896,
          6.9123893,
          6.7810364,
          6.688301,
          6.7475715,
          6.7273865,
          6.741077,
          6.9804873,
          6.7117705,
          6.6960464,
          6.7279034,
          7.332376,
          0.6268585,
          -13.143214,
          6.3675995,
          6.834576,
          6.7364626,
          6.7386584,
          6.7172484,
          6.713956,
          0.56816936,
          -13.145014,
          6.3413205,
          0.5401769,
          -13.145104,
          6.3209743,
          0.559009,
          -13.1412735,
          6.3656344,
          -0.40612793,
          -13.139826,
          -2.5127144,
          -1.8490962,
          0.56234044,
          -2.9727101,
          -4.884769,
          1.441076,
          0.6305394,
          -13.143917,
          6.3829613,
          6.73324,
          6.7171345,
          6.702096,
          0.6013063,
          -13.14588,
          -6.345619,
          6.4618034,
          6.895726,
          7.03607,
          0.5482025,
          -13.142159,
          0.4702137,
          -22.724495,
          -6.3454804,
          6.4044843,
          7.586579,
          7.0644484,
          6.819568,
          6.8480606,
          6.873871,
          0.5525426,
          -13.145757,
          6.434289,
          7.3137794,
          6.791603,
          0.55018586,
          -13.144392,
          0.63652825,
          -13.144066,
          0.5494268,
          -13.144712,
          0.61189985,
          -13.141974,
          0.49713132,
          -22.725777,
          -6.3462095,
          0.62747455,
          -13.144004,
          0.62771624,
          -13.143258,
          0.6255337,
          -13.144922,
          -0.059341095,
          -22.72587,
          -6.3463187,
          0.6164919,
          -13.142969,
          0.6088295,
          -13.143421,
          0.59983563,
          -13.142055,
          0.60138065,
          -13.145548,
          2.6656168,
          3.391235,
          0.72689146,
          0.84205157,
          0.7479129,
          0.84769046,
          0.5586534,
          -13.142608,
          0.61776954,
          -13.143289,
          0.53531,
          -13.142268,
          6.3334966,
          0.550862,
          -13.1448345,
          0.5567833,
          -13.143459,
          6.3602104,
          0.5431081,
          -13.141797,
          -0.31512874,
          6.7314205,
          6.656595,
          6.717632,
          6.737553,
          6.710371,
          6.6853466,
          6.742738,
          0.61153895,
          -22.726046,
          -6.346429,
          0.4972184,
          -13.141868,
          0.5648191,
          -13.147403,
          6.4395647,
          6.6589994,
          6.7039523,
          6.709208,
          6.7207904,
          7.4526105,
          6.803282,
          6.7130737,
          6.6969166,
          6.718716,
          6.702673,
          7.3578115,
          0.5741545,
          -13.143472,
          0.56506777,
          -13.142407,
          0.56090605,
          -13.140954,
          6.44956,
          6.8696327,
          0.5441441,
          -22.725374,
          0.5619867,
          -13.146959,
          0.55409396,
          -13.142629,
          0.5735656,
          -13.143718,
          0.62388164,
          -13.147753,
          0.622599,
          -13.146311,
          -6.346714,
          6.4732037,
          6.9154344,
          6.8026886,
          6.783176,
          0.49374175,
          0.62093157,
          -0.52851117,
          0.5717068,
          0.4934044,
          5.7591,
          0.583747,
          0.5307253,
          0.6126642,
          0.56210977,
          0.58635277,
          7.9261622,
          0.78193,
          7.936362,
          8.064338,
          0.5485969,
          0.5336557,
          0.5014099,
          0.47927353,
          0.45156628,
          7.9317107,
          8.080398,
          8.067467,
          8.066291,
          8.068918,
          8.066509,
          8.067697,
          8.064524,
          0.55518484,
          7.943887,
          0.5913708,
          0.56092197,
          -0.6703495,
          -0.4056202,
          0.18613668,
          -0.1320004,
          -0.1970642,
          0.12420455,
          0.17083743,
          -0.781311,
          -2.274381,
          0.35406432,
          2.5968301,
          2.4569807,
          -0.06296741,
          2.3759584,
          0.5597918,
          0.43571067,
          0.3450529,
          -3.6056442,
          0.6307582,
          -13.143913,
          0.5640788,
          -13.144757,
          6.3612213,
          6.682104,
          6.6888022,
          6.738601,
          0.44333845,
          -13.143018,
          0.62697446,
          -13.1452,
          6.433029,
          7.3439074,
          7.0002923,
          0.52370083,
          -13.145045,
          0.624695,
          -13.145291,
          -6.347508,
          0.6186984,
          -13.14466,
          6.447038,
          6.91596,
          7.066125,
          0.5680792,
          -22.727003,
          -6.345527,
          0.62558925,
          -13.145075,
          -0.117297605,
          -13.146852,
          -1.1924025,
          0.51288766,
          -13.142794,
          0.58626956,
          -13.142906,
          -0.19436221,
          -13.145785,
          -4.7213135,
          0.5405383,
          -22.726282,
          -6.3467035,
          -0.5597663,
          -13.148307,
          6.307388,
          6.7124543,
          6.7392945,
          6.697199,
          7.267488,
          6.713902,
          6.7238383,
          6.7225466,
          7.3375797,
          6.7137313,
          6.7101817,
          6.723091,
          7.3979483,
          -0.48789325,
          0.56844765,
          -13.146589,
          0.5561246,
          -13.14527,
          0.6183558,
          -13.145661,
          0.54221267,
          -13.144961,
          0.61402124,
          -13.146467,
          -0.118947536,
          -13.147178,
          0.54632926,
          -13.142097,
          0.5782947,
          -13.143627
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "--\ntitle: \"Large Language Models: A New Moore's Law?\"\nthumbnail: /blog/assets/33_large_language_mode..."
          ],
          [
           "### Deep Learning, Deep Pockets?\n\nAs you would expect, training a 530-billion parameter model on hum..."
          ],
          [
           "I'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? ..."
          ],
          [
           "Downsizing efforts are also under way in the Natural Language Processing community, using transfer l..."
          ],
          [
           "You guessed it, that's another way to do transfer learning, and it'll help you save on everything!\n ..."
          ],
          [
           "However, the Machine Learning community is still struggling with this topic, and for good reason. Op..."
          ],
          [
           "Instead of chasing trillion-parameter models (place your bets), wouldn't all be better off if we bui..."
          ],
          [
           "--\ntitle: \"Why weâ€™re switching to Hugging Face Inference Endpoints, and maybe you should too\"\nthumbn..."
          ],
          [
           "Now, you can reasonably argue that ECS was not the best approach to serving ML models, but it served..."
          ],
          [
           "- Requester region: eu-east-1\n- Requester instance size: t3-medium\n- Inference endpoint region: eu-e..."
          ],
          [
           "```\nWhat we see from these results is pretty encouraging. The application that will consume these en..."
          ],
          [
           "```\n\nWe can say a couple of things about this. Firstly, we want a managed solution to deployment, we..."
          ],
          [
           "## Other considerations\n\n### Deployment Options\n\nCurrently you can deploy an Inference Endpoint from..."
          ],
          [
           "```\n\nFor me, whatâ€™s lacking is a [custom terraform provider](https://www.hashicorp.com/blog/writing-..."
          ],
          [
           "--\ntitle: \"DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub\" \nthumbnail: /blog/assets..."
          ],
          [
           "We are happy to share that we recently added another feature to help you analyze datasets on the Hub..."
          ],
          [
           "```\n\nCreate a connection to DuckDB and install and load the `httpfs` extension to allow reading and ..."
          ],
          [
           "```\n\nTo learn more, check out the [documentation](https://huggingface.co/docs/datasets-server/parque..."
          ],
          [
           "--\ntitle: \"Building an AI WebTV\"\nthumbnail: /blog/assets/156_ai_webtv/thumbnail.gif\nauthors:\n- user:..."
          ],
          [
           "The individual video sequences are purposely made to be short, meaning the WebTV should be seen as a..."
          ],
          [
           "ðŸ‘‰Â  You will need to use the same prompt for both the generation and upscaling.\n\n## Calling the video..."
          ],
          [
           "export const generateVideo = async (prompt: string) => {\n  const api = await client(\"*** URL OF THE ..."
          ],
          [
           "```\n\n\n## Post-processing\n\nOnce an individual take (a video clip) is upscaled, it is then passed to F..."
          ],
          [
           "let playlist = 'ffconcat version 1.0\\n'\nallFilePaths.forEach(filePath => {\n  playlist += `file '${fi..."
          ],
          [
           "```\n\nThis will generate the following playlist content:\n\n```bash\nffconcat version 1.0\nfile 'video1.m..."
          ],
          [
           "```\n\nThere are many different configuration options for FFmpeg, for more information in the [officia..."
          ],
          [
           "<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\n   <video\n      alt=\"demo5...."
          ],
          [
           "We've seen it with large language models and their ability to synthesize convincing content that mim..."
          ],
          [
           "<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\n   <video\n      alt=\"demo18..."
          ],
          [
           "<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\n   <video\n      alt=\"demo2...."
          ],
          [
           "**Wrong direction:** the model sometimes has trouble with movement and direction. For instance, here..."
          ],
          [
           "**Text or objects inserted into the image:** the model sometimes injects words from the prompt into ..."
          ],
          [
           "## Maintaining consistency between scenes\n\nIf you plan to create sequences of multiple videos, you w..."
          ],
          [
           "--\ntitle: Fine tuning CLIP with Remote Sensing (Satellite) images and captions\nthumbnail: /blog/asse..."
          ],
          [
           "Over the next two weeks, teams participated in lectures from Hugging Face and Google, trained one or..."
          ],
          [
           "The ability to search through large collections of images using text queries is an immensely powerfu..."
          ],
          [
           "In addition, we used the [UCM Dataset](https://mega.nz/folder/wCpSzSoS#RXzIlrv--TDt3ENZdKN8JA) and t..."
          ],
          [
           "#### Data Augmentation\n\nIn order to regularize our dataset and prevent overfitting due to the size o..."
          ],
          [
           "### Evaluation\n\n#### Metrics\n\nA subset of the RSICD test set was used for evaluation. We found 30 ca..."
          ],
          [
           "| Model-name                               | k=1   | k=3   | k=5   | k=10  |\n| ---------------------..."
          ],
          [
           "| bs128x8-lr5e-5-imgaugs-textaugs-3/ckpt-5 | 0.823 | 0.946 | 0.971 | 0.992 |\n| bs128x8-lr5e-5-wd02/c..."
          ],
          [
           "_1 - our best model, 2 - our second best model_\n\n\n#### Demo\n\nYou can access the [CLIP-RSICD Demo](ht..."
          ],
          [
           "--\ntitle: \"Multivariate Probabilistic Time Series Forecasting with Informer\" \nthumbnail: /blog/asset..."
          ],
          [
           "## Introduction\n\nA few months ago we introduced the [Time Series Transformer](https://huggingface.co..."
          ],
          [
           "## Informer - Under The Hood\n\nBased on the vanilla Transformer ([Vaswani et al., 2017](https://arxiv..."
          ],
          [
           "### ProbSparse Attention\n\nThe main idea of ProbSparse is that the canonical self-attention scores fo..."
          ],
          [
           "$$\n\\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}} )V\n$$\n\nWhere \\\\(Q\\in \\math..."
          ],
          [
           "This is good! But how can we select the \\\\(u\\\\) \"active\" queries to create \\\\(Q_{reduce}\\\\)? Let's d..."
          ],
          [
           "But how can we calculate the term \\\\(q_ik_j^T\\\\) in non-quadratic time? Recall that most of the dot-..."
          ],
          [
           "# calculate u to find the Top-u queries under the sparsity measurement\n    u = min(sampling_factor *..."
          ],
          [
           "```\nNote that in the implementation, \\\\(U_{part}\\\\) contain \\\\(L_Q\\\\) in the calculation, for stabil..."
          ],
          [
           "Let's see this in code:\n    \n```python\nfrom torch import nn\n\n# ConvLayer is a class with forward pas..."
          ],
          [
           "```\n    \nBy reducing the input of each layer by two, we get a memory usage of \\\\(O(N\\cdot T \\log T)\\..."
          ],
          [
           "```\n\n## Load Dataset\n\nIn this blog post, we'll use the `traffic_hourly` dataset, which is available ..."
          ],
          [
           "```\n\nEach example contains a few keys, of which `start` and `target` are the most important ones. Le..."
          ],
          [
           "```\n\nThe initial values are exactly the same as the corresponding training example. However, this ex..."
          ],
          [
           "```\n\nWe now use `datasets`' [`set_transform`](https://huggingface.co/docs/datasets/v2.7.0/en/package..."
          ],
          [
           "```\n\n## Define the Model\n\nNext, let's instantiate a model. The model will be trained from scratch, h..."
          ],
          [
           "```\n\nThis means that this would look back up to 721 hours (~30 days) for each time step, as addition..."
          ],
          [
           "```\n\nIn this case, there are four additional features, namely \"hour of day\", \"day of week\", \"day of ..."
          ],
          [
           "```\n\nNote that hours and days are encoded as values between `[-0.5, 0.5]` from GluonTS. For more inf..."
          ],
          [
           "```\n\n## Define Transformations\n\nNext, we define the transformations for the data, in particular for ..."
          ],
          [
           "```\n\nThe transformations below are annotated with comments, to explain what they do. At a high level..."
          ],
          [
           "return Chain(\n        # step 1: remove static/dynamic fields if not specified\n        [RemoveFields(..."
          ],
          [
           "),\n            # step 4: add temporal features based on freq of the dataset\n            # these serv..."
          ],
          [
           "FieldName.TARGET: \"values\",\n                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n        ..."
          ],
          [
           "```\n\n## Define `InstanceSplitter`\n\nFor training/validation/testing we next create an `InstanceSplitt..."
          ],
          [
           "return InstanceSplitter(\n        target_field=\"values\",\n        is_pad_field=FieldName.IS_PAD,\n     ..."
          ],
          [
           "```\n\n## Create DataLoaders\n\nNext, it's time to create the DataLoaders, which allow us to have batche..."
          ],
          [
           "# we initialize a Training instance\n    instance_splitter = create_instance_splitter(config, \"train\"..."
          ],
          [
           "```\n\n\n```python\ndef create_backtest_dataloader(\n    config: PretrainedConfig,\n    freq,\n    data,\n  ..."
          ],
          [
           "transformation = create_transformation(freq, config)\n    transformed_data = transformation.apply(dat..."
          ],
          [
           "```\n\n\n```python\ntrain_dataloader = create_train_dataloader(\n    config=config,\n    freq=freq,\n    da..."
          ],
          [
           "```\n\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the ..."
          ],
          [
           "```\n\nNote that the model is returning a loss. This is possible as the decoder automatically shifts t..."
          ],
          [
           "model, optimizer, train_dataloader = accelerator.prepare(\n    model,\n    optimizer,\n    train_datalo..."
          ],
          [
           "```\n\n```python\n# view training\nloss_history = np.array(loss_history).reshape(-1)\nx = range(loss_hist..."
          ],
          [
           "```\n\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/infor..."
          ],
          [
           "```\n\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `i..."
          ],
          [
           "```\n\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values pr..."
          ],
          [
           "```\n\n\n```python\nplt.scatter(mase_metrics, smape_metrics, alpha=0.2)\nplt.xlabel(\"MASE\")\nplt.ylabel(\"s..."
          ],
          [
           "```\n\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/infor..."
          ],
          [
           "```\n\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/infor..."
          ],
          [
           "So the vanilla Transformer still performs best here! In the future, we hope to better benchmark thes..."
          ],
          [
           "--\ntitle: \"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\"..."
          ],
          [
           "For example, the [IMDB dataset](https://huggingface.co/datasets/imdb) specifies `en` in the YAML met..."
          ],
          [
           "However, there is a major caveat to this. Most datasets (around 87%) do not specify the language use..."
          ],
          [
           "### Predicting the Languages of Datasets Using Machine Learning\n\nWeâ€™ve already seen that many of the..."
          ],
          [
           "```\n\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole datase..."
          ],
          [
           "We pass 20 examples to the model representing rows from a dataset. This results in 20 individual lan..."
          ],
          [
           "We discard the script information since this isn't currently captured consistently as metadata on th..."
          ],
          [
           "#### Next Steps \n\nAs the number of datasets on the Hub grows, metadata becomes increasingly importan..."
          ],
          [
           "--\ntitle: \"Generating Human-level Text with Contrastive Search in Transformers ðŸ¤—\"\nthumbnail: /blog/a..."
          ],
          [
           "**[Remark]** For users who are not familiar with text generation, please refer more details to [this..."
          ],
          [
           "```\n\n****\n\n<span id='problems_of_decoding_methods'/>\n\n### 4. Problems of Existing Decoding Methods:\n..."
          ],
          [
           "```\n\n<details open>\n<summary><b>Model Output:</b></summary>\n\n```\nOutput:\n---------------------------..."
          ],
          [
           "```\n</details>\n\n**[Remark]** From the result generated by greedy search, we can see obvious pattern ..."
          ],
          [
           "```\n\n<details open>\n<summary><b>Model Output:</b></summary>\n\n```\nOutput:\n---------------------------..."
          ],
          [
           "```\n</details>\n\n**[Remark]** While nucleus sampling can generate text free of repetitions, the seman..."
          ],
          [
           "<center class=\"half\">\n    <img src=\"assets/115_introducing_contrastive_search/formulation.png\" width..."
          ],
          [
           "<span id='contrastive_generation'/>\n\n#### 5.2. Generating Text with Contrastive Search:\n\nBelow, we u..."
          ],
          [
           "```\n\nThe arguments are as follows:\n* `--top_k`: The hyperparameter \\\\(k\\\\) in contrastive search.\n* ..."
          ],
          [
           "```\nOutput:\n----------------------------------------------------------------------------------------..."
          ],
          [
           "\"The game of Go is a complex game in which players have to be very careful not to overextend their\nt..."
          ],
          [
           "```\n</details>\n\n**[Remark]** We see that the generated text is of exceptionally high quality. The en..."
          ],
          [
           "****\n\n<span id='more_examples'/>\n\n### 6. More Generated Examples:\n\nIn this section, we provide more ..."
          ],
          [
           "```\n</details>\n\n<span id='gpt2_greedy_example_one'/>\n\n##### 6.1.1. Generating Text with Greedy Searc..."
          ],
          [
           "```\nOutput:\n----------------------------------------------------------------------------------------..."
          ],
          [
           "The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\nare..."
          ],
          [
           "```\n</details>\n\n<span id='gpt2_nucleus_example_one'/>\n\n##### 6.1.2. Generating Text with Nucleus Sam..."
          ],
          [
           "```\nOutput:\n----------------------------------------------------------------------------------------..."
          ],
          [
           "The scientists think that it could be ancient folklore that has survived and is no longer attributed..."
          ],
          [
           "```\n</details>\n\n\n<span id='gpt2_contrastive_example_one'/>\n\n##### 6.1.3. Generating Text with Contra..."
          ],
          [
           "```\nOutput:\n----------------------------------------------------------------------------------------..."
          ],
          [
           "After analyzing the data, the team determined that the herd consisted of at least three species of u..."
          ],
          [
           "```\n</details>\n\n\n<span id='opt_example_two'/>\n\n#### 6.2. Example Two - OPT:\n\nIn this part, we use th..."
          ],
          [
           "```\n</details>\n\n<details>\n<summary><b>Model Output: [click to expand]</b></summary>\n\n```\nOutput:\n---..."
          ],
          [
           "```\n</details>\n\n\n<span id='opt_greedy_example_two'/>\n\n##### 6.2.2. Generating Text with Nucleus Samp..."
          ],
          [
           "```\n</details>\n\n\n\n<span id='opt_contrastive_example_two'/>\n\n##### 6.2.3. Generating Text with Contra..."
          ],
          [
           "```\n</details>\n\n<details open>\n<summary><b>Model Output:</b></summary>\n\n```\nOutput:\n----------------..."
          ],
          [
           "```\n</details>\n\n****\n\n<span id='resources'/>\n\n### 7. Resources:\n\nFor more details of contrastive sea..."
          ],
          [
           "```\n\n\n\n****\n\n<span id='references'/>\n\n## Reference:\n> [1] Su et al., 2022 [\"A Contrastive Framework ..."
          ],
          [
           "--\ntitle: \"AMD + ðŸ¤—: Large Language Models Out-of-the-Box Acceleration with AMD GPU\"\nthumbnail: /blog..."
          ],
          [
           "```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"01-..."
          ],
          [
           "```\n\nOne of the major aspects we have been working on is the ability to run Hugging Face Transformer..."
          ],
          [
           "* Flash Attention v2 from AMD Open Source efforts in [ROCmSoftwarePlatform/flash-attention](https://..."
          ],
          [
           "We are very excited to make these state of the art acceleration tools available and easy to use to H..."
          ],
          [
           "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <img alt=\"\" src=\"assets/optimum_amd/trans..."
          ],
          [
           "Performance-wise, we spent a lot of time benchmarking Text Generation Inference on AMD Instinct GPUs..."
          ],
          [
           "Missing bars for A100 correspond to out of memory errors, as Llama 70B weights 138 GB in float16, an..."
          ],
          [
           "Of course we'll soon be working on performance optimization for the MI300 lineup, ensuring that both..."
          ],
          [
           "--\ntitle: \"Machine Learning Experts - Lewis Tunstall\"\nthumbnail: /blog/assets/60_lewis_tunstall_inte..."
          ],
          [
           "*Note: Transcription has been slightly modified/reformatted to deliver the highest-quality reading e..."
          ],
          [
           "This collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been he..."
          ],
          [
           "So for example, if you're trying to build a chatbot you need this model to be very fast and responsi..."
          ],
          [
           "OpenAI actually provided in their blog posts some examples of the essays that this model had created..."
          ],
          [
           "### You and other experts at Hugging Face have been working hard on the Hugging Face Course. How did..."
          ],
          [
           "And he actually used that to apply to Hugging Face.\n\n### No way?!\n\n**Lewis:** He's joining the Big S..."
          ],
          [
           "So this accelerates the whole field in a really powerful way. And I can imagine these applications u..."
          ],
          [
           "### That is super interesting and powerful.\n\n**Lewis:** Maybe one thing to mention is that the whole..."
          ],
          [
           "Although that may work, a lot of the time what happens is you introduce a lot of complexity into the..."
          ],
          [
           "### If you could go back and do one thing differently at the beginning of your career in machine lea..."
          ],
          [
           "### What are some of the industries you're most excited to see machine learning applied? \n\n**Lewis:*..."
          ],
          [
           "I think there's hope that in my lifetime I will have a laundry-folding robot.\n\n### What have you bee..."
          ],
          [
           "### What are some of your favorite Machine Learning papers?\n\n**Lewis:** Depends on how we measure th..."
          ],
          [
           "But this example showed that you can actually be quite creative and help mathematicians find new ide..."
          ],
          [
           "**Lewis:** So when Oâ€™Reilly is telling you â€œWe're going to get our illustrator now to design the cov..."
          ],
          [
           "### I love it. Well, it looks absolutely amazing. A lot of these types of books tend to be quite dry..."
          ],
          [
           "**Lewis:** See ya, Britney. Bye.\n\nThank you for listening to Machine Learning Experts!\n\n<a href=\"htt..."
          ],
          [
           "--\ntitle: 'Welcome fastai to the Hugging Face Hub'\nthumbnail: /blog/assets/64_fastai/fastai_hf_blog...."
          ],
          [
           "Because of all this, and more (the writer of this post started his journey thanks to the fast.ai cou..."
          ],
          [
           "![Fastai Models in the Hub](assets/64_fastai/hf_hub_fastai.png)\n\nIn addition to free model hosting a..."
          ],
          [
           "```\n\n## Creating a fastai `Learner`\n\nHere we train the [first model in the fastbook](https://github...."
          ],
          [
           "```\n\n3. Use the `token` argument of the `push_to_hub_fastai` function.\n\nYou can input `push_to_hub_f..."
          ],
          [
           "```\n\nThe `Learner` is now in the Hub in the repo named [`espejelomar/identify-my-cat`](https://huggi..."
          ],
          [
           "First, upload an image of a cat (or possibly a dog?). The [Colab notebook with this tutorial](https:..."
          ],
          [
           "```\nIt works ðŸ‘‡!\n\n```py\n_,_,probs = learner.predict(img)\nprint(f\"Probability it's a cat: {100*probs[1..."
          ],
          [
           "```\n\n```python\nimport torch\nimport transformers\nfrom fastai.text.all import *\n\nfrom blurr.text.data...."
          ],
          [
           "```\n\nTry it with a couple sentences and review their sentiment (negative or positive) with `learner_..."
          ],
          [
           "```\n\n\n## What's next?\n\nTake the [fast.ai course](https://course.fast.ai/) (a new version is coming s..."
          ],
          [
           "--\ntitle: \"StackLLaMA: A hands-on guide to train LLaMA with RLHF\" \nthumbnail: /blog/assets/138_stack..."
          ],
          [
           "By combining these approaches, we are releasing the StackLLaMA model. This model is available on the..."
          ],
          [
           "## Stack Exchange dataset\n\nGathering human feedback is a complex and expensive endeavor. In order to..."
          ],
          [
           "## Efficient training strategies\n\nEven training the smallest LLaMA model requires an enormous amount..."
          ],
          [
           "In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters (depending on the..."
          ],
          [
           "```bash\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py\nto..."
          ],
          [
           "```\n\n## Supervised fine-tuning\n\nBefore we start training reward models and tuning our model with RL,..."
          ],
          [
           "```python\n# load model in 8bit\nmodel = AutoModelForCausalLM.from_pretrained(\n        args.model_path..."
          ],
          [
           "```\n\nWe train the model for a few thousand steps with the causal language modeling objective and sav..."
          ],
          [
           "A trick that works well instead of direct feedback is training a reward model on human annotations c..."
          ],
          [
           "```python\nclass RewardTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=Fal..."
          ],
          [
           "```\n\nWe utilize a subset of a 100,000 pair of candidates and evaluate on a held-out set of 50,000. W..."
          ],
          [
           "```\n\nThe same template was used for SFT, RM and RLHF stages.\n\nA common issue with training the langu..."
          ],
          [
           "# Compute sentiment score\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n   ..."
          ],
          [
           "```\n\nWe train for 20 hours on 3x8 A100-80GB GPUs, using the ðŸ¤— research cluster, but you can also get..."
          ],
          [
           "In general in RL, you want to achieve the highest reward. In RLHF we use a Reward Model, which is im..."
          ],
          [
           "One needs to be careful when generating the responses and we suggest to always use a simple sampling..."
          ],
          [
           "## Citation\n\n```bibtex\n@misc {beeching2023stackllama,\n    author       = { Edward Beeching and\n     ..."
          ],
          [
           "```\n\n## Acknowledgements\n\nWe thank Philipp Schmid for sharing his wonderful [demo](https://huggingfa..."
          ],
          [
           "--\ntitle:  Deploy LLMs with Hugging Face Inference Endpoints\nthumbnail: /blog/assets/155_inference_e..."
          ],
          [
           "Before we start, let's refresh our knowledge about Inference Endpoints. \n\n## What is Hugging Face In..."
          ],
          [
           "You can get started with Inference Endpoints at: [https://ui.endpoints.huggingface.co/](https://ui.e..."
          ],
          [
           "![Select Instance Type](assets/155_inference_endpoints_llm/instance-selection.png \"Select Instance T..."
          ],
          [
           "```\n\nYou can use different parameters to control the generation, defining them in the `parameters` a..."
          ],
          [
           "## 3. Stream responses in Javascript and Python\n\nRequesting and generating text with LLMs can be a t..."
          ],
          [
           "```\n\nWe can create a `InferenceClient` providing our endpoint URL and credential alongside the hyper..."
          ],
          [
           "```\n\nWe can create a `HfInferenceEndpoint` providing our endpoint URL and credential alongside the h..."
          ],
          [
           "```\n\nReplace the `process.stdout` call with the `yield` or with a function you want to stream the to..."
          ],
          [
           "--\ntitle: \"2D Asset Generation: AI for Game Development #4\"\nthumbnail: /blog/assets/124_ml-for-games..."
          ],
          [
           "Requirements:\n- Your preferred image-editing software, such as [Photoshop](https://www.adobe.com/pro..."
          ],
          [
           "In this section, I'll walk through how I generated a corn icon for the farming game. As a starting p..."
          ],
          [
           "<div align=\"center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "[Dreambooth](https://dreambooth.github.io/), [textual inversion](https://textual-inversion.github.io..."
          ],
          [
           "--\ntitle: \"Supercharged Customer Service with Machine Learning\"\nthumbnail: /blog/assets/61_superchar..."
          ],
          [
           "Assuming that a) messages of very unsatisfied customers represent only a fraction of all messages an..."
          ],
          [
           "Let's take a look at all available Datasets on the [Hugging Face Hub](https://huggingface.co/dataset..."
          ],
          [
           "Now we can inspect those datasets in more detail by reading through the dataset card, which ideally ..."
          ],
          [
           "Let's quickly go over the dataset cards of the models above:\n\n-   *GLUE* is a collection of small da..."
          ],
          [
           "As a final note, we recommend making use of Hub's dataset functionality even when working with priva..."
          ],
          [
           "Let's take a look at all models that have been fine-tuned on Amazon Reviews Multi. You can find the ..."
          ],
          [
           "However, both of the above resources are currently suboptimal. The model summary is not always kept ..."
          ],
          [
           "## Training / Fine-tuning a model with ðŸ¤— Transformers and ðŸ¤— Datasets\n\nIn this section, we will jump ..."
          ],
          [
           "```\n\nAlso, we install the ðŸ¤— Transformers and ðŸ¤— Datasets libraries to run this notebook. Since we wil..."
          ],
          [
           "```\n\n\n\n### Preprocess the dataset\n\nBefore we can start training the model, we should bring the datas..."
          ],
          [
           "```\n\n\n\nGreat, that was fast ðŸ”¥. Let's take a look at the structure of the dataset.\n\n\n```python\nprint(..."
          ],
          [
           "```\n\n**Output:**\n```\n    Stars: 1\n    Review: This product caused severe burning of my skin. I have ..."
          ],
          [
           "```\n\n\n\n\nAs mentioned before, we will use the `\"review_body\"` as the model's input and `\"stars\"` as t..."
          ],
          [
           "```\n\n\nTo apply this function to all data samples in our dataset, we use the [`map`](https://huggingf..."
          ],
          [
           "```\n\n**Output:**\n```\n    Input IDS: [1, 329, 714, 2044, 3567, 5127, 265, 312, 1158, 260, 273, 286, 4..."
          ],
          [
           "```\n\n```\n    Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when in..."
          ],
          [
           "```\n\n\n\nNext, we load a data collator. A [data collator](https://huggingface.co/docs/transformers/mai..."
          ],
          [
           "```\n\nNext, we define the `compute_metrics` which will be applied to the predicted outputs of the mod..."
          ],
          [
           "```\n\n\nPutting it all together, we can finally instantiate the Trainer by passing all required compon..."
          ],
          [
           "**Output:**\n<div>\n<table><p>\n  <tbody>\n <tr style=\"text-align: left;\">\n  <td>Step</td>\n  <td>Trainin..."
          ],
          [
           "<td>0.910928</td>\n    <td>0.608400</td>\n  </tr>\n  <tr>\n    <td>30000</td>\n    <td>0.806700</td>\n    ..."
          ],
          [
           "**Output:**..."
          ],
          [
           "```\n    ***** Running Evaluation *****\n      Num examples = 5000\n      Batch size = 8\n    Saving mod..."
          ],
          [
           "```\n\n### Evaluate / Analyse the model\n\nNow that we have fine-tuned the model we need to be very care..."
          ],
          [
           "```\n    ***** Running Prediction *****\n      Num examples = 5000\n      Batch size = 8\n```\n\n\n**Output..."
          ],
          [
           "```\n\n\n\nThe results are very similar to performance on the validation dataset, which is usually a goo..."
          ],
          [
           "# Second let's compute how many satisfied messages we unnecessarily reply to\n    satisfied_label_idx..."
          ],
          [
           "```\n\n\nWe again instantiate the `Trainer` to easily run the evaluation.\n\n\n```python\ntrainer = Trainer..."
          ],
          [
           "```\n\n\nand again upload everything on the Hub.\n\n\n```python\ntrainer.push_to_hub()\n```\n\n**Output:**\n```..."
          ],
          [
           "```\n\n\n\nThe data is now saved [here](https://huggingface.co/patrickvonplaten/deberta_amazon_reviews_v..."
          ],
          [
           "If you're looking for **highly optimized** solutions which don't require any technical knowledge, yo..."
          ],
          [
           "--\ntitle: \"Accelerating Document AI\" \nthumbnail: /blog/assets/112_document-ai/thumbnail.png\nauthors:..."
          ],
          [
           "There are at least six general use cases for building document AI solutions. These use cases differ ..."
          ],
          [
           "![png](assets/112_document-ai/ocr.png)\n\nOCR is a backbone of Document AI use cases as it's essential..."
          ],
          [
           "A basic approach is applying OCR on a document image, after which a [BERT](https://huggingface.co/do..."
          ],
          [
           "<html itemscope itemtype=\"https://schema.org/FAQPage\">\n  <div itemscope itemprop=\"mainEntity\" itemty..."
          ],
          [
           "![png](assets/112_document-ai/DIT.png)\n\nDocument layout analysis with DiT.\n\nDocument layout analysis..."
          ],
          [
           "The first version of LayoutLM (now known as LayoutLMv1) was released in 2020 and dramatically improv..."
          ],
          [
           "![png](assets/112_document-ai/layoutlm.png)\n\nData scientists are finding document layout analysis an..."
          ],
          [
           "The approach for table detection and structure recognition is similar to document layout analysis in..."
          ],
          [
           "</div>\n    </div>\n        </div>\n\n<html itemscope itemtype=\"https://schema.org/FAQPage\">\n  <div item..."
          ],
          [
           "DocVQA is typically evaluated using the Average Normalized Levenshtein Similarity (ANLS) metric. For..."
          ],
          [
           "<html itemscope itemtype=\"https://schema.org/FAQPage\">\n  <div itemscope itemprop=\"mainEntity\" itemty..."
          ],
          [
           "Data preparation for Document AI is critical and challenging. It's crucial to have properly annotate..."
          ],
          [
           "The flexibility of building your models leads to many options for data scientists. Our strong recomm..."
          ],
          [
           "</div>\n    </div>\n        </div>\n\n\n### Next Steps\n\nAre you seeing the possibilities of Document AI? ..."
          ],
          [
           "A table of the currently available Transformers models achieving state-of-the-art performance on Doc..."
          ],
          [
           "| model | paper | license | checkpoints |\n| --- | --- | --- | --- |\n| [Donut](https://huggingface.co..."
          ],
          [
           "| [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) | [arxiv](http..."
          ],
          [
           "| [LiLT](https://huggingface.co/docs/transformers/main/en/model_doc/lilt) | [arxiv](https://arxiv.or..."
          ],
          [
           "</div>\n    </div>\n        </div>\n\n<html itemscope itemtype=\"https://schema.org/FAQPage\">\n  <div item..."
          ],
          [
           "</div>\n    </div>\n        </div>\n\n </html>..."
          ],
          [
           "--\ntitle: \"How we sped up transformer inference 100x for ðŸ¤— API customers\"\nthumbnail: /blog/assets/09..."
          ],
          [
           "-| Naive version                                                                                    ..."
          ],
          [
           "## Compilation FTW: the hard to get 10x\nNow this is where it gets really tricky. In order to get the..."
          ],
          [
           "To reach that bar, as Machine Learning Engineers at Hugging Face we certainly have an unfair advanta..."
          ],
          [
           "--\ntitle: \"Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chine..."
          ],
          [
           "In addition, the Chinese AI community has been actively engaged in creating trendy Spaces, such as [..."
          ],
          [
           "We are excited to announce that we will continue to strengthen our ties with the Chinese AI communit..."
          ],
          [
           "--\ntitle: \"How to generate text: using different decoding methods for language generation with Trans..."
          ],
          [
           "This blog post gives a brief overview of different decoding strategies\nand more importantly shows ho..."
          ],
          [
           "```\n\n``` python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntorch_dev..."
          ],
          [
           "```\n\n\n## Greedy Search\n\nGreedy search is the simplest decoding method.\nIt selects the word with the ..."
          ],
          [
           "```\n\n```\nOutput:\n-----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n\n\nAlright\\! We have generated our first short text with GPT2 ðŸ˜Š. The\ngenerated words following t..."
          ],
          [
           "<img src=\"/blog/assets/02_how-to-generate/beam_search.png\" alt=\"beam search\" style=\"margin: auto; di..."
          ],
          [
           "```\n\n```\nOutput:\n-----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n```\nOutput:\n-----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n```\nOutput:\n-----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n\nAs can be seen, the five beam hypotheses are only marginally different\nto each other - which s..."
          ],
          [
           "$$ w_t \\sim P(w|w_{1:t-1}) $$\n\nTaking the example from above, the following graphic visualizes langu..."
          ],
          [
           "```\n\n```\nOutput:\n-----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n\n\nInteresting\\! The text seems alright - but when taking a closer look, it\nis not very coherent..."
          ],
          [
           "```\n\n```\nOutput:\n-----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n\n\nOK. There are less weird n-grams and the output is a bit more coherent\nnow\\! While applying t..."
          ],
          [
           "Let's see how *Top-K* can be used in the library by setting `top_k=50`:\n\n\n\n``` python\n# set seed to ..."
          ],
          [
           "```\n\n```\nOutput:\n-----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n\n\nNot bad at all\\! The text is arguably the most *human-sounding* text so\nfar. One concern thou..."
          ],
          [
           "<img src=\"/blog/assets/02_how-to-generate/top_p_sampling.png\" alt=\"Top p sampling\" style=\"margin: au..."
          ],
          [
           "```\n\n```\nOutput:\n-----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n\n```\nOutput:\n----------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n\nCool, now you should have all the tools to let your model write your\nstories with `transformer..."
          ],
          [
           "## Appendix\n\n`generate` has evolved into a highly composable method, with flags to manipulate the re..."
          ],
          [
           "--\ntitle: \"Accelerate your models with ðŸ¤— Optimum Intel and OpenVINO\"\nthumbnail: /blog/assets/113_ope..."
          ],
          [
           "â€‹Let us show you how to get started in minutes!â€‹\n\n## Quantizing a Vision Transformer with Optimum In..."
          ],
          [
           "```\n\nNext, moving to a Python environment, we import the appropriate modules and download the origin..."
          ],
          [
           "```\n\nAs usual with image datasets, we need to apply the same image transformations that were used at..."
          ],
          [
           "```\n\nWe're now ready to quantize the model. The `OVQuantizer.quantize()` method quantizes the model ..."
          ],
          [
           "```\n\nA minute or two later, the model has been quantized. We can then easily load it with our [`OVMo..."
          ],
          [
           "```\n\nâ€‹To verify that quantization did not have a negative impact on accuracy, we applied an evaluati..."
          ],
          [
           "```\n\nLooking at the quantized model, we see that its memory size decreased by **3.8x** from 344MB to..."
          ],
          [
           "--\ntitle: Guiding Text Generation with Constrained Beam Search in ðŸ¤— Transformers\nthumbnail: /blog/as..."
          ],
          [
           "However, this is actually a very non-trivial problem. This is because the task requires us to force ..."
          ],
          [
           "And what if you have multiple constraints with varying requirements? What if you want to force the p..."
          ],
          [
           "```\n!pip install -q git+https://github.com/huggingface/transformers.git\n```\n\n\n```python\nfrom transfo..."
          ],
          [
           "```\n\n    Output:\n    -------------------------------------------------------------------------------..."
          ],
          [
           "```\n\n    Output:\n    -------------------------------------------------------------------------------..."
          ],
          [
           "force_word = \"scared\"\nforce_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]\n\nforce_words_i..."
          ],
          [
           "```\n\n    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n    Output:\n    -..."
          ],
          [
           "![Beam search step 1](https://raw.githubusercontent.com/huggingface/blog/main/assets/53_constrained_..."
          ],
          [
           "Let's say that we're trying to force the phrase `\"is fast\"` in the generated output. \n\nIn the tradit..."
          ],
          [
           "Banks solve this problem by creating a *balance* between fulfilling the constraints and creating sen..."
          ],
          [
           "And finally notice how we ended up at a sensible output that contains our constraint phrase: `\"The d..."
          ],
          [
           "print(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))..."
          ],
          [
           "```\n\n    Output:\n    -------------------------------------------------------------------------------..."
          ],
          [
           "```\n\nor:\n```python\nstarting_text = \"The woman\"\ntemplate = [\"the\", \"\", \"\", \"University\", \"\", \"in\"]\n\np..."
          ],
          [
           "Thanks to everybody that gave guidance for this feature contribution: Patrick von Platen for being i..."
          ],
          [
           "--\ntitle: \"Making a web app generator with open ML models\"\nthumbnail: /blog/assets/153_text_to_webap..."
          ],
          [
           "Some of those techniques are now available as ready-to-use NPM libraries:\n\n- Using AI/ML libraries s..."
          ],
          [
           "## Architecture\n\nWe are going to use NodeJS to create our generative AI web server.\n\nThe model will ..."
          ],
          [
           "```\n\nThen, we can install the Hugging Face Inference client:\n\n```html\nnpm install @huggingface/infer..."
          ],
          [
           "```\n\nYou can now tell the inference client to use our private endpoint and call our model:\n\n```javas..."
          ],
          [
           "```\n\nStart your web server:\n\n```bash\nnpm run start\n```\n\nand open `https://localhost:3000?prompt=some..."
          ],
          [
           "```\n\n### Preventing hallucination\n\nIt can be difficult to reliably prevent hallucinations and failur..."
          ],
          [
           "```\n\n## Adding support for images\n\nWe now have a system that can generate HTML, CSS and JS code, but..."
          ],
          [
           "```\n\nYou can also try to be more specific, for example:\n\n```\nOnly generate a few images and use desc..."
          ],
          [
           "```\n\nTo make this work, you will have to make some changes:\n\n```javascript\n...\n\n// going to localhos..."
          ],
          [
           "```\n\n## Going further\n\nThe final demo Space includes a [more complete example](https://huggingface.c..."
          ],
          [
           "--\ntitle: 'Liftoff! How to get started with your first ML project ðŸš€'\nthumbnail: /blog/assets/84_firs..."
          ],
          [
           "> Compute dense vector representations for sentences, paragraphs, and images\n\nIn a nutshell, Sentenc..."
          ],
          [
           "Comparing sentences by similarity means that if we have a collection of sentences or paragraphs, we ..."
          ],
          [
           "Second, Sentence Transformers is an accessible entry-point to many important ML concepts that you ca..."
          ],
          [
           "Third, embeddings are key for several industrial applications. Google searches use embeddings to [ma..."
          ],
          [
           "1. **Do a brain dump of everything you know the toolâ€™s capable of**: For Sentence Transformers this ..."
          ],
          [
           "4. **Ideate:** Spend some time brainstorming on what different combination of the elements from the ..."
          ],
          [
           "For my first Sentence Transformers project, I remembered that I had a little dataset of popular song..."
          ],
          [
           "<div class=\"hidden xl:block\">\n<div style=\"display: flex; flex-direction: column; align-items: center..."
          ],
          [
           "## What can you expect to learn from your first project?\n\nSince every project is unique, your learni..."
          ],
          [
           "Further reading:\n\n- [Getting Started with Embeddings](https://huggingface.co/blog/getting-started-wi..."
          ],
          [
           "--\ntitle: \"Fit More and Train Faster With ZeRO via DeepSpeed and FairScale\"\nthumbnail: /blog/assets/..."
          ],
          [
           "This blog post will describe how you can benefit from ZeRO regardless of whether you own just a sing..."
          ],
          [
           "```\nexport BS=16\npython -m torch.distributed.launch --nproc_per_node=2 ./finetune_trainer.py \\\n--mod..."
          ],
          [
           "```\n\nWe are just using the `DistributedDataParallel` (DDP) and nothing else to boost the performance..."
          ],
          [
           "Let's look at the results of these six test runs:\n\n| Method                    | max BS |  train tim..."
          ],
          [
           "If you would like to experiment with this benchmark yourself or want to know more details about the ..."
          ],
          [
           "```\nexport BS=1\nCUDA_VISIBLE_DEVICES=0 ./finetune_trainer.py \\\n--model_name_or_path t5-3b --n_train ..."
          ],
          [
           "```\net voila! We get a batch size of 20 trained just fine. I could probably push it even further. Th..."
          ],
          [
           "```\nWe can't compare these to the baseline, since the baseline won't even start and immediately fail..."
          ],
          [
           "This idea could be difficult to grasp, and you will find my attempt at an explanation [here](https:/..."
          ],
          [
           "```\nRuntimeError: CUDA out of memory. Tried to allocate 1.48 GiB (GPU 0; 23.65 GiB total capacity;\n1..."
          ],
          [
           "```\nThe program wants to allocate ~1.5GB and the GPU still has some 6-7GBs of unused memory, but it ..."
          ],
          [
           "You can, of course, modify your own trainer to integrate DeepSpeed and FairScale, based on each proj..."
          ],
          [
           "* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/ab..."
          ],
          [
           "We were quite astonished at the amazing level of support we received from the FairScale and DeepSpee..."
          ],
          [
           "--\ntitle: \"Ethics and Society Newsletter #1\" \nthumbnail: /blog/assets/103_ethics-soc-1/thumbnail.png..."
          ],
          [
           "To this end, we share some of our recent thinking and work in the new Hugging Face _Ethics and Socie..."
          ],
          [
           "- We ground the creation of these tools and artifacts in _responsibility_ for the impacts of what we..."
          ],
          [
           "Building from these basics, we are taking an approach to operationalizing values that center the con..."
          ],
          [
           "In the coming months, we will be putting together several other pieces on values, tensions, and ethi..."
          ],
          [
           "--\ntitle: \"Open LLM Leaderboard: DROP deep dive\"\nthumbnail: /blog/assets/evaluating-mmlu-leaderboard..."
          ],
          [
           "We added it to the Open LLM Leaderboard three weeks ago, and observed that the f1-scores of pretrain..."
          ],
          [
           "Normalization happens in several steps, both for generation and gold:\n1) **Split on separators** `|`..."
          ],
          [
           "## Diving into the results\nExtending our investigations, our friends at [Zeno](https://zenoml.com) j..."
          ],
          [
           "We hypothesized that both these problems could be fixed by using `\\n` instead of `.` as an end of ge..."
          ],
          [
           "In 10% of the cases, the gold answer is a floating number (for example `12.25`) and model prediction..."
          ],
          [
           "Thanks to the many community members who pointed out issues on DROP scores, and many thanks to the E..."
          ],
          [
           "--\ntitle: \"Evaluating Language Model Bias with ðŸ¤— Evaluate\"\nthumbnail: /blog/assets/112_evaluating-ll..."
          ],
          [
           "The workflow has two main steps:\n- Prompting the language model with a predefined set of prompts (ho..."
          ],
          [
           "```python\n>>> male_prompts = [\n'The janitor reprimanded the accountant because he',\n'The carpenter a..."
          ],
          [
           "```\n\nAlthough we define these prompts directly for the sake of example here, more can be extracted d..."
          ],
          [
           "```\nAs you can see above, a simple difference in pronoun can result in a higher toxicity ratio for f..."
          ],
          [
           "```\n\nAnd as before, we use GPT-2 to generate completions:\n```python\n>>> profession1_completions = [\"..."
          ],
          [
           "```\nBased on the Regard scores above, the completions for profession 1 (truck drivers) have a more n..."
          ],
          [
           "```\n\nHigher HONEST scores mean more hurtful completions. Based on the model completions above, we ha..."
          ],
          [
           "*- Written by Sasha Luccioni and Meg Mitchell, drawing on work from the Evaluate crew and the Societ..."
          ],
          [
           "--\ntitle: \"Can foundation models label data like humans?\"\nthumbnail: /blog/assets/llm-leaderboard/le..."
          ],
          [
           "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/llm-leaderboa..."
          ],
          [
           "## Evaluating preferences of open-source models\n\nAny point in a training process where humans are ne..."
          ],
          [
           "To do this, we curated a held-out set of instruction prompts and completions from a popular set of o..."
          ],
          [
           "With these completions, we set off to evaluate the quality of the models with Scale AI and GPT-4. \nT..."
          ],
          [
           "****************Elo rankings without ties (bootstrapped from 1000 rounds of sampling games)*********..."
          ],
          [
           "****************Elo rankings w/ ties (bootstrapped from 1000 rounds of sampling games)**************..."
          ],
          [
           "**Elo rankings w/ ties (bootstrapped from 1000 rounds of sampling games)**\n\n*Reminder, in the Likert..."
          ],
          [
           "```\n### Question\n{question}\n\n### The Start of Assistant 1's Answer\n{answer_1}\n### The End of Assista..."
          ],
          [
           "```\n\nThe histogram of responses from GPT-4 starts to show a clear issue with LLM based evaluation: *..."
          ],
          [
           "## Related work\n\nWe are not the only ones to share the GPT-4 may not be a perfect tool for training ..."
          ],
          [
           "Below weâ€™ve included a couple examples of what the evaluations look like to give you a sense why and..."
          ],
          [
           "I am excited about what lies ahead and can't wait to join the team at [Company Name]. Thank you agai..."
          ],
          [
           "This answer only takes up 34 characters compared to longer explanations like sunlight reaching earth..."
          ],
          [
           "### Ablations\n\n**GPT-4 Elo with score rather than ranking**\n\nOther evaluation benchmarks use a ranki..."
          ],
          [
           "```\n\nThis resulted in the histogram of rankings below, which flipped the bias from before (but did n..."
          ],
          [
           "## Takeaways and discussion\n\nThere is a lot here, but the most important insights in our experiments..."
          ],
          [
           "Continuing with this, it is worth noting that ChatGPT (a slightly less high performance model) actua..."
          ],
          [
           "- **Correct generation parameters**: in the early stages of our experiments, we had to spend substan..."
          ],
          [
           "### Resources and citation\n\n- More information on our labeling instructions can be found [here](http..."
          ],
          [
           "```\n@article{rajani2023llm_labels,\n  author = {Rajani, Nazneen, and Lambert, Nathan and Han, Sheon a..."
          ],
          [
           "--\ntitle: \"Student Ambassador Programâ€™s call for applications is open!\"\nthumbnail: /blog/assets/67_a..."
          ],
          [
           "ðŸŽŽ Network of peers with whom ambassadors can collaborate. \n\nðŸ§‘ðŸ»â€ðŸ’» Workshops and support from the Hugg..."
          ],
          [
           "--\ntitle: \"Training a language model with ðŸ¤—Â Transformers using TensorFlow and TPUs\"\nthumbnail: /blog..."
          ],
          [
           "Unlike our Colab example, however, this example is designed to be **scalable** and much closer to a ..."
          ],
          [
           "## What to expect\n\nWeâ€™re going to train a [RoBERTa](https://huggingface.co/docs/transformers/model_d..."
          ],
          [
           "## Getting the data and training a tokenizer\n\nAs mentioned, we used the [WikiText dataset (v1)](http..."
          ],
          [
           "## Tokenizing the data and creating TFRecords\n\nOnce the tokenizer is trained, we can use it on all t..."
          ],
          [
           "## Training a model on data in GCS\n\nIf youâ€™re familiar with using ðŸ¤—Â Transformers, then you already k..."
          ],
          [
           "```\n\nBut since weâ€™re in the TPU territory, we need to perform this initialization under a strategy s..."
          ],
          [
           "```\n\nSimilarly, the optimizer also needs to be initialized under the same strategy scope with which ..."
          ],
          [
           "```\n\nIf `args.dataset` contains the `gs://` identifier, TensorFlow will understand that it needs to ..."
          ],
          [
           "[{'score': 0.1003185287117958,\n  'token': 52,\n  'token_str': 'be',\n  'sequence': 'Goal of my life is..."
          ],
          [
           "```\n\n## Conclusion\n\nIf thereâ€™s one thing we want to emphasize with this example, itâ€™s that TPU train..."
          ],
          [
           "--\ntitle: \"A Dive into Vision-Language Models\"\nthumbnail: /blog//assets/128_vision_language_pretrain..."
          ],
          [
           "## Table of contents\n\n1. [Introduction](#introduction)\n2. [Learning Strategies](#learning-strategies..."
          ],
          [
           "To predict something like that, the model needs to understand both the input image and the text prom..."
          ],
          [
           "Weâ€™ll cover the following themes in the pre-training objectives: \n- **Contrastive Learning:** Aligni..."
          ],
          [
           "Contrastive learning is a commonly used pre-training objective for vision models and has proven to b..."
          ],
          [
           "### 2) PrefixLM\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/docume..."
          ],
          [
           "Models that leverage a unified multi-modal architecture to fuse visual information into a language m..."
          ],
          [
           "Models such as [Frozen](https://arxiv.org/abs/2106.13884) and [ClipCap](https://arxiv.org/abs/2111.0..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "### 4) Masked-Language Modeling / Image-Text Matching\n\nAnother line of vision-language models uses a..."
          ],
          [
           "For the ITM objective, given an image and caption pair, the task is to predict whether the caption m..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "### Pre-training datasets\n\nVision-language models are typically pre-trained on large multi-modal dat..."
          ],
          [
           "Even image-text datasets consisting solely of human-generated captions, such as Flickr30K, are inher..."
          ],
          [
           "Models fine-tuned on the question-answering downstream task, such as [ViLT](https://arxiv.org/abs/21..."
          ],
          [
           "Note that vision-language models are used for various classical NLP and computer vision tasks such a..."
          ],
          [
           "* [CLIP](https://huggingface.co/docs/transformers/model_doc/clip)\n* [FLAVA](https://huggingface.co/d..."
          ],
          [
           "* [TrOCR](https://huggingface.co/docs/transformers/main/en/model_doc/trocr) (an instance of the `Vis..."
          ],
          [
           "While models such as CLIP, FLAVA, BridgeTower, BLIP, LiT and `VisionEncoderDecoder` models provide j..."
          ],
          [
           "Letâ€™s go ahead and experiment with some of these models. We will use [ViLT](https://huggingface.co/d..."
          ],
          [
           "```\n\nNext, we will download a random image of two cats and preprocess both the image and our  query ..."
          ],
          [
           "```\n\nStraight-forward, right? Letâ€™s do another demonstration with CLIPSeg and see how we can perform..."
          ],
          [
           "```\n\nSimilar to ViLT, itâ€™s important to refer to the [original work](https://arxiv.org/abs/2112.1000..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "We also see a massive surge of works that leverage joint vision-language representations for image m..."
          ],
          [
           "While robotics research hasnâ€™t leveraged vision-language models on a wide scale yet, we see works su..."
          ],
          [
           "We are continuing to integrate the most impactful computer vision and multi-modal models and would l..."
          ],
          [
           "--\ntitle: \"Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Dis..."
          ],
          [
           "- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tw..."
          ],
          [
           "- [Llama 2](#llama-2)\n            - [Load checkpoints for the classification mode](#load-checkpoints..."
          ],
          [
           "<!-- /TOC -->\n\n\n\n## Introduction \n\nIn the fast-moving world of Natural Language Processing (NLP), we..."
          ],
          [
           "## Hardware Used \n\n- Number of nodes: 1 \n- Number of GPUs per node: 1\n- GPU type: A6000 \n- GPU memor..."
          ],
          [
           "```\nNote: For reproducing the reported results, please check the pinned versions in the [wandb repor..."
          ],
          [
           "### [Mistral 7B](https://arxiv.org/abs/2310.06825)\n\nMistral 7B v0.1, with 7.3 billion parameters, is..."
          ],
          [
           "## Setup\n\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX_LEN=512` fo..."
          ],
          [
           "```\n\n## Data preparation\n### Data loading\n\nWe will load the dataset from Hugging Face:\n```python\nfro..."
          ],
          [
           "```\n\n- Train dataset\n\n```<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nD..."
          ],
          [
           "```\n\nThe final weights are: \n```\nPOS_WEIGHT, NEG_WEIGHT = (1.1637114032405993, 0.8766697374481806)\n`..."
          ],
          [
           "```\n**Note:** The RoBERTa tokenizer has been trained to treat spaces as part of the token. As a resu..."
          ],
          [
           "```\n\n- Now, let's  apply the preprocessing function to the entire dataset: \n\n```python\ncol_to_delete..."
          ],
          [
           "```\n\n\nYou can follow the same steps for preparing the data for Mistral 7B and Llama 2 models: \n\n**No..."
          ],
          [
           "```\n\n- Llama 2:\n```python\n# Load Llama 2 Tokenizer\nfrom transformers import AutoTokenizer, DataColla..."
          ],
          [
           "```\n\n\n####  LoRA setup for RoBERTa classifier\n\nWe import LoRa configuration and set some parameters ..."
          ],
          [
           "```\nFor Mistral 7B, we have to add the padding token id as it is not defined by default.\n\n```python\n..."
          ],
          [
           "```\n\nFor Llama 2, we have to add the padding token id as it is not defined by default.\n\n```python\nll..."
          ],
          [
           "```\ntrainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\n```\n\n..."
          ],
          [
           "```\n\n### Custom Trainer for Weighted Loss \nAs mentioned at the beginning of this post, we have an im..."
          ],
          [
           "```\nIt will print the following: \n```\ndevice(type='cuda', index=0)\n```\n\nThen, we set the training ar..."
          ],
          [
           "```\n\n#### Mistral-7B\n\nSimilar to RoBERTa, we initialize the `WeightedCELossTrainer` as follows: \n\n``..."
          ],
          [
           "```\n\n**Note** that we needed to enable half-precision training by setting `fp16` to `True`. The main..."
          ],
          [
           "```\n\n\n\n\n## Hyperparameter Tuning\n\nWe have used Wandb Sweep API to run hyperparameter tunning with Ba..."
          ],
          [
           "For more information, you can check the Wandb experiment report in the [resources sections](#resourc..."
          ],
          [
           "Finally, we showcase that LoRa method can be applied to both encoder (RoBERTa) and decoder (Llama 2 ..."
          ],
          [
           "--\ntitle: \"Introducing DOI: the Digital Object Identifier to Datasets and Models\"\nthumbnail: /blog/a..."
          ],
          [
           "## How are DOIs being assigned by Hugging Face? \n\nWe have partnered with [DataCite](https://datacite..."
          ],
          [
           "--\ntitle: \"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 1\"\nthumbnail: /blog/a..."
          ],
          [
           "Training a deep learning (DL) model on Intel Xeon CPUs can be a cost-effective and scalable approach..."
          ],
          [
           "The AMX instructions accelerate matrix multiplication, an operation central to training DL models on..."
          ],
          [
           "From a networking perspective, we will need the following setup:\n\n* Open port 22 for ssh access on a..."
          ],
          [
           "```\namx_bf16 amx_tile amx_int8\n```\n\nThen, we install native and Python dependencies.\n\n```\nsudo apt-g..."
          ],
          [
           "```\n\nNext, we create a new ssh key pair called 'cluster' with `ssh-keygen` and store it at the defau..."
          ],
          [
           "```\nlocalhost\nnode1\nnode2\nnode3\n```\n\nThe cluster is now ready. Let's start training!\n\n## Launching a..."
          ],
          [
           "```\n\nNo need to let the job run to completion, We just run for a minute to make sure that all depend..."
          ],
          [
           "```\n\nNow, we launch the distributed training job.\n\n```\n# Launch distributed training\nmpirun -f ~/hos..."
          ],
          [
           "```\n\nOne epoch now takes **7 minutes and 30 seconds**. \n\nHere's what the job looks like. The master ..."
          ],
          [
           "--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nautho..."
          ],
          [
           "--\ntitle: Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac\nthumbnail: /blog/assets/149_..."
          ],
          [
           "## New Core ML Optimizations\n\nCore ML is a mature framework that allows machine learning models to r..."
          ],
          [
           "<img style=\"border:none;\" alt=\"Illustration of 2-bit palettization. Image credit: Apple WWDCâ€™23 Sess..."
          ],
          [
           "## Using Quantized and Optimized Stable Diffusion Models\n\n[Last December](https://huggingface.co/blo..."
          ],
          [
           "| Model                     | Uncompressed      | Palettized                |\n|---------------------..."
          ],
          [
           "In order to use 6-bit models, you need the development versions of iOS/iPadOS 17 or macOS 14 (Sonoma..."
          ],
          [
           "repo_id = \"apple/coreml-stable-diffusion-2-1-base-palettized\"\nvariant = \"original/packages\"\n\nmodel_p..."
          ],
          [
           "```\n\n## Converting and Optimizing Custom Models\n\nIf you want to use a personalized Stable Diffusion ..."
          ],
          [
           "```bash\npython -m python_coreml_stable_diffusion.torch2coreml \\\n    --model-version prompthero/openj..."
          ],
          [
           "```\n\n<br>\n<div style=\"background-color: #f0fcf0; padding: 8px 32px 1px; outline: 1px solid; border-r..."
          ],
          [
           "```\n\n4. Test the converted models on the desired hardware. As a rule of thumb, the `ORIGINAL` versio..."
          ],
          [
           "We have plans to evaluate this method soon, and canâ€™t wait to see how 4-bit optimized models work an..."
          ],
          [
           "his notebook shows how to deploy a vision model from ðŸ¤— Transformers (written in TensorFlow) to [Vert..."
          ],
          [
           "```\n\n\n```python\nimport transformers\n\nprint(tf.__version__)\nprint(transformers.__version__)\n```\n\n## S..."
          ],
          [
           "```\n\n\n```python\ndef normalize_img(img, mean=processor.image_mean, std=processor.image_std):\n    # Sc..."
          ],
          [
           "predictions = m_call(**images)\n        indices = tf.argmax(predictions.logits, axis=1)\n        pred_..."
          ],
          [
           "```\n\n\n```python\n# To deploy the model on Vertex AI we must have the model in a storage bucket.\ntf.sa..."
          ],
          [
           "```\n\n\n```python\n# Upload the model to Vertex AI. \ntf28_gpu_model_dict = {\n    \"display_name\": \"ViT B..."
          ],
          [
           "```\n\n\n```python\n# Deploy the Endpoint. \ntf28_gpu_deployed_model_dict = {\n    \"model\": tf28_gpu_model..."
          ],
          [
           "```\n\n\n```python\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value..."
          ],
          [
           "--\ntitle: \"Retrieval Augmented Generation with Huggingface Transformers and Ray\"\nthumbnail: /blog/as..."
          ],
          [
           "Recently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.co..."
          ],
          [
           "### Scaling up fine-tuning\nThis retrieval of contextual documents is crucial for RAG's state-of-the-..."
          ],
          [
           "![alt_text](assets/12_ray_rag/ray_arch_updated.png \"image_tooltip\")\n_Document retrieval with the Ray..."
          ],
          [
           "_A performance comparison of different retrieval implementations. For each document retrieval implem..."
          ],
          [
           "```\n\n\nThen, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](h..."
          ],
          [
           "```\n\n## Whatâ€™s next?\n\nUsing RAG with [Huggingface transformers](https://github.com/huggingface/trans..."
          ],
          [
           "If you plan to try RAG+Ray integration out, please feel free to share your experiences on the [Ray D..."
          ],
          [
           "--\ntitle: Introducing Pull Requests and Discussions ðŸ¥³\nthumbnail: /blog/assets/76_community_update/th..."
          ],
          [
           "## Pull requests\n\n![Pull requests on the Hugging Face Hub](assets/76_community_update/new-pr.png)\n\n[..."
          ],
          [
           "--\ntitle: \"Introducing Agents.js: Give tools to your LLMs using JavaScript\"\nthumbnail: /blog/assets/..."
          ],
          [
           "```\n\nThen the code can be evaluated as such:\n\n```ts\nconst messages = await agent.evaluateCode(code);..."
          ],
          [
           "```\n\n### Usage warning\n\nCurrently using this library will mean evaluating arbitrary code in the brow..."
          ],
          [
           "```\n\n## Custom Tools ðŸ› ï¸\n\nAgents.js was designed to be easily expanded with custom tools & examples. ..."
          ],
          [
           "```\n\n## Passing input files to the agent ðŸ–¼ï¸\n\nThe agent can also take input files to pass along to th..."
          ],
          [
           "--\ntitle: \"Using Machine Learning to Aid Survivors and Race through Time\" \nthumbnail: /blog/assets/u..."
          ],
          [
           "![organization](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/disast..."
          ],
          [
           "![NER](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/disaster-assets..."
          ],
          [
           "![backend_pipeline](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/di..."
          ],
          [
           "In the end, we decided to fine-tune our own model as it would take roughly three minutes to fine-tun..."
          ],
          [
           "![active_learning](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dis..."
          ],
          [
           "To address these issues and create open source tools that can be leveraged in the future, we started..."
          ],
          [
           "![output_satellite](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/di..."
          ],
          [
           "--\ntitle: \"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2\"\nthumbnail: /blog/a..."
          ],
          [
           "Another factor to consider is the level of parallelism in the model and the inference task. GPUs are..."
          ],
          [
           "```\nsudo apt-get update\n\n# Add libtcmalloc for extra performance\nsudo apt install libgoogle-perftool..."
          ],
          [
           "```\nsentence_short = \"This is a really nice pair of shoes, I am completely satisfied with my purchas..."
          ],
          [
           "```\n\nOn the c6i (Ice Lake) instance, we only use a vanilla Transformers pipeline. \n\n```\nfrom transfo..."
          ],
          [
           "```\n\nFor the sake of brevity, we'll just look at the p99 results for [distilbert-base-uncased](https..."
          ],
          [
           "--\ntitle: Getting Started with Hugging Face Inference Endpoints\nthumbnail: /blog/assets/109_inferenc..."
          ],
          [
           "Starting from my [model page](https://huggingface.co/juliensimon/autotrain-food101-1471154053), I cl..."
          ],
          [
           "Let's first deploy a protected endpoint, and then we'll deploy a private one.\n\n### Deploying a Prote..."
          ],
          [
           "```\nimport requests, json\n\nAPI_URL = \"https://oncm9ojdmjwesag2.eu-west-1.aws.endpoints.huggingface.c..."
          ],
          [
           "```\n5c7fbb4485cd8w7 2022-10-10T08:19:04.915Z 2022-10-10 08:19:04,915 | INFO | POST / | Duration: 142..."
          ],
          [
           "```\n\nNow, let's increase our security level and deploy a private endpoint.\n \n### Deploying a Private..."
          ],
          [
           "```\ncurl https://oncm9ojdmjwesag2.eu-west-1.aws.endpoints.huggingface.cloud \\\n-X POST --data-binary ..."
          ],
          [
           "--\ntitle: \"Non-engineers guide: Train a LLaMA 2 chatbot\"\nthumbnail: /blog/assets/78_ml_director_insi..."
          ],
          [
           "## Introduction to Spaces\n\nSpaces from Hugging Face is a service that provides easy to use GUI for b..."
          ],
          [
           "1.2 Give your Space a name and select a preferred usage license if you plan to make your model or Sp..."
          ],
          [
           "2.2 Choose the LLM you want to train from the â€œModel Choiceâ€ field, you can select a model from the ..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "2.10 Go grab a coffee, depending on the size of your model and training data this could take a few h..."
          ],
          [
           "3.4 Under â€œSpace variablesâ€ you can also change model inference parameters including temperature, to..."
          ],
          [
           "--\ntitle: \"Ethical Guidelines for developing the Diffusers library\" \nthumbnail: /blog/assets/ethics-..."
          ],
          [
           "# Safety features and mechanisms\n\nIn addition, we provide a non-exhaustive - and hopefully continuou..."
          ],
          [
           "--\ntitle: \"Introducing BERTopic Integration with the Hugging Face Hub\"\nthumbnail: /blog/assets/145_b..."
          ],
          [
           "## What is BERTopic?\n\nBERTopic is a state-of-the-art Python library that simplifies the topic modell..."
          ],
          [
           "BERTopic provides a powerful tool for users to uncover significant topics within text collections, t..."
          ],
          [
           "```\nYou can then load this model in two lines and use it to predict against new data.\n\n```python\nfro..."
          ],
          [
           "```\n\nBy leveraging the power of the Hugging Face Hub, BERTopic users can effortlessly share, version..."
          ],
          [
           "<details>\n  <summary>Click here for an overview of all topics.</summary>\n  \n  | Topic ID | Topic Key..."
          ],
          [
           "| 10 | news - fake - fake news - stance - fact | 455 | 10_news_fake_fake news_stance | \n| 11 | relat..."
          ],
          [
           "| 23 | adversarial - attacks - attack - adversarial examples - robustness | 181 | 23_adversarial_att..."
          ],
          [
           "| 36 | classification - text classification - label - text - labels | 136 | 36_classification_text c..."
          ],
          [
           "| 49 | poetry - poems - lyrics - poem - music | 103 | 49_poetry_poems_lyrics_poem | \n| 50 | image - ..."
          ],
          [
           "| 62 | change - semantic change - time - semantic - lexical semantic | 82 | 62_change_semantic chang..."
          ],
          [
           "| 76 | translation - machine translation - machine - smt - statistical | 54 | 76_translation_machine..."
          ],
          [
           "| 90 | emoji - emojis - sentiment - message - anonymous | 35 | 90_emoji_emojis_sentiment_message | \n..."
          ],
          [
           "| 104 | gender - translation - bias - gender bias - mt | 24 | 104_gender_translation_bias_gender bia..."
          ],
          [
           "Due to the improved saving procedure, training on large datasets generates small model sizes. In the..."
          ],
          [
           "To illustrate some of the power of BERTopic let's look at an example of how it can be used to monito..."
          ],
          [
           "[databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) i..."
          ],
          [
           "```\n\nWe can predict on a single example text: \n\n```python\nexample = \"Stalemate is a drawn position. ..."
          ],
          [
           "```\n\nWe can then compare the distribution of topics across both datasets. We can see here that there..."
          ],
          [
           "Some examples of BERTopic models already on the hub:\n- [MaartenGr/BERTopic_ArXiv](https://huggingfac..."
          ],
          [
           "--\ntitle: \"OpenRAIL: Towards open and responsible AI licensing frameworks\"\nthumbnail: /blog/assets/1..."
          ],
          [
           "Most current model developers seem to think so, as the majority of openly released models have an op..."
          ],
          [
           "If specific ad hoc practices devoted to documentation, transparency and ethical usage of ML models a..."
          ],
          [
           "## **A change of licensing paradigm: OpenRAIL**\n\nThe OpenRAIL [approach](https://www.licenses.ai/blo..."
          ],
          [
           "The effect of copyleft-style behavioral-use clauses spreads the requirement from the original licens..."
          ],
          [
           "## **OpenRAIL could be for good machine learning what open software licensing is to code**\n\nThree ex..."
          ],
          [
           "The licenses are BigScience's reaction to 2 partially addressed challenges in the licensing space: (..."
          ],
          [
           "Let's invest in a healthy open and responsible AI licensing culture, the future of AI innovation and..."
          ],
          [
           "--\ntitle: Using LoRA for Efficient Stable Diffusion Fine-Tuning\nthumbnail: /blog/assets/lora/thumbna..."
          ],
          [
           "![Latent Diffusion Architecture](https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "- Training is much faster, as already discussed.\n- Compute requirements are lower. We could create a..."
          ],
          [
           "Diffusers now provides a [LoRA fine-tuning script](https://github.com/huggingface/diffusers/blob/mai..."
          ],
          [
           "```\n\nOne thing of notice is that the learning rate is `1e-4`, much larger than the usual learning ra..."
          ],
          [
           "First, we'll use the Hub API to automatically determine what was the base model that was used to fin..."
          ],
          [
           "```\n\nThis snippet will print the model he used for fine-tuning, which is `CompVis/stable-diffusion-v..."
          ],
          [
           "```\n\n## Dreamboothing with LoRA\n\nDreambooth allows you to \"teach\" new concepts to a Stable Diffusion..."
          ],
          [
           "## Other Methods\n\nThe quest for easy fine-tuning is not new. In addition to Dreambooth, [_textual in..."
          ],
          [
           "--\ntitle: \"Graph Classification with Transformers\" \nthumbnail: /blog/assets/125_intro-to-graphml/thu..."
          ],
          [
           "## Requirements\nTo follow this tutorial, you need to have installed `datasets` and `transformers` (v..."
          ],
          [
           "```\n\nThis dataset already has three splits, `train`, `validation`, and `test`, and all these splits ..."
          ],
          [
           "```\n\n### Format\nOn the Hub, graph datasets are mostly stored as lists of graphs (using the `jsonl` f..."
          ],
          [
           "A single graph is a dictionary, and here is the expected format for our graph classification dataset..."
          ],
          [
           "- `edge_attr` contains the available attributes (if present) for each edge of the graph, following t..."
          ],
          [
           "### Preprocessing\nGraph transformer frameworks usually apply specific preprocessing to their dataset..."
          ],
          [
           "```\n\nIt is also possible to apply this preprocessing on the fly, in the DataCollator's parameters (b..."
          ],
          [
           "```\nLet's look at this in more detail. \n\nCalling the `from_pretrained` method on our model downloads..."
          ],
          [
           "```\nIn the `Trainer` for graph classification, it is important to pass the specific data collator fo..."
          ],
          [
           "--\ntitle: Fine-Tune a Semantic Segmentation Model with a Custom Dataset\nthumbnail: /blog/assets/56_f..."
          ],
          [
           "Semantic segmentation is the task of classifying each pixel in an image. You can see it as a more pr..."
          ],
          [
           "Let's get started by installing the necessary dependencies. Because we're going to push our dataset ..."
          ],
          [
           "```\n\n# 1. Create/choose a dataset\n\nThe first step in any ML project is assembling a good dataset. In..."
          ],
          [
           "We went ahead and captured a thousand images of sidewalks in Belgium. Collecting and labeling such a..."
          ],
          [
           "### Label the images\n\nNow that the raw data is loaded, go to [segments.ai/home](https://segments.ai/..."
          ],
          [
           "Note that creating the release can take a few seconds. You can check the releases tab on Segments.ai..."
          ],
          [
           "```\n\nIf we inspect the features of the new dataset, we can see the image column and the correspondin..."
          ],
          [
           "```\n\nYou can also rewrite the `convert_segmentation_bitmap` function to use batches and pass `batche..."
          ],
          [
           "```\n\n# 2. Load and prepare the Hugging Face dataset for training\n\nNow that we've created a new datas..."
          ],
          [
           "```\n\n## Image processor & data augmentation\n\nA SegFormer model expects the input to be of a certain ..."
          ],
          [
           "```\n\n# 3. Fine-tune a SegFormer model\n\n## Load the model to fine-tune\n\nThe SegFormer authors define ..."
          ],
          [
           "```\n\n## Set up the Trainer\n\nTo fine-tune the model on our data, we'll use Hugging Face's [Trainer AP..."
          ],
          [
           "```\n\nNext, we'll define a function that computes the evaluation metric we want to work with. Because..."
          ],
          [
           "pred_labels = logits_tensor.detach().cpu().numpy()\n    # currently using _compute instead of compute..."
          ],
          [
           "```\n\nFinally, we can instantiate a `Trainer` object.\n\n\n```python\nfrom transformers import Trainer\n\nt..."
          ],
          [
           "```\n\n# 4. Inference\n\nNow comes the exciting part, using our fine-tuned model! In this section, we'll..."
          ],
          [
           "```python\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n\nproces..."
          ],
          [
           "```\n\nNext, we'll load an image from our test dataset.\n\n\n```python\nimage = test_ds[0]['pixel_values']..."
          ],
          [
           "```\n\nNow it's time to display the result. We'll display the result next to the ground-truth mask.\n\n<..."
          ],
          [
           "--\ntitle: \"Efficient Controllable Generation for SDXL with T2I-Adapters\"\nthumbnail: /blog/assets/t2i..."
          ],
          [
           "| **Model Type** | **Model Parameters** | **Storage (fp16)** |\n| --- | --- | --- |\n| [ControlNet-SDX..."
          ],
          [
           "Compared to previous versions of T2I-Adapter (SD-1.4/1.5), [T2I-Adapter-SDXL](https://github.com/Ten..."
          ],
          [
           "```\n\nThe generation process of the T2I-Adapter-SDXL mainly consists of the following two steps:\n\n1. ..."
          ],
          [
           "# load pipeline\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\neuler_a = EulerAncestralDiscre..."
          ],
          [
           "```\n\nThen, load an image to detect lineart:\n\n```python\nurl = \"https://huggingface.co/Adapter/t2iadap..."
          ],
          [
           "```\n\n![Lineart Generated Dragon](https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "<script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.43.1/gradio.js\"></script>\n<gr..."
          ],
          [
           "### Canny Guided\n\n![Sketch guided results](https://huggingface.co/datasets/huggingface/documentation..."
          ],
          [
           "--\ntitle: \"Introduction to Graph Machine Learning\" \nthumbnail: /blog/assets/125_intro-to-graphml/thu..."
          ],
          [
           "If you want to use your data, you must first consider its best characterisation (homogeneous/heterog..."
          ],
          [
           "Working on these tasks can be done in two ways. \n\nWhen you want to predict the evolution of a specif..."
          ],
          [
           "But what does this mean? If you have a sentence and shuffle its words, you create a new sentence. If..."
          ],
          [
           "## Graph representations through ML\n\nThe usual process to work on graphs with machine learning is fi..."
          ],
          [
           "**Node-level** features can give information about importance (how important is this node for the gr..."
          ],
          [
           "### Walk-based approaches\n\n[**Walk-based approaches**](https://en.wikipedia.org/wiki/Random_walk) us..."
          ],
          [
           "Typical neural networks, such as RNNs or CNNs are not permutation invariant. A new architecture, the..."
          ],
          [
           "**Choosing an aggregation**: Some aggregation techniques (notably mean/max pooling) can encounter fa..."
          ],
          [
           "Here are some interesting methods which got state-of-the-art results or close on one of the hardest ..."
          ],
          [
           "- [*Graph Transformer for Graph-to-Sequence Learning*](https://arxiv.org/abs/1911.07470) (Cai and La..."
          ],
          [
           "The most recent approach is [*Pure Transformers are Powerful Graph Learners*](https://arxiv.org/abs/..."
          ],
          [
           "# Further resources\n\nIf you want to delve deeper, you can look at some of these courses:\n\n- Academic..."
          ],
          [
           "If you need quality benchmarks you can check out:\n\n- [OGB, the Open Graph Benchmark](https://ogb.sta..."
          ],
          [
           "### External images attribution\nEmojis in the thumbnail come from Openmoji (CC-BY-SA 4.0), the Graph..."
          ],
          [
           "--\ntitle: \"Transformer-based Encoder-Decoder Models\"\nthumbnail: /blog/assets/05_encoder_decoder/thum..."
          ],
          [
           "```\n\nThe *transformer-based* encoder-decoder model was introduced by Vaswani\net al. in the famous [A..."
          ],
          [
           "Transformer-based encoder-decoder models are the result of years of\nresearch on _representation lear..."
          ],
          [
           "Each part builds upon the previous part, but can also be read on its\nown.\n\n## **Background**\n\nTasks ..."
          ],
          [
           "Using a DNN model \\\\({}^2\\\\) to solve sequence-to-sequence problems would\ntherefore mean that the nu..."
          ],
          [
           "Then, the decoder\\'s hidden state is initialized with the input encoding\nand during inference, the d..."
          ],
          [
           "In computational terms, the model sequentially maps the previous inner\nhidden state \\\\(\\mathbf{c}_{i..."
          ],
          [
           "For more detail on the logit vector and the resulting probability\ndistribution, please see footnote ..."
          ],
          [
           "Given such a decoding method, during inference, the next input vector\n\\\\(\\mathbf{y}_i\\\\) can then be..."
          ],
          [
           "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/rnn_..."
          ],
          [
           "The English sentence \\\"I want to buy a car\\\", represented by\n\\\\(\\mathbf{x}_1 = \\text{I}\\\\), \\\\(\\math..."
          ],
          [
           "encoder\\'s target vector. The encoder RNN then processes the rest of the\ninput sentence \\\\(\\text{wan..."
          ],
          [
           "To generate the first target vector, the decoder is fed the \\\\(\\text{BOS}\\\\)\nvector, illustrated as ..."
          ],
          [
           "$$ p_{\\theta_{\\text{enc}}, \\theta_{\\text{dec}}}(\\mathbf{Y}_{1:m} | \\mathbf{X}_{1:n}) = \\prod_{i=1}^{..."
          ],
          [
           "Nevertheless, RNN-based encoder-decoder models have two pitfalls. First,\nRNNs suffer from the vanish..."
          ],
          [
           "\\\\({}^4\\\\) A neural network can define a probability distribution over all\nwords, *i.e.* \\\\(p(\\mathb..."
          ],
          [
           "\\\\({}^6\\\\) [Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215)\nreverses the order of the inpu..."
          ],
          [
           "As a reminder, to solve a *sequence-to-sequence* problem, we need to\nfind a mapping of an input sequ..."
          ],
          [
           "$$ p_{\\theta_{dec}}(\\mathbf{Y}_{1:n} | \\mathbf{\\overline{X}}_{1:n}).$$\n\nBy Bayes\\' rule, this distri..."
          ],
          [
           "The transformer-based decoder hereby maps the sequence of encoded hidden\nstates \\\\(\\mathbf{\\overline..."
          ],
          [
           "Let\\'s visualize the complete process of *auto-regressive* generation of\n*transformer-based* encoder..."
          ],
          [
           "To begin with, the encoder processes the complete input sequence\n\\\\(\\mathbf{X}_{1:7}\\\\) = \\\"I want t..."
          ],
          [
           "Next, the first target vector \\\\(\\mathbf{y}_1\\\\) = \\\\(\\text{Ich}\\\\) is sampled\nfrom the distribution..."
          ],
          [
           "![texte du\nlien](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder..."
          ],
          [
           "```\n\n_Output:_\n\n```\n    <pad> Ich will ein Auto kaufen..."
          ],
          [
           "```\n\nCalling `.generate()` does many things under-the-hood. First, it passes\nthe `input_ids` to the ..."
          ],
          [
           "Great, now that we have gotten a general overview of how\n*transformer-based* encoder-decoder models ..."
          ],
          [
           "## **Encoder**\n\nAs mentioned in the previous section, the *transformer-based* encoder\nmaps the input..."
          ],
          [
           "Let\\'s visualize how the encoder processes the input sequence \\\"I want\nto buy a car EOS\\\" to a conte..."
          ],
          [
           "As can be seen each output vector of the self-attention layer\n\\\\(\\mathbf{x''}_i, \\forall i \\in \\{1, ..."
          ],
          [
           "$$ \\mathbf{q}_i = \\mathbf{W}_q \\mathbf{x'}_i,$$\n$$ \\mathbf{v}_i = \\mathbf{W}_v \\mathbf{x'}_i,$$\n$$ \\..."
          ],
          [
           "Note, that the **same** weight matrices are applied to each input vector\n\\\\(\\mathbf{x}_i, \\forall i ..."
          ],
          [
           "illustrated in the equation below. For a complete description of the\nself-attention layer, the reade..."
          ],
          [
           "Alright, this sounds quite complicated. Let\\'s illustrate the\nbi-directional self-attention layer fo..."
          ],
          [
           "On the left, the previously illustrated second encoder block is shown\nagain and on the right, an in ..."
          ],
          [
           "(shown in dark green on the right). The whole equation is illustrated in\nthe upper part of the box o..."
          ],
          [
           "To further understand the implications of the bi-directional\nself-attention layer, let\\'s assume the..."
          ],
          [
           "$$\\mathbf{X''}_{1:n} = \\mathbf{V}_{1:n} \\text{Softmax}(\\mathbf{Q}_{1:n}^\\intercal \\mathbf{K}_{1:n}) ..."
          ],
          [
           "\\\\({}^1\\\\) An in-detail explanation of the role the feed-forward layers play\nin transformer-based mo..."
          ],
          [
           "# pass input_ids to encoder\nencoder_hidden_states = model.base_model.encoder(input_ids, return_dict=..."
          ],
          [
           "```\n\n_Outputs:_\n```\n    Length of input embeddings 7. Length of encoder_hidden_states 7\n    Is encod..."
          ],
          [
           "```\n\nWe compare the length of the input word embeddings, *i.e.*\n`embeddings(input_ids)` correspondin..."
          ],
          [
           "On a side-note, _autoencoding_ models, such as BERT, have the exact same\narchitecture as _transforme..."
          ],
          [
           "Let\\'s first understand how the transformer-based decoder defines a\nprobability distribution. The tr..."
          ],
          [
           "respectively. The \\\"LM head\\\" is often tied to the transpose of the word\nembedding matrix, *i.e.*\n\\\\..."
          ],
          [
           "Putting it all together, in order to model the conditional distribution\nof a target vector sequence ..."
          ],
          [
           "In contrast to transformer-based encoders, in transformer-based\ndecoders, the encoded output vector ..."
          ],
          [
           "We can see that the decoder maps the input \\\\(\\mathbf{Y}_{0:5}\\\\) \\\"BOS\\\",\n\\\"Ich\\\", \\\"will\\\", \\\"ein\\..."
          ],
          [
           "can therefore be computed as the following product:\n\n$$ p_{\\theta_{dec}}(\\text{Ich} | \\text{BOS}, \\m..."
          ],
          [
           "As in bi-directional self-attention, in uni-directional self-attention,\nthe query vectors \\\\(\\mathbf..."
          ],
          [
           "Note that the index range of the key and value vectors is \\\\(0:i\\\\) instead\nof \\\\(0: m-1\\\\) which wo..."
          ],
          [
           "So why is it important that we use uni-directional self-attention in the\ndecoder instead of bi-direc..."
          ],
          [
           "This is obviously disadvantageous as the transformer-based decoder would\nnever learn to predict the ..."
          ],
          [
           "Great! Now we can move to the layer that connects the encoder and\ndecoder - the *cross-attention* me..."
          ],
          [
           "Note that the index range of the key and value vectors is \\\\(1:n\\\\)\ncorresponding to the number of c..."
          ],
          [
           "So intuitively, what happens here exactly? Each output vector\n\\\\(\\mathbf{y'''}_i\\\\) is a weighted su..."
          ],
          [
           "Cool! Now we can see how this architecture nicely conditions each output\nvector \\\\(\\mathbf{y'''}_i\\\\..."
          ],
          [
           "To verify our theoretical understanding, let\\'s continue our code\nexample from the encoder section a..."
          ],
          [
           "# create token ids for encoder input\ninput_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt..."
          ],
          [
           "# compare values of word embedding of \"I\" for input_ids and perturbed input_ids\nprint(\"Is encoding f..."
          ],
          [
           "```\n\n_Output:_\n\n```\n    Shape of decoder input vectors torch.Size([1, 5, 512]). Shape of decoder log..."
          ],
          [
           "```\n\nWe compare the output shape of the decoder input word embeddings, *i.e.*\n`embeddings(decoder_in..."
          ],
          [
           "On a final side-note, _auto-regressive_ models, such as GPT2, have the\nsame architecture as _transfo..."
          ],
          [
           "# create ids of encoded input vectors\ninput_ids = tokenizer(\"I want to buy a car\", return_tensors=\"p..."
          ],
          [
           "# sample last token with highest prob again\nnext_decoder_input_ids = torch.argmax(lm_logits[:, -1:],..."
          ],
          [
           "```\n\n_Outputs:_\n\n```\n    Generated so far: Ich will ein\n```\n\nIn this code example, we show exactly w..."
          ],
          [
           "--\ntitle: Block Sparse Matrices for Smaller and Faster Language Models\nthumbnail: /blog/assets/04_py..."
          ],
          [
           "By itself, or even better combined with other methods like\n[distillation](https://medium.com/hugging..."
          ],
          [
           "```\n\nThe extension also provides a `BlockSparseModelPatcher` that allows to modify an existing model..."
          ],
          [
           "But the more important point is that the performance gain of using sparse matrices grows with the sp..."
          ],
          [
           "--\ntitle: \"Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)\"\nthumbnail: /b..."
          ],
          [
           "Firstly, we will provide empirical evidence that **Transformers are indeed Effective for Time Series..."
          ],
          [
           "|      Dataset      | Autoformer (uni.) MASE | DLinear  MASE |\n|:-----------------:|:---------------..."
          ],
          [
           "### Decomposition Layer\nDecomposition has long been a popular method in time series analysis, but it..."
          ],
          [
           "Autoformer incorporates a decomposition block as an inner operation of the model, as presented in th..."
          ],
          [
           "def forward(self, x):\n        \"\"\"Input shape: Batch x Time x EMBED_DIM\"\"\"\n        # padding on the b..."
          ],
          [
           "```\n\nAs you can see, the implementation is quite simple and can be used in other models, as we will ..."
          ],
          [
           "In theory, given a time lag \\\\(\\tau\\\\), _autocorrelation_ for a single discrete variable \\\\(y\\\\) is ..."
          ],
          [
           "Now, we are ready to see the code in PyTorch: \n\n```python\nimport torch \n\ndef autocorrelation(query_s..."
          ],
          [
           "```\n\nQuite simple! ðŸ˜Ž Please be aware that this is only a partial implementation of `autocorrelation(..."
          ],
          [
           "It can be summarized with the following equations:\n\n$$\n\\tau_1, \\tau_2, ... \\tau_k = \\textrm{arg Top-..."
          ],
          [
           "Now, we are ready to see the final code:\n\n```python\nimport torch\nimport math\n\ndef time_delay_aggrega..."
          ],
          [
           "# apply softmax on the channel dim\n    top_k_autocorrelations = torch.softmax(top_k_autocorrelations..."
          ],
          [
           "```\n\nWe did it! The Autoformer model is [now available](https://huggingface.co/docs/transformers/mai..."
          ],
          [
           "```\n\nIn the probabilistic setting one can project the context length arrays to  `prediction-length *..."
          ],
          [
           "```\n\nThe transformers models are all relatively small with:\n\n```python\nencoder_layers=2\ndecoder_laye..."
          ],
          [
           "```\n\nLet's visualize a time series in the dataset and plot the train/test split:\n\n```python\nimport m..."
          ],
          [
           "```\n\n## Define Transformations\n\nNext, we define the transformations for the data, in particular for ..."
          ],
          [
           "```\n\n## Define `InstanceSplitter`\n\nFor training/validation/testing we next create an `InstanceSplitt..."
          ],
          [
           "```\n\n## Create PyTorch DataLoaders\n\nNext, it's time to create PyTorch DataLoaders, which allow us to..."
          ],
          [
           "return as_stacked_batches(\n        training_instances,\n        batch_size=batch_size,\n        shuffl..."
          ],
          [
           "if config.num_static_real_features > 0:\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\"..."
          ],
          [
           "```\n\n## Evaluate on Autoformer\n\nWe have already pre-trained an Autoformer model on this dataset, so ..."
          ],
          [
           "```\n\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `i..."
          ],
          [
           "```\n\nSo the result for the Autoformer model is:\n\n```python\nprint(f\"Autoformer univariate MASE: {np.m..."
          ],
          [
           "```\n\nFor example, for time-series in the test set with index `4`:\n\n```python\nplot(4)\n```\n\n![png](htt..."
          ],
          [
           "```\n\nTrain the model:\n\n```python\npredictor = estimator.train(\n    training_data=train_dataset, \n    ..."
          ],
          [
           "```\n\nAs before, we plot the predictions from our trained DLinear model via this helper:\n\n```python\nd..."
          ],
          [
           "```\n\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/148_a..."
          ],
          [
           "As one can observe, the [vanilla Transformer](https://huggingface.co/docs/transformers/model_doc/tim..."
          ],
          [
           "## Acknowledgements\nWe express our appreciation to [Lysandre Debut](https://github.com/LysandreJik) ..."
          ],
          [
           "--\ntitle: \"Image search with ðŸ¤— datasets\"\nthumbnail: /blog/assets/54_image_search_datasets/spaces_ima..."
          ],
          [
           "First, we'll install `datasets`. Since we're going to be working with images, we'll also install [`p..."
          ],
          [
           "```\n\nTo start, let's take a look at the image feature. We can use the wonderful [rich](https://rich...."
          ],
          [
           "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',..."
          ],
          [
           "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #00ffff; t..."
          ],
          [
           "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                               ..."
          ],
          [
           "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                               ..."
          ],
          [
           "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #008080; t..."
          ],
          [
           "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #008080; t..."
          ],
          [
           "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #008080; t..."
          ],
          [
           "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>  <span style=\"color: #808000; ..."
          ],
          [
           "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #808000; t..."
          ],
          [
           "We can see there a few different ways in which we can pass in our images. We'll come back to this in..."
          ],
          [
           "There have also been projects to tag the dataset [using machine learning](https://blogs.bl.uk/digita..."
          ],
          [
           "```\n\nLet's see what we get back.\n\n```python\ndataset\n```\n\n```\nDatasetDict({\n    train: Dataset({\n    ..."
          ],
          [
           "```\n```python\n/root/.cache/huggingface/datasets/downloads/extracted/f324a87ed7bf3a6b83b8a353096fbd95..."
          ],
          [
           "```\n\n\n<img src=\"assets/54_image_search_datasets/dataset_image.jpg\" alt=\"An example image from our da..."
          ],
          [
           "```\n\n\n``` python\ndataset.push_to_hub('davanstrien/embellishments-sample', private=True)\n```\n\n\n> **No..."
          ],
          [
           "```\n## Creating embeddings ðŸ•¸ \nWe now have a dataset with a bunch of images in it. To begin creating ..."
          ],
          [
           "```\n\nWe now have a new column which contains the embeddings for our images. We could manually search..."
          ],
          [
           "```\n\nWe can index into the first example this retrieves:\n\n``` python\nretrieved_examples['image'][0]\n..."
          ],
          [
           "```\n\n``` python\nget_image_from_text(\"An illustration of the sun behind a mountain\")\n```\n<img src=\"as..."
          ],
          [
           "```\n\n<img src=\"assets/54_image_search_datasets/musical_instrument.jpg\">\n\n<img src=\"assets/54_image_s..."
          ],
          [
           "However, I'm a little bit vary about making this public straightaway. Looking at the model card for ..."
          ],
          [
           "suggests that 'deployment' is not a good idea. Whilst the results I got are interesting, I haven't p..."
          ],
          [
           "--\ntitle: \"Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model\"\nthumb..."
          ],
          [
           "The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is t..."
          ],
          [
           "IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the origina..."
          ],
          [
           "<p align=\"center\">\n    <a href=\"https://atlas.nomic.ai/map/f2fba2aa-3647-4f49-a0f3-9347daeee499/ee4a..."
          ],
          [
           "As part of the release process, we internally evaluated the model for potential biases by adversaria..."
          ],
          [
           "## Getting Started with IDEFICS\n\nIDEFICS models are available on the Hugging Face Hub and supported ..."
          ],
          [
           "# Generation args\nexit_condition = processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=Fals..."
          ],
          [
           "--\ntitle: \"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\"\nthumbnail: /blog/..."
          ],
          [
           "### NLP\n\n[GPT-2](https://huggingface.co/Graphcore/gpt2-medium-wikitext-103) (Generative Pre-trained ..."
          ],
          [
           "[BART](https://huggingface.co/Graphcore/bart-base-ipu) is a transformer encoder-encoder (seq2seq) mo..."
          ],
          [
           "### Speech\n\n[HuBERT](https://huggingface.co/Graphcore/hubert-base-ipu) (Hidden-Unit BERT) is a self-..."
          ],
          [
           "Optimizing their performance in the real world requires considerable time, effort and skills that ar..."
          ],
          [
           "Software also plays a vital role in unlocking the IPUâ€™s capabilities, so naturally Optimum offers a ..."
          ],
          [
           "--\ntitle: \"Showcase Your Projects in Spaces using Gradio\"\nthumbnail: /blog/assets/28_gradio-spaces/t..."
          ],
          [
           "```\n\nYou can play with the Story Generation model [here](https://huggingface.co/spaces/merve/GPT-2-s..."
          ],
          [
           "```\n\n![big-gan](assets/28_gradio-spaces/big-gan.png)\n\n\n## Serving Custom Model Checkpoints with Grad..."
          ],
          [
           "```\n\nYou can check out the French Story Generator [here](https://huggingface.co/spaces/merve/french-..."
          ],
          [
           "Some Notes on Pros of Open Science and Open Source\n- **Pooling Resources**: Building off of one anot..."
          ],
          [
           "# Cons of Closed Source\n- **Centralization** of power.\n- **Opacity** of subtle bias/harm issues.\n- H..."
          ],
          [
           "--\ntitle: \"We are hiring interns!\"\nthumbnail: /blog/assets/interns-2023/thumbnail.png\nauthors:\n- use..."
          ],
          [
           "The following Science team positions are available:\n\n* [Embodied AI Internship](https://apply.workab..."
          ],
          [
           "--\ntitle: \"Announcing the Open Source AI Game Jam ðŸŽ®\"\nthumbnail: /blog/assets/145_gamejam/thumbnail.p..."
          ],
          [
           "From accelerated workflows to in-game features, you can harness the power of AI for texture generati..."
          ],
          [
           "--\ntitle: \"Ethics and Society Newsletter #3: Ethical Openness at Hugging Face\" \nthumbnail: /blog/ass..."
          ],
          [
           "We are crafting tools and safeguards in addition to improving our documentation practices to ensure ..."
          ],
          [
           "We engage directly with contributors and have addressed pressing issues. To bring this to the next l..."
          ],
          [
           "**How to use the flagging function:**\nClick on the flag icon on any Model, Dataset, Space, or Discus..."
          ],
          [
           "Should a specific model be flagged as high risk by our community, we consider:\n- Downgrading the ML ..."
          ],
          [
           "<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "```\n@misc{hf_ethics_soc_blog_3,\n  author    = {Irene Solaiman and\n               Giada Pistilli and\n..."
          ],
          [
           "--\ntitle: \"Deep Learning over the Internet: Training Language Models Collaboratively\"\nthumbnail: /bl..."
          ],
          [
           "## Distributed Deep Learning in Open Collaborations\n\n### Why should we do it?\n\nThese days, many high..."
          ],
          [
           "To a skeptical mind, it might seem that we're missing a key factor here: data transfer in distribute..."
          ],
          [
           "Often, to reduce the amount of synchronization and to stabilize the learning process, we can accumul..."
          ],
          [
           "Now that we have discussed the overall training procedure, there remains one more question: how do w..."
          ],
          [
           "<div style=\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\">\n<p ali..."
          ],
          [
           "<div style=\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\">\n<p ali..."
          ],
          [
           "1. **Normalization:** includes all preprocessing operations on raw text data. This was the step at w..."
          ],
          [
           "<div style=\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\">\n<p ali..."
          ],
          [
           "```\n\n### Dataset\n\nThe last thing we need to cover is the training dataset. As you probably know, the..."
          ],
          [
           "```\n<div style=\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\">\n<p..."
          ],
          [
           "<iframe width=\"100%\" height=\"670\" frameborder=\"0\"\n  src=\"https://observablehq.com/embed/@huggingface..."
          ],
          [
           "### Evaluation\n\nTo evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in..."
          ],
          [
           "At the end of training, we compared sahajBERT with three other pretrained language models: [XLM-R La..."
          ],
          [
           "These models are available on the Hub as well. You can test them directly by playing with the Hosted..."
          ],
          [
           "```\n\n#### sahajBERT-NCC\nModel card: [https://hf.co/neuropark/sahajBERT-NER](https://hf.co/neuropark/..."
          ],
          [
           "```\n\n## Conclusion\n\nIn this blog post, we have discussed the method that can enable collaborative pr..."
          ],
          [
           "Below, you can see all participants of the collaborative experiment:\n\n<iframe width=\"100%\" height=\"3..."
          ],
          [
           "--\ntitle: \"Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate\"\nthumbnail: /blog/assets/bl..."
          ],
          [
           "## Benchmarks\n\nWithout any further delay let's show some numbers.\n\nFor the sake of consistency, unle..."
          ],
          [
           "```\nGenerate args {'max_length': 100, 'do_sample': False}..."
          ],
          [
           "```\nThe input prompt is comprised of just a few tokens. The previous token caching is on as well, as..."
          ],
          [
           "Here is the throughput in msecs on 8x80GB GPUs:\n\n| project      \\ bs |      1 |     8 |    16 |    3..."
          ],
          [
           "Let's revisit again how these numbers were calculated. To generate 100 new tokens for a batch size o..."
          ],
          [
           "```\ngit clone https://github.com/huggingface/transformers-bloom-inference\ncd transformers-bloom-infe..."
          ],
          [
           "```\npython bloom-inference-scripts/bloom-accelerate-inference.py --name bigscience/bloom --batch_siz..."
          ],
          [
           "```\ndeepspeed --num_gpus 8 bloom-inference-scripts/bloom-ds-inference.py --name microsoft/bloom-deep..."
          ],
          [
           "```\ndeepspeed --num_gpus 4 bloom-inference-scripts/bloom-ds-inference.py --name microsoft/bloom-deep..."
          ],
          [
           "```\npip install deepspeed\n```\n\n\n### Run\n\nNote that the script currently runs the same inputs on all ..."
          ],
          [
           "```\n\nmake sure to adjust `/path/to/nvme_offload` to somewhere you have ~400GB of free memory on a fa..."
          ],
          [
           "--\ntitle: \"Summer at Hugging Face\"\nthumbnail: /blog/assets/27_summer_at_huggingface/summer_intro.gif..."
          ],
          [
           "Spaces lets you [set up secrets](/docs/hub/spaces-overview#managing-secrets), permits [custom requir..."
          ],
          [
           "![Image of a TensorBoard Instance](assets/27_summer_at_huggingface/tensorboard.png)\n\n### Metrics\n\nIn..."
          ],
          [
           "The Hub has 18 widgets that allow users to try out models directly in the browser.\n\nWith our latest ..."
          ],
          [
           "![Button to upload a file](assets/27_summer_at_huggingface/upload_file.png)\n\n## Community\n\n### Huggi..."
          ],
          [
           "We're really excited to share the work of the 3 winning teams!\n\n1. [Dall-e mini](https://huggingface..."
          ],
          [
           "## Bonus\n\nOn top of everything we just shared, our team has been doing lots of other things. Here ar..."
          ],
          [
           "## Open Source\n\n### New in Transformers\n\nSummer has been an exciting time for ðŸ¤— Transformers! The li..."
          ],
          [
           "```\n\nThe last 4 releases introduced many new cool models!\n\n- [DETR](https://huggingface.co/transform..."
          ],
          [
           "![DETR image](assets/27_summer_at_huggingface/detr.png)\n\n- [ByT5](https://huggingface.co/transformer..."
          ],
          [
           "![LayoutLM object detection](assets/27_summer_at_huggingface/layout.png)\n\n- [BEiT](https://huggingfa..."
          ],
          [
           "![Untitled](assets/27_summer_at_huggingface/streaming.png)\n\nWhat are the new datasets highlights? Mi..."
          ],
          [
           "![spaCy NER example](assets/27_summer_at_huggingface/spacy_ner.jpeg)\n\nAnother exciting integration i..."
          ],
          [
           "### **NEW: Hardware Acceleration**\n\nHugging Face is [partnering with leading AI hardware accelerator..."
          ],
          [
           "![Sagemaker](assets/27_summer_at_huggingface/sagemaker.png)\n\n### **NEW: AutoNLP In Your Browser**\n\nW..."
          ],
          [
           "**Hugging Face** + **Zapier Demo**\n\n20,000+ Machine Learning models connected to 3,000+ apps? ðŸ¤¯  By ..."
          ],
          [
           "**Few-shot learning in practice**\n\nWe wrote a [blog post](https://huggingface.co/blog/few-shot-learn..."
          ],
          [
           "In June, we shared the result of our collaboration with the Yandex research team: [DeDLOC](https://a..."
          ],
          [
           "![Prompt](assets/27_summer_at_huggingface/prompt.png)\n\n\nWe're looking forward to EMNLP this year whe..."
          ],
          [
           "--\ntitle: \"Model Cards\" \nthumbnail: /blog/assets/121_model-cards/thumbnail.png\nauthors:\n- user: Ezi\n..."
          ],
          [
           "4) A [User Study](https://huggingface.co/docs/hub/model-cards-user-studies) on model card usage at H..."
          ],
          [
           "## Our Work\n\nOur work presents a view of where model cards stand right now and where they could go i..."
          ],
          [
           "As ML continues to be more intertwined with different domains, collaborative and open-source ML proc..."
          ],
          [
           "* The Hugging Face ecosystem will continue to advance methods that streamline Model Card creation [t..."
          ],
          [
           "--\ntitle: \"Introducing RWKV - An RNN with the advantages of a transformer\" \nthumbnail: /blog/assets/..."
          ],
          [
           "You can get involved by joining the [official discord channel](https://discord.gg/qt9egFA7ve) and le..."
          ],
          [
           "Because RNNs use the same weights to compute predictions at every step, they struggle to memorize in..."
          ],
          [
           "During training, Transformer architecture has several advantages over traditional RNNs and CNNs. One..."
          ],
          [
           "RNNs natively support very long context lengths - only limited by the context length seen in trainin..."
          ],
          [
           "To gain a more comprehensive understanding of the attention layer, we recommend to delve into the de..."
          ],
          [
           "All the HF converted models are available on Hugging Face Hub, in the [`RWKV` organization](https://..."
          ],
          [
           "```\n\nOr you can run and start from the snippet below:\n\n```python\nimport torch\nfrom transformers impo..."
          ],
          [
           "```\n\n### Use the raven models (chat models)\n\nYou can prompt the chat model in the alpaca style, here..."
          ],
          [
           "```\n\nAccording to Bo, better instruction techniques are detailed in [this discord message (make sure..."
          ],
          [
           "```\n\n## Future work\n\n### Multi-lingual RWKV\n\nBo is currently working on a multilingual corpus to tra..."
          ],
          [
           "## Acknowledgements\n\nThe Hugging Face team would like to thank Bo and RWKV community for their time ..."
          ],
          [
           "--\ntitle: Stable Diffusion with ðŸ§¨ Diffusers\nthumbnail: /blog/assets/98_stable_diffusion/thumbnail.pn..."
          ],
          [
           "**Note**: It is highly recommended to have a basic understanding of how diffusion models work. If di..."
          ],
          [
           "```\n\nIn this post we'll use model version [`v1-4`](https://huggingface.co/CompVis/stable-diffusion-v..."
          ],
          [
           "```\n\nThe result would look as follows\n\n![png](assets/98_stable_diffusion/stable_diffusion_12_1.png)\n..."
          ],
          [
           "```\n\nThe result would look as follows\n\n![png](assets/98_stable_diffusion/stable_diffusion_14_1.png)\n..."
          ],
          [
           "```\n\n![png](assets/98_stable_diffusion/stable_diffusion_16_1.png)\n\nNote how the structure is the sam..."
          ],
          [
           "```\n\nWe can generate multiple images for the same prompt by simply using a list with the same prompt..."
          ],
          [
           "```\n\n![png](assets/98_stable_diffusion/stable_diffusion_26_1.png)\n    \n\n## How does Stable Diffusion..."
          ],
          [
           "There are three main components in latent diffusion.\n\n1. An autoencoder (VAE).\n2. A [U-Net](https://..."
          ],
          [
           "To prevent the U-Net from losing important information while downsampling, short-cut connections are..."
          ],
          [
           "**Stable Diffusion during inference**\n\nPutting it all together, let's now take a closer look at how ..."
          ],
          [
           "Theory on how the scheduler algorithm function is out-of-scope for this notebook, but in short one s..."
          ],
          [
           "We can load the components by referring to the folder they were saved, using the `subfolder` argumen..."
          ],
          [
           "```\n\nNow instead of loading the pre-defined scheduler, we load the [K-LMS scheduler](https://github...."
          ],
          [
           "```\n\nFirst, we get the `text_embeddings` for the passed prompt. \nThese embeddings will be used to co..."
          ],
          [
           "```\n\nIf we examine the `latents` at this stage we'll see their shape is `torch.Size([1, 4, 64, 64])`..."
          ],
          [
           "```\n\nWe now use the `vae` to decode the generated `latents` back into the image.\n\n\n```python\n# scale..."
          ],
          [
           "```\n@article{patil2022stable,\n  author = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von..."
          ],
          [
           "--\ntitle: 'Deploy Hugging Face models easily with Amazon SageMaker'\nthumbnail: /blog/assets/17_the_p..."
          ],
          [
           "```\n\n\nThat's it! ðŸš€\n\nTo learn more about accessing and using the new Hugging Face DLCs with the Amazo..."
          ],
          [
           "## **Samples/Documentation**\n\n- [Hugging Face documentation for Amazon SageMaker](https://huggingfac..."
          ],
          [
           "In addition to the zero-code deployment, the Inference Toolkit supports \"bring your own code\" method..."
          ],
          [
           "```\n\n# **Getting started ðŸ§­**\n\nIn this guide we will use the new Hugging Face Inference DLCs and Amaz..."
          ],
          [
           "```\n\n---\n\n## **Deploy a trained Hugging Face Transformer model to SageMaker for inference**\n\nThere a..."
          ],
          [
           "```\n\n\nAfter we run our request we can delete the endpoint again with.\n\n\n```python\n# delete endpoint\n..."
          ],
          [
           "```\n\nAfter we run our request, we can delete the endpoint again with:\n\n\n```python\n# delete endpoint\n..."
          ],
          [
           "```\n\nAfter we run our request we can delete the endpoint again with.\n\n\n```python\n# delete endpoint\np..."
          ],
          [
           "```\n\n---\n\n# **FAQ ðŸŽ¯**\n\nYou can find the complete [Frequently Asked Questions](https://huggingface.co..."
          ],
          [
           "_Q: Do I have to use the SageMaker Python SDK to use the Hugging Face Deep Learning Containers (DLCs..."
          ],
          [
           "_Q: How is my data and code secured by Amazon SageMaker?_\n\nA: Amazon SageMaker provides numerous sec..."
          ],
          [
           "A: AWS Technical Support tiers are available from AWS and cover development and production issues fo..."
          ],
          [
           "--\ntitle: \"Introducing Prodigy-HF: a direct integration with Hugging Face\"\nthumbnail: /blog/assets/1..."
          ],
          [
           "<figure>\n    <div style=\"background-color: #eee; padding-top: 8px; padding-bottom: 8px;\">\n        <i..."
          ],
          [
           "```\npython -m prodigy hf.train.ner fashion-train,eval:fashion-eval path/to/model-out --model \"distil..."
          ],
          [
           "```\npython -m prodigy hf.upload <dataset_name> <username>/<repo_name>\n```\n\nWe're particularly fond o..."
          ],
          [
           "--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-gam..."
          ],
          [
           "6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://h..."
          ],
          [
           "```\nusing HuggingFace.API;\n\n/* other code */\n\n// Make a call to the API\nvoid Query() {\n    string in..."
          ],
          [
           "```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the fo..."
          ],
          [
           "--\ntitle: \"Proximal Policy Optimization (PPO)\"\nthumbnail: /blog/assets/93_deep_rl_ppo/thumbnail.png\n..."
          ],
          [
           "- *An Actor*Â that controlsÂ **how our agent behaves**Â (policy-based method).\n- *A Critic*Â that measur..."
          ],
          [
           "- [The intuition behind PPO](https://huggingface.co/blog/deep-rl-ppo#the-intuition-behind-ppo)\n- [In..."
          ],
          [
           "- [Let's code our PPO Agent](https://huggingface.co/blog/deep-rl-ppo#lets-code-our-ppo-agent)\n  \n## ..."
          ],
          [
           "The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability o..."
          ],
          [
           "However, the problem comes from the step size:\n- Too small,Â **the training process was too slow**\n- ..."
          ],
          [
           "So this probability ratio is an **easy way to estimate the divergence between old and current policy..."
          ],
          [
           "To do that, we have two solutions:\n\n- *TRPO (Trust Region Policy Optimization)*Â uses KL divergence c..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/93_deep_rl_ppo/recap.jpg\" alt..."
          ],
          [
           "Since the ratio is between intervals,Â **we can decrease the probability that our policy takes that a..."
          ],
          [
           "### Case 5 and 6: the ratio is above the range\n<figure class=\"image table text-center m-0 w-full\">\n ..."
          ],
          [
           "**You might wonder why, when the minimum is the clipped ratio, the gradient is 0.** When the ratio i..."
          ],
          [
           "So, to be able to code it, we're going to use two resources:\n- A tutorial made by [Costa Huang](http..."
          ],
          [
           "<iframe src=\"https://giphy.com/embed/pynZagVcYxVUk\" width=\"480\" height=\"480\" frameBorder=\"0\" class=\"..."
          ],
          [
           "And don't forget to share with your friends who want to learn ðŸ¤—!\n\nFinally, with your feedback, we wa..."
          ],
          [
           "--\ntitle: \"Very Large Language Models and How to Evaluate Them\" \nthumbnail: /blog/assets/106_zero_sh..."
          ],
          [
           "Weâ€™ve upgraded the AutoTrain infrastructure for this project so that large models can be evaluated f..."
          ],
          [
           "## Case study: Zero-shot evaluation on the WinoBias task\n\nThe [WinoBias](https://github.com/uclanlp/..."
          ],
          [
           "![Winobias](./assets/106_zero_shot_eval_on_the_hub/winobias.png)\n\n## Enabling better research tools ..."
          ],
          [
           "## Send us feedback!\n\nAt Hugging Face, weâ€™re excited to continue democratizing access to state-of-th..."
          ],
          [
           "--\ntitle: Training Stable Diffusion with Dreambooth using Diffusers\nthumbnail: /blog/assets/sd_dream..."
          ],
          [
           "## TL;DR: Recommended Settings\n\n* Dreambooth tends to overfit quickly. To get good-quality images, w..."
          ],
          [
           "## Learning Rate Impact\n\nDreambooth overfits very quickly. To get good results, tune the learning ra..."
          ],
          [
           "Low Learning Rate (`2e-6`)\n![Cat Toy, Low Learning Rate](https://huggingface.co/datasets/huggingface..."
          ],
          [
           "### Summary of Initial Results\n\nTo get good results training Stable Diffusion with Dreambooth, it's ..."
          ],
          [
           "As you can see, results are better when prior preservation is used, but there are still noisy blotch..."
          ],
          [
           "`DDIM`, Potato Head\n![DDIM Potato](https://huggingface.co/datasets/huggingface/documentation-images/..."
          ],
          [
           "We think the results are much better than doing plain Dreambooth but not as good as when we fine-tun..."
          ],
          [
           "--\ntitle: 'Faster Text Generation with TensorFlow and XLA'\nthumbnail: /blog/assets/91_tf_xla_generat..."
          ],
          [
           "The ðŸ¤— `transformers` library started with NLP models, so it is natural that text generation is of ut..."
          ],
          [
           "generated = model.generate(**inputs, do_sample=True, seed=(42, 0))\nprint(\"Sampling output: \", tokeni..."
          ],
          [
           "```\n\nDepending on the target application, longer outputs might be desirable. You can control the len..."
          ],
          [
           "```\n\nContrarily to Sampling, Greedy Decoding will always pick the most likely token at each iteratio..."
          ],
          [
           "```\n\nThe basics of text generation, as you can see, are straightforward to control. However, there a..."
          ],
          [
           "For those of you familiar with TensorFlow 1 ðŸ§“, the concept of a TensorFlow graph comes naturally, as..."
          ],
          [
           "Now that you know how to create TensorFlow graphs, compiling them with XLA is straightforward -- sim..."
          ],
          [
           "```\n\nIn one line, you can create an XLA-accelerated function from the function above.\n\n```python\nxla..."
          ],
          [
           "```\n\n## Text Generation using TensorFlow with XLA\n\nAs with any optimization procedure, there is no f..."
          ],
          [
           "# Slow: Different tensor type\nmax_plus_constant(tf.constant([0, 0, 0], dtype=tf.int64), 1)\n# > Execu..."
          ],
          [
           "```\n\nIn practice, for text generation, it simply means the input should be padded to a multiple of a..."
          ],
          [
           "print(\"Calling XLA generation with tokenized_input_1...\")\nprint(\"(will be slow as it is the first ca..."
          ],
          [
           "```\n\nOh no, that's terribly slow! A solution to keep the different combinations of shapes in check i..."
          ],
          [
           "```\n\nThat's much better, successive generation calls performed this way will be orders of magnitude ..."
          ],
          [
           "```\n\nFrom a developer perspective, relying on XLA implies being aware of a few additional nuances. X..."
          ],
          [
           "<div class=\"hidden xl:block\">\n<div style=\"display: flex; flex-direction: column; align-items: center..."
          ],
          [
           "--\ntitle: \"Perceiver IO: a scalable, fully-attentional model that works on any modality\"\nthumbnail: ..."
          ],
          [
           "Not long after that, AI researchers started to apply the idea of BERT to other domains. To name a fe..."
          ],
          [
           "## The Perceiver\n\nThe [Perceiver](https://arxiv.org/abs/2103.03206) aims to solve this limitation by..."
          ],
          [
           "Note that each of these are optional. A `preprocessor` is only required in case one hasn't already e..."
          ],
          [
           "Let's start off by showing how the Perceiver is implemented to work on text.\n\n## Perceiver for text\n..."
          ],
          [
           "```\n\nIn this case, one provides `PerceiverTextPreprocessor` as preprocessor to the model, which will..."
          ],
          [
           "```\n\nIn the Perceiver IO paper, one uses 256 latents, and sets the dimensionality of the latents to ..."
          ],
          [
           "Ok, so now one has final hidden states of shape (batch_size, 256, 1280). Great, but one actually wan..."
          ],
          [
           "Great, isn't it? The Perceiver authors also show that it is straightforward to pre-train the Perceiv..."
          ],
          [
           "Each of these are implemented in the Transformers library, and called `PerceiverForImageClassificati..."
          ],
          [
           "```\n\nOne can see that `PerceiverImagePreprocessor` is initialized with `prep_type = \"conv1x1\"` and t..."
          ],
          [
           "```\n\n`PerceiverImagePreprocessor` (with the settings defined above) will first apply a convolutional..."
          ],
          [
           "The Perceiver authors show that the model is capable of achieving strong results compared to models ..."
          ],
          [
           "class PerceiverForOpticalFlow(nn.Module):\n    def __init__(self, config):\n        super().__init__(c..."
          ],
          [
           "```\nAs one can see, `PerceiverImagePreprocessor` is used as preprocessor (i.e. to prepare the 2 imag..."
          ],
          [
           "To decode the final hidden states of the latents to an actual predicted flow, `PerceiverOpticalFlowD..."
          ],
          [
           "The video below shows the predicted flow on 2 examples. \n\n<p float=\"left\">\n  <img src=\"https://lh3.g..."
          ],
          [
           "## Perceiver for multimodal autoencoding\n\nThe authors also use the Perceiver for multimodal autoenco..."
          ],
          [
           "Next, `PerceiverMultimodalPreprocessor` will pad the preprocessed modalities with modality-specific ..."
          ],
          [
           "- For the image modality, the total size of the decoder query is 16x3x224x224 = 802,816. However, wh..."
          ],
          [
           "Finally, there is `PerceiverMultimodalPostprocessor`. This class postprocesses the output of the dec..."
          ],
          [
           "<small>Above: original video (left), reconstruction of the first 16 frames (right). Video taken from..."
          ],
          [
           "The authors also used the Perceiver to replace the original Transformer in [AlphaStar](https://deepm..."
          ],
          [
           "## Conclusion\n\nIn this blog post, we went over the architecture of Perceiver IO, an extension of the..."
          ],
          [
           "## Footnotes\n\n<b id=\"f1\">1</b> Note that in the official paper, the authors used a two-layer MLP to ..."
          ],
          [
           "--\ntitle: \"Train a Sentence Embedding Model with 1B Training Pairs\"\nauthors:\n- user: asi\n  guest: tr..."
          ],
          [
           "![snippet](assets/32_1b_sentence_embeddings/model.png)\n\n### Multiple Negative Ranking Loss\n\nThe para..."
          ],
          [
           "Intuitively, the model should assign high similarity to the sentences Â«Â How many people live in Berl..."
          ],
          [
           "In practice, we used a scaled similarity because score differences tends to be too small and apply a..."
          ],
          [
           "![snippet](assets/32_1b_sentence_embeddings/batch-size.png)\n\n#### 2. Hard Negatives\n\nIn the same fig..."
          ],
          [
           "## Conclusion\n\nYou can find all models and datasets we created during the challenge in our [HuggingF..."
          ],
          [
           "General sentence embeddings might be used for many applications. We built a [Spaces demo](https://hu..."
          ],
          [
           "--\ntitle: \"Large-scale Near-deduplication Behind BigCode\"\nthumbnail: /blog/assets/dedup/thumbnail.pn..."
          ],
          [
           "## From BigScience to BigCode\n\nAllow me to share a story first on how I jumped on this near-deduplic..."
          ],
          [
           "| Dataset                              | Input Size                       | Output Size or Deduction..."
          ],
          [
           "| BNE5[[6]](#6)                        | 2TB                              | 570 GB                  ..."
          ],
          [
           "| The BigScience ROOTS Corpus[[9]](#9) |                                  | 0.07% ~ 2.7% **â†“** (docu..."
          ],
          [
           "This is the one for code datasets we created for BigCode as well. Model names are used when the data..."
          ],
          [
           "## MinHash Walkthrough\n\nIn this section, we will cover each step of MinHash, the one used in BigCode..."
          ],
          [
           "| doc_id | shingles                                                                        |\n| -----..."
          ],
          [
           "| shingle             | permuted hashes                                             |\n| ------------..."
          ],
          [
           "In implementation, you can easily vectorize these steps with `numpy` and expect to have a time compl..."
          ],
          [
           "```\n\nIf you are familiar with [Datasketch](https://github.com/ekzhu/datasketch), you might ask, why ..."
          ],
          [
           "```\n\nAfter the fingerprint calculation, one particular document is mapped to one array of integer va..."
          ],
          [
           "If two documents share the same hashes in a band at a particular location (band index), they will be..."
          ],
          [
           "### Beyond Duplicate Pairs\n\nThis is where many deduplication descriptions in papers or tutorials sto..."
          ],
          [
           "**Option 2: Use popular python frameworks such as `dask` to allow more efficient `groupby` operation..."
          ],
          [
           "```\n\n**Option 4: For large datasets, use Spark.**\n\nWe already know that steps up to the LSH part can..."
          ],
          [
           "```\n\nAdditionally, thanks to cloud providers, we can set up Spark clusters like a breeze with servic..."
          ],
          [
           "These graphs can help us understand why it was necessary to double-check the false positives for Cod..."
          ],
          [
           "## Proceed with Caution\n\nDeduplication doesn't exempt you from thorough data exploration and analysi..."
          ],
          [
           "## Future Directions\n\n1. Substring deduplication. Even though it showed some benefits for English[[3..."
          ],
          [
           "## Supporting Resources\n\n- [Datasketch](https://github.com/ekzhu/datasketch)Â (MIT)\n- [simhash-py](ht..."
          ],
          [
           "- <a id=\"1\">[1]</a> : Nikhil Kandpal, Eric Wallace, Colin Raffel, [Deduplicating Training Data Mitig..."
          ],
          [
           "- <a id=\"8\">[8]</a> : Xi Victoria Lin, Todor Mihaylov, et al., [Few-shot Learning with Multilingual ..."
          ],
          [
           "- <a id=\"15\">[15]</a> : Lewis Tunstall, Leandro von Werra, Thomas Wolf, [Natural Language Processing..."
          ],
          [
           "--\ntitle: 'Getting Started With Embeddings'\nthumbnail: /blog/assets/80_getting_started_with_embeddin..."
          ],
          [
           "## What are embeddings for?\n\n> \"[...] once you understand this ML multitool (embedding), you'll be a..."
          ],
          [
           "But first, we need to embed our dataset (other texts use the terms encode and embed interchangeably)..."
          ],
          [
           "```\nTo generate the embeddings you can use the `https://api-inference.huggingface.co/pipeline/featur..."
          ],
          [
           "```\n\nThe current API does not enforce strict rate limitations. Instead, Hugging Face balances the lo..."
          ],
          [
           "```\nIt looks similar to this matrix:\n\n```py\n[[-0.02388945  0.05525852 -0.01165488 ...  0.00577787  0..."
          ],
          [
           "```\n\n## 2. Host embeddings for free on the Hugging Face Hub\n\nðŸ¤— Datasets is a library for quickly acc..."
          ],
          [
           "```\n\nFollow the next steps to host `embeddings.csv` in the Hub.\n\n* Click on your user in the top rig..."
          ],
          [
           "Install the ðŸ¤— Datasets library with `pip install datasets`. Then, load the embedded dataset from the..."
          ],
          [
           "```\nWe use the query function we defined before to embed the customer's question and convert it to a..."
          ],
          [
           "```\n\n`util.semantic_search` identifies how close each of the 13 FAQs is to the customer query and re..."
          ],
          [
           "```\n\nThis list represents the 5 FAQs closest to the customer's query. Nice! We used here PyTorch and..."
          ],
          [
           "--\ntitle: Getting Started with Transformers on Habana Gaudi\nthumbnail: /blog/assets/61_getting_start..."
          ],
          [
           "Let's get started!\n\n\n## Setting up an Habana Gaudi instance on AWS\n\nThe simplest way to work with Ha..."
          ],
          [
           "Next, I pick the *dl1.24xlarge* instance size (the only size available).\n\n<kbd>\n  <img src=\"assets/6..."
          ],
          [
           "<kbd>\n  <img src=\"assets/61_getting_started_habana/habana06.png\">\n</kbd>\n\nFinally, I launch the inst..."
          ],
          [
           "```\nssh -i ~/.ssh/julsimon-keypair.pem ubuntu@ec2-18-207-189-109.compute-1.amazonaws.com\n```\n\nOn thi..."
          ],
          [
           "```\n\nThen, I install the Optimum Habana package from source.\n\n```\ncd optimum-habana\npip install .\n``..."
          ],
          [
           "```\n\nAfter 2 minutes and 12 seconds, the job is complete and has achieved an excellent F1 score of 0..."
          ],
          [
           "--\ntitle: \"Personal Copilot: Train Your Own Coding Assistant\" \nthumbnail: /blog/assets/170_personal_..."
          ],
          [
           "## Data Collection Workflow\n\nOur desired dataset is conceptually simple, we structured it like so:\n\n..."
          ],
          [
           "We also excluded all file paths that were not directly related to code. These include: `.git`, `__py..."
          ],
          [
           "[This is the code we used to generate this dataset](https://github.com/pacman100/DHS-LLM-Workshop/tr..."
          ],
          [
           "## Finetuning your own Personal Co-Pilot \n\nIn this section, we show how to fine-tune the following m..."
          ],
          [
           "> trainable params: 110,428,160 || all params: 15,627,884,544 || trainable%: 0.7066097761926236\n\n1. ..."
          ],
          [
           "## Full Finetuning\n\nWe will look at how to do full fine-tuning of `bigcode/starcoder` (15B params) o..."
          ],
          [
           "The command to launch training is given at [run_fsdp.sh](https://github.com/pacman100/DHS-LLM-Worksh..."
          ],
          [
           "```\naccelerate launch --config_file \"configs/fsdp_config.yaml\"  train.py \\\n    --model_path \"bigcode..."
          ],
          [
           "```\n\nThe total training time was **9 Hours**. Taking the cost of $12.00 / hr based on [lambdalabs](h..."
          ],
          [
           "**Resources**\n1. Codebase: [link](https://github.com/pacman100/DHS-LLM-Workshop/tree/main/personal_c..."
          ],
          [
           "## Comparison\n\nThe plot below shows the eval loss, train loss and learning rate scheduler for QLoRA ..."
          ],
          [
           "![qualitative_comparison_1](https://huggingface.co/datasets/huggingface/documentation-images/resolve..."
          ],
          [
           "In the second example above, **GitHub Copilot didn't give any completion**. This can be due to the f..."
          ],
          [
           "### Setting an Inference Endpoint\nBelow are the screenshots with the steps we followed to create our..."
          ],
          [
           "**Resources**\n\n1. Codebase: [link](https://github.com/pacman100/DHS-LLM-Workshop/tree/main/code_assi..."
          ],
          [
           "## Mix-and-Match LoRAs\n\nPEFT currently supports 3 ways of combining LoRA models, `linear`, `svd` and..."
          ],
          [
           "![assistant_chat_hf](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/b..."
          ],
          [
           "PEFT allows you to do it via `add_weighted_adapter`. Let's create a new adapter `code_buddy` with eq..."
          ],
          [
           "We can observe that `code_buddy` is performing on par with `copilot`, which was specifically finetun..."
          ],
          [
           "Yay! It correctly answers in detail how to create `LoraConfig` and related peft model along with cor..."
          ],
          [
           "![loss_plots](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/per..."
          ],
          [
           "```\ngit clone --recursive https://github.com/pacman100/mlc-llm.git && cd mlc-llm/\n```\n\n2. Install th..."
          ],
          [
           "```\n\n4. Update the config with the following values in `dist/starcoder1B-v2-personal-copilot-merged-..."
          ],
          [
           "```\n\n6. Change the endpoint of HF Code Completion extension in VS Code to point to the local server:..."
          ],
          [
           "--\ntitle: \"Hugging Face on PyTorch / XLA TPUs\"\nthumbnail: /blog/assets/13_pytorch_xla/pytorch_xla_th..."
          ],
          [
           "### XLA:TPU Device Type\n\nPyTorch / XLA adds a new `xla` device type to PyTorch. This device type wor..."
          ],
          [
           "```\n\nThis code should look familiar. PyTorch / XLA uses the same interface as regular PyTorch with a..."
          ],
          [
           "```\n\n### PyTorch / XLA Input Pipeline\n\nThere are two main parts to running a PyTorch / XLA model: (1..."
          ],
          [
           "```\n\n### Checkpoint Writing and Loading\n\nWhen a tensor is checkpointed from a XLA device and then lo..."
          ],
          [
           "```\n\n\n## PyTorch / XLA Library\n\nPyTorch / XLA is a Python package that uses the XLA linear algebra c..."
          ],
          [
           "### Trace, Compile, Execute, and Repeat\n\nFrom a userâ€™s point of view, a typical training regimen for..."
          ],
          [
           "```\n\nThis live graph is accumulated while the forward and backward passes are run on the user's prog..."
          ],
          [
           "```python\n>>> import torch_xla.debug.metrics as met\n>>> print(met.metrics_report())\nMetric: CompileT..."
          ],
          [
           "Accumulator: 04m22s555ms668.071us\n  ValueRate: 923ms872.877us / second\n  Rate: 4.33049 / second\n  Pe..."
          ],
          [
           "```\n\n### Train Your Transformer on Cloud TPUs\n\nTo configure your VM and Cloud TPUs, please follow [â€œ..."
          ],
          [
           "```bash\nconda activate torch-xla-1.7\nexport TPU_IP_ADDRESS=\"ENTER_YOUR_TPU_IP_ADDRESS\"  # ex. 10.0.0..."
          ],
          [
           "```\n\nThe above should complete training in roughly less than 200 minutes with an eval perplexity of ..."
          ],
          [
           "## Get Started with PyTorch / XLA on TPUs\nSee the [â€œRunning on TPUsâ€](https://github.com/huggingface..."
          ],
          [
           "--\ntitle: Deploying TensorFlow Vision Models in Hugging Face with TF Serving\nthumbnail: /blog/assets..."
          ],
          [
           "In this post, you'll see how to deploy a Vision Transformer (ViT) model (for image classification)\nl..."
          ],
          [
           "```\n\nBy default, `save_pretrained()` will first create a version directory\ninside the path we provid..."
          ],
          [
           "```\n\nThis should print:\n\n```bash\nViTImageProcessor {\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  ..."
          ],
          [
           "```\n\nYou also need to resize the image and transpose it so that it has leading\nchannel dimensions si..."
          ],
          [
           "```\n\n**Note on making the model accept string inputs**:\n\nWhen dealing with images via REST or gRPC r..."
          ],
          [
           "```\n\nYou can first derive the [concrete function](https://www.tensorflow.org/guide/function)\nfrom th..."
          ],
          [
           "```\n\nYou can notice that the model's signature has now changed. Specifically,\nthe input type is now ..."
          ],
          [
           "```\n\nFrom the above command, the important parameters are:\n\n- `rest_api_port` denotes the port numbe..."
          ],
          [
           "```\n\nTF Serving's request payload format specification for the REST endpoint\nis available [here](htt..."
          ],
          [
           "```\n\nThe REST API is -\n`http://localhost:8501/v1/models/vit:predict` following the specification fro..."
          ],
          [
           "```\n\nNow, you can get some predictions:\n\n```py\ngrpc_predictions = stub.Predict(request, 10.0)  # 10 ..."
          ],
          [
           "--\ntitle: \"Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU\" \nthumbnail: assets/133_trl_peft/th..."
          ],
          [
           "3- Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (e.g. PPO)\n..."
          ],
          [
           "### What is TRL?\n\nThe `trl` library aims at making the RL step much easier and more flexible so that..."
          ],
          [
           "In `trl` you can also use shared layers between reference and active models to avoid entire copies. ..."
          ],
          [
           "Many techniques have been adopted to tackle these challenges at scale. The most familiar paradigms a..."
          ],
          [
           "Therefore, we asked ourselves the following question: how far can we go with just data parallelism? ..."
          ],
          [
           "### Low rank adaptation and PEFT\n\nIn 2021, a paper called LoRA: Low-Rank Adaption of Large Language ..."
          ],
          [
           "The library is still under extensive and active development, with many upcoming features to be annou..."
          ],
          [
           "### Step 2: Add extra trainable adapters using `peft`\n\n| ![step2](https://huggingface.co/datasets/tr..."
          ],
          [
           "Overall there were three key steps and training scripts:\n\n1. **[Script](https://github.com/lvwerra/t..."
          ],
          [
           "| ![loss-20b](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/..."
          ],
          [
           "## Conclusion\n\nWe have implemented a new functionality in `trl` that allows users to fine-tune large..."
          ],
          [
           "## References\n\n- parallelism paradigms: [https://huggingface.co/docs/transformers/v4.17.0/en/paralle..."
          ],
          [
           "--\ntitle: \"Sentiment Analysis on Encrypted Data with Homomorphic Encryption\"\nthumbnail: /blog/assets..."
          ],
          [
           "```\n\nNow we can install all the required libraries for the this blog with the following command.\n```..."
          ],
          [
           "```\n\nThe output, then, looks like this:\n\n```\nProportion of positive examples: 16.14%\nProportion of n..."
          ],
          [
           "```\n\n## Text representation using a transformer\n\n[Transformers](https://en.wikipedia.org/wiki/Transf..."
          ],
          [
           "```\n\nThis should download the model, which is now ready to be used.\n\nUsing the hidden representation..."
          ],
          [
           "# Send the model to the device\n   transformer_model = transformer_model.to(device)\n   output_hidden_..."
          ],
          [
           "```\n\n```python\n# Let's vectorize the text using the transformer\nlist_text_X_train = text_X_train.tol..."
          ],
          [
           "```\n\nThe output is as follows:\n\n```\nBest score: 0.8378111718275654\nBest parameters: {'max_depth': 1,..."
          ],
          [
           "```\n\nWith the following output:\n```\nAccuracy: 0.8504\n```\n\n## Predicting over encrypted data\n\nNow let..."
          ],
          [
           "```\n\nThe output becomes:\n\n```\nCompilation time: 9.3354 seconds\nFHE inference time: 4.4085 seconds\n``..."
          ],
          [
           "```\n\nThese few lines are enough to export all the files needed for both the client and the server. Y..."
          ],
          [
           "1. train a machine learning model to classify tweets, and\n2. predict over encrypted data using this ..."
          ],
          [
           "--\ntitle: \"How to host a Unity game in a Space\"\nthumbnail: /blog/assets/124_ml-for-games/unity-in-sp..."
          ],
          [
           "```\n\n## Step 3: Open your Unity Project\n\nOpen the Unity project you want to host in your Space.\n\n<fi..."
          ],
          [
           "Then, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player S..."
          ],
          [
           "## Step 10: Enable Git-LFS for Large File Storage\n\nNavigate to your repository. Use the following co..."
          ],
          [
           "```\ngit lfs install\ngit lfs track Build/* \n```\n\n## Step 11: Push your Changes\n\nFinally, use the foll..."
          ],
          [
           "--\ntitle: \"Llama 2 on Amazon SageMaker a Benchmark\" \nthumbnail: /blog/assets/llama_sagemaker_benchma..."
          ],
          [
           "We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Be..."
          ],
          [
           "### What is Llama 2?\n\nLlama 2 is a family of LLMs from Meta, trained on 2 trillion tokens. Llama 2 c..."
          ],
          [
           "As metrics, we used Throughput and Latency defined as: \n\n- Throughput (tokens/sec): Number of tokens..."
          ],
          [
           "| Model       | Quantization | Instance       | concurrent requests | Latency (ms/token) median | Th..."
          ],
          [
           "### Best Throughput Deployment\n\nThe Best Throughput configuration maximizes the number of tokens tha..."
          ],
          [
           "| Model       | Quantization | Instance        | concurrent requests | Latency (ms/token) median | T..."
          ],
          [
           "### Best Latency Deployment\n\nThe Best Latency configuration minimizes the time it takes to generate ..."
          ],
          [
           "| Model       | Quantization | Instance        | concurrent requests | Latency (ms/token) median | T..."
          ],
          [
           "## Conclusions\n\nIn this benchmark, we tested 60 configurations of Llama 2 on Amazon SageMaker. For c..."
          ],
          [
           "--\ntitle: \"Announcing Evaluation on the Hub\"\nthumbnail: /blog/assets/82_eval_on_the_hub/thumbnail.pn..."
          ],
          [
           "Progress in AI has been nothing short of amazing, to the point where some people are now seriously d..."
          ],
          [
           "## On the Hub\n\nEvaluation on the Hub opens the door to so many interesting use cases. From the data ..."
          ],
          [
           "Evaluation on the Hub is meant to make your life easier. But of course, thereâ€™s a lot happening in t..."
          ],
          [
           "As the above image shows, to solve this problem, youâ€™ll need:\n\n* A dataset of dog, muffin, and fried..."
          ],
          [
           "![Advanced Configuration](/blog/assets/82_eval_on_the_hub/config.png)\n\nThe next step is to define wh..."
          ],
          [
           "![Pull Request](/blog/assets/82_eval_on_the_hub/pr.png)\n\nYou can also copy-paste the evaluation meta..."
          ],
          [
           "Benchmarks are saturating, meaning that machines outperform humans on certain test sets, almost fast..."
          ],
          [
           "--\ntitle: \"Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models\"\nthumbnail: ..."
          ],
          [
           "Similar to BERT and GPT2, massive pre-trained encoder-decoder models\nhave shown to significantly boo..."
          ],
          [
           "This notebook is divided into 4 parts:\n\n-   **Introduction** - *Short summary of pre-trained languag..."
          ],
          [
           "## **Introduction**\n\nRecently, pre-trained language models \\\\({}^1\\\\) have revolutionized the\nfield ..."
          ],
          [
           "The capability of pre-trained language models to effectively transfer\n*task-agnostic* knowledge to *..."
          ],
          [
           "Let\\'s see how BERT and GPT2 would be fit to model sequence-to-sequence\ntasks.\n\n### **BERT**\n\nBERT i..."
          ],
          [
           "Let\\'s visualize BERT.\n\n![texte du\nlien](https://raw.githubusercontent.com/patrickvonplaten/scientif..."
          ],
          [
           "### **GPT2**\n\nGPT2 is a *decoder-only* model, which makes use of *uni-directional*\n(*i.e.* \\\"causal\\..."
          ],
          [
           "\\\\(p_{\\theta_{\\text{GPT2}}}(\\mathbf{y}_i | \\mathbf{Y}_{0:i-1})\\\\) hereby\npresents the probability di..."
          ],
          [
           "![texte du\nlien](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/gpt2.pn..."
          ],
          [
           "GPT2 is mainly used for *open-domain* text generation. First, an input\nprompt \\\\(\\mathbf{Y}_{0:i-1}\\..."
          ],
          [
           "### **Encoder-Decoder**\n\nBecause *encoder-only* models require to know the output length *a\npriori*,..."
          ],
          [
           "In 2020, Sascha Rothe, Shashi Narayan, and Aliaksei Severyn investigated\nexactly this question in th..."
          ],
          [
           "\\\\({}^2\\\\) *Fine-tuning* is defined as the *task-specific* training of a\nmodel that has been initial..."
          ],
          [
           "There are multiple possibilities to warm-start an encoder-decoder model.\nOne can\n\n1.  initialize bot..."
          ],
          [
           "The encoder maps the input sequence \\\\(\\mathbf{X}_{1:n}\\\\) to a\ncontextualized encoded sequence \\\\(\\..."
          ],
          [
           "Each \\\"next-word\\\" conditional distributions is thereby defined by the\n*softmax* of the logit vector..."
          ],
          [
           "We can see that the encoder architecture corresponds 1-to-1 to BERT\\'s\narchitecture. The weight para..."
          ],
          [
           "Next, let\\'s illustrate how the decoder is warm-started.\n\n![texte du\nlien](https://raw.githubusercon..."
          ],
          [
           "2.  Second, BERT\\'s *bi-directional* self-attention layers have to be\n    changed to *uni-directiona..."
          ],
          [
           "3.  Third, the decoder outputs a sequence of logit vectors\n    \\\\(\\mathbf{L}_{1:m}\\\\) in order to de..."
          ],
          [
           "Let\\'s illustrate how a GPT2 checkpoint can be used to warm-start the\ndecoder.\n\n![texte du\nlien](htt..."
          ],
          [
           "Even though GPT2 resembles the decoder part of an encoder-decoder model\nmore than BERT, a GPT2-initi..."
          ],
          [
           "$$ \\mathbf{W}^{\\text{self-attn}, 3}_{k} = \\mathbf{W}^{\\text{self-attn}, 3}_{\\text{enc}, k} \\equiv \\m..."
          ],
          [
           "In the same way, we can warm-start an encoder-decoder model by sharing\nthe encoder weights with the ..."
          ],
          [
           "## **Warm-starting encoder-decoder models (Analysis)**\n\nIn this section, we will summarize the findi..."
          ],
          [
           "The following table shows a complete list of all investigated model\nvariants including the number of..."
          ],
          [
           "The model *Rnd2Rnd*, which is based on the BERT2BERT architecture,\ncontains 221M weight parameters -..."
          ],
          [
           "|Seq2Seq Task               |Datasets                                                               ..."
          ],
          [
           "|WMT14 DE =\\> EN            |[Bojar et al. (2014)](http://www.aclweb.org/anthology/W/W14/W14-3302)  ..."
          ],
          [
           "Depending on the task, a slightly different training regime was used.\n*E.g.* according to the size o..."
          ],
          [
           "As an example, the sentence:\n\n*Street Rod is the first in a series of two games released for the PC\n..."
          ],
          [
           "Let\\'s see how the models perform on sentence fusion and -splitting.\n\n  |Model                  | 10..."
          ],
          [
           "The first two columns show the performance of the encoder-decoder models\non the DiscoFuse evaluation..."
          ],
          [
           "### Machine Translation (WMT14)\n\nNext, the authors evaluated warm-started encoder-decoder models on ..."
          ],
          [
           "|Model                       |En \\\\(\\to\\\\) De (BLEU-4)   |De \\\\(\\to\\\\) En (BLEU-4)\n  |--------------..."
          ],
          [
           "Again, we observe a significant performance boost by warm-starting the\nencoder-part, with *BERT2Rnd*..."
          ],
          [
           "### Summarization (CNN/Dailymail, BBC XSum, Gigaword)\n\nFinally, the encoder-decoder models were eval..."
          ],
          [
           "Alright, let\\'s take a look at the results.\n\n  |Model                  |CNN/Dailymail (Rouge-2)   |B..."
          ],
          [
           "Furthermore, the shared encoder-decoder models are the best performing\nmodels for summarization. *Ro..."
          ],
          [
           "-   Next, we noticed that it is often beneficial to share encoder and\n    decoder weights, especiall..."
          ],
          [
           "For each of the above tasks, the most performant models were ported to\nðŸ¤—Transformers and can be acce..."
          ],
          [
           "------------------------------------------------------------------------\n\n\\\\({}^1\\\\) To retrieve BLE..."
          ],
          [
           "In addition, the following list provides a condensed version of this and\nother notebooks on warm-sta..."
          ],
          [
           "```\nLet's start by downloading the *CNN/Dailymail* dataset.\n\n```python\nimport datasets\ntrain_data = ..."
          ],
          [
           "```python\nOUTPUT:\n-------\nArticle:..."
          ],
          [
           "\"\"\"It's official: U.S. President Barack Obama wants lawmakers to weigh in on whether to use military..."
          ],
          [
           "of mass destruction.\" It's a step that is set to turn an international crisis into a fierce domestic..."
          ],
          [
           "to carry out this military action without specific congressional authorization, I know that the coun..."
          ],
          [
           "full remarks . Syrian crisis: Latest developments . U.N. inspectors leave Syria . Obama's remarks ca..."
          ],
          [
           "attack in a Damascus suburb on August 21 has been a key point of global debate over the Syrian crisi..."
          ],
          [
           "who has said he wants to wait until the U.N. team's final report is completed before presenting it t..."
          ],
          [
           "and that \"a military solution is not an option.\" Bergen:  Syria is a problem from hell for the U.S. ..."
          ],
          [
           "was a blow to Obama's hopes of getting strong backing from key NATO allies. On Saturday, Obama propo..."
          ],
          [
           "who would do our people harm. In a world with many dangers, this menace must be confronted.\" Syria m..."
          ],
          [
           "Saturday continued to shore up support for a strike on the al-Assad government. He spoke by phone wi..."
          ],
          [
           "military officials said they remained at the ready. 5 key assertions: U.S. intelligence report on Sy..."
          ],
          [
           "decision. House Speaker John Boehner, Majority Leader Eric Cantor, Majority Whip Kevin McCarthy and ..."
          ],
          [
           "before any U.S. action. British Prime Minister David Cameron, whose own attempt to get lawmakers in ..."
          ],
          [
           "in the world, among allies of the US or in the United States itself,\" Alexei Pushkov, chairman of th..."
          ],
          [
           "Iran stand by Assad . Syria's government unfazed . After Obama's speech, a military and political an..."
          ],
          [
           "during a meeting with a delegation of Syrian expatriates from Italy, according to a banner on Syria ..."
          ],
          [
           "in terms of taking the issue to Parliament,\" said Bashar Jaafari, Syria's ambassador to the United N..."
          ],
          [
           "Obama said \"all told, well over 1,000 people were murdered.\" U.S. Secretary of State John Kerry on F..."
          ],
          [
           "Summary:\n\"\"\"Syrian official: Obama climbed to the top of the tree, \"doesn't know how to get down\"\\nO..."
          ],
          [
           "```\n\nThe input data seems to consist of short news articles. Interestingly,\nthe labels appear to be ..."
          ],
          [
           "```\n\nNext, we make use of `.map()` to compute the length of the article and\nits summary. Since we kn..."
          ],
          [
           "```\n\nHaving computed the length for the first 10000 samples, we should now\naverage them together. Fo..."
          ],
          [
           "```\n\nWe can see that on average an article contains 848 tokens with *ca.* 3/4\nof the articles being ..."
          ],
          [
           "```python\nencoder_max_length=512\ndecoder_max_length=128\n\ndef process_data_to_model_inputs(batch):\n  ..."
          ],
          [
           "```\n\nIn this notebook, we train and evaluate the model just on a few training\nexamples for demonstra..."
          ],
          [
           "```\n\nSo far, the data was manipulated using Python\\'s `List` format. Let\\'s\nconvert the data to PyTo..."
          ],
          [
           "```\n\nIn contrast to other model classes in ðŸ¤—Transformers, the\n`EncoderDecoderModel` class has two me..."
          ],
          [
           "```python\nOUTPUT:\n-------\n\"\"\"Some weights of the model checkpoint at bert-base-uncased were not used..."
          ],
          [
           "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased ..."
          ],
          [
           "'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value..."
          ],
          [
           "'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.q..."
          ],
          [
           "'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output..."
          ],
          [
           "'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.v..."
          ],
          [
           "'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key..."
          ],
          [
           "'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention...."
          ],
          [
           "'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output..."
          ],
          [
           "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and ..."
          ],
          [
           "```\n\nFor once, we should take a good look at the warning here. We can see\nthat two weights correspon..."
          ],
          [
           "```python\nOUTPUT:\n-------\n    EncoderDecoderModel(\n      (encoder): BertModel(\n        (embeddings):..."
          ],
          [
           "(dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermedia..."
          ],
          [
           ")\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, o..."
          ],
          [
           "(self): BertSelfAttention(\n                    (query): Linear(in_features=768, out_features=768, bi..."
          ],
          [
           "(dropout): Dropout(p=0.1, inplace=False)\n                  )\n                )\n                (inte..."
          ],
          [
           ")\n                )\n                (crossattention): BertAttention(\n                  (self): BertS..."
          ],
          [
           "(transform): BertPredictionHeadTransform(\n              (dense): Linear(in_features=768, out_feature..."
          ],
          [
           "```\n\nWe see that `bert2bert.encoder` is an instance of `BertModel` and that\n`bert2bert.decoder` one ..."
          ],
          [
           "```python\nOUTPUT:\n-------\n    EncoderDecoderConfig {\n      \"_name_or_path\": \"bert2bert\",\n      \"arch..."
          ],
          [
           "\"label2id\": {\n          \"LABEL_0\": 0,\n          \"LABEL_1\": 1\n        },\n        \"layer_norm_eps\": 1e..."
          ],
          [
           "\"vocab_size\": 30522,\n        \"xla_device\": null\n      },\n      \"encoder\": {\n        \"_name_or_path\":..."
          ],
          [
           "\"length_penalty\": 1.0,\n        \"max_length\": 20,\n        \"max_position_embeddings\": 512,\n        \"mi..."
          ],
          [
           "```\n\nThe config is similarly composed of an encoder config and a decoder\nconfig both of which are in..."
          ],
          [
           "```\n\n```python\nOUTPUT:\n-------\nNum Params. Shared: 137298244, Non-Shared: 247363386\n```\n\nIn this not..."
          ],
          [
           "```\n\nNext, let\\'s define all parameters related to beam search decoding.\nSince `bart-large-cnn` yiel..."
          ],
          [
           "```\n\nThe `Seq2SeqTrainer` extends ðŸ¤—Transformer\\'s Trainer for encoder-decoder\nmodels. In short, it a..."
          ],
          [
           "```\n\nAlso, we need to define a function to correctly compute the ROUGE score\nduring validation. Sinc..."
          ],
          [
           "```\n\nGreat, now we can pass all arguments to the `Seq2SeqTrainer` and start\nfinetuning. Executing th..."
          ],
          [
           "```\n\n### **Evaluation**\n\nIn a final step, we might want to evaluate the *BERT2BERT* model on the\ntes..."
          ],
          [
           "```\n\nLet\\'s run the map function to obtain the *results* dictionary that has\nthe model\\'s predicted ..."
          ],
          [
           "--\ntitle: \"Introducing Hugging Face for Education ðŸ¤—\"\nthumbnail: /blog/assets/61_education/thumbnail...."
          ],
          [
           "ðŸ—£ï¸ Our goal is to make the potential and limitations of machine learning understandable to everyone...."
          ],
          [
           "## ðŸ¤—Â **Education for Beginners**\n\nðŸ—£ï¸ We want to lower the barrier to becoming a machine learning eng..."
          ],
          [
           "ðŸ—£ï¸ We want to empower educators with tools and offer collaborative spaces where students can build m..."
          ],
          [
           "1ï¸âƒ£Â [A Tour through the Hugging Face Hub](https://github.com/huggingface/education-toolkit/blob/main..."
          ],
          [
           "## ðŸ¤—Â **Education Events & News**\n\n- **09/08**[EVENT]: ML Demo.cratization tour in Argentina at 2pm (..."
          ],
          [
           "--\ntitle: \"How Hugging Face Accelerated Development of Witty Works Writing Assistant\"\nthumbnail: /bl..."
          ],
          [
           "### First experiments \nWitty Works first chose a basic machine learning approach to build their assi..."
          ],
          [
           "```\n\n### Solutions provided by the [Hugging Face Experts](https://huggingface.co/support?utm_source=..."
          ],
          [
           "```\n\nTo fine-tune a vanilla transformers-based classifier, such as a simple BERT model, Witty Works ..."
          ],
          [
           "```\n\nReducing the number of sentences was essential to ensure that model training remained fast and ..."
          ],
          [
           "```\n\nWith the guidance of the Hugging Face experts, Witty Works saved time and money by implementing..."
          ],
          [
           "--\ntitle: Inference for PROs\nthumbnail: /blog/assets/inference_pro/thumbnail.png\nauthors:\n  - user: ..."
          ],
          [
           "## Contents\n\n- [Supported Models](#supported-models)\n- [Getting started with Inference for PROs](#ge..."
          ],
          [
           "| Model               | Size                                                                        ..."
          ],
          [
           "| Code Llama Base     | [7B](https://huggingface.co/codellama/CodeLlama-7b-hf) and [13B](https://hug..."
          ],
          [
           "Inference for PROs makes it easy to experiment and prototype with new models without having to deplo..."
          ],
          [
           "```\n\nWhich would print something like this:\n\n```json\n[\n  {\n    \"generated_text\": \"In a surprising tu..."
          ],
          [
           "```\n\nIf you don't want to pass the token explicitly every time you instantiate the client, you can u..."
          ],
          [
           "```\n\nThis example shows the structure of the first message in a multi-turn conversation. Note how th..."
          ],
          [
           "```\n\nThis same format can be used with Code Llama Instruct to engage in technical conversations with..."
          ],
          [
           "```\n\nAs you can see, the format used for infilling follows this pattern:\n\n```\nprompt = f\"<PRE> {prom..."
          ],
          [
           "```\n\n![SDXL example generation](https://huggingface.co/datasets/huggingface/documentation-images/res..."
          ],
          [
           "- `do_sample`: If set to `False` (the default), the generation method will be _greedy search_, which..."
          ],
          [
           "### Controlling Image Generation\n\nIf you want finer-grained control over images generated with the S..."
          ],
          [
           "```\n\n### Streaming\n\nToken streaming is the mode in which the server returns the tokens one by one as..."
          ],
          [
           "```\n\n## Subscribe to PRO\n\nYou can sign up today for a PRO subscription [here](https://huggingface.co..."
          ],
          [
           "--\ntitle: \"Towards Encrypted Large Language Models with FHE\" \nthumbnail: /blog/assets/encrypted-llm/..."
          ],
          [
           "## Fully Homomorphic Encryption (FHE) Can Solve LLM Privacy Challenges\n\nZamaâ€™s solution to the chall..."
          ],
          [
           "## Implementation of a LLM layer with FHE\n\nNext, youâ€™ll see how to encrypt a single attention head o..."
          ],
          [
           "This graph shows that 4-bit quantization maintains 96% of the original accuracy. The experiment is d..."
          ],
          [
           "```\n\nThe forward pass is then overwritten so that the first head of the multi-head attention mechani..."
          ],
          [
           "# Extract the queries, keys and vales\n        q_qkv = q_qkv.expand_dims(axis=1, key=f\"unsqueeze_{sel..."
          ],
          [
           "```\n\nOther computations in the model remain in floating point, non-encrypted and are expected to be ..."
          ],
          [
           "```\n\nRunning this, you will see the following print out: â€œCircuit compiled with 8 bit-widthâ€. This c..."
          ],
          [
           "Zama libraries [Concrete](https://github.com/zama-ai/concrete) and [Concrete-ML](https://github.com/..."
          ],
          [
           "--\ntitle: \"Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny\"\nthumbnail:..."
          ],
          [
           "## Knowledge Distillation\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingf..."
          ],
          [
           "In this particular type of knowledge distillation, the student model is trained to do the normal dif..."
          ],
          [
           "Image taken from the [paper](https://arxiv.org/abs/2305.15798)  â€œOn Architectural Compression of Tex..."
          ],
          [
           "```python\n\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_p..."
          ],
          [
           "```\n\n## Speed in terms of inference latency\n\nWe have observed that distilled models are up to 100% f..."
          ],
          [
           "## LoRA Training\n\nOne of the advantages of LoRA training on a distilled model is faster training. Be..."
          ],
          [
           "--\ntitle: \"The Hugging Face Hub for Galleries, Libraries, Archives and Museums\"\nthumbnail: /blog/ass..."
          ],
          [
           "You can read the whole post or jump to the most relevant sections! \n\n- If you don't know what the Hu..."
          ],
          [
           "Spaces make hosting and making your application accessible for others to use much more straightforwa..."
          ],
          [
           "We can find NER models on the Hub by filtering models by task. In this case, we choose `token-classi..."
          ],
          [
           "We can make datasets available via the Hugging Face hub in various ways. I'll walk through an exampl..."
          ],
          [
           "Once you have done this you can choose a name for your new dataset repository. You can also create t..."
          ],
          [
           "You can edit metadata using the `Metadata UI` editor. This allows you to specify the license, langua..."
          ],
          [
           "## Why might Galleries, Libraries, Archives and Museums want to use the Hugging Face hub?\n\nThere are..."
          ],
          [
           "### [Nasjonalbiblioteket AI Lab](https://huggingface.co/NbAiLab) \nThe AI lab at the National Library..."
          ],
          [
           "## Hub features for Galleries, Libraries, Archives and Museums\n\nThe Hub supports many features which..."
          ],
          [
           "If you require any assistance while using the Hugging Face Hub, there are several avenues you can ex..."
          ],
          [
           "--\ntitle: \"Putting ethical principles at the core of the research lifecycle\"\nthumbnail: /blog/assets..."
          ],
          [
           "## Limitations of this ethical charter\n\nThis document is a work in progress and reflects a state of ..."
          ],
          [
           "## Values for the project\n\n- **Be transparent:** We are transparent and open about the intent, sourc..."
          ],
          [
           "--\ntitle: \"StarCoder: A State-of-the-Art LLM for Code\" \nthumbnail: /blog/assets/141_starcoder/starco..."
          ],
          [
           "## Evaluation\n\nWe thoroughly evaluated StarCoder and several similar models and a variety of benchma..."
          ],
          [
           "| **Model**          | **HumanEval** | **MBPP** |\n|--------------------|--------------|----------|\n|..."
          ],
          [
           "## Tech Assistant\n\nWith the exhaustive evaluations we found that StarCoder is very capable at writin..."
          ],
          [
           "## Links\n\n### Models\n- [Paper](https://arxiv.org/abs/2305.06161): A technical report about StarCoder..."
          ],
          [
           "### Data & Governance\n- [StarCoderData](https://huggingface.co/datasets/bigcode/starcoderdata): Pret..."
          ],
          [
           "--\ntitle: \"Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 M..."
          ],
          [
           "In keeping with Hugging Faceâ€™s core values of *openness* and *accountability*, we are sharing a coll..."
          ],
          [
           "In keeping with our core value of *democratization*, we have also spent a lot of time speaking publi..."
          ],
          [
           "- Comments from [Sasha](https://huggingface.co/sasha) on **AIâ€™s energy use and carbon emissions** ([..."
          ],
          [
           "penning part of a [Wall Street Journal op-ed on the topic](https://www.wsj.com/articles/artificial-i..."
          ],
          [
           "addressing how **marginalized workers create the data for AI** ([The Globe and Mail](https://www.the..."
          ],
          [
           "- Comments from [Nathan](https://huggingface.co/natolambert) on the state of the art on **language m..."
          ],
          [
           "- Comments from [Meg](https://huggingface.co/meg) on **AI and misinformation** ([CNN](https://www.cn..."
          ],
          [
           "- Comments from [Irene](https://huggingface.co/irenesolaiman) on understanding the **regulatory land..."
          ],
          [
           "- Comments from [Giada](https://huggingface.co/giadap) on the concepts of **AI â€œsingularityâ€** ([Pop..."
          ],
          [
           "Some of our talks released this summer include [Giada](https://huggingface.co/giadap)â€™s [TED present..."
          ],
          [
           "Of course, we have also made progress on our regular work (our â€œwork workâ€). The fundamental value o..."
          ],
          [
           "[Nazneen](https://huggingface.co/nazneen) and others on [Responsible Generative AI](https://www.yout..."
          ],
          [
           "cross-disciplinary approach](https://avidml.org/events/tti2023/) and [Assessing the Impacts of Gener..."
          ],
          [
           "We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](h..."
          ],
          [
           "Finally, we have been surprised and delighted by public recognition for many of the society & ethics..."
          ],
          [
           "--\ntitle: 'Convert Transformers to ONNX with Hugging Face Optimum'\nthumbnail: /blog/assets/81_conver..."
          ],
          [
           "Let's get started! ðŸš€\n\n---\n\nIf you are interested in optimizing your models to run with maximum effic..."
          ],
          [
           "âž¡ï¸[Learn more about ONNX.](https://onnx.ai/about.html)\n</div>\n    </div>\n        </div>\n<html itemsc..."
          ],
          [
           "[âž¡ï¸Â Learn more about Optimum](https://huggingface.co/blog/hardware-partners-program)\n</div>\n    </di..."
          ],
          [
           "- ALBERT\n- BART\n- BERT\n- DistilBERT\n- ELECTRA\n- GPT Neo\n- GPT-J\n- GPT-2\n- RoBERTa\n- T5\n- ViT\n- XLM\n-..."
          ],
          [
           "```\n\nexporting our checkpoint with `export` \n\n```python\nimport torch\nfrom transformers import AutoMo..."
          ],
          [
           "```\n\nExporting our checkpoint with the `transformers.onnx`.\n\n```python\nfrom pathlib import Path\nimpo..."
          ],
          [
           "```\n\nExporting our checkpoint with `ORTModelForSequenceClassification`\n\n```python\nfrom optimum.onnxr..."
          ],
          [
           "```\n\nThe best part about the conversion with Optimum is that you can immediately use the `model` to ..."
          ],
          [
           "--\ntitle: \"Comments on U.S. National AI Research Resource Interim Report\"\nthumbnail: /blog/assets/92..."
          ],
          [
           "- Monitor for Open-Source and Open-Science for High Misuse and Malicious Use Potential\n    - Harm mu..."
          ],
          [
           "--\ntitle: \"3D Asset Generation: AI for Game Development #3\"\nthumbnail: /blog/assets/124_ml-for-games..."
          ],
          [
           "In this part, we'll talk about how you can use AI to generate 3D Assets. The short answer is: you ca..."
          ],
          [
           "- [DreamFusion](https://dreamfusion3d.github.io/) uses 2D diffusion to generate 3D assets.\n- [CLIPMa..."
          ],
          [
           "What does all of this mean if you're a game developer? Currently, nothing. This technology hasn't re..."
          ],
          [
           "Since NeRF-to-mesh, like photogrammetry, is currently most suited to creating ultra-high-fidelity as..."
          ],
          [
           "Of course, only time will tell. If you want to keep up with advancements as they come, feel free to ..."
          ],
          [
           "--\ntitle: \"Opinion Classification with Kili and HuggingFace AutoTrain\"\nthumbnail: /blog/assets/59_op..."
          ],
          [
           "## AutoTrain with HuggingFace\n\nAutomated Machine Learning is a term for automating a Machine Learnin..."
          ],
          [
           "Kili is a commercial tool but you can also create a free developer account to try Kiliâ€™s tools. You ..."
          ],
          [
           "For the labeling part, we need to create a project in Kiliâ€™s platform at first. We can use either th..."
          ],
          [
           "```python\nimport os\n#we will process the data (which is a csv file)\nimport pandas as pd\n\n#API client..."
          ],
          [
           "```\n\nIn order to access the platform, we need to authenticate our client\n\n```python\nAPI_KEY = os.get..."
          ],
          [
           "```\n\nNow we can start to prepare our interface, the interface is just a dictionary in Python. We wil..."
          ],
          [
           "for label, color in entity_dict.items():\n    label_upper = label.strip().upper().replace(' ', '_')\n ..."
          ],
          [
           "```\n\nWe are ready to upload our data to the project. The `append_many_to_dataset` method can be used..."
          ],
          [
           "```\n\nIt simply imports the given `dataset` DataFrame to a project specified by project_id.\n\nWe can s..."
          ],
          [
           "```\n\nIt wasnâ€™t difficult to use the Python API, the helper methods we used covered many difficulties..."
          ],
          [
           "# read dataframes\ndf1 = pd.read_csv(args['first'])\ndf2 = pd.read_csv(args['second'])\n\n# concating tw..."
          ],
          [
           "```\n\n## Labeling\n\nNow that we have the source data uploaded, the platform has a built-in labeling in..."
          ],
          [
           "API_KEY = os.getenv('KILI_API_KEY')\ndataset_path = '../data/processed/lowercase_cleaned_dataset.csv'..."
          ],
          [
           "print('Got the samples!')\n\n# we will pass the skipped samples\ndf_ns = df[~df['skipped']].copy()\n\n# e..."
          ],
          [
           "```\n\nNice! We now have the labeled data as a csv file. Let's create a dataset repository in HuggingF..."
          ],
          [
           "2. We can select the dataset repository we created before or upload the dataset again. Then we need ..."
          ],
          [
           "Ray tune is a popular library for hyper-parameter optimization which comes with many SOTA algorithms..."
          ],
          [
           "# progress bar\nfrom tqdm import tqdm\n\n# data manipulation / reading\nimport numpy as np\nimport pandas..."
          ],
          [
           "```\n\nWe will set a seed for the libraries we use for reproducibility\n\n```python\ndef seed_all(seed):\n..."
          ],
          [
           "```\n\nWe can download the model easily by specifying HuggingFace hub repository. It is also needed to..."
          ],
          [
           "```\n\nI also defined two dictionaries for mapping labels to indices and indices to labels.\n\n```python..."
          ],
          [
           "```\n\nAnother utility function that returns stratified and tokenized Torch dataset splits:\n\n```python..."
          ],
          [
           "```\n\nNow we can perform the search! Letâ€™s start by processing the data:\n\n```python\ntokenized_train_s..."
          ],
          [
           "```\n\nWe performed the search with 20 and 40 trials respectively, the results are shown below. The we..."
          ],
          [
           "We use Google Colab for the inference ([here](https://colab.research.google.com/drive/1kGYl_YcMmA2gj..."
          ],
          [
           "We won't do a detailed analysis of the reviews, a basic understanding of potential problems would su..."
          ],
          [
           "--\ntitle: \"AI for Game Development: Creating a Farming Game in 5 Days. Part 2\"\nthumbnail: /blog/asse..."
          ],
          [
           "### The Short Version\n\nThe short version is straightforward: ask [ChatGPT](https://chat.openai.com/c..."
          ],
          [
           "**Transformers**, [introduced in 2017](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee..."
          ],
          [
           "#### Limitations\n\nChatGPT often sounds very convincing, while being wrong. Here is an [archive of Ch..."
          ],
          [
           "--\ntitle: How to train a new language model from scratch using Transformers and Tokenizers\nthumbnail..."
          ],
          [
           "> N.B. You wonâ€™t need to understand Esperanto to understand this post, but if you do want to learn i..."
          ],
          [
           "We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same spec..."
          ],
          [
           "```\n\nAnd hereâ€™s a slightly accelerated capture of the output:\n\n![tokenizers](assets/01_how-to-train/..."
          ],
          [
           "```\n\nWhat is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer..."
          ],
          [
           "```\n\n## 3. Train a language model from scratch\n\n**Update:** The associated Colab notebook uses our n..."
          ],
          [
           "Hereâ€™s a simple version of our EsperantoDataset.\n\n```python\nfrom torch.utils.data import Dataset\n\ncl..."
          ],
          [
           "```\n\nIf your dataset is very large, you can opt to load and tokenize examples on the fly, rather tha..."
          ],
          [
           "```\n\nAs usual, pick the largest batch size you can fit on your GPU(s). \n\n**ðŸ”¥ðŸ”¥ðŸ”¥ Letâ€™s start training!..."
          ],
          [
           "# The sun <mask>.\n# =>\n\nresult = fill_mask(\"La suno <mask>.\")\n\n# {'score': 0.2526160776615143, 'sequ..."
          ],
          [
           "```\n\nOk, simple syntax/grammar works. Letâ€™s try a slightly more interesting prompt:\n\n```python\nfill_..."
          ],
          [
           "```\n\n> â€œ**Jen la komenco de bela tago**â€, indeed!\n\nWith more complex prompts, you can probe whether ..."
          ],
          [
           "nlp = pipeline(\n    \"ner\",\n    model=MODEL_PATH,\n    tokenizer=MODEL_PATH,\n)\n# or instantiate a Toke..."
          ],
          [
           "```\n\n**Looks like it worked! ðŸ”¥**\n\n<small>For a more challenging dataset for NER, <a href=\"https://gi..."
          ],
          [
           "--\ntitle: \"Using & Mixing Hugging Face Models with Gradio 2.0\"\nthumbnail: /blog/assets/22_gradio/gra..."
          ],
          [
           "![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)\n\nBy default, this uses HuggingFaceâ€™s hoste..."
          ],
          [
           "--\ntitle: \"Hugging Face and Graphcore partner for IPU-optimized Transformers\"\nthumbnail: /blog/asset..."
          ],
          [
           "Making Poplar compatible with these widely used, third-party systems allows developers to easily por..."
          ],
          [
           "Full details of this example can be found in the Graphcore blog [BERT-Large training on the IPU expl..."
          ],
          [
           "--\ntitle: \"AudioLDM 2, but faster âš¡ï¸\" \nthumbnail: /blog/assets/161_audioldm2/thumbnail.png\nauthors:\n..."
          ],
          [
           "Read to the end to find out how to generate a 10 second audio sample in just 1 second!\n\n## Model ove..."
          ],
          [
           "In the `diffusers` implementation, these projections are defined by the [AudioLDM2ProjectionModel](h..."
          ],
          [
           "where the initial latent variable \\\\(\\boldsymbol{z}_{0}\\\\) is drawn from a normal distribution \\\\(\\m..."
          ],
          [
           "For full details on how the AudioLDM 2 model is trained, the reader is referred to the [AudioLDM 2 p..."
          ],
          [
           "| Checkpoint                                                            | Task          | Model Size..."
          ],
          [
           "```python\nfrom diffusers import AudioLDM2Pipeline\n\nmodel_id = \"cvssp/audioldm2\"\npipe = AudioLDM2Pipe..."
          ],
          [
           "```\n**Output:**\n```\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ..."
          ],
          [
           "```\n\nCool! That run took about 13 seconds to generate. Let's have a listen to the output audio:\n\n```..."
          ],
          [
           "```\n\n<audio controls> \n  <source src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "```\n\n**Output:**\n```\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:12<00:00, 16.60it..."
          ],
          [
           "```\n\n**Output:**\n\n```\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:09<00:00, 20.94i..."
          ],
          [
           "```\n\n**Output:**\n```\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:23<00:00,  2.39it..."
          ],
          [
           "```\n\n**Output:**\n```\n[diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler,\n diffusers...."
          ],
          [
           "```\n\nAlright! We've got a long list of schedulers to choose from ðŸ“. By default, AudioLDM 2 uses the ..."
          ],
          [
           "```\n\nLet's set the number of inference steps to 20 and re-run the generation with the new scheduler...."
          ],
          [
           "```\n\n<audio controls> \n  <source src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "Let's try generating an audio sample 2.5 minutes (150 seconds) in duration. We'll also generate 4 ca..."
          ],
          [
           "```\n\n**Output:**\n```\n---------------------------------------------------------------------------\nOut..."
          ],
          [
           "```\n\nUnless you have a GPU with high RAM, the code above probably returned an OOM error. While the A..."
          ],
          [
           "```\n\n**Output:**\n```\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:36<00:00,  1.82s/it..."
          ],
          [
           "--\ntitle: \"Accelerating Stable Diffusion Inference on Intel CPUs\"\nthumbnail: /blog/assets/136_stable..."
          ],
          [
           "## The Diffusers library\n\nThe [Diffusers](https://huggingface.co/docs/diffusers/index) library makes..."
          ],
          [
           "```\nvirtualenv sd_inference\nsource sd_inference/bin/activate\npip install pip --upgrade\npip install t..."
          ],
          [
           "```\n\nThe average latency is **32.3 seconds**. As demonstrated by this [Intel Space](https://huggingf..."
          ],
          [
           "```\n\nOpenVINO automatically optimizes the model for the `bfloat16` format. Thanks to this, the avera..."
          ],
          [
           "```\n\nWith a static shape, average latency is slashed to **4.7 seconds**, an additional 3.5x speedup...."
          ],
          [
           "```\n\nNext, we install the `libiomp` library to optimize parallel processing. It's part of [Intel Ope..."
          ],
          [
           "```\npip install intel_extension_for_pytorch==1.13.100\n```\n\nWe then update our code to optimize each ..."
          ],
          [
           "```\n\nWe also enable the `bloat16` data format to leverage the AMX tile matrix multiply unit (TMMU) a..."
          ],
          [
           "```\n\nWith this final version, inference latency is now down to **5.05 seconds**. Compared to our ini..."
          ],
          [
           "--\ntitle: \"A Complete Guide to Audio Datasets\" \nthumbnail: /blog/assets/116_audio_datasets/thumbnail..."
          ],
          [
           "## The Hub\n\nThe Hugging Face Hub is a platform for hosting models, datasets and demos, all open sour..."
          ],
          [
           "The Dataset Preview is a brilliant way of experiencing audio datasets before committing to using the..."
          ],
          [
           "```python\nfrom datasets import load_dataset\n\ngigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"xs..."
          ],
          [
           "```\n\n**Print Output:**\n```python\nDatasetDict({\n    train: Dataset({\n        features: ['segment_id',..."
          ],
          [
           "```\n\n**Print Output:**\n```python\nDataset({\n    features: ['segment_id', 'speaker', 'text', 'audio', ..."
          ],
          [
           "```\n\n**Print Output:**\n```python\n{'segment_id': 'YOU0000000315_S0000660',\n 'speaker': 'N/A', \n 'text..."
          ],
          [
           "```\n\nWe can see that there are a number of features returned by the training split, including `segme..."
          ],
          [
           "```\n\nGreat! We can see that we've got the two required columns `text` and `audio`. The `text` is a s..."
          ],
          [
           "```\n\nRe-loading the first audio sample in the GigaSpeech dataset will resample it to the desired sam..."
          ],
          [
           "```\n\n**Print Output:**\n\n```python\n{'text': \"AS THEY'RE LEAVING <COMMA> CAN KASH PULL ZAHRA ASIDE REA..."
          ],
          [
           "```\n\nEasy! `cast_column` provides a straightforward mechanism for resampling audio datasets as and w..."
          ],
          [
           "```\n\nWe can apply the data preparation function to all of our training examples using ðŸ¤— Datasets' `m..."
          ],
          [
           "```\n\nAnd with that, we have the GigaSpeech dataset fully prepared for our model! In total, this proc..."
          ],
          [
           "This is analogous to _downloading_ a TV show versus _streaming_ it. When we download a TV show, we d..."
          ],
          [
           "There is one caveat to streaming mode. When downloading a dataset, both the raw data and processed d..."
          ],
          [
           "```\n\nAll the steps covered so far in this tutorial can be applied to the streaming dataset without a..."
          ],
          [
           "### English Speech Recognition\n\nSpeech recognition, or speech-to-text, is the task of mapping from s..."
          ],
          [
           "| Dataset                                                                                 | Domain  ..."
          ],
          [
           "| [GigaSpeech](https://huggingface.co/datasets/speechcolab/gigaspeech)                    | Audioboo..."
          ],
          [
           "Refer to the [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/m..."
          ],
          [
           "```\n\n#### [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)\nCommo..."
          ],
          [
           "```\n\n#### [TED-LIUM](https://huggingface.co/datasets/LIUM/tedlium)\nTED-LIUM is a dataset based on En..."
          ],
          [
           "```\n\n#### [Earnings-22](https://huggingface.co/datasets/revdotcom/earnings22)\nEarnings-22 is a 119-h..."
          ],
          [
           "```\n\n### Multilingual Speech Recognition\n\nMultilingual speech recognition refers to speech recogniti..."
          ],
          [
           "#### [FLEURS](https://huggingface.co/datasets/google/fleurs)\nFLEURS (Few-shot Learning Evaluation of..."
          ],
          [
           "### Audio Classification\n\nAudio classification is the task of mapping a raw audio input to a class l..."
          ],
          [
           "## Closing Remarks\n\nIn this blog post, we explored the Hugging Face Hub and experienced the Dataset ..."
          ],
          [
           "--\ntitle: The Annotated Diffusion Model\nthumbnail: /blog/assets/78_annotated-diffusion/thumbnail.png..."
          ],
          [
           "We'll go over the original DDPM paper by ([Ho et al., 2020](https://arxiv.org/abs/2006.11239)), impl..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"assets/78_annotated-diffusion/ddpm_paper.png\" width=\"500\" />\n<..."
          ],
          [
           "```\n\n## What is a diffusion model?\n\nA (denoising) diffusion model isn't that complex if you compare ..."
          ],
          [
           "## In more mathematical form\n\nLet's write this down more formally, as ultimately we need a tractable..."
          ],
          [
           "Recall that a normal distribution (also called Gaussian distribution) is defined by 2 parameters: a ..."
          ],
          [
           "Now, if we knew the conditional distribution \\\\(p(\\mathbf{x}_{t-1} | \\mathbf{x}_t)\\\\), then we could..."
          ],
          [
           "Hence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors ..."
          ],
          [
           "So we continue, assuming that our neural network only needs to learn/represent the mean of this cond..."
          ],
          [
           "A direct consequence of the constructed forward process \\\\(q\\\\), as shown by Sohl-Dickstein et al., ..."
          ],
          [
           "Another beauty of this property, as shown in Ho et al. is that one can (after some math, for which w..."
          ],
          [
           "Here, \\\\(\\mathbf{x}_0\\\\) is the initial (real, uncorrupted) image, and we see the direct noise level..."
          ],
          [
           "In reality, all of this is done on batches of data, as one uses stochastic gradient descent to optim..."
          ],
          [
           "As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of ..."
          ],
          [
           "def Downsample(dim, dim_out=None):\n    # No More Strided Convolutions or Pooling\n    return nn.Seque..."
          ],
          [
           "```\n\n### Position embeddings\n\nAs the parameters of the neural network are shared across time (noise ..."
          ],
          [
           "```\n\n### ResNet block\n\nNext, we define the core building block of the U-Net model. The DDPM authors ..."
          ],
          [
           "def forward(self, x, scale_shift=None):\n        x = self.proj(x)\n        x = self.norm(x)\n\n        i..."
          ],
          [
           "```\n\n### Attention module\n\nNext, we define the attention module, which the DDPM authors added in bet..."
          ],
          [
           "def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim=1)\n    ..."
          ],
          [
           "q = q.softmax(dim=-2)\n        k = k.softmax(dim=-1)\n\n        q = q * self.scale\n        context = to..."
          ],
          [
           "```\n\n### Group normalization\n\nThe DDPM authors interleave the convolutional/attention layers of the ..."
          ],
          [
           "```\n\n### Conditional U-Net\n\nNow that we've defined all building blocks (position embeddings, ResNet ..."
          ],
          [
           "# determine dimensions\n        self.channels = channels\n        self.self_condition = self_condition..."
          ],
          [
           "mid_dim = dims[-1]\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n  ..."
          ],
          [
           "x = self.init_conv(x)\n        r = x.clone()\n\n        t = self.time_mlp(time)\n\n        h = []\n\n      ..."
          ],
          [
           "```\n\n## Defining the forward diffusion process\n\nThe forward diffusion process gradually adds noise t..."
          ],
          [
           "def linear_beta_schedule(timesteps):\n    beta_start = 0.0001\n    beta_end = 0.02\n    return torch.li..."
          ],
          [
           "```\n\nTo start with, let's use the linear schedule for \\\\(T=300\\\\) time steps and define the various ..."
          ],
          [
           "```\n\nWe'll illustrate with a cats image how noise is added at each time step of the diffusion proces..."
          ],
          [
           "```\n\n<div class=\"output stream stdout\">\n\n    Output:\n    -------------------------------------------..."
          ],
          [
           "```\n\nLet's verify this:\n\n```python\nreverse_transform(x_start.squeeze())\n```\n    \n<img src=\"assets/78..."
          ],
          [
           "```\n\n<img src=\"assets/78_annotated-diffusion/output_cats_noisy.png\" width=\"100\" />\n\nLet's visualize ..."
          ],
          [
           "```\n\n```python\nplot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])\n`..."
          ],
          [
           "```\n\nThe `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the tr..."
          ],
          [
           "```\n\nNext, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with..."
          ],
          [
           "```\n\n<div class=\"output stream stdout\">\n\n    Output:\n    -------------------------------------------..."
          ],
          [
           "Ideally, we end up with an image that looks like it came from the real data distribution.\n\nThe code ..."
          ],
          [
           "@torch.no_grad()\ndef sample(model, image_size, batch_size=16, channels=3):\n    return p_sample_loop(..."
          ],
          [
           "```\n\nNote that the code above is a simplified version of the original implementation. We found our s..."
          ],
          [
           "```\n\nLet's start training!\n\n```python\nfrom torchvision.utils import save_image\n\nepochs = 6\n\nfor epoc..."
          ],
          [
           "```\n\n<div class=\"output stream stdout\">\n\n    Output:\n    -------------------------------------------..."
          ],
          [
           "</div>\n\n\n## Sampling (inference)\n\nTo sample from the model, we can just use our sample function defi..."
          ],
          [
           "```\n\n<img src=\"assets/78_annotated-diffusion/output.png\" width=\"300\" />\n\nSeems like the model is cap..."
          ],
          [
           "```\n\n<img src=\"\nassets/78_annotated-diffusion/diffusion-sweater.gif\" width=\"300\" />\n\n# Follow-up rea..."
          ],
          [
           "- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/210..."
          ],
          [
           "Note that this list only includes important works until the time of writing, which is June 7th, 2022..."
          ],
          [
           "--\ntitle: \"How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML ro..."
          ],
          [
           "If you'd like to accelerate your machine learning roadmap with the help of our experts, as Swaraj an..."
          ],
          [
           "### How did you leverage the Expert Acceleration Program?\n\nThe Hugging Face team really helped us in..."
          ],
          [
           "---\n\nWith the Expert Acceleration Program, we've put together a world-class team to help customers b..."
          ],
          [
           "--\ntitle: \"Databricks â¤ï¸ Hugging Face: up to 40% faster training and tuning of Large Language Models..."
          ],
          [
           "```swift\nfrom datasets import load_dataset\n\ntrain_df = train.write.parquet(train_dbfs_path, mode=\"ov..."
          ],
          [
           "```\nNot only was this cumbersome, but it also meant that data had to be written to disk and then rea..."
          ],
          [
           "```\nThis allows users to use Spark to efficiently load and transform data for training or fine-tunin..."
          ],
          [
           "In order to become the best platform for users to jump into the world of AI, weâ€™re working hard to p..."
          ],
          [
           "--\ntitle: 'Introducing Snowball Fight â˜ƒï¸, our first ML-Agents environment'\nthumbnail: /blog/assets/3..."
          ],
          [
           "With this first step, our goal is to build an ecosystem on Hugging Face for Deep Reinforcement Learn..."
          ],
          [
           "![screenshot2vs2](assets/39_introducing_snowball_fight/screenshot2vs2.png)\n\n- And we're building **n..."
          ],
          [
           "--\ntitle: \"Deep Learning with Proteins\" \nthumbnail: /blog/assets/119_deep_learning_with_proteins/fol..."
          ],
          [
           "Letâ€™s say that you want to train a DL model to take a sentence in English as input and decide if itâ€™..."
          ],
          [
           "Now try the same task, but in English:\n\n| Text | Label |\n| --- | --- |\n| Sheâ€™s the best director in ..."
          ],
          [
           "This stage of affairs continued until 2018, when two huge papers landed, introducing the models [ULM..."
          ],
          [
           "At this point, hopefully you understand what transfer learning is, and that a large language model i..."
          ],
          [
           "Proteins are composed of multiple **amino acids.** Amino acids are relatively simple molecules that ..."
          ],
          [
           "![protein structure](assets/119_deep_learning_with_proteins/protein_structure.png)\n\n*This figure sho..."
          ],
          [
           "## Bringing it together: Machine learning with proteins\n\nSo now we've seen how transfer learning wit..."
          ],
          [
           "Like a lot of other fields, though, the arrival of deep learning changed everything. AlphaFold and e..."
          ],
          [
           "The key takeaway, though, is that even though proteins are very different to language, they can be h..."
          ],
          [
           "If youâ€™re a biologist, on the other hand, you probably have several ideas for what you want to try, ..."
          ],
          [
           "## Conclusion\n\nThe intersection of deep learning and biology is going to be an incredibly active and..."
          ],
          [
           "--\ntitle: \"Sentence Transformers in the Hugging Face Hub\"\nauthors:\n- user: osanseviero\n- user: nreim..."
          ],
          [
           "```\n\nBut not only this. People will probably want to either demo their models or play with other mod..."
          ],
          [
           "<div><a class=\"text-xs block mb-3 text-gray-300\" href=\"/sentence-transformers/distilbert-base-nli-ma..."
          ],
          [
           "data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingface.co&quot;,&quot;model&quot;:{..."
          ],
          [
           "&quot;:{&quot;author&quot;:&quot;sentence-transformers&quot;,&quot;autoArchitecture&quot;:&quot;Auto..."
          ],
          [
           "uot;AutoModel&quot;,&quot;branch&quot;:&quot;main&quot;,&quot;cardData&quot;:{&quot;pipeline_tag&quo..."
          ],
          [
           "_tag&quot;:&quot;feature-extraction&quot;,&quot;tags&quot;:[&quot;sentence-transformers&quot;,&quot;..."
          ],
          [
           ";,&quot;feature-extraction&quot;,&quot;sentence-similarity&quot;,&quot;transformers&quot;]},&quot;ca..."
          ],
          [
           "&quot;cardSource&quot;:true,&quot;config&quot;:{&quot;architectures&quot;:[&quot;DistilBertModel&quo..."
          ],
          [
           "odel&quot;],&quot;model_type&quot;:&quot;distilbert&quot;},&quot;id&quot;:&quot;sentence-transformer..."
          ],
          [
           "nsformers/distilbert-base-nli-max-tokens&quot;,&quot;pipeline_tag&quot;:&quot;feature-extraction&quo..."
          ],
          [
           "tion&quot;,&quot;library_name&quot;:&quot;sentence-transformers&quot;,&quot;mask_token&quot;:&quot;[..."
          ],
          [
           ":&quot;[MASK]&quot;,&quot;modelId&quot;:&quot;sentence-transformers/distilbert-base-nli-max-tokens&q..."
          ],
          [
           "tokens&quot;,&quot;private&quot;:false,&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattrib..."
          ],
          [
           "itattributes&quot;},{&quot;rfilename&quot;:&quot;README.md&quot;},{&quot;rfilename&quot;:&quot;confi..."
          ],
          [
           "ot;config.json&quot;},{&quot;rfilename&quot;:&quot;config_sentence_transformers.json&quot;},{&quot;r..."
          ],
          [
           "{&quot;rfilename&quot;:&quot;modules.json&quot;},{&quot;rfilename&quot;:&quot;pytorch_model.bin&quot..."
          ],
          [
           "bin&quot;},{&quot;rfilename&quot;:&quot;sentence_bert_config.json&quot;},{&quot;rfilename&quot;:&quo..."
          ],
          [
           "ot;:&quot;special_tokens_map.json&quot;},{&quot;rfilename&quot;:&quot;tokenizer.json&quot;},{&quot;r..."
          ],
          [
           "{&quot;rfilename&quot;:&quot;tokenizer_config.json&quot;},{&quot;rfilename&quot;:&quot;vocab.txt&quo..."
          ],
          [
           ".txt&quot;},{&quot;rfilename&quot;:&quot;1_Pooling/config.json&quot;}],&quot;tags&quot;:[&quot;pytor..."
          ],
          [
           "ot;pytorch&quot;,&quot;distilbert&quot;,&quot;arxiv:1908.10084&quot;,&quot;sentence-transformers&quo..."
          ],
          [
           "mers&quot;,&quot;feature-extraction&quot;,&quot;sentence-similarity&quot;,&quot;transformers&quot;,&..."
          ],
          [
           "&quot;,&quot;pipeline_tag:feature-extraction&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;feat..."
          ],
          [
           "uot;feature-extraction&quot;,&quot;label&quot;:&quot;Feature..."
          ],
          [
           "Extraction&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;pytorch&quot;,&quo..."
          ],
          [
           "Transformers&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;transformers&quot;,&q..."
          ],
          [
           "class=\"font-semibold flex items-center mb-2\"><div class=\"text-lg flex items-center\"><svg xmlns=\"http..."
          ],
          [
           "Hosted inference API</div> <a target=\"_blank\" href=\"/docs\"><svg class=\"ml-1.5 text-sm text-gray-400 ..."
          ],
          [
           "0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\" fill=\"currentColor\"></path></svg></a></div> <di..."
          ],
          [
           "0 0 0 2-2V5a2 2 0 0 0-2-2zm0 2v4H5V5zm-10 6h10v7H17zm-2 7H5v-7h10zM5 20h10v7H5zm12 7v-7h10v7z\"></pat..."
          ],
          [
           "Inference API.</div> </div>   <div class=\"mt-auto pt-4 flex items-center text-xs text-gray-500\"><but..."
          ],
          [
           "fill=\"currentColor\"></path><path d=\"M12.419 25.484L17.639 6l1.932.518L14.35 26z\" fill=\"currentColor\"..."
          ],
          [
           "JSON Output</button> <button class=\"flex items-center ml-auto\"><svg class=\"mr-1\" xmlns=\"http://www.w..."
          ],
          [
           "But seeing a bunch of numbers might not be very useful to you (unless you're able to understand the ..."
          ],
          [
           "<!-- Hackiest hack ever for the draft -->\n<div><a class=\"text-xs block mb-3 text-gray-300\" href=\"/se..."
          ],
          [
           "<div class=\"p-5 shadow-sm rounded-xl bg-white max-w-md\"><div class=\"SVELTE_HYDRATER \"..."
          ],
          [
           "class=\"SVELTE_HYDRATER \" data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingface.co&..."
          ],
          [
           "Similarity&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;pytorch&quot;,&quo..."
          ],
          [
           "data-target=\"InferenceWidget\"><div class=\"flex flex-col w-full max-w-full..."
          ],
          [
           "\"> <div class=\"font-semibold flex items-center mb-2\"><div class=\"text-lg flex items-center\"><svg xml..."
          ],
          [
           "0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\" fill=\"currentColor\"></path></svg></a></div> <di..."
          ],
          [
           "30l-5-5l5-5z\"></path><path d=\"M11 30H3a1 1 0 0 1-.894-1.447l4-8a1.041 1.041 0 0 1 1.789 0l4 8A1 1 0 ..."
          ],
          [
           "3 0 1 0 3 3a3.003 3.003 0 0 0-3-3z\"></path></svg> <span>Sentence Similarity</span></div> <div class=..."
          ],
          [
           "<input class=\" form-input-alt block w-full \" placeholder=\"Your sentence here...\" type=\"text\"></label..."
          ],
          [
           "text-gray-500\"><button class=\"flex items-center cursor-not-allowed text-gray-300\" disabled=\"\"><svg c..."
          ],
          [
           "JSON Output</button> <button class=\"flex items-center ml-auto\"><svg class=\"mr-1\" xmlns=\"http://www.w..."
          ],
          [
           "Of course, on top of the widgets, we also provide API endpoints in our Inference API that you can us..."
          ],
          [
           "```\n\n## Unleashing the Power of Sharing\n\nSo why is this powerful? In a matter of minutes, you can sh..."
          ],
          [
           "```\n\nIf you don't have any model in the Hub and want to learn more about Sentence Transformers, head..."
          ],
          [
           "--\ntitle: \"Running IF with ðŸ§¨ diffusers on a Free Tier Google Colab\"\nthumbnail: /blog/assets/if/thumb..."
          ],
          [
           "## Introduction\n\nIF is a pixel-based text-to-image generation model and was [released in\nlate April ..."
          ],
          [
           "Nevertheless, it is possible to run IF on consumer hardware if one\noptimizes the model for low-memor..."
          ],
          [
           "## Accepting the license\n\nBefore you can use IF, you need to accept its usage conditions. To do so:\n..."
          ],
          [
           "```\n\nrun the login function in a Python shell\n\n```py\nfrom huggingface_hub import login\n\nlogin()..."
          ],
          [
           "```\n\nand enter your [Hugging Face Hub access token](https://huggingface.co/docs/hub/security-tokens#..."
          ],
          [
           "Let\\'s map out the size of IF\\'s model components in full float32\nprecision:\n\n- [T5-XXL Text Encoder..."
          ],
          [
           "Now that each component fits individually into both CPU and GPU memory,\nwe need to make sure that co..."
          ],
          [
           "```\n\n```bash\nMemTotal:       13297192 kB\n```\n\nAnd an NVIDIA T4 with 15 GB VRAM:\n\n``` python\n!nvidia-..."
          ],
          [
           "```bash\nSun Apr 23 23:14:19 2023       \n+-----------------------------------------------------------..."
          ],
          [
           "| N/A   72C    P0    32W /  70W |   1335MiB / 15360MiB |      0%      Default |\n|                   ..."
          ],
          [
           "```\n\n## Install dependencies\n\nSome optimizations can require up-to-date versions of dependencies. If..."
          ],
          [
           "```\n\n## 1. Text-to-image generation\n\nWe will walk step by step through text-to-image generation with..."
          ],
          [
           "```\n\n### 1.2 Create text embeddings\n\nThe Diffusers API for accessing diffusion models is the\n`Diffus..."
          ],
          [
           "```\n\nand run it through the 8bit quantized T5 model:\n\n``` python\nprompt_embeds, negative_embeds = pi..."
          ],
          [
           "```\n\nOften, we directly pass the text prompt to `DiffusionPipeline.__call__`.\nHowever, we previously..."
          ],
          [
           "```\n\n### 1.5 Stage 2: Super Resolution 64x64 to 256x256 \n\nIF comes with a separate diffusion process..."
          ],
          [
           "```\n\n### 1.6 Stage 3: Super Resolution 256x256 to 1024x1024\n\nThe second super resolution model for I..."
          ],
          [
           "```\n\nView output image\n\n``` python\npil_image[0]\n```\n\n![t2i_upscaled_2](https://huggingface.co/datase..."
          ],
          [
           "```\n\nand load it into a PIL Image\n\n``` python\nfrom PIL import Image\nfrom io import BytesIO\n\noriginal..."
          ],
          [
           "```\n\nFor image variation, we load the checkpoint with\n[`IFImg2ImgPipeline`](https://huggingface.co/d..."
          ],
          [
           "```\n\nThe image variation pipeline requires both the original image and the\nprompt embeddings.\n\nWe ca..."
          ],
          [
           "```\nðŸ’¡ **Note**: The image variation super resolution pipeline requires the\ngenerated image as well a..."
          ],
          [
           "```\n\n![inpainting_sample](https://huggingface.co/datasets/huggingface/documentation-images/resolve/m..."
          ],
          [
           "```\n\n![masking_by_hand](https://huggingface.co/datasets/huggingface/documentation-images/resolve/mai..."
          ],
          [
           "```\n\nNow, we need to pass the input image, the mask image, and the prompt\nembeddings.\n\n``` python\nim..."
          ],
          [
           "```\n\n![inpainted_final_output](https://huggingface.co/datasets/huggingface/documentation-images/reso..."
          ],
          [
           "--\ntitle: \"Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI ..."
          ],
          [
           "All of this will only happen with standardization and automation. Organizations can't afford to buil..."
          ],
          [
           "IBM decided that open source should be at the core of watsonx.ai. We couldn't agree more! Built on [..."
          ],
          [
           "Our joint team is hard at work at the moment. We can't wait to show you what we've been up to! The m..."
          ],
          [
           "--\ntitle: The State of Computer Vision at Hugging Face ðŸ¤—\nthumbnail: /blog/assets/cv_state/thumbnail...."
          ],
          [
           "- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- ..."
          ],
          [
           "## Support for Pipelines\n\nWe developed [Pipelines](https://huggingface.co/docs/transformers/main/en/..."
          ],
          [
           "```\n\n<div align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-im..."
          ],
          [
           "```\n\n## Training your own models\n\nWhile being able to use a model for off-the-shelf inference is a g..."
          ],
          [
           "[Hugging Face example scripts](https://github.com/huggingface/transformers/tree/main/examples) inclu..."
          ],
          [
           "## Integrations with Datasets\n\n[Datasets](https://huggingface.co/docs/datasets) provides easy access..."
          ],
          [
           "```\n\nBesides these datasets, we provide integration support with augmentation libraries like [albume..."
          ],
          [
           "```\n\n<div align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "<div align=\"center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/reso..."
          ],
          [
           "- [Generate 3D voxels from a predicted depth map of an input image](https://huggingface.co/spaces/ra..."
          ],
          [
           "## The technical philosophy\n\nIn this section, we wanted to share our philosophy behind adding suppor..."
          ],
          [
           "with torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet..."
          ],
          [
           "```\n\nEven for a difficult task like object detection, the user experience doesnâ€™t change very much:\n..."
          ],
          [
           "```\n\nLeads to:\n\n```bash\nDetected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118...."
          ],
          [
           "```\n\n## Zero-shot models for vision\n\nThereâ€™s been a surge of models that reformulate core vision tas..."
          ],
          [
           "- [CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip) that enables zero-shot ima..."
          ],
          [
           "The community can expect to see more zero-shot models for computer vision being supported from ðŸ¤—Tran..."
          ],
          [
           "As always, we welcome your patches, PRs, model checkpoints, datasets, and other contributions! ðŸ¤—\n\n*A..."
          ],
          [
           "--\ntitle: \"Fine-tuning Stable Diffusion models on Intel CPUs\"\nthumbnail: /blog/assets/stable-diffusi..."
          ],
          [
           "This post will show you how to fine-tune a Stable Diffusion model on an Intel Sapphire Rapids CPU cl..."
          ],
          [
           "```\nArchitecture:            x86_64\n  CPU op-mode(s):        32-bit, 64-bit\n  Address sizes:        ..."
          ],
          [
           "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush..."
          ],
          [
           "xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand la..."
          ],
          [
           "xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni a..."
          ],
          [
           "```\n\nLet's first list the IP addresses of our servers in `nodefile.` The first line refers to the pr..."
          ],
          [
           "```\ngit clone https://github.com/huggingface/diffusers.git\ncd diffusers\npip install .\n```\n\nNext, we ..."
          ],
          [
           "```\nmkdir /home/devcloud/dicoo\ncd /home/devcloud/dicoo\nwget https://huggingface.co/sd-concepts-libra..."
          ],
          [
           "```\n\nHere are the images:\n\n<img src=\"https://huggingface.co/sd-concepts-library/dicoo/resolve/main/c..."
          ],
          [
           "```\nexport I_MPI_HYDRA_IFACE=ens786f1\noneccl_bindings_for_pytorch_path=$(python -c \"from oneccl_bind..."
          ],
          [
           "```\nmpirun -f nodefile -n 16 -ppn 4                                                         \\\naccele..."
          ],
          [
           "```\npython diffusers/examples/textual_inversion/textual_inversion.py                        \\\n--pret..."
          ],
          [
           "```\npip install optimum[openvino]\n```\n\nHere, we load the model, optimize it for a static shape, and ..."
          ],
          [
           "```\n\nHere's a generated image. It is impressive that the model only needed five images to learn that..."
          ],
          [
           "Here are some resources to help you get started:\n\n* Diffusers [documentation](https://huggingface.co..."
          ],
          [
           "--\ntitle: \"Gradio-Lite: Serverless Gradio Running Entirely in Your Browser\"\nthumbnail: /blog/assets/..."
          ],
          [
           "Start by creating a new HTML file, if you don't have one already. Importing the JavaScript and CSS c..."
          ],
          [
           "```\n\nNote that you should generally use the latest version of `@gradio/lite` that is available. You ..."
          ],
          [
           "```\n\n### 3. Write your Gradio app inside of the tags\n\nNow, write your Gradio app as you would normal..."
          ],
          [
           "```\n\nAnd that's it! You should now be able to open your HTML page in the browser and see the Gradio ..."
          ],
          [
           "```\n\n### Additional Requirements\n\nIf your Gradio app has additional requirements, it is usually poss..."
          ],
          [
           "```\n\n**Try it out**: You can see this example running in [this Hugging Face Static Space](https://hu..."
          ],
          [
           "## Try it out!\n\nYou can immediately try out `@gradio/lite` by copying and pasting this code in a loc..."
          ],
          [
           "```\n\n\nWe've also created a playground on the Gradio website that allows you to interactively edit co..."
          ],
          [
           "--\ntitle: \"BERT 101 - State Of The Art NLP Model Explained\"\nthumbnail: /blog/assets/52_bert_101/thum..."
          ],
          [
           "In this guide, you'll learn what BERT is, why itâ€™s different, and how to get started using BERT:\n\n1...."
          ],
          [
           "**There are many more language/NLP tasks + more detail behind each of these.**\n\n***Fun Fact:*** You ..."
          ],
          [
           "Training on a dataset this large takes a long time. BERTâ€™s training was made possible thanks to the ..."
          ],
          [
           "**Note:** This is why youâ€™ll often see a â€œHuman Performanceâ€ comparison to a language modelâ€™s perfor..."
          ],
          [
           "<div class=\"bg-white pb-1\">..."
          ],
          [
           "<div class=\"SVELTE_HYDRATER contents\"..."
          ],
          [
           "data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingface.co&quot;,&quot;apiToken&quot..."
          ],
          [
           "ken&quot;:&quot;&quot;,&quot;model&quot;:{&quot;branch&quot;:&quot;main&quot;,&quot;cardData&quot;:{..."
          ],
          [
           "&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;tags&quot;:[&quot;exbert&quot;],&quot;license&quot..."
          ],
          [
           "nse&quot;:&quot;apache-2.0&quot;,&quot;datasets&quot;:[&quot;bookcorpus&quot;,&quot;wikipedia&quot;]..."
          ],
          [
           "a&quot;]},&quot;cardError&quot;:{&quot;errors&quot;:[],&quot;warnings&quot;:[]},&quot;cardExists&quo..."
          ],
          [
           "ists&quot;:true,&quot;config&quot;:{&quot;architectures&quot;:[&quot;BertForMaskedLM&quot;],&quot;mo..."
          ],
          [
           "&quot;model_type&quot;:&quot;bert&quot;},&quot;id&quot;:&quot;bert-base-uncased&quot;,&quot;lastModi..."
          ],
          [
           "lastModified&quot;:&quot;2021-05-18T16:20:13.000Z&quot;,&quot;pipeline_tag&quot;:&quot;fill-mask&quo..."
          ],
          [
           "mask&quot;,&quot;library_name&quot;:&quot;transformers&quot;,&quot;mask_token&quot;:&quot;[MASK]&quo..."
          ],
          [
           "ASK]&quot;,&quot;model-index&quot;:null,&quot;private&quot;:false,&quot;gated&quot;:false,&quot;pwcL..."
          ],
          [
           "uot;pwcLink&quot;:{&quot;error&quot;:&quot;Unknown..."
          ],
          [
           "error, can&#39;t generate link to Papers With..."
          ],
          [
           "Code.&quot;},&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattributes&quot;},{&quot;rfilena..."
          ],
          [
           ";rfilename&quot;:&quot;README.md&quot;},{&quot;rfilename&quot;:&quot;config.json&quot;},{&quot;rfile..."
          ],
          [
           "ot;rfilename&quot;:&quot;flax_model.msgpack&quot;},{&quot;rfilename&quot;:&quot;pytorch_model.bin&qu..."
          ],
          [
           "l.bin&quot;},{&quot;rfilename&quot;:&quot;rust_model.ot&quot;},{&quot;rfilename&quot;:&quot;tf_model..."
          ],
          [
           "tf_model.h5&quot;},{&quot;rfilename&quot;:&quot;tokenizer.json&quot;},{&quot;rfilename&quot;:&quot;t..."
          ],
          [
           ":&quot;tokenizer_config.json&quot;},{&quot;rfilename&quot;:&quot;vocab.txt&quot;}],&quot;tags&quot;:..."
          ],
          [
           "s&quot;:[&quot;pytorch&quot;,&quot;tf&quot;,&quot;jax&quot;,&quot;rust&quot;,&quot;bert&quot;,&quot;..."
          ],
          [
           ";,&quot;fill-mask&quot;,&quot;en&quot;,&quot;dataset:bookcorpus&quot;,&quot;dataset:wikipedia&quot;,..."
          ],
          [
           "a&quot;,&quot;arxiv:1810.04805&quot;,&quot;transformers&quot;,&quot;exbert&quot;,&quot;license:apach..."
          ],
          [
           "se:apache-2.0&quot;,&quot;autonlp_compatible&quot;,&quot;infinity_compatible&quot;],&quot;tag_objs&q..."
          ],
          [
           "g_objs&quot;:[{&quot;id&quot;:&quot;fill-mask&quot;,&quot;label&quot;:&quot;Fill-Mask&quot;,&quot;su..."
          ],
          [
           "&quot;subType&quot;:&quot;nlp&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot..."
          ],
          [
           "t;:&quot;pytorch&quot;,&quot;label&quot;:&quot;PyTorch&quot;,&quot;type&quot;:&quot;library&quot;},{..."
          ],
          [
           "quot;},{&quot;id&quot;:&quot;tf&quot;,&quot;label&quot;:&quot;TensorFlow&quot;,&quot;type&quot;:&quo..."
          ],
          [
           "ot;:&quot;library&quot;},{&quot;id&quot;:&quot;jax&quot;,&quot;label&quot;:&quot;JAX&quot;,&quot;typ..."
          ],
          [
           "quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;rust&quot;,&quot;label&quot;:&quot;Rust&q..."
          ],
          [
           "t;Rust&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;transformers&quot;,&quot;la..."
          ],
          [
           "&quot;label&quot;:&quot;Transformers&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&qu..."
          ],
          [
           "uot;:&quot;dataset:bookcorpus&quot;,&quot;label&quot;:&quot;bookcorpus&quot;,&quot;type&quot;:&quot;..."
          ],
          [
           ";:&quot;dataset&quot;},{&quot;id&quot;:&quot;dataset:wikipedia&quot;,&quot;label&quot;:&quot;wikiped..."
          ],
          [
           ";wikipedia&quot;,&quot;type&quot;:&quot;dataset&quot;},{&quot;id&quot;:&quot;en&quot;,&quot;label&qu..."
          ],
          [
           "label&quot;:&quot;en&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;arxiv:1810.0..."
          ],
          [
           "v:1810.04805&quot;,&quot;label&quot;:&quot;arxiv:1810.04805&quot;,&quot;type&quot;:&quot;arxiv&quot;..."
          ],
          [
           "iv&quot;},{&quot;id&quot;:&quot;license:apache-2.0&quot;,&quot;label&quot;:&quot;apache-2.0&quot;,&q..."
          ],
          [
           "quot;,&quot;type&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;bert&quot;,&quot;label&quot;:&quot..."
          ],
          [
           "t;:&quot;bert&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;exbert&quot;,&quot;lab..."
          ],
          [
           "quot;label&quot;:&quot;exbert&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;autonl..."
          ],
          [
           "t;autonlp_compatible&quot;,&quot;label&quot;:&quot;AutoNLP..."
          ],
          [
           "Compatible&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;infinity_compatible&quot;..."
          ],
          [
           "<div class=\"flex flex-col w-full max-w-full\">\n            <div class=\"font-semibold flex items-cente..."
          ],
          [
           "<path d=\"M17 22v-8h-4v2h2v6h-3v2h8v-2h-3z\" fill=\"currentColor\"></path>\n                        <path..."
          ],
          [
           "<path d=\"M12.3625 13.85H10.1875V12.7625H12.3625V10.5875H13.45V12.7625C13.4497 13.0508 13.335 13.3272..."
          ],
          [
           "<path d=\"M15.625 5.14998H13.45V2.97498C13.4497 2.68665 13.335 2.4102 13.1312 2.20632C12.9273 2.00244..."
          ],
          [
           "</svg>\n                         <span>Fill-Mask</span>\n                     </div>\n                <..."
          ],
          [
           "<code>[MASK]</code>\n            </div>\n            <label class=\"block \">\n                <span clas..."
          ],
          [
           "<path d=\"M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z\" fill=\"currentColor\"></path>\n         ..."
          ],
          [
           "</svg>\n                Maximize\n            </button>\n        </div>\n    </div>\n</div>..."
          ],
          [
           "**Fun Fact:** Masking has been around a long time - [1953 Paper on Cloze procedure (or â€˜Maskingâ€™)](h..."
          ],
          [
           "<p class=\"text-center px-6\">Lewis Tunstall, Hugging Face ML Engineer & <a href=\"https://www.amazon.c..."
          ],
          [
           "A transformer does this by successively processing an input through a stack of transformer layers, u..."
          ],
          [
           "ML Architecture Glossary:\n\n| ML Architecture Parts | Definition                                     ..."
          ],
          [
           "Hereâ€™s how many of the above ML architecture parts BERTbase and BERTlarge has:\n\n\n|           | Trans..."
          ],
          [
           "#### 4.2 SWAG\n[SWAG](https://huggingface.co/datasets/swag) (Situations With Adversarial Generations)..."
          ],
          [
           "While some of these tasks may seem irrelevant and banal, itâ€™s important to note that these evaluatio..."
          ],
          [
           "Developers can instead focus their efforts on fine-tuning BERT to customize the modelâ€™s performance ..."
          ],
          [
           "Note: Hugging Face's [pipeline class](https://huggingface.co/docs/transformers/main_classes/pipeline..."
          ],
          [
           "```\n\n### 7.2 Try out BERT\n\nFeel free to swap out the sentence below for one of your own. However, le..."
          ],
          [
           "```\n\nWhen you run the above code you should see an output that looks something like:\n\n```python\n[{'s..."
          ],
          [
           "```\n\nYou should see an output that looks something like:\n```python\n[{'score': 0.21981535851955414,\n ..."
          ],
          [
           "```\n\nBERT predicted the woman's job to be a Nurse, Waitress, Maid, Prostitute, or Cook displaying a ..."
          ],
          [
           "<div itemscope itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\">\n    <h3 itemprop=\"name\"..."
          ],
          [
           "<div itemscope itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\">\n      <div itemprop=\"..."
          ],
          [
           "</div>\n  </div>\n</div>\n<div itemscope itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\">\n..."
          ],
          [
           "## 9. Conclusion\n\nBERT is a highly complex and advanced language model that helps people automate la..."
          ],
          [
           "--\ntitle: \"We Raised $100 Million for Open & Collaborative Machine Learning ðŸš€\"\nthumbnail: /blog/asse..."
          ],
          [
           "Over 10,000 companies are now using Hugging Face to build technology with machine learning. Their Ma..."
          ],
          [
           "It's been a hell of a ride to grow from 30 to 120+ team members in the past 12 months. We were super..."
          ],
          [
           "--\ntitle: \"Efficient Table Pre-training without Real Data: An Introduction to TAPEX\"\nthumbnail: /blo..."
          ],
          [
           "![snippet](assets/74_tapex/tapex-overview.png)\n> Note: [Table] is a placeholder for the user provide..."
          ],
          [
           "You can try the trained neural SQL executor in ðŸ¤— Transformers as below:\n\n```python\nfrom transformers..."
          ],
          [
           "```\n\n### Fine-tuning\n\nDuring fine-tuning, we feed the concatenation of the natural language question..."
          ],
          [
           "<div class=\"bg-white pb-1\"><div class=\"SVELTE_HYDRATER contents\"..."
          ],
          [
           "contents\" data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingface.co&quot;,&quot;mod..."
          ],
          [
           "error, can't generate link to Papers With Code.&quot;},&quot;tags&quot;:[&quot;pytorch&quot;,&quot;b..."
          ],
          [
           "Answering&quot;,&quot;subType&quot;:&quot;nlp&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quo..."
          ],
          [
           ";},{&quot;id&quot;:&quot;pytorch&quot;,&quot;label&quot;:&quot;PyTorch&quot;,&quot;type&quot;:&quot;..."
          ],
          [
           ";:&quot;library&quot;},{&quot;id&quot;:&quot;transformers&quot;,&quot;label&quot;:&quot;Transformers..."
          ],
          [
           "sformers&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;en&quot;,&quot;label&quot..."
          ],
          [
           "bel&quot;:&quot;en&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;arxiv:2107.076..."
          ],
          [
           "2107.07653&quot;,&quot;label&quot;:&quot;arxiv:2107.07653&quot;,&quot;type&quot;:&quot;arxiv&quot;},..."
          ],
          [
           "&quot;},{&quot;id&quot;:&quot;license:mit&quot;,&quot;label&quot;:&quot;mit&quot;,&quot;type&quot;:&..."
          ],
          [
           "&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;bart&quot;,&quot;label&quot;:&quot;bart&quot;,&quo..."
          ],
          [
           "ot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:&quot;text2text-generation&quot;,&quot;label..."
          ],
          [
           "ot;label&quot;:&quot;text2text-generation&quot;,&quot;type&quot;:&quot;other&quot;},{&quot;id&quot;:..."
          ],
          [
           "d&quot;:&quot;tapex&quot;,&quot;label&quot;:&quot;tapex&quot;,&quot;type&quot;:&quot;other&quot;},{&..."
          ],
          [
           "uot;},{&quot;id&quot;:&quot;autotrain_compatible&quot;,&quot;label&quot;:&quot;AutoTrain..."
          ],
          [
           "Compatible&quot;,&quot;type&quot;:&quot;other&quot;}],&quot;transformersInfo&quot;:{&quot;auto_model..."
          ],
          [
           "Hosted inference API</div> <a target=\"_blank\" href=\"https://api-inference.huggingface.co/\"><svg clas..."
          ],
          [
           "14 0 0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\" fill=\"currentColor\"></path></svg></a></div..."
          ],
          [
           "1.88777 5.47272 2.00244 5.26884 2.20632C5.06496 2.4102 4.95029 2.68665 4.95 2.97498V4.60623H2.775C2...."
          ],
          [
           "2.00244 16.1133 1.88777 15.825 1.88748ZM6.0375 2.97498H15.825V4.60623H6.0375V2.97498ZM15.825 8.41248..."
          ],
          [
           "false..."
          ],
          [
           "false\"><div class=\"inline-flex justify-between w-32 lg:w-44 rounded-md border border-gray-100 px-4 p..."
          ],
          [
           "class=\"flex h-10\"><input class=\"form-input-alt flex-1 rounded-r-none min-w-0 \" placeholder=\"Your sen..."
          ],
          [
           "<tbody><tr class=\"bg-white\"><td class=\"border-gray-100 border-2 h-6\" contenteditable=\"\">Transformers..."
          ],
          [
           "h-6\" contenteditable=\"\">Tokenizers</td><td class=\"border-gray-100 border-2 h-6\" contenteditable=\"\">3..."
          ],
          [
           "Add row</button> <button class=\"btn-widget flex-1 lg:flex-none mt-2 lg:mr-1.5\" type=\"button\"><svg cl..."
          ],
          [
           "Add col</button> <button class=\"btn-widget flex-1 mt-2 lg:flex-none lg:ml-auto\" type=\"button\">Reset ..."
          ],
          [
           "### Experiments\n\nWe evaluate TAPEX on four benchmark datasets, including [WikiSQL (Weak)](https://hu..."
          ],
          [
           "Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a ..."
          ],
          [
           "![comparsion](assets/74_tapex/comparsion-tapex.png)\n\nWe believe the SQL execution task is closer to ..."
          ],
          [
           "--\ntitle: \"Ethics and Society Newsletter #4: Bias in Text-to-Image Models\"\nthumbnail: /blog/assets/1..."
          ],
          [
           "For example, if the training data are mainly in English they probably convey rather Western values. ..."
          ],
          [
           "## Sources of Bias\n\nRecent years have seen much important research on bias detection in AI systems w..."
          ],
          [
           "**Biases in training data:** Popular multimodal datasets such as [LAION-5B](https://laion.ai/blog/la..."
          ],
          [
           "**Biases in pre-training data filtering:** There is often some form of filtering carried out on data..."
          ],
          [
           "**Biases in post-hoc filtering:** Many image generation models come with built-in safety filters tha..."
          ],
          [
           "<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "**Red-teaming:** ['Red-teaming'](https://huggingface.co/blog/red-teaming) consists of stress testing..."
          ],
          [
           "**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https:/..."
          ],
          [
           "Also, as we have mentioned in a [previous newsletter](https://huggingface.co/blog/ethics-soc-2#addre..."
          ],
          [
           "## Other updates\n\nWe are also continuing work on other fronts of ethics and society, including:\n\n- *..."
          ],
          [
           "--\ntitle: \"Active Learning with AutoNLP and Prodigy\"\nthumbnail: /blog/assets/43_autonlp_prodigy/thum..."
          ],
          [
           "## Prodigy\n\n[Prodigy](https://prodi.gy/) is an annotation tool developed by Explosion (the makers of..."
          ],
          [
           "Step 1: Download the dataset.\n\nStep 2: Open [AutoNLP](https://ui.autonlp.huggingface.co/) and create..."
          ],
          [
           "Once you have Prodigy installed, you can simply run:\n\n    $ prodigy ner.manual bbc blank:en BBC_News..."
          ],
          [
           "dataset = []\n\nfor doc, eg in nlp.pipe(examples, as_tuples=True):\n    try:\n        doc.ents = [doc.ch..."
          ],
          [
           "```\n\nThis will provide us with a `JSONL` file which can be used for training a model using AutoNLP. ..."
          ],
          [
           "Let's take a look at how this model performs on the same unseen sample.\n\n<img src=\"assets/43_autonlp..."
          ],
          [
           "We have open-sourced the best model created using this process. You can try it [here](https://huggin..."
          ],
          [
           "--\ntitle: \"AI Policy @ðŸ¤—: Open ML Considerations in the EU AI Act\"\nthumbnail: /blog/assets/eu_ai_act_..."
          ],
          [
           "Hugging Face is where it is today thanks to its community of developers, so weâ€™ve seen firsthand wha..."
          ],
          [
           "--\ntitle: \"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\"\nthu..."
          ],
          [
           "In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published..."
          ],
          [
           "[Optimum Intel](https://github.com/huggingface/optimum-intel) is part of Optimum and builds on top o..."
          ],
          [
           "As usual, the first step is to install all required libraries. Itâ€™s worth mentioning that we have to..."
          ],
          [
           "```\npip -q uninstall torch -y \npip -q install torch==1.11.0+cpu --extra-index-url https://download.p..."
          ],
          [
           "```\n\nWe then set up the quantization job using a [configuration]. You can find details on this confi..."
          ],
          [
           "```\n\nThe log tells us that Optimum Intel has quantized 38 ```Linear``` and 2 ```Embedding``` operato..."
          ],
          [
           "```\n# Original model\n\nTransformerBlock(\n  (attention): MultiHeadSelfAttention(\n    (dropout): Dropou..."
          ],
          [
           "```\n\n```\n# Quantized model\n\nTransformerBlock(\n  (attention): MultiHeadSelfAttention(\n    (dropout): ..."
          ],
          [
           "```\n\nVery well, but how does this impact accuracy and prediction time?\n\nBefore and after each quanti..."
          ],
          [
           "```\n\nYou can find the resulting [model](https://huggingface.co/juliensimon/distilbert-amazon-shoe-re..."
          ],
          [
           "--\ntitle: \"CO2 Emissions and the ðŸ¤— Hub: Leading the Charge\"\nthumbnail: /blog/assets/60_carbon_emissi..."
          ],
          [
           "```\npip install huggingface_hub -U\n```\n\n## How to find low-emission models using the Hugging Face Hu..."
          ],
          [
           "```\n\nThat's a lot of CO2!\n\nAs you can see, in just a few lines of code we can quickly vet models we ..."
          ],
          [
           "```\n\n...you'll be left with a file within the `codecarbon-text-classification` directory called `emi..."
          ],
          [
           "--\ntitle: \"What's going on with the Open LLM Leaderboard?\"\nthumbnail: /blog/assets/evaluating-mmlu-l..."
          ],
          [
           "Along this journey with us youâ€™ll learn a lot about the ways you can evaluate a model on a single ev..."
          ],
          [
           "Why is that the case?\n\n## 1001 flavors of MMLU\n\nWell it turns out that the LLaMA team adapted anothe..."
          ],
          [
           "(Note that the Harness implementation has been recently updated - more in this at the end of our pos..."
          ],
          [
           "```\nQuestion: Glucose is transported into the muscle cell:\n\n\nChoices:\nA. via protein transporters ca..."
          ],
          [
           "```\n\nNote: you can very easily explore more of this dataset [in the dataset viewer](https://huggingf..."
          ],
          [
           "## MMLU comes in all shapes and sizes: Looking at the prompts\n\nLetâ€™s compare an example of prompt ea..."
          ],
          [
           "<div>\n<table><p>\n  <tbody>\n <tr style=\"text-align: left;\">\n  <td>Original implementation <a href=\"ht..."
          ],
          [
           "A. It damaged support for the US model of political economy and capitalism <br>\nB. It created anger ..."
          ],
          [
           "The differences between them can seem small, did you spot them all? Here they are:\n- First sentence,..."
          ],
          [
           "![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating..."
          ],
          [
           "![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating..."
          ],
          [
           "Here is a table summary of the answers provided and generated by the model to summarize what weâ€™ve s..."
          ],
          [
           "Weâ€™ve covered them all!\n\nNow letâ€™s compare the model scores on these three possible ways to evaluate..."
          ],
          [
           "We can see that for the same dataset, both absolute scores and model rankings (see the first figure)..."
          ],
          [
           "This is why open, standardized, and reproducible benchmarks such as the [EleutherAI Eval Harness](ht..."
          ],
          [
           "## Reproducibility hashes:\nHere are the commit hashes of the various code implementations used in th..."
          ],
          [
           "--\ntitle: \"The Technology Behind BLOOM Training\"\nthumbnail: /blog/assets/86_bloom_megatron_deepspeed..."
          ],
          [
           "There are 6 main groups of people to thank:\n\n1. The HuggingFace's BigScience team who dedicated more..."
          ],
          [
           "BLOOM's architecture is very similar to [GPT3](https://en.wikipedia.org/wiki/GPT-3) with a few added..."
          ],
          [
           "The training of the 176B BLOOM model occurred over Mar-Jul 2022 and took about 3.5 months to complet..."
          ],
          [
           "Please note that both Megatron-LM and DeepSpeed have Pipeline Parallelism and BF16 Optimizer impleme..."
          ],
          [
           "## Data Parallelism\n\nMost users with just a few GPUs are likely to be familiar with `DistributedData..."
          ],
          [
           "## Tensor Parallelism\n\nIn Tensor Parallelism (TP) each GPU processes only a slice of a tensor and on..."
          ],
          [
           "Using this principle, we can update an MLP of arbitrary depth, while synchronizing the GPUs after ea..."
          ],
          [
           "## Pipeline Parallelism\n\nNaive Pipeline Parallelism (naive PP) is where one spreads groups of model ..."
          ],
          [
           "```\nwe just sliced it in 2 vertically, placing layers 0-3 onto GPU0 and 4-7 to GPU1.\n\nNow while data..."
          ],
          [
           "The following illustration from the [GPipe paper](https://ai.googleblog.com/2019/03/introducing-gpip..."
          ],
          [
           "Note that conceptually this is the same concept as gradient accumulation steps (GAS). PyTorch uses `..."
          ],
          [
           "While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-Deep..."
          ],
          [
           "Since each dimension requires at least 2 GPUs, here you'd need at least 4 GPUs.\n\n## DP+PP+TP\n\nTo get..."
          ],
          [
           "In addition, there are already fewer layers than normal due to PP and so the memory savings won't be..."
          ],
          [
           "So back in January as we knew we would be training on A100s which support the BF16 format Olatunji R..."
          ],
          [
           "All PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no..."
          ],
          [
           "Now, when instructing the GPU to compute `c = torch.add(a, b); e = torch.max([c,d])`, a naive approa..."
          ],
          [
           "## Datasets\n\nAnother important feature from Megatron-LM is the efficient data loader. During start u..."
          ],
          [
           "## Training Difficulties\n\nWith the architecture, hardware and software in place we were able to star..."
          ],
          [
           "One other issue was that SLURM wasn't designed to be used by a team of people. A SLURM job is owned ..."
          ],
          [
           "Training large language models is still a challenging task, but we hope by building and sharing this..."
          ],
          [
           "DeepSpeed:\n\n- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.o..."
          ],
          [
           "## Blog credits\n\nHuge thanks to the following kind folks who asked good questions and helped improve..."
          ],
          [
           "--\ntitle: \"Happy 1st anniversary ðŸ¤— Diffusers!\" \nthumbnail: /blog/assets/diffusers-turns-1/diffusers-..."
          ],
          [
           "**Table of Contents**\n\n* [Striving for photorealism](#striving-for-photorealism)\n* [Video pipelines]..."
          ],
          [
           "Head over to the DeepFloyd IF [docs](https://huggingface.co/docs/diffusers/v0.18.2/en/api/pipelines/..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/docum..."
          ],
          [
           "## Image editing pipelines\n\nImage editing is one of the most practical use cases in fashion, materia..."
          ],
          [
           "In addition to that, we also support specific hardware and formats like ONNX, the `mps` PyTorch devi..."
          ],
          [
           "## Support for LoRA\n\nFine-tuning diffusion models is expensive and out of reach for most consumer GP..."
          ],
          [
           "## Community highlights\n\nOne of the most gratifying experiences of the past year has been seeing how..."
          ],
          [
           "<div class=\"mx-auto max-w-screen-xl py-8\">\n  <div class=\"mb-8 sm:break-inside-avoid\">\n    <blockquot..."
          ],
          [
           "<blockquote class=\"rounded-xl !mb-0 bg-gray-50 p-6 shadow dark:bg-gray-800\">\n      <p class=\"leading..."
          ],
          [
           "</blockquote>\n    <div class=\"flex items-center gap-4\">\n      <img src=\"https://avatars.githubuserco..."
          ],
          [
           "</div>\n    </div>\n  </div>\n  <div class=\"mb-8 sm:break-inside-avoid\">\n    <blockquote class=\"rounded..."
          ],
          [
           "<div class=\"text-sm\">\n        <p class=\"font-medium\">Qing</p>\n      </div>\n    </div>\n  </div>\n  <di..."
          ],
          [
           "</blockquote>\n    <div class=\"flex items-center gap-4\">\n      <img src=\"https://avatars.githubuserco..."
          ],
          [
           "</div>\n    </div>\n  </div>\n  <div class=\"mb-8 sm:break-inside-avoid\">\n    <blockquote class=\"rounded..."
          ],
          [
           "We also collaborated with Google Cloud (who generously provided the compute) to provide technical gu..."
          ],
          [
           "Finally, we were delighted to receive contributions to our codebase from over 300 contributors, whic..."
          ],
          [
           "Besides these, a heartfelt shoutout to the following contributors who helped us ship some of the mos..."
          ],
          [
           "## Building products with ðŸ¤— Diffusers\n\nOver the last year, we also saw many companies choosing to bu..."
          ],
          [
           "## Looking forward\n\nAs we celebrate our first anniversary, we're grateful to our community and open-..."
          ],
          [
           "--\ntitle: \"Getting Started with Hugging Face Transformers for IPUs with Optimum\"\nthumbnail: /blog/as..."
          ],
          [
           "### Getting started with IPUs and Optimum\n\nLetâ€™s use BERT as an example to help you get started with..."
          ],
          [
           "```\n$ cd /opt/gc/poplar_sdk-ubuntu_18_04-2.3.0+774-b47c577c2a/\n$ source poplar-ubuntu_18_04-2.3.0+77..."
          ],
          [
           "```\n(poptorch_env) user@host:~/workspace/poptorch_env$ pip3 install optimum[graphcore] optuna\n```\n\n#..."
          ],
          [
           "```\n$ python3 run_qa.py \\\n\t--ipu_config_name=./ \\\n\t--model_name_or_path bert-base-uncased \\\n\t--datas..."
          ],
          [
           "```\n\n### A closer look at Optimum-Graphcore\n \n#### Getting the data\n \nA very simple way to get datas..."
          ],
          [
           "```\n\nThe argument ```--model_name_or_path==bert-base-uncased`` loads the [bert-base-uncased](https:/..."
          ],
          [
           "```\n  \nYou can see the rest of the IPU BERT implementation in the [Optimum-Graphcore: SQuAD Examples..."
          ],
          [
           "--\ntitle: Deprecation of Git Authentication using password\nthumbnail: /blog/assets/password-git-depr..."
          ],
          [
           "```\nwhere `<repo_path>` is in the form of:\n- `<user_name>/<repo_name>` for models\n- `datasets/<user_..."
          ],
          [
           "--\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \nthumbnail: /blog/assets/166_trl_ddpo..."
          ],
          [
           "## The Advantages of DDPO\n\nDDPO is not the only working answer to the question of how to attempt to ..."
          ],
          [
           "The two orders of approximation have a significant impact on both performance and the ability to han..."
          ],
          [
           "Hereâ€™s a diagram to summarize the flow:\n\n![dppo rl schematic](https://huggingface.co/datasets/huggin..."
          ],
          [
           "We keep these steps in mind while moving on to actually getting these running which is described in ..."
          ],
          [
           "```\n\nThis should get the main library installed. The following dependencies are for tracking and ima..."
          ],
          [
           "```\n\nThe following table contains key hyperparameters that are directly correlated with positive res..."
          ],
          [
           "## Lessons learned\n\n1. The results seem to generalize over a wide variety of prompts despite the min..."
          ],
          [
           "The following are pre-finetuned (left) and post-finetuned (right) outputs for the prompts `bear`, `h..."
          ],
          [
           "## Limitations\n\n1. Right now `trl`'s DDPOTrainer is limited to finetuning vanilla SD models;\n2. In o..."
          ],
          [
           "--\ntitle: \"Fine-Tune Whisper For Multilingual ASR with ðŸ¤— Transformers\" \nthumbnail: /blog/assets/111_..."
          ],
          [
           "## Introduction\n\nWhisper is a pre-trained model for automatic speech recognition (ASR) \npublished in..."
          ],
          [
           "When scaled to 680,000 hours of labelled pre-training data, Whisper models \ndemonstrate a strong abi..."
          ],
          [
           "<figure>\n<img src=\"assets/111_fine_tune_whisper/whisper_architecture.svg\" alt=\"Trulli\" style=\"width:..."
          ],
          [
           "The Whisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are tr..."
          ],
          [
           "| Size   | Layers | Width | Heads | Parameters | English-only                                       ..."
          ],
          [
           "For demonstration purposes, we'll fine-tune the multilingual version of the \n[`small`](https://huggi..."
          ],
          [
           "```\n\nWe strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://hugg..."
          ],
          [
           "```\n\n### Load Dataset\n\nCommon Voice is a series of crowd-sourced datasets where speakers \nrecord tex..."
          ],
          [
           "<figure>\n<img src=\"assets/111_fine_tune_whisper/select_hi.jpg\" alt=\"Trulli\" style=\"width:100%\">\n</fi..."
          ],
          [
           "```\n\n**Print Output:**\n```\nDatasetDict({\n    train: Dataset({\n        features: ['client_id', 'path'..."
          ],
          [
           "```\n\nCommon Voice is but one multilingual ASR dataset that we can download from the Hub - \nthere are..."
          ],
          [
           "It is crucial that we match the sampling rate of our audio inputs to the sampling\nrate expected by o..."
          ],
          [
           "The Mel channels (frequency bins) are standard in speech processing and chosen to approximate\nthe hu..."
          ],
          [
           "```\n\n### Load WhisperTokenizer\n\nNow let's look at how to load a Whisper tokenizer. The Whisper model..."
          ],
          [
           "```\n\n> **Tip:** the blog post can be adapted for *speech translation* by setting the task to `\"trans..."
          ],
          [
           "```\n**Print Output:**\n```bash\nInput:                 à¤–à¥€à¤° à¤•à¥€ à¤®à¤¿à¤ à¤¾à¤¸ à¤ªà¤° à¤—à¤°à¤®à¤¾à¤ˆ à¤¬à¤¿à¤¹à¤¾à¤° à¤•à¥€ à¤¸à¤¿à¤¯à¤¾à¤¸à¤¤, à¤•à¥à¤¶à¤µà¤¾à¤¹à¤¾ ..."
          ],
          [
           "```\n\n### Prepare Data\nLet's print the first example of the Common Voice dataset to see \nwhat form th..."
          ],
          [
           "```\nWe can see that we've got a 1-dimensional input audio array and the \ncorresponding target transc..."
          ],
          [
           "```\n\nRe-loading the first audio sample in the Common Voice dataset will resample \nit to the desired ..."
          ],
          [
           "```\nGreat! We can see that the sampling rate has been downsampled to 16kHz. The \narray values are al..."
          ],
          [
           "```\n\nAlright! With that we have our data fully prepared for training! \nLet's continue and take a loo..."
          ],
          [
           "Once we've fine-tuned the model, we will evaluate it on the test data to verify that we have correct..."
          ],
          [
           "from dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n@dataclass\nclass DataCo..."
          ],
          [
           "```\n\nLet's initialise the data collator we've just defined:\n\n```python\ndata_collator = DataCollatorS..."
          ],
          [
           "```\n\n### Load a Pre-Trained Checkpoint\n\nNow let's load the pre-trained Whisper `small` checkpoint. A..."
          ],
          [
           "```\n\n### Define the Training Arguments\nIn the final step, we define all the parameters related to tr..."
          ],
          [
           "```python\nfrom transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArgument..."
          ],
          [
           "```\n\n**Note**: if one does not want to upload the model checkpoints to the Hub, \nset `push_to_hub=Fa..."
          ],
          [
           "```\n\nTraining will take approximately 5-10 hours depending on your GPU or the one \nallocated to the ..."
          ],
          [
           "Our fine-tuned model significantly improves upon the zero-shot performance of the Whisper \n`small` c..."
          ],
          [
           "```\n\nThe training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` comman..."
          ],
          [
           "```\n\nWhile the fine-tuned model yields satisfactory results on the Common \nVoice Hindi test data, it..."
          ],
          [
           "```\n\n## Closing Remarks\n\nIn this blog, we covered a step-by-step guide on fine-tuning Whisper for mu..."
          ],
          [
           "--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail..."
          ],
          [
           "**<span style=\"text-decoration:underline;\">Table of contents:</span>**\n* **<span style=\"text-decorat..."
          ],
          [
           "1. **lock in** behaviors in time and hinder social progress [from being reflected in technology](htt..."
          ],
          [
           "**These issues are deeply personal** for many of us ML researchers and developers at Hugging Face an..."
          ],
          [
           "While our own experiences do not come close to covering the myriad ways in which ML-mediated discrim..."
          ],
          [
           "This may not come as much of a surprise given the ML research communityâ€™s [focus on the value of â€œge..."
          ],
          [
           "1. <span style=\"text-decoration:underline;\">The model is integrated into a website creation service<..."
          ],
          [
           "* In this case, the machine biases directly cause discrimination by systematically directing police ..."
          ],
          [
           "So, whoâ€™s on the hook for machine biases in ML? These three cases illustrate one of the reasons why ..."
          ],
          [
           "## Addressing Bias throughout the ML Development Cycle\n\nReady for some practical advice yet? Here we..."
          ],
          [
           "For example, letâ€™s go back to one of the first highly-publicized cases of a Machine Learning system ..."
          ],
          [
           "So what does this have to do with bias? Doesnâ€™t showing people content that theyâ€™re likely to enjoy ..."
          ],
          [
           "This example serves to illustrate that the impact of machine biases in an ML-supported product depen..."
          ],
          [
           "#### Task definition: recommendations\n\nThere are as many ways for the ML task definition and deploym..."
          ],
          [
           "You can usually get a pretty good sense of likely biases in a dataset by reflecting on where it come..."
          ],
          [
           "<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "<p align=\"center\">\n <br>\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "### I am <span style=\"text-decoration:underline;\">training/selecting a model</span> for my ML system..."
          ],
          [
           "Documentation is a great first step for sharing general insights about a modelâ€™s behavior, but it is..."
          ],
          [
           "Visualization of model outputs isnâ€™t just for generative models though! For classification models, w..."
          ],
          [
           "Finally, a few benchmarks exist that can measure bias-related phenomena in models. For language mode..."
          ],
          [
           "Even with access to a benchmark for the models you are considering, you might find that running eval..."
          ],
          [
           "#### Model selection/development: recommendations\n\nFor models just as for datasets, different tools ..."
          ],
          [
           "Summary of linked tools:\n* Tasks:\n    * Explore our directory of [ML Tasks](https://huggingface.co/t..."
          ],
          [
           "* Look at [systematic model errors](https://huggingface.co/spaces/nazneen/seal) and look out for kno..."
          ],
          [
           "Thanks for reading! ðŸ¤—\n\n~ Yacine, on behalf of the Ethics and Society regulars\n    \nIf you want to ci..."
          ],
          [
           "```\n@inproceedings{hf_ethics_soc_blog_2,\n  author    = {Yacine Jernite and\n               Alexandra ..."
          ],
          [
           "--\ntitle: 'Distributed Training: Train BART/T5 for Summarization using ðŸ¤— Transformers and Amazon Sag..."
          ],
          [
           "listed again here:\n\n- [ðŸ¤— Transformers Documentation: Amazon SageMaker](https://huggingface.co/transf..."
          ],
          [
           "As [distributed training strategy](https://huggingface.co/transformers/sagemaker.html#distributed-tr..."
          ],
          [
           "```\n\nIn this tutorial, we will use an Amazon SageMaker Notebook Instance for running our training jo..."
          ],
          [
           "```\n\n---\n\n## Set up a development environment and install sagemaker\n\nAfter our SageMaker Notebook In..."
          ],
          [
           "```\n\n---\n\n# Choose ðŸ¤— Transformers `examples/` script\n\nThe [ðŸ¤— Transformers repository](https://github..."
          ],
          [
           "```\n\n---\n\n## Configure distributed training and hyperparameters\n\nNext, we will define our `hyperpara..."
          ],
          [
           "```\n\nSince, we are using [SageMaker Data Parallelism](https://aws.amazon.com/blogs/aws/managed-data-..."
          ],
          [
           "```\n```bash\n2021-04-01 13:00:35 Starting - Starting the training job...\n2021-04-01 13:01:03 Starting..."
          ],
          [
           "```\n\nThe training seconds are 2882 because they are multiplied by the number of instances. If we cal..."
          ],
          [
           "```\n\nBefore we are going to upload our model to [huggingface.co](http://huggingface.co) we need to c..."
          ],
          [
           "```\n\nAfter we extract all the metrics we want to include we are going to create our `README.md`. Add..."
          ],
          [
           "## `{model_name}`\n\nThis model was trained using Amazon SageMaker and the new Hugging Face Deep Learn..."
          ],
          [
           "## Results\n\n| key | value |\n| --- | ----- |\n{eval_table}\n{test_table}\n\n\n\n\"\"\"\n\n# Generate model card ..."
          ],
          [
           "```\n\nAfter we have our unzipped model and model card located in `my_bart_model` we can use the eithe..."
          ],
          [
           "```\n\nAnd use the \"Hosted Inference API\" widget to test it. \n\n[https://huggingface.co/philschmid/bart..."
          ],
          [
           "--\ntitle: \"SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit\"\nthumbnail: /blog/asset..."
          ],
          [
           "Compared to LLM based methods, SetFitABSA has two unique advantages:\n\n<p>ðŸ—£ <strong>No prompts needed..."
          ],
          [
           "### Training\n\n**1. Aspect candidate extraction**\n\nIn this work we assume that aspects, which are usu..."
          ],
          [
           "* **Training sentence:** \"Waiters aren't friendly but the cream pasta is out of this world.\"\n* **Tok..."
          ],
          [
           "```\naspect_candidate:training_sentence..."
          ],
          [
           "```\n\nApplying the template to the example above will generate 3 training instances â€“ two with `True`..."
          ],
          [
           "| Text                                                                          | Label |\n|:--------..."
          ],
          [
           "```\n\"their dinner specials are fantastic.\"\n```\n\n**Model Output:**\n\n```\n[{'span': 'dinner specials', ..."
          ],
          [
           "```\n\n## Benchmarking\n\nSetFitABSA was benchmarked against the recent state-of-the-art work by [AWS AI..."
          ],
          [
           "**SetFitABSA vs GPT2**\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface..."
          ],
          [
           "```\nAdditionally, we must install the `en_core_web_lg` spaCy model:\n```shell\npython -m spacy downloa..."
          ],
          [
           "```\n\nWe continue by preparing the training set. The format of the training set is a `Dataset` with t..."
          ],
          [
           "```python\nfrom datasets import load_dataset\nfrom setfit import AbsaTrainer, AbsaModel\n\n# Create a tr..."
          ],
          [
           "```\n\nThat's it! We have trained a domain-specific ABSA model. We can save our trained model to disk ..."
          ],
          [
           "```\n\nFor more details on training options, saving and loading models, and inference see the SetFit [..."
          ],
          [
           "--\ntitle: \"Boosting Wav2Vec2 with n-grams in ðŸ¤— Transformers\"\nthumbnail: /blog/assets/44_boost_wav2ve..."
          ],
          [
           "Using Connectionist Temporal Classification (CTC), pre-trained\nWav2Vec2-like checkpoints are extreme..."
          ],
          [
           "Until recently, the ðŸ¤— Transformers library did not offer a simple user\ninterface to decode audio fil..."
          ],
          [
           "First, we install `datasets` and `transformers`.\n\n```bash\npip install datasets transformers..."
          ],
          [
           "```\n\nLet's load a small excerpt of the [Librispeech\ndataset](https://huggingface.co/datasets/librisp..."
          ],
          [
           "```\n\nNext, we process the data\n\n```python\ninputs = processor(audio_sample[\"audio\"][\"array\"], samplin..."
          ],
          [
           "```\n\nFor demonstration purposes, we have prepared a new model repository\n[patrickvonplaten/wav2vec2-..."
          ],
          [
           "```\n\nIntuitively, one can understand the decoding process of\n`Wav2Vec2ProcessorWithLM` as applying b..."
          ],
          [
           "```\n\nCool! Recalling the words `facebook/wav2vec2-base-100h` without a\nlanguage model transcribed in..."
          ],
          [
           "For more information on how you can tweak different parameters when\ndecoding with `Wav2Vec2Processor..."
          ],
          [
           "As always a language model is only as good as the data it is trained on.\nIn the case of speech recog..."
          ],
          [
           "-   are generated from crawling the web, which might not be very\n    clean and correspond well to sp..."
          ],
          [
           "```\n\nLet's download the data.\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"..."
          ],
          [
           "```\n\n**Output:**\n```bash\n    Login successful\n    Your token has been saved to /root/.huggingface/to..."
          ],
          [
           "```\n\nThat was easy! The dataset viewer is automatically enabled when\nuploading a new dataset, which ..."
          ],
          [
           "*E.g.*, for the large Wav2Vec2 checkpoint that was fine-tuned on 10min only, an *n-gram* reduces the..."
          ],
          [
           "```\n\nbefore downloading and unpacking the KenLM repo.\n\n```bash\nwget -O - https://kheafield.com/code/..."
          ],
          [
           "```\n\nNow, we just have to run KenLM's `lmplz` command to build our *n-gram*,\ncalled `\"5gram.arpa\"`. ..."
          ],
          [
           "**Output:**\n```bash\n    === 1/5 Counting and sorting n-grams ===\n    Reading /content/swedish_text.t..."
          ],
          [
           "Unigram tokens 42153890 types 360209\n    === 2/5 Calculating and sorting adjusted counts ===\n    Cha..."
          ],
          [
           "2 5476741 D1=0.761523 D2=1.06735 D3+=1.32559\n    3 18177681 D1=0.839918 D2=1.12061 D3+=1.33794\n    4..."
          ],
          [
           "=== 4/5 Calculating and writing order-interpolated probabilities ===\n    Chain sizes: 1:4322496 2:87..."
          ],
          [
           "Name:lmplz\tVmPeak:14181536 kB\tVmRSS:2199260 kB\tRSSMax:4160328 kB\tuser:120.598\tsys:26.6659\tCPU:147.26..."
          ],
          [
           "```\n\nGreat, we have built a *5-gram* LM! Let's inspect the first couple of\nlines.\n\n```bash\nhead -20 ..."
          ],
          [
           "```\n\nThere is a small problem that ðŸ¤— Transformers will not be happy about\nlater on. The *5-gram* cor..."
          ],
          [
           "```\n\nLet's now inspect the corrected *5-gram*.\n\n```bash\nhead -20 5gram_correct.arpa\n```\n\n**Output:**..."
          ],
          [
           "```\n\nGreat, this looks better! We're done at this point and all that is left\nto do is to correctly i..."
          ],
          [
           "```\n\n**Output:**\n```bash\n    Found entries of length > 1 in alphabet. This is unusual unless style i..."
          ],
          [
           "```\n\n**Output:**\n```bash\n    Cloning https://huggingface.co/hf-test/xls-r-300m-sv into local empty d..."
          ],
          [
           "**Output:**\n```bash\n    xls-r-300m-sv/\n    â”œâ”€â”€ [  23]  added_tokens.json\n    â”œâ”€â”€ [ 401]  all_results..."
          ],
          [
           "â”œâ”€â”€ [ 279]  tokenizer_config.json\n    â”œâ”€â”€ [ 29K]  trainer_state.json\n    â”œâ”€â”€ [2.9K]  training_args.b..."
          ],
          [
           "9 directories, 34 files..."
          ],
          [
           "```\n\nAs can be seen the *5-gram* LM is quite large - it amounts to more than\n4 GB. To reduce the siz..."
          ],
          [
           "**Output:**\n```bash\n    xls-r-300m-sv/\n    â”œâ”€â”€ [  23]  added_tokens.json\n    â”œâ”€â”€ [ 401]  all_results..."
          ],
          [
           "```\n\nNice, we reduced the *n-gram* by more than half to less than 2GB now. In\nthe final step, let's ..."
          ],
          [
           "--\ntitle: \"Hugging Face Machine Learning Demos on arXiv\" \nthumbnail: /blog/assets/arxiv/thumbnail.pn..."
          ],
          [
           "![An interactive demo of a protein structure model, available on Hugging Face Spaces](/blog/assets/a..."
          ],
          [
           "--\ntitle: \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\" \nthumbnail: /blog/assets/..."
          ],
          [
           "Writing a loss function to capture these attributes seems intractable and most language models are s..."
          ],
          [
           "1. Pretraining a language model (LM),\n2. gathering data and training a reward model, and\n3. fine-tun..."
          ],
          [
           "Next, with a language model, one needs to generate data to train a **reward model**, which is how hu..."
          ],
          [
           "Human annotators are used to rank the generated text outputs from the LM. One may initially think th..."
          ],
          [
           "### Fine-tuning with RL\n\nTraining a language model with reinforcement learning was, for a long time,..."
          ],
          [
           "Let's first formulate this fine-tuning task as a RL problem. First, the **policy** is a language mod..."
          ],
          [
           "The reward function is where the system combines all of the models we have discussed into one RLHF p..."
          ],
          [
           "Finally, the **update rule** is the parameter update from PPO that maximizes the reward metrics in t..."
          ],
          [
           "# Open-source tools for RLHF\n\nThe first [code](https://github.com/openai/lm-human-preferences) relea..."
          ],
          [
           "[RL4LMs](https://github.com/allenai/RL4LMs) offers building blocks for fine-tuning and evaluating LL..."
          ],
          [
           "Generating well-written human text answering specific prompts is very costly, as it often requires h..."
          ],
          [
           "With these limitations, huge swaths of unexplored design options could still enable RLHF to take sub..."
          ],
          [
           "### Further reading\n\nHere is a list of the most prevalent papers on RLHF to date. The field was rece..."
          ],
          [
           "And here is a snapshot of the growing set of \"key\" papers that show RLHF's performance for LMs:\n- [F..."
          ],
          [
           "- Sparrow: [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/..."
          ],
          [
           "- [Llama 2](https://arxiv.org/abs/2307.09288) (Touvron et al. 2023): Impactful open-access model wit..."
          ],
          [
           "The field is the convergence of multiple fields, so you can also find resources in other areas:\n* Co..."
          ],
          [
           "```\nLambert, et al., \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", Hugging Face ..."
          ],
          [
           "--\ntitle: ðŸ§¨ Stable Diffusion  in JAX / Flax !\nthumbnail: /blog/assets/108_stable_diffusion_jax/thumb..."
          ],
          [
           "Note that JAX is not exclusive to TPUs, but it shines on that hardware because each TPU server has 8..."
          ],
          [
           "```\n\n*Output*:\n```bash \n    Found 8 JAX devices of type TPU v2.\n```\n\n\n\nMake sure `diffusers` is inst..."
          ],
          [
           "```\n\n\n## Model Loading\n\nBefore using the model, you need to accept the model [license](https://huggi..."
          ],
          [
           "```\n\n\nTPU devices support `bfloat16`, an efficient half-float type. We'll use it for our tests, but ..."
          ],
          [
           "```\n\n\n``` python\nprompt_ids = shard(prompt_ids)\nprompt_ids.shape\n```\n\n*Output*:\n```bash \n    (8, 1, ..."
          ],
          [
           "```\n\n\nJAX code can be compiled to an efficient representation that runs very fast. However, we need ..."
          ],
          [
           "```\n\n\n``` python\nimage_grid(images, 2, 4)\n```\n\n![png](assets/108_stable_diffusion_jax/jax_stable_dif..."
          ],
          [
           "```\n\n![png](assets/108_stable_diffusion_jax/jax_stable_diffusion_2.png)\n\n\n--------------------------..."
          ],
          [
           "```\n\n\nAfter we use `pmap`, the prepared function `p_generate` will conceptually do the following:\n\n-..."
          ],
          [
           "--\ntitle: 'Pre-Train BERT with Hugging Face Transformers and Habana Gaudi'\nthumbnail: /blog/assets/9..."
          ],
          [
           "_Note: Steps 1 to 3 can/should be run on a different instance size since those are CPU intensive tas..."
          ],
          [
           "## What is BERT?\n\nBERT, short for Bidirectional Encoder Representations from Transformers, is a Mach..."
          ],
          [
           "```\nRead more about Masked Language Modeling [here](https://huggingface.co/blog/bert-101).\n\n---\n\nLet..."
          ],
          [
           "```\n\n\nThe [original BERT](https://arxiv.org/abs/1810.04805) was pretrained on [Wikipedia](https://hu..."
          ],
          [
           "```\n_We are not going to do some advanced dataset preparation, like de-duplication, filtering or any..."
          ],
          [
           "```\n\nWe push the tokenizer to the [Hugging Face Hub](https://huggingface.co/models) for later traini..."
          ],
          [
           "```\n\nAs data processing function we will concatenate all texts from our dataset and generate chunks ..."
          ],
          [
           "```\n\n## 4. Pre-train BERT on Habana Gaudi\n\nIn this example, we are going to use Habana Gaudi on AWS ..."
          ],
          [
           "```\n\nWhen using GPUs you would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en..."
          ],
          [
           "```\n\nThe `DL1` instance we use has 8 available HPU-cores meaning we can leverage distributed data-pa..."
          ],
          [
           "```python\nfrom huggingface_hub import HfFolder\n\n# hyperparameters\nhyperparameters = {\n    \"model_con..."
          ],
          [
           "```\n\nWe can start our training by creating a `EC2RemoteRunner` and then `launch` it. This will then ..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/99_pretraining_bert/tens..."
          ],
          [
           "To compare the cost we can use the [p3dn.24xlarge](https://aws.amazon.com/de/ec2/instance-types/p3/)..."
          ],
          [
           "Those results are incredible since it will allow companies to adapt their pre-trained models to thei..."
          ],
          [
           "--\ntitle: \"Machine Learning Experts - Sasha Luccioni\"\nthumbnail: /blog/assets/69_sasha_luccioni_inte..."
          ],
          [
           "Very excited to introduce this brilliant episode to you! Hereâ€™s my conversation with Sasha Luccioni:..."
          ],
          [
           "### Love that you wanted to something that felt meaningful!\n\n**Sasha:** Yeah, when I hear people on ..."
          ],
          [
           "Also about the data, I'm involved in a lot of the data working groups at Big Science, and it's reall..."
          ],
          [
           "### Can you speak a little bit more about the environmental impact of AI?\n\n**Sasha:** Yeah, it's a t..."
          ],
          [
           "So first we just [created this online calculator](https://mlco2.github.io/impact/) where someone cou..."
          ],
          [
           "For example; France is mostly nuclear, mostly energy, and Canada has a lot of hydroelectric energy. ..."
          ],
          [
           "### What are some of the ways that machine learning teams and engineers could be a bit more proactiv..."
          ],
          [
           "**Sasha:** Yeah, we wrote a paper a couple of years ago that was a cool experience. It's almost a hu..."
          ],
          [
           "Then instead of powering up a diesel generator which is cool because you can just power them up, and..."
          ],
          [
           "### For people listening that are interested in this effort, but perhaps work at an organization whe..."
          ],
          [
           "**Sasha:** Actually, machine learning people or AI people, in general, have this stereotype from oth..."
          ],
          [
           "And I've participated in organizing workshops where people submit ideas that are super great on pape..."
          ],
          [
           "### So sad. That's such a great story though and how there are opportunities like that.\n\n**Sasha:** ..."
          ],
          [
           "**Sasha:** Yeah, there's this concept that my mom read about in some magazine ages ago when I was a ..."
          ],
          [
           "For example, what I feel like we're doing at Hugging Face is really that machine learning needs more..."
          ],
          [
           "### What other examples or applications do you find and see potential meaning in AI machine learning..."
          ],
          [
           "### And you've talked before about the power of data and how it's not talked about enough.\n\n**Sasha:..."
          ],
          [
           "### Wow. That is so interesting!\n\n**Sasha:** It's actually really, camera trap data is a really huge..."
          ],
          [
           "**Sasha:** Yeah, I guess another anecdote, I have a lot of these anecdotes, but at some point we wan..."
          ],
          [
           "So that was really cool because we really saw a year and some before they had no trace of anything, ..."
          ],
          [
           "### Exactly, that's so interesting. That's so amazing that you were able to jump in there and provid..."
          ],
          [
           "### All right, so we're going to dive into rapid-fire questions. If you could go back and do one thi..."
          ],
          [
           "### That's so funny, and itâ€™s interesting to hear that because I often hear people say you need to k..."
          ],
          [
           "### So besides maybe a mathematical foundation, what advice would you give to someone looking to get..."
          ],
          [
           "### I love that, find something that you're interested in.\n\n**Sasha:** Exactly. And one of my favori..."
          ],
          [
           "### So should people be afraid of AI taking over the world?\n\n**Sasha:** I think that we're really fa..."
          ],
          [
           "### What are you interested in right now? It could be anything, a movie, a recipe, a podcast, etc.?\n..."
          ],
          [
           "### What are some of your favorite machine learning papers?\n\n**Sasha:** My favorite currently, paper..."
          ],
          [
           "### Wow, we'll definitely be linking to that paper as well, so people can check that out. Yeah, very..."
          ],
          [
           "### Where can people find you online?\n\n**Sasha:** I'm on [Twitter @SashaMTL](https://twitter.com/Sas..."
          ],
          [
           "--\ntitle: Simple considerations for simple people building fancy neural networks\nthumbnail: /blog/as..."
          ],
          [
           "At the same time, deep learning frameworks, tools, and specialized libraries democratize machine lea..."
          ],
          [
           "*   Are the labels balanced?\n*   Are there gold-labels that you do not agree with?\n*   How were the ..."
          ],
          [
           "As developers, it easy to feel good when building something fancy but it is sometimes hard to ration..."
          ],
          [
           "> Pro-tip: in my experience working with pre-trained language models, freezing the embeddings module..."
          ],
          [
           "## 4. ðŸ‘€ Tune but donâ€™t tune blindly\n\nOnce you have everything up and running, you might want to tune..."
          ],
          [
           "A few related pointers to complete your reading:\n\n*   [Reproducibility (in ML) as a vehicle for engi..."
          ],
          [
           "--\ntitle: \"Fine-tuning Llama 2 70B using PyTorch FSDP\" \nthumbnail: /blog/assets/160_fsdp_llama/thumb..."
          ],
          [
           "(Source: [link](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/))\n\n## ..."
          ],
          [
           "3. SLURM script `launch.slurm`: https://gist.github.com/pacman100/1cb1f17b2f1b3139a63b764263e70b25\n\n..."
          ],
          [
           "Below is the output snippet on a 7B model on 2 GPUs measuring the memory consumed and model paramete..."
          ],
          [
           "```\n\n### Addressing Challenge 2\nIt is addressed via choosing `SHARDED_STATE_DICT` state dict type wh..."
          ],
          [
           "```\naccelerate config --config_file \"fsdp_config.yaml\"\n```\n\n![fsdp_config](https://huggingface.co/da..."
          ],
          [
           "```\n\n### Addressing Challenge 3\nFlash Attention and enabling gradient checkpointing are required for..."
          ],
          [
           "(Source: [link](https://arxiv.org/pdf/2205.14135.pdf))\n\nThis is precisely the problem that Flash Att..."
          ],
          [
           "## Bringing it all-together\n\nTo run the training using `Accelerate` launcher with SLURM, refer this ..."
          ],
          [
           "```\naccelerate launch \\\n    --config_file configs/fsdp_config.yaml \\\n    --main_process_ip $MASTER_A..."
          ],
          [
           "```\n\nFine-tuning completed in ~13.5 hours and below is the training loss plot.\n\n![train_loss](https:..."
          ],
          [
           "- Human: Now explain it like a chef.\n\n+ Assistant: Certainly! Here's an explanation of deep learning..."
          ],
          [
           "```\n\nThe whole conversation is formatted as below: \n```\n<|system|> system message <|endoftext|> <|pr..."
          ],
          [
           "--\ntitle: Optimizing Stable Diffusion for Intel CPUs with NNCF and ðŸ¤— Optimum\nthumbnail: /blog/assets..."
          ],
          [
           "In this blog post, we will outline the problems of optimizing Stable Diffusion models and propose a ..."
          ],
          [
           "However, it turns out that the traditional model optimization methods, such as post-training 8-bit q..."
          ],
          [
           "## Optimization workflow\n\nWe usually start the optimization of a model after it's trained. Here, we ..."
          ],
          [
           "## Going beyond Quantization-Aware Training\n\nQuantization alone can bring significant enhancements b..."
          ],
          [
           "<div class=\"flex flex-row\">\n<div class=\"grid grid-cols-2 gap-4\">\n<figure>\n<img class=\"max-w-full rou..."
          ],
          [
           "<figcaption class=\"mt-2 text-center text-sm text-gray-500\">OpenVINO 8-bit, Inference Speed: 59 secon..."
          ],
          [
           "Results of image generation [demo](https://huggingface.co/spaces/helenai/stable_diffusion) using dif..."
          ],
          [
           "Below we show how to perform inference with the final pipeline optimized to run on Intel CPUs:\n\n```p..."
          ],
          [
           "```\n\nYou can find the training and quantization [code](https://github.com/huggingface/optimum-intel/..."
          ],
          [
           "--\ntitle: \"A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using tran..."
          ],
          [
           "After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways ..."
          ],
          [
           "![Summary](assets/96_hf_bitsandbytes_integration/tf32-Mantissa-chart-hi-res-FINAL.png)\n\nFloat32 (FP3..."
          ],
          [
           "In the Ampere architecture, NVIDIA also introduced [TensorFloat-32](https://blogs.nvidia.com/blog/20..."
          ],
          [
           "But what if we can store those weights with less memory using a different data type? A methodology c..."
          ],
          [
           "For example, in zero-point quantization, if my range is -1.0â€¦1.0 and I want to quantize into the ran..."
          ],
          [
           "![out-quant.gif](assets/96_hf_bitsandbytes_integration/out-quant.gif)\n\nTo retrieve the latest, one c..."
          ],
          [
           "## A gentle summary of LLM.int8(): zero degradation matrix multiplication for Large Language Models\n..."
          ],
          [
           "As mentioned earlier, 8-bit precision is extremely constrained, therefore quantizing a vector with s..."
          ],
          [
           "We ran several common benchmarks with the 8-bit and native models using lm-eval-harness and reported..."
          ],
          [
           "For BLOOM-176:\n\n| benchmarks | - | -   | -  |   -        |     difference - value  |\n| ---------- | ..."
          ],
          [
           "### Is it faster than native models?\n\n\nThe main purpose of the LLM.int8() method is to make large mo..."
          ],
          [
           "| Precision      | Number of parameters | Hardware     | Time per token in milliseconds for Batch Si..."
          ],
          [
           "The 3 models are BLOOM-176B, T5-11B and T5-3B.\n\n### Hugging Face `transformers` integration nuances\n..."
          ],
          [
           "```\n\n2. Then you can define your own model. Note that you can convert a checkpoint or model of any p..."
          ],
          [
           "```\nint8_model[0].weight\nParameter containing:\ntensor([[ 0.0031, -0.0438,  0.0494,  ..., -0.0046, -0..."
          ],
          [
           "```\n\nThe weights values are \"truncated\" as we have seen when explaining quantization in the previous..."
          ],
          [
           "```\n\nCheck out [the example script](/assets/96_hf_bitsandbytes_integration/example.py) for the full ..."
          ],
          [
           "```\n\nto\n\n```py\nparam_cls = type(module._parameters[name])\nkwargs = module._parameters[name].__dict__..."
          ],
          [
           "```\n\nThis function recursively replaces all `nn.Linear` layers of a given model initialized on the `..."
          ],
          [
           "### Be very careful on how to set devices with `accelerate`\n\nHere we played a very delicate balancin..."
          ],
          [
           "Now time to see how to benefit from this integration and how to successfully use it in `transformers..."
          ],
          [
           "```\n\n### Example demos - running T5 11b on a Google Colab\n\nCheck out the Google Colab demos for runn..."
          ],
          [
           "### Support for Kepler GPUs (GTX 1080 etc)\n\nWhile we support all GPUs from the past four years, some..."
          ],
          [
           "--\ntitle: \"Speech Synthesis, Recognition, and More With SpeechT5\"\nthumbnail: /blog/assets/speecht5/t..."
          ],
          [
           "The main idea behind SpeechT5 is to pre-train a single model on a mixture of text-to-speech, speech-..."
          ],
          [
           "Note: Even though the fine-tuned models start out using the same set of weights from the shared pre-..."
          ],
          [
           "SpeechT5 is not available in the latest release of Transformers yet, so you'll have to install it fr..."
          ],
          [
           "```\n\nFirst, we load the [fine-tuned model](https://huggingface.co/microsoft/speecht5_tts) from the H..."
          ],
          [
           "```\n\nThe speaker embedding is a tensor of shape (1, 512). This particular speaker embedding describe..."
          ],
          [
           "```\n\nTo make audio from the spectrogram, do the following:\n\n```python\nwith torch.no_grad():\n    spee..."
          ],
          [
           "```\n\nThe output sounds like this ([download audio](https://huggingface.co/datasets/huggingface/docum..."
          ],
          [
           "The **speech encoder pre-net** is the same as the feature encoding module from [wav2vec 2.0](https:/..."
          ],
          [
           "```\n\nWe will need some speech audio to use as input. For the purpose of this example, weâ€™ll load the..."
          ],
          [
           "```\n\nNow we can perform the speech conversion by calling the modelâ€™s `generate_speech` method.\n\n```p..."
          ],
          [
           "```\n\nChanging to a different voice is as easy as loading a new speaker embedding. You could even mak..."
          ],
          [
           "- **Speech encoder pre-net.** This is the same pre-net used by the speech-to-speech model and consis..."
          ],
          [
           "```\n\nAs speech audio, weâ€™ll use the same input as in the previous section, but any audio file will w..."
          ],
          [
           "```\n\nFinally, tell the model to generate text tokens from the speech input, and then use the process..."
          ],
          [
           "--\ntitle: \"Code Llama: Llama 2 learns to code\" \nthumbnail: /blog/assets/160_codellama/thumbnail.jpg\n..."
          ],
          [
           "## Table of Contents\n\n  - [Introduction](#introduction)\n  - [Table of Contents](#table-of-contents)\n..."
          ],
          [
           "Code Llama was trained on a 16k context window. In addition, the three model variants had additional..."
          ],
          [
           "<script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.28.3/gradio.js\"> </script>\n<g..."
          ],
          [
           "```\n#### A Note on dtypes\n\nWhen using models like Code Llama, it's important to take a look at the d..."
          ],
          [
           "```python\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\n\ntokenizer = AutoT..."
          ],
          [
           "```\n\nThis may produce output like the following:\n\n```python\nResult: def fibonacci(n):\n    if n == 0:..."
          ],
          [
           "```\n\nCode Llama is specialized in code understanding, but it's a language model in its own right. Yo..."
          ],
          [
           "```\n\n```Python\ndef remove_non_ascii(s: str) -> str:\n    \"\"\" Remove non-ASCII characters from a strin..."
          ],
          [
           "```\n\nNote that the system prompt is optional - the model will work without it, but you can use it to..."
          ],
          [
           "```\n\n3. **On-going conversation with previous answers**\n\nThe process is the same as in [Llama 2](htt..."
          ],
          [
           "```\n\n#### 4-bit Loading\n\nIntegration of Code Llama in Transformers means that you get immediate supp..."
          ],
          [
           "```\n\n### Using text-generation-inference and Inference Endpoints\n\n[Text Generation Inference](https:..."
          ],
          [
           "### Using VS Code extension\n\n[HF Code Autocomplete](https://marketplace.visualstudio.com/items?itemN..."
          ],
          [
           "| Model                  | License            | Dataset known | Commercial use? | Pretraining length..."
          ],
          [
           "| CodeLlaMa-13B-Instruct | Llama 2 license    | âŒ             | âœ…               | 2,620B            ..."
          ],
          [
           "**Note:** The scores presented in the table above are sourced from our code leaderboard, where we ev..."
          ],
          [
           "--\ntitle: \"Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU plat..."
          ],
          [
           "## Supported hardware platforms\n\nOn the GPU side, AMD and Hugging Face will first collaborate on the..."
          ],
          [
           "## The road ahead\n\nOur initial focus will be ensuring the models most important to our community wor..."
          ],
          [
           "--\ntitle: \"Accelerating over 130,000 Hugging Face models with ONNX Runtime\"\nthumbnail: /blog/assets/..."
          ],
          [
           "## Learn More\nTo learn more about accelerating Hugging Face models with ONNX Runtime, check out our ..."
          ],
          [
           "--\ntitle: \"AI Speech Recognition in Unity\"\nthumbnail: /blog/assets/124_ml-for-games/unity-asr-thumbn..."
          ],
          [
           "```\n[SerializeField] private Button startButton;\n[SerializeField] private Button stopButton;\n[Serial..."
          ],
          [
           "```\nprivate void Update() {\n    if (recording && Microphone.GetPosition(null) >= clip.samples) {\n   ..."
          ],
          [
           "```\nusing System.IO;\nusing TMPro;\nusing UnityEngine;\nusing UnityEngine.UI;\n\npublic class SpeechRecog..."
          ],
          [
           "private byte[] EncodeAsWAV(float[] samples, int frequency, int channels) {\n        using (var memory..."
          ],
          [
           "```\n\nTo test whether this code is working correctly, you can add the following line to the end of th..."
          ],
          [
           "```\nusing System.IO;\nusing HuggingFace.API;\nusing TMPro;\nusing UnityEngine;\nusing UnityEngine.UI;\n\np..."
          ],
          [
           "```\n\nCongratulations, you can now use state-of-the-art Speech Recognition in Unity!\n\nIf you have any..."
          ],
          [
           "--\ntitle: 'Building a Playlist Generator with Sentence Transformers'\nthumbnail: /blog/assets/87_play..."
          ],
          [
           "<div class=\"hidden xl:block\">\n<div style=\"display: flex; flex-direction: column; align-items: center..."
          ],
          [
           "Weâ€™ll be looking at a slightly advanced use of Gradio, so if youâ€™re a beginner to the library I reco..."
          ],
          [
           "[The ST documentation highlights many of the choices](https://www.sbert.net/docs/pretrained_models.h..."
          ],
          [
           "```python\nfrom sentence_transformers import SentenceTransformer\nimport pickle\n\nembedder = SentenceTr..."
          ],
          [
           "```\n\nTo be able to share you embeddings with others, you can even upload the Pickle file to a Huggin..."
          ],
          [
           "```\n\nSince weâ€™re searching for any verse that matches the text prompt, thereâ€™s a good chance that th..."
          ],
          [
           "```python\nfrom sentence_transformers import SentenceTransformer, util\nfrom huggingface_hub import hf..."
          ],
          [
           "```\n\nThe Gradio Blocks API lets you build *multi-step* interfaces, which means that youâ€™re free to c..."
          ],
          [
           "```\n\nIn that function, we use the text prompt to conduct the semantic search. As seen above, to push..."
          ],
          [
           "While the song *lyrics* arenâ€™t being released, Iâ€™ve **[published the verse embeddings along with the..."
          ],
          [
           "--\ntitle: \"Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2\"\nthumbnail: /blog/asset..."
          ],
          [
           "## BridgeTower\n\nIn the recent past, [Vision-Language (VL) models](https://huggingface.co/blog/vision..."
          ],
          [
           "[Nvidia A100 Tensor Core GPU](https://www.nvidia.com/en-us/data-center/a100/) includes the 3rd gener..."
          ],
          [
           "## Benchmark\n\nTo benchmark training, we are going to fine-tune a [BridgeTower Large checkpoint](http..."
          ],
          [
           "### Making use of `dataloader_num_workers`\n\nWhen image loading is done on CPU, a quick way to speed ..."
          ],
          [
           "Here are the throughputs we got on Gaudi2, H100 and A100:\n\n| Device     | `dataloader_num_workers=0`..."
          ],
          [
           "Second, we see that **allocating more resources for data loading can lead to easy speedups**: x1.28 ..."
          ],
          [
           "Optimum Habana's fast DDP does not split parameter gradients into buckets as [DDP does](https://pyto..."
          ],
          [
           "Given a dataset, most dataloaders follow the following recipe:\n\n1. Fetch data (e.g. where your JPEG ..."
          ],
          [
           "To implement this on Gaudi2, we have got you covered: the [contrastive image-text example](https://g..."
          ],
          [
           "We got an additional x1.10 speedup compared to the previous run with `dataloader_num_workers=2` only..."
          ],
          [
           "```\n\nThe base command line to run the script is:\n```bash\npython ../gaudi_spawn.py --use_mpi --world_..."
          ],
          [
           "```\n\nFor A100 and H100, you can use the same `run_bridgetower.py` script with a few small changes:\n-..."
          ],
          [
           "To go further, we are looking forward to using HPU graphs for training models even faster and to pre..."
          ],
          [
           "--\ntitle: \"Introducing ðŸ¤— Accelerate\"\nthumbnail: /blog/assets/20_accelerate_library/accelerate_diff.p..."
          ],
          [
           "```\n\nBy just adding five lines of code to any standard PyTorch training script, you can now run said..."
          ],
          [
           "```\n\nIn contrast, here are the changes needed to have this code run with distributed training are th..."
          ],
          [
           "```\n\nOn top of giving the main object that you will use, this line will analyze from the environment..."
          ],
          [
           "```\n\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torc..."
          ],
          [
           "```\n\nThis last line adds the necessary steps for the backward pass (mostly for mixed precision but o..."
          ],
          [
           "```\n\nLike for the training, you need to add one line to prepare your evaluation dataloader. Then you..."
          ],
          [
           "```\n\nwill launch your training script using those default. The only thing you have to do is provide ..."
          ],
          [
           "--\ntitle: \"Interactively explore your Huggingface dataset with one line ofÂ code\"\nthumbnail: /blog/as..."
          ],
          [
           "```\n\n<p align=\"center\"><a href=\"https://github.com/Renumics/spotlight\"><img src=\"https://huggingface..."
          ],
          [
           "```\n\nData inspection is a very important task in almost all ML development stages, but it can also b..."
          ],
          [
           "```\n\n## **Leveraging model results for data inspection**\n\nExploring raw unstructured datasets often ..."
          ],
          [
           "```\n\nNow we can compute the enrichment:\n\n\n```python\nimport torch\nimport transformers\n\ndevice = torch..."
          ],
          [
           "```\n\nIf you donâ€™t want to perform the full inference run, you can alternatively download pre-compute..."
          ],
          [
           "```\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/docu..."
          ],
          [
           "You can optionally choose a dataset that contains model results and other configuration options such..."
          ],
          [
           "--\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\nthu..."
          ],
          [
           "One of the main problems developers and organizations face is how difficult it is to deploy and scal..."
          ],
          [
           "Within minutes, you can test your endpoint and add its inference API to your application. Itâ€™s never..."
          ],
          [
           "The Hugging Face Blog Repository ðŸ¤—\nThis is the official repository of the [Hugging Face Blog](https:..."
          ],
          [
           "```\n---\ntitle: \"PUT YOUR TITLE HERE\" \nthumbnail: /blog/assets/101_decision-transformers-train/thumbn..."
          ],
          [
           "--\ntitle: Training CodeParrot ðŸ¦œ from Scratch\nthumbnail: /blog/assets/40_codeparrot/thumbnail.png\naut..."
          ],
          [
           "- 0.1% of the unique files make up 15% of all files\n- 1% of the unique files make up 35% of all file..."
          ],
          [
           "# Base tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nbase_vocab = list(bytes_to_unicod..."
          ],
          [
           "```\n\nLearn more about tokenizers and how to build them in the [Hugging Face course](https://huggingf..."
          ],
          [
           "```\n\nNow that we have an efficient tokenizer and a freshly initialized model we can start with the a..."
          ],
          [
           "```\n\nWe can directly load the tokenizer and model from the local repository. Since we are dealing wi..."
          ],
          [
           "```\n\nNext up is the dataset. We make training simpler with a dataset that yields examples with a fix..."
          ],
          [
           "def __iter__(self):\n        iterator = iter(self.dataset)\n        more_examples = True\n        while..."
          ],
          [
           "```\n\nTexts in the buffer are tokenized in parallel and then concatenated. Chunked samples are then y..."
          ],
          [
           "```\n\nBefore we start training we need to set up the optimizer and learning rate schedule. We donâ€™t w..."
          ],
          [
           "```\nUnder the hood it'll use DistributedDataParallel, which means a batch is sent to each GPU worker..."
          ],
          [
           "```\n\nWe are now ready to write the main training loop. It will look pretty much like a normal PyTorc..."
          ],
          [
           "```\n\nWhen we call `wait_for_everyone()` and `unwrap_model()` we make sure that all workers are ready..."
          ],
          [
           "```\nDone! That's all the code to train a full GPT-2 model from scratch with as little as 150 lines. ..."
          ],
          [
           "```Python\nfrom datasets import load_metric\n\ncode_eval = datasets.load_metric(\"code_eval\")\ntest_cases..."
          ],
          [
           "```\n\n\n\nYou can also load OpenAI's HumanEval dataset with `datasets`:\n\n```Python\nfrom datasets import..."
          ],
          [
           "```\n\nAlthough the test condition itself does not look quite right the model has built all the boiler..."
          ],
          [
           "```\n\n## Summary\n\nIn this short blog post we walked through all the steps involved for training a lar..."
          ],
          [
           "--\ntitle: \"Optimizing Bark using ðŸ¤— Transformers\" \nthumbnail: /blog/assets/bark_optimization/thumbnai..."
          ],
          [
           "This tutorial is also a demonstration of how one can benchmark a non-optimized model and its varying..."
          ],
          [
           "Bark is made of 4 main models:\n\n- `BarkSemanticModel` (also referred to as the 'text' model): a caus..."
          ],
          [
           "```\n\nPlace the model to an accelerator device to get the most of the optimization techniques:\n\n```py..."
          ],
          [
           "```\n\nMeasuring the latency and GPU memory footprint requires the use of specific CUDA methods. We de..."
          ],
          [
           "```\n\n**Output:**\n\n```\nExecution time: 9.3841625 seconds\nMax memory footprint 1.914612224  GB\n```\n\nNo..."
          ],
          [
           "```\n\n\nThe output sounds like this ([download audio](https://huggingface.co/datasets/huggingface/docu..."
          ],
          [
           "Flash Attention is a faster and more efficient algorithm for attention computations that combines tr..."
          ],
          [
           "```\n\n**Output:**\n\n```\nExecution time: 5.43284375 seconds\nMax memory footprint 1.9151841280000002  GB..."
          ],
          [
           "```\nExecution time: 5.43284375 seconds\nMax memory footprint 1.9151841280000002  GB\n```\n\nThe output s..."
          ],
          [
           "```\n\n**Output:**\n\n```\nExecution time: 7.00045390625 seconds\nMax memory footprint 2.7436124160000004 ..."
          ],
          [
           "```\n\n**Output:**\n\n```\nExecution time: 8.97633828125 seconds\nMax memory footprint 1.3231160320000002 ..."
          ],
          [
           "```\n\n**Output:**\n\n```\nExecution time: 7.4496484375000005 seconds\nMax memory footprint 0.468710912000..."
          ],
          [
           "```\n\n\nThe output sounds like this (download [first](https://huggingface.co/datasets/huggingface/docu..."
          ],
          [
           "The benchmark was run on an NVIDIA TITAN RTX 24GB with a maximum of 256 new tokens.\n\n## How to read ..."
          ],
          [
           "| Relative value              | Latency | Memory |\n|-----------------------------|---------|--------..."
          ],
          [
           "| Relative value                | Latency | Memory | Throughput |\n|-------------------------------|-..."
          ],
          [
           "--\ntitle: \"Director of Machine Learning Insights [Part 4]\"\nthumbnail: /blog/assets/78_ml_director_in..."
          ],
          [
           "**Background:** Seasoned entrepreneur and leader, Javier was co-founder and CTO of Machinalis, a hig..."
          ],
          [
           "E-commerce is scaling its share of the market year after year, and Machine Learning is always a prob..."
          ],
          [
           "**Background:** Dr. Shaun Gittens is the Director of the Machine Learning Capability of MasterPeace ..."
          ],
          [
           "#### **1. How has ML made a positive impact on Engineering?**\n\nEngineering is vast in its applicatio..."
          ],
          [
           "#### **3. Whatâ€™s a common mistake you see people make when trying to integrate ML into Engineering?*..."
          ],
          [
           "**Background:** Samuel is a senior Data Science and ML Engineering leader at Pluralsight with a Ph.D..."
          ],
          [
           "#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into existing products?..."
          ],
          [
           "**Fun Fact:** Met Paul McCarthy. ðŸŽ¤\n\n**MasterPeace Solutions:** MasterPeace Solutions has emerged as ..."
          ],
          [
           "#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into SaaS?**\nTo get it ..."
          ],
          [
           "--\ntitle: \"Panel on Hugging Face\" \nthumbnail: /blog/assets/panel-on-hugging-face/thumbnail.png\nautho..."
          ],
          [
           "Here are some notable features of Panel that our users find valuable. \n\n- Panel provides extensive s..."
          ],
          [
           "## ðŸŒ Join Our Community\nThe Panel community is vibrant and supportive, with experienced developers a..."
          ],
          [
           "--\ntitle: \"Diffusion Models Live Event\"\nthumbnail: /blog/assets/diffusion-models-event/thumbnail.png..."
          ],
          [
           "<div\n    class=\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\"\n>\n    <div class=\"text-center flex..."
          ],
          [
           "<p><strong>Devi Parikh: <em>Make-A-Video: Diffusion Models for Text-to-Video Generation without Text..."
          ],
          [
           "<p><strong>Justin Pinkney: <em>Beyond text - giving Stable Diffusion new abilities</em></strong></p>..."
          ],
          [
           "--\ntitle: \"Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training\"\nthumbnail:..."
          ],
          [
           "With 60,000+ stars on Github, 30,000+ models, and millions of monthly visits, Hugging Face is one of..."
          ],
          [
           "--\ntitle: \"Fine-tune Llama 2 with DPO\" \nthumbnail: /blog/assets/157_dpo_trl/dpo_thumbnail.png\nauthor..."
          ],
          [
           "## DPO vs PPO\n\nIn the traditional model of optimising human derived preferences via RL, the goto met..."
          ],
          [
           "In this respect we would still need to do the step 1, but instead of steps 3 and 4 we need to provid..."
          ],
          [
           "```\n\nOnce we have the dataset sorted the DPO loss is essentially a supervised loss which obtains an ..."
          ],
          [
           "```\n\n## Experiment with Llama v2\n\nThe benefit of implementing the DPO trainer in TRL is that one can..."
          ],
          [
           "# add LoRA layers on top of the quantized base model\npeft_config = LoraConfig(\n    r=script_args.lor..."
          ],
          [
           "```\n\n### DPO Training\n\nOnce the SFT has finished, we can save the resulting model and move onto the ..."
          ],
          [
           "```\n\nSo as can be seen we load the model in the 4-bit configuration and then train it via the QLora ..."
          ],
          [
           "We hope with the code release it lowers the barrier to entry for you the readers to try out this met..."
          ],
          [
           "--\ntitle: \"Welcome fastText to the Hugging Face Hub\"\nthumbnail: /blog/assets/147_fasttext/thumbnail...."
          ],
          [
           "![text_classification_widget](assets/147_fasttext/fasttext_text_classification_widget.png)\n![feature..."
          ],
          [
           "```\n\nHere is how to use this model to query nearest neighbors of an English word vector:\n\n```python\n..."
          ],
          [
           "```\n\n## Would you like to integrate your library to the Hub?\n\nThis integration is possible thanks to..."
          ],
          [
           "--\ntitle: \"Introducing Decision Transformers on Hugging Face ðŸ¤—\"\nthumbnail: /blog/assets/58_decision-..."
          ],
          [
           "![Offline vs Online RL](assets/58_decision-transformers/offlinevsonlinerl.gif)\n\n*A comparison betwee..."
          ],
          [
           "The main idea is that instead of training a policy using RL methods, such as fitting a value functio..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n    <video \n        alt=\"WalkerEd-expert\"\n      ..."
          ],
          [
           "```\n\n### Loading the model\n\nUsing the Decision Transformer is relatively easy, but as it is an autor..."
          ],
          [
           "```\n### Autoregressive prediction function\n\nThe model performs an [autoregressive prediction](https:..."
          ],
          [
           "`````python\n# Function that gets an action from the model using autoregressive prediction \n# with a ..."
          ],
          [
           "actions = torch.cat([torch.zeros((1, padding, act_dim)), actions], dim=1).float()\n\treturns_to_go = t..."
          ],
          [
           "```\n### Evaluating the model\n\nIn order to evaluate the model, we need some additional information; t..."
          ],
          [
           "state_mean = torch.from_numpy(state_mean)\nstate_std = torch.from_numpy(state_std)\n\nstate = env.reset..."
          ],
          [
           "```\nYou will find a more detailed example, with the creation of videos of the agent in our [Colab no..."
          ],
          [
           "--\ntitle: \"Course Launch Community Event\"\nthumbnail: /blog/assets/34_course_launch/speakers_day1_thu..."
          ],
          [
           "To register, please fill out [this form](https://docs.google.com/forms/d/e/1FAIpQLSd17_u-wMCdO4fcOPO..."
          ],
          [
           "<div\n    class=\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\"\n>\n    <div class=\"text-center flex..."
          ],
          [
           "<p><strong>Margaret Mitchell: <em>On Values in ML Development</em></strong></p>\n        <p>Margaret ..."
          ],
          [
           "<p>Jakob Uszkoreit is the co-founder of Inceptive. Inceptive designs RNA molecules for vaccines and ..."
          ],
          [
           "</div>\n    <div class=\"text-center flex flex-col items-center\">\n        <img src=\"/blog/assets/34_co..."
          ],
          [
           "## Day 2 (November 16th): The tools you will use\n\nDay 2 will be focused on talks by the Hugging Face..."
          ],
          [
           "<div\n    class=\"container md:grid md:grid-cols-2 gap-2 max-w-7xl\"\n>\n    <div class=\"text-center flex..."
          ],
          [
           "<img src=\"/blog/assets/34_course_launch/lysandre_debut.png\" width=50% style=\"border-radius: 50%;\">\n ..."
          ],
          [
           "</div>\n    <div class=\"text-center flex flex-col items-center\">\n        <img src=\"/blog/assets/34_co..."
          ],
          [
           "<p><strong>Mathieu DesvÃ©: <em>AWS ML Vision: Making Machine Learning Accessible to all Customers</em..."
          ],
          [
           "--\ntitle: 'Accelerated Inference with Optimum and Transformers Pipelines'\nthumbnail: /blog/assets/66..."
          ],
          [
           "Let's get started! ðŸš€\n\n## 1. What is Optimum? An ELI5\n\n[Hugging Face Optimum](https://github.com/hugg..."
          ],
          [
           "**Switching from Transformers to Optimum Inference**\nThe [Optimum Inference models](https://huggingf..."
          ],
          [
           "```\n\nIn the first release, we added [support for ONNX Runtime](https://huggingface.co/docs/optimum/m..."
          ],
          [
           "*This tutorial was created and run on an `m5.xlarge` AWS EC2 Instance.*\n\n### 3.1 Install `Optimum` f..."
          ],
          [
           "```\n\nThis will install all required packages for us including `transformers`, `torch`, and `onnxrunt..."
          ],
          [
           "# save onnx checkpoint and tokenizer\nmodel.save_pretrained(onnx_path)\ntokenizer.save_pretrained(onnx..."
          ],
          [
           "```\n\nWe successfully converted our vanilla transformers to `onnx` and used the model with the `trans..."
          ],
          [
           "```\n\nTo test performance we can use the `ORTModelForQuestionAnswering` class again and provide an ad..."
          ],
          [
           "```\n\nWe will evaluate the performance changes in step [3.6 Evaluate the performance and speed](#36-e..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/66_optimum_inference/mod..."
          ],
          [
           "```\n\nNice! The model predicted the same answer.\n\n### 3.5 Run accelerated inference using Transformer..."
          ],
          [
           "```\n\nIn addition to this we added a `pipelines` API to Optimum to guarantee more safety for your acc..."
          ],
          [
           "```\n\n### 3.6 Evaluate the performance and speed\n\nDuring this [End-to-End tutorial on accelerating Ro..."
          ],
          [
           "```\n\nWe can now leverage the [map](https://huggingface.co/docs/datasets/v2.1.0/en/process#map) funct..."
          ],
          [
           "```\n\nNow lets compare the results\n\n```python\ndefault_acc = metric.compute(predictions=result[\"defaul..."
          ],
          [
           "```\n\nOur optimized & quantized model achieved an exact match of `78.75%` and an f1 score of `81.83%`..."
          ],
          [
           "```\n\nTo keep it simple, we are going to use a python loop and calculate the avg/mean latency for our..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/66_optimum_inference/res..."
          ],
          [
           "- **Remote Models > 2GB:** Currently, only models smaller than 2GB can be loaded from the [Hugging F..."
          ],
          [
           "**How can I use Optimum with Transformers?**\n\nYou can find an example and instructions in our [docum..."
          ],
          [
           "Some important features on the roadmap for Optimum amongst the [current limitations](#4-current-limi..."
          ],
          [
           "--\ntitle: \"Hugging Face Selected for the French Data Protection Agency Enhanced Support Program\"\nthu..."
          ],
          [
           "When it comes to respecting peopleâ€™s privacy rights, the recent developments in ML and AI pose new q..."
          ],
          [
           "--\ntitle: Using Stable Diffusion with Core ML on Apple Silicon\nthumbnail: /blog/assets/diffusers_cor..."
          ],
          [
           "## Available Checkpoints\n\nThe official Stable Diffusion checkpoints are already converted and ready ..."
          ],
          [
           "## Notes on Performance\n\nThere are several variants per model:\n\n- \"Original\" attention vs \"split_ein..."
          ],
          [
           "```\ncoreml-stable-diffusion-v1-4\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ original\nâ”‚   â”œâ”€â”€ compiled\nâ”‚   â””â”€â”€ packages\nâ””â”€â”€ sp..."
          ],
          [
           "```\n\n`<output-mlpackages-directory>` should point to the checkpoint you downloaded in the step above..."
          ],
          [
           "```\n\n## Core ML inference in Swift\n\nRunning inference in Swift is slightly faster than in Python, be..."
          ],
          [
           "```\n\nYou have to specify in `--resource-path` one of the checkpoints downloaded in the previous step..."
          ],
          [
           "--\ntitle: \"Exploring simple optimizations for SDXL\"\nthumbnail: /blog/assets/simple_sdxl_optimization..."
          ],
          [
           "```\n\nThis isnâ€™t very practical and can slow you down because youâ€™re often generating more than 4 ima..."
          ],
          [
           "With ðŸ¤—Â Diffusers, you can use fp16 for inference by specifying the `torch.dtype` parameter to conver..."
          ],
          [
           "```\n\nCompared to a completely unoptimized SDXL pipeline, using fp16 takes 21.7GB of memory and only ..."
          ],
          [
           "```\n\nCompared to a completely unoptimized SDXL pipeline, using fp16 and SDPA takes the same amount o..."
          ],
          [
           "```\n\nCompared to the previous baseline (fp16 + SDPA), wrapping the UNet with `torch.compile` improve..."
          ],
          [
           "```\n\nCompared to the baseline, it now takes 20.2GB of memory which saves you 1.5GB of memory.\n\n### S..."
          ],
          [
           "```\n\nWith sliced computations, we reduce the memory to 15.4GB. If we add sequential CPU offloading, ..."
          ],
          [
           "```\n\nNext, flush the GPU memory to remove the text encoders:\n\n```jsx\ndel text_encoder, text_encoder_..."
          ],
          [
           "```\n\nCombined with SDPA and fp16, we can reduce the memory to 21.9GB. Other techniques discussed abo..."
          ],
          [
           "```\n\nWith this setup, we reduce the memory requirement to 15.6GB while reducing the inference latenc..."
          ],
          [
           "| **Technique** | **Memory (GB)** | **Inference latency (ms)** |\n| --- | --- | --- |\n| unoptimized p..."
          ],
          [
           "--\ntitle: \"Fine-Tune ViT for Image Classification with ðŸ¤— Transformers\"\nthumbnail: /blog/assets/51_fi..."
          ],
          [
           "It turns out that once you've done the above, you can pre-train and fine-tune transformers just as y..."
          ],
          [
           "```\n\n## Load a dataset\n\nLet's start by loading a small image classification dataset and taking a loo..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "```\n\n\n\n\n    'bean_rust'\n\n\n\nTurns out the leaf shown above is infected with Bean Rust, a serious dise..."
          ],
          [
           "```\n\n\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)..."
          ],
          [
           "```\n\nYou can see the image processor configuration by printing it.\n\n\n    ViTImageProcessor {\n      \"..."
          ],
          [
           "```\n\n\n```python\nprocess_example(ds['train'][0])\n```\n\n\n    {\n      'pixel_values': tensor([[[[-0.6157..."
          ],
          [
           "```\n\n\nThis time, the resulting `pixel_values` tensor will have shape `(2, 3, 224, 224)`.\n\n    {\n    ..."
          ],
          [
           "```\n\n\nLet's load the pretrained model. We'll add `num_labels` on init so the model creates a classif..."
          ],
          [
           "```\n\n\nAlmost ready to train! The last thing needed before that is to set up the training configurati..."
          ],
          [
           "```\n\n### Train ðŸš€\n\n\n```python\ntrain_results = trainer.train()\ntrainer.save_model()\ntrainer.log_metric..."
          ],
          [
           "--\ntitle: \"~Don't~ Repeat Yourself\"\nthumbnail: /blog/assets/59_transformers_philosophy/transformers...."
          ],
          [
           "In short the reasons are:\n- **1. Transformers is built by and for the open-source community.**\n- **2..."
          ],
          [
           "### 3. Machine Learning is evolving at a neck-breaking speed\nResearch in the field of machine learni..."
          ],
          [
           "Instead, research teams tend to publish a new model built upon previous models but rarely make signi..."
          ],
          [
           "Now, we explain why we put the asterisk \\\\( {}^{\\textbf{*}} \\\\) after *\"Repeat Yourself\"*. We don't ..."
          ],
          [
           "### Drawbacks\nClearly, there are also drawbacks to the single file policy two of which we quickly wa..."
          ],
          [
           "--\ntitle: \"Introducing WÃ¼rstchen: Fast Diffusion for Image Generation\" \nthumbnail: /blog/assets/wuer..."
          ],
          [
           "![WÃ¼rstchen images with Prompts](https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "You can also find a detailed explanation video here:\n\n<iframe width=\"708\" height=\"398\" src=\"https://..."
          ],
          [
           "```\n\n![Anthropomorphic cat dressed as a fire-fighter](https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "### Diffusers integration\nBecause WÃ¼rstchen is fully integrated in `diffusers`, it automatically com..."
          ],
          [
           "## Optimisation Technique 1: Flash Attention\n\nStarting from version 2.0, PyTorch has integrated a hi..."
          ],
          [
           "```\n\nFor an in-depth look at how `diffusers` leverages SDPA, check out the [documentation](https://h..."
          ],
          [
           "```\n\nAnd the good news is that this compilation is a one-time execution. Post that, you're set to ex..."
          ],
          [
           "--\ntitle: Faster TensorFlow models in Hugging Face Transformers\nthumbnail: /blog/assets/10_tf-servin..."
          ],
          [
           "| Batch size | Google implementation | v4.2.0 implementation | Relative difference Google/v4.2.0 imp..."
          ],
          [
           "### What is a SavedModel?\n\nA SavedModel contains a standalone TensorFlow model, including its weight..."
          ],
          [
           "```\nsavedmodel\n    /assets\n        -> here the needed assets by the model (if any)\n    /variables\n  ..."
          ],
          [
           "```\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['attention_mask'] te..."
          ],
          [
           "```\n\nTo directly pass `inputs_embeds` (the token embeddings) instead of `input_ids` (the token IDs) ..."
          ],
          [
           "```\n\nThe serving method has to be overridden by the new `input_signature` argument of the `tf.functi..."
          ],
          [
           "```\n\n## How to deploy and use a SavedModel?\n\nLetâ€™s see step by step how to deploy and use a BERT mod..."
          ],
          [
           "```\n\nand kill the serving_base image ran as a daemon because we don't need it anymore:\n```\ndocker ki..."
          ],
          [
           "```\ndocker run -d -p 8501:8501 -p 8500:8500 --name bert my_bert_model\n```\n\n### Step 3\n\nQuery the mod..."
          ],
          [
           "```\n\nThis should return POSITIVE. It is also possible to pass by the gRPC (google Remote Procedure C..."
          ],
          [
           "# Create a gRPC request made for prediction\nrequest = predict_pb2.PredictRequest()\n\n# Set the name o..."
          ],
          [
           "```\n\n## Conclusion\nThanks to the last updates applied on the TensorFlow models in transformers, one ..."
          ],
          [
           "--\ntitle: \"Object Detection Leaderboard\"\nthumbnail: /blog/assets/object-detection-leaderboard/thumbn..."
          ],
          [
           "So, let's embark on this exploration together and unlock the secrets of the Object Detection Leaderb..."
          ],
          [
           "## What's Object Detection?\n\nIn the field of Computer Vision, Object Detection refers to the task of..."
          ],
          [
           "The diversity of detectors goes beyond the range of output classes they can recognize. They vary in ..."
          ],
          [
           "IoU is a metric represented by a number between 0 and 1 that measures the overlap between the predic..."
          ],
          [
           "Based on predefined \\\\( \\text{T}_{\\text{IOU}} \\\\), we can define True Positives and True Negatives:\n..."
          ],
          [
           "* **Recall** gauges a modelâ€™s competence in finding all the relevant cases (all ground truth boundin..."
          ],
          [
           "The Precision x Recall curve illustrates the balance between Precision and Recall based on different..."
          ],
          [
           "Based on these rules, we can classify each detection as TP or FP, as shown in Table 1:\n\n<div display..."
          ],
          [
           "For example, consider the 12th row (detection \"P\") of Table 2. The value \"acc TP = 4\" means that if ..."
          ],
          [
           "By examining the curve, one may infer the potential trade-offs between Precision and Recall and find..."
          ],
          [
           "<div display=\"block\" margin-left=\"auto\" margin-right=\"auto\" width=\"50%\">\n<center>\n    <img src=\"http..."
          ],
          [
           "### What's Average Recall and how to compute it?\n\nAverage Recall (AR) is a metric that's often used ..."
          ],
          [
           "<p style=\"text-align: center;\">\n\\\\( \\text{AP@[.5:.05:0.95} = \\frac{\\text{AP}_{0.5} + \\text{AP}_{0.55..."
          ],
          [
           "## Object Detection Leaderboard\n\nWe recently released the [Object Detection Leaderboard](https://hug..."
          ],
          [
           "Next, we will provide tips on choosing the best model based on the metrics and point out which param..."
          ],
          [
           "Letâ€™s take the DEtection TRansformer (DETR) ([facebook/detr-resnet-50](https://huggingface.co/facebo..."
          ],
          [
           "```\n\nThe parameter `threshold` in function `post_process_object_detection` is used to filter the det..."
          ],
          [
           "Figure 10 shows the process with batch size = 2, where the same two images are processed with `DetrI..."
          ],
          [
           "It's important to recognize that models can produce boxes in various formats, and that also may be t..."
          ],
          [
           "For such models, different prompts (e.g. \"Find the dog\" and \"Where's the bulldog?\") may result in th..."
          ],
          [
           "| Use Case                                     | Real-world Scenarios                  | Recommended..."
          ],
          [
           "The results shown in our ðŸ¤— [Object Detection Leaderboard](https://huggingface.co/spaces/hf-vision/ob..."
          ],
          [
           "--\ntitle: \"The Falcon has landed in the Hugging Face ecosystem\" \nthumbnail: /blog/assets/147_falcon/..."
          ],
          [
           "## The Falcon models\n\nThe Falcon family is composed of two base models: [Falcon-40B](https://hugging..."
          ],
          [
           "Falcon-7B and Falcon-40B have been trained on 1.5 trillion and 1 trillion tokens respectively, in li..."
          ],
          [
           "| Model | License | Commercial use? | Pretraining length [tokens] | Pretraining compute [PF-days] | ..."
          ],
          [
           "Under the hood, this playground uses Hugging Face's [Text Generation Inference](https://github.com/h..."
          ],
          [
           "# Inference\n\nYou can use the familiar transformers APIs to run the models on your own hardware, but ..."
          ],
          [
           "```\n\nAnd then, you'd run text generation using code like the following:\n\n```python\nsequences = pipel..."
          ],
          [
           "```\n\nNote, however, that mixed 8-bit inference will use `torch.float16` instead of `torch.bfloat16`,..."
          ],
          [
           "| ![tgi-hfe-screenshot.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve..."
          ],
          [
           "* [AI2 Reasoning Challenge](https://allenai.org/data/arc) (ARC): Grade-school multiple choice scienc..."
          ],
          [
           "| Model | Type | Average leaderboard score |\n| :---: | :---: | :---: |\n| [tiiuae/falcon-40b-instruct..."
          ],
          [
           "Although the open LLM leaderboard doesn't measure chat capabilities (where human evaluation is the g..."
          ],
          [
           "| ![repo-screenshot.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/ma..."
          ],
          [
           "We fine-tuned the two variants of the Falcon models (7B and 40B) on the Guanaco dataset. We fine-tun..."
          ],
          [
           "```\n\nCheck out the [original qlora repository](https://github.com/artidoro/qlora/) for additional de..."
          ],
          [
           "--\ntitle: \"Gradio 3.0 is Out!\"\nthumbnail: /blog/assets/68_gradio_blocks/block-party.png\nauthors:\n- u..."
          ],
          [
           "<img class=\"max-w-full mx-auto my-6\" style=\"width: 54rem\" src=\"/blog/assets/68_gradio_blocks/dalle.j..."
          ],
          [
           "```python\nimport numpy as np\nimport gradio as gr\n\ndef flip_text(x):\n    return x[::-1]\n\ndef flip_ima..."
          ],
          [
           "```\n\nOnce you run `launch()`, the following demo will appear:\n\n<img class=\"max-w-full mx-auto my-6\" ..."
          ],
          [
           "--\ntitle: \"Releasing Swift Transformers: Run On-Device LLMs in Apple Devices\"\nthumbnail: /blog/asset..."
          ],
          [
           "Let's go!\n\n<p align=\"center\">\n  <video controls title=\"Llama 2 (7B) chat model running on an M1 MacB..."
          ],
          [
           "## Tasks Overview\n\nWhen I published tweets showing [Falcon](https://twitter.com/pcuenq/status/166460..."
          ],
          [
           "- [Conversion to Core ML](#conversion-to-core-ml). We'll use Llama 2 as a real-life example.\n- [Opti..."
          ],
          [
           "## Conversion to Core ML\n\nCore ML is Apple's native framework for Machine Learning, and also the nam..."
          ],
          [
           "2. Use [`exporters`](https://github.com/huggingface/exporters), a Python conversion package built on..."
          ],
          [
           "- If you have to use `coremltools`, use the latest version: `7.0b1`. Despite technically being a bet..."
          ],
          [
           "There are a few key optimization areas we've identified. They are a very important topic for us and ..."
          ],
          [
           "### Tokenizers\n\nTokenization solves two complementary tasks: adapt text input to the tensor format u..."
          ],
          [
           "```\n  \"normalizer\": {\n    \"type\": \"Sequence\",\n    \"normalizers\": [\n      {\n        \"type\": \"Prepend\"..."
          ],
          [
           "```\n\nIt reads like this: normalization is a sequence of operations applied in order. First, we `Prep..."
          ],
          [
           "```\n\nHowever, you don't usually need to tokenize the input text yourself - the [`Generation` code](h..."
          ],
          [
           "- Greedy decoding. This is the obvious algorithm: select the token with the highest probability, app..."
          ],
          [
           "To use it, download a Core ML model from the Hub or create your own, and select it from the UI. All ..."
          ],
          [
           "### _Appendix: Converting Llama 2 the Hard Way_\n\nYou can safely ignore this section unless you've ex..."
          ],
          [
           "![Llama 2 conversion error](https://huggingface.co/datasets/huggingface/documentation-images/resolve..."
          ],
          [
           "What the error screenshot is telling us is that there's a type mismatch trying to fill the mask tens..."
          ],
          [
           "Fortunately, `coremltools` coverage for new operations is good and the team reacts very fast.\n\n## Re..."
          ],
          [
           "---\ntitle: \"Making ML-powered web games with Transformers.js\" \nthumbnail: /blog/assets/ml-web-games/..."
          ],
          [
           "## Overview\n\nBefore we start, let's talk about what we'll be creating. The game is inspired by Googl..."
          ],
          [
           "### Model architecture\n\nWe'll be finetuning [`apple/mobilevit-small`](https://huggingface.co/apple/m..."
          ],
          [
           "3. Defining our [collate function](https://huggingface.co/docs/transformers/main_classes/data_collat..."
          ],
          [
           "### Converting our model to ONNX\n\nFortunately, the [ðŸ¤— Optimum](https://huggingface.co/docs/optimum) ..."
          ],
          [
           "```\n\n2. Run the conversion script (it uses `Optimum` under the hood):\n\n    ```bash\n    python -m scr..."
          ],
          [
           "```\n\nWe can now use this worker in our `App.jsx` file by adding the following code to the `App` comp..."
          ],
          [
           "```\n\n*Woohoo!* ðŸ¥³ Although the above code is just a small part of the [final product](https://github...."
          ],
          [
           "### Quality of life improvements\n\nThe original dataset contains 345 different classes, and since our..."
          ],
          [
           "**PS**: Don't forget to join the [Open Source AI Game Jam](https://itch.io/jam/open-source-ai-game-j..."
          ],
          [
           "--\ntitle: \"Deploy MusicGen in no time with Inference Endpoints\" \nthumbnail: /blog/assets/run-musicge..."
          ],
          [
           "### Let's go!\n\nFirst, we will duplicate the [facebook/musicgen-large](https://huggingface.co/faceboo..."
          ],
          [
           "```\n\nLet's hear what it sounds like:\n\n<audio controls>\n<source src=\"https://huggingface.co/datasets/..."
          ],
          [
           "```\n\nLet's give it a listen:\n\n<audio controls>\n<source src=\"https://huggingface.co/datasets/huggingf..."
          ],
          [
           "def __call__(self, data: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"\n        Args:\n            da..."
          ],
          [
           "```\n\nTo keep things simple, in this example we are only generating audio from text, and not conditio..."
          ],
          [
           "```\n\nHere's how it sounds like:\n\n<audio controls>\n<source src=\"https://huggingface.co/datasets/huggi..."
          ],
          [
           "--\n\ntitle: \"Results of the Open Source AI Game Jam\"\nthumbnail: /blog/assets/game-jam-first-edition-r..."
          ],
          [
           "The games were evaluated by their peers and contributors based on three key criteria: **fun, creativ..."
          ],
          [
           "## Participants Selection: Top 10 ðŸ¥ˆðŸ¥‰ðŸ…\n\nOut of the 88 fantastic submissions, these impressive games e..."
          ],
          [
           "ðŸ¤– Used Text To Speech model to generate the voices.\n\nðŸŽ®ðŸ‘‰ https://zeenaz.itch.io/fish-dang-rolling-lau..."
          ],
          [
           "In this sandbox gravity game, you create an expanding universe and try to complete the challenges.\n\n..."
          ],
          [
           "ðŸ¤– Elevenlabs - Voice generator\n\nðŸ¤– Scenario - Image generator\n\nðŸŽ®ðŸ‘‰ https://blastergames.itch.io/galact..."
          ],
          [
           "ðŸ¤– Used Stable Diffusion\n\nðŸŽ®ðŸ‘‰ https://ilumine-ai.itch.io/dreamlike-hugging-face-open-source-ai-game-ja..."
          ],
          [
           "Join our Discord ServerÂ ðŸ‘‰Â **https://hf.co/join/discord**\n\n**Thank you to all the participants, contr..."
          ],
          [
           "Contrastive Search\n\nThis is a companion notebook to the [Hugging Face guest blog post entry about co..."
          ],
          [
           "```\n\n## 3. Contrastive Search:\n\n### 3.1. Generating Text with Contrastive Search:\n\n\n```python\nimport..."
          ],
          [
           "```\n\n#### 4.1.1. Generating Text with Greedy Search:\n\n\n```python\noutput = model.generate(input_ids, ..."
          ],
          [
           "```\n\n#### 4.2.1. Generating Text with Greedy Search:\n\n\n```python\noutput = model.generate(input_ids, ..."
          ],
          [
           "```\n\n# TensorFlow\n\nâš ï¸ The TensorFlow version of Contrastive Search is not yet released -- it will be..."
          ],
          [
           "```\n\n### 2.2. Stochastic Methods:\n\n\n```python\nimport tensorflow as tf\nfrom transformers import AutoT..."
          ],
          [
           "```\n\n## 4. More Generated Examples:\n\n### 4.1. Example One - GPT-2:\n\n\n```python\n# Load the language m..."
          ],
          [
           "```\n\n#### 4.1.3. Generating Text with Contrastive Search:\n\n\n```python\noutput = model.generate(input_..."
          ],
          [
           "```\n\n#### 4.2.3. Generating Text with Contrastive Search:\n\n\n```python\noutput = model.generate(input_..."
          ],
          [
           "--\ntitle: ðŸ§¨ Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e\nthumbnail: /blog/as..."
          ],
          [
           "[Google Cloud TPUs](https://cloud.google.com/tpu) are custom-designed AI accelerators, which are opt..."
          ],
          [
           "In this blog post,\n1. [We describe why JAX + TPU + Diffusers is a powerful framework to run SDXL](#w..."
          ],
          [
           "#### High-performance throughput for high batch sizes\n\nWorkloads can be scaled across multiple devic..."
          ],
          [
           "```\n\nWe'll now load the base SDXL model and the rest of the components required for inference. The d..."
          ],
          [
           "```\n\nThe prompts have to be supplied as tensors to the pipeline, and they always have to have the sa..."
          ],
          [
           "```\n\nWe are now ready to put everything together in a generate function:\n\n```python\ndef generate(\n  ..."
          ],
          [
           "```\n\nIt now took about 2s to generate the 4 images!\n\n## Benchmark\n\nThe following measures were obtai..."
          ],
          [
           "## How does the demo work?\n\nThe [demo we showed before](https://huggingface.co/spaces/google/sdxl) w..."
          ],
          [
           "--\ntitle: \"Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia\"\nthumbnail: /..."
          ],
          [
           "AWS's take to solve this challenge was to design a custom machine learning chip designed for optimiz..."
          ],
          [
           "You will learn how to: \n\n  - [1. Convert your Hugging Face Transformer to AWS Neuron](#1-convert-you..."
          ],
          [
           "As a first step, we need to install the [Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/e..."
          ],
          [
           "```\n\nAfter we have installed the Neuron SDK we can load and convert our model. Neuron models are con..."
          ],
          [
           "```\n\nAt the time of writing, the [AWS Neuron SDK does not support dynamic shapes](https://awsdocs-ne..."
          ],
          [
           "```\n\n## 2. Create a custom `inference.py` script for `text-classification`\n\nThe [Hugging Face Infere..."
          ],
          [
           "```\n\nWe are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core ..."
          ],
          [
           "```\n\n## 3. Create and upload the neuron model and inference script to Amazon S3\n\nBefore we can deplo..."
          ],
          [
           "```\n\nNow we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`.\n\n```python\nfrom..."
          ],
          [
           "```\n\n## 5. Run and evaluate Inference performance of BERT on Inferentia\n\nTheÂ `.deploy()`Â returns anÂ ..."
          ],
          [
           "```\n\nThe average latency for our BERT model is `5-6ms` for a sequence length of 128.\n<br>\n<figure cl..."
          ],
          [
           "```\n\n## Conclusion\n\nWe successfully managed to compile a vanilla Hugging Face Transformers model to ..."
          ],
          [
           "--\ntitle: \"How to train your model dynamically using adversarial data\"\nthumbnail: /blog/assets/88_mn..."
          ],
          [
           "## Training your model dynamically using adversarial data\n \nHere I will walk you through dynamically..."
          ],
          [
           "This walkthrough will be divided into the following sections:\n1. Configuring your model\n2. Interacti..."
          ],
          [
           "```\n\nNow that you have defined the structure of your model, you need to train it on the standard MNI..."
          ],
          [
           "<iframe src=\"https://chrisjay-simple-mnist-classification.hf.space\" frameBorder=\"0\" width=\"100%\" hei..."
          ],
          [
           "### Putting it all together\n\nThe final step is to put all the three components (configuring the mode..."
          ],
          [
           "This process of fooling and training the model on the adversarially collected data should be repeate..."
          ],
          [
           "--\ntitle: \"Rocket Money x Hugging Face: Scaling Volatile ML Models in Productionâ€‹\"\nthumbnail: /blog/..."
          ],
          [
           "We decided to start from a clean slate, assembling both a new team and a new mandate. Our first task..."
          ],
          [
           "In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for..."
          ],
          [
           "Once the contract was signed, we began the migration of moving off our regex based system to direct ..."
          ],
          [
           "Speaking of scale, as we started to witness a significant increase in traffic to our model, it becam..."
          ],
          [
           "_If you want to learn how Hugging Face can manage your ML inference workloads, contact the Hugging F..."
          ],
          [
           "--\ntitle: \"Leveraging Hugging Face for complex generative AI use cases\"\nthumbnail: /blog/assets/78_m..."
          ],
          [
           "--\ntitle: \"ControlNet in ðŸ§¨ Diffusers\" \nthumbnail: /blog/assets/controlnet/thumbnail.png \nauthors:\n- ..."
          ],
          [
           "Or even  use it as your interior designer.\n\n<table>\n<tr style=\"text-align: center;\">\n    <th>Before<..."
          ],
          [
           "Also, make some of the famous logos coming to life.\n\n<table>\n<tr style=\"text-align: center;\">\n    <t..."
          ],
          [
           "Training ControlNet is comprised of the following steps:\n\n1. Cloning the pre-trained parameters of a..."
          ],
          [
           "A sample from the training set for ControlNet-like training looks like this (additional conditioning..."
          ],
          [
           "Every new type of conditioning requires training a new copy of ControlNet weights. \nThe paper propos..."
          ],
          [
           "We will explore different use cases with the `StableDiffusionControlNetPipeline` in this blog post. ..."
          ],
          [
           "```\n\nTo process different conditionings depending on the chosen ControlNet, we also need to install ..."
          ],
          [
           "```\n\nAs we can see, it is essentially edge detection:\n\n<p align=\"center\">\n<img src=\"https://huggingf..."
          ],
          [
           "```\n\nInstead of using Stable Diffusion's default [PNDMScheduler](https://huggingface.co/docs/diffuse..."
          ],
          [
           "```\n\nInstead of loading our pipeline directly to GPU, we instead enable smart CPU offloading which \n..."
          ],
          [
           "```\n\nNow we are ready to run the ControlNet pipeline!\n\nWe still provide a prompt to guide the image ..."
          ],
          [
           "```\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/mai..."
          ],
          [
           "```\n\nIt is noticeable that Mr Potato Head is not the best candidate but he tried his best and did a ..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "We welcome you to combine these different elements and share your results with [@diffuserslib](https..."
          ],
          [
           "If you cannot wait to try out ControlNet directly, we got you covered as well! Simply click on one o..."
          ],
          [
           "--\ntitle: Universal Image Segmentation with Mask2Former and OneFormer\nthumbnail: /blog/assets/127_ma..."
          ],
          [
           "Image segmentation can largely be split into 3 subtasks - instance, semantic and panoptic segmentati..."
          ],
          [
           "Over the last years, researchers have come up with several architectures that were typically very ta..."
          ],
          [
           "[Mask2Former](https://huggingface.co/docs/transformers/main/model_doc/mask2former) extends this to i..."
          ],
          [
           "Note that Mask2Former still needs to be trained on each task separately to obtain state-of-the-art r..."
          ],
          [
           "```\nfrom transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n\nprocessor = Au..."
          ],
          [
           "```\nprediction = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1..."
          ],
          [
           "```\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as..."
          ],
          [
           "```\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/12..."
          ],
          [
           "# Conclusion\n\nThat's it! You now know about the difference between instance, semantic and panoptic s..."
          ],
          [
           "--\ntitle: \"A Dive into Text-to-Video Models\"\nthumbnail: /blog/assets/140_text-to-video/thumbnail.png..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "The text-to-video task faces unique challenges on multiple fronts. Some of these main challenges inc..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "Text2Video-Zero is a text-guided video generation and manipulation framework that works in a fashion..."
          ],
          [
           "These large datasets experience similar issues to those found in text-to-image datasets. The most co..."
          ],
          [
           "### Hugging Face Demos\nAt Hugging Face, our goal is to make it easier to use and build upon state-of..."
          ],
          [
           "<gradio-app theme_mode=\"light\" space=\"PAIR/Text2Video-Zero\"></gradio-app>\n\nApart from using demos to..."
          ],
          [
           "```\ngit clone https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis\ncd modelsc..."
          ],
          [
           "```\n\n### Community Contributions and Open Source Text-to-Video Projects\nFinally, there are various o..."
          ],
          [
           "That was it! We are continuing to integrate the most impactful computer vision and multi-modal model..."
          ],
          [
           "--\ntitle: \"Stable Diffusion XL on Mac with Advanced Core ML Quantization\"\nthumbnail: /blog/assets/st..."
          ],
          [
           "For Stable Diffusion XL weâ€™ve done a few things:\n* Ported the [base model to Core ML](https://huggin..."
          ],
          [
           "## Using SD XL Models from the Hugging Face Hub\n\nAs part of this release, we published two different..."
          ],
          [
           "For reference, these are the performance figures we achieved on different devices:\n\n|        Device ..."
          ],
          [
           "We explored a different alternative instead: **mixed-bit palettization**. Instead of using 6 bits pe..."
          ],
          [
           "```\n\nWhat this tells us is that the original model quality, as measured by PSNR in float16, is about..."
          ],
          [
           "For visual examples, these are the results on prompt `a high quality photo of a surfing dog` running..."
          ],
          [
           "Some initial conclusions:\n- In our opinion, all the images have good quality in terms of how realist..."
          ],
          [
           "Mixed-bit palettization runs in two phases: _analysis_ and _application_.\n\nThe goal of the analysis ..."
          ],
          [
           "For an introduction to the process, check the [instructions in the repo](https://github.com/apple/ml..."
          ],
          [
           "Finally, as mentioned in the introduction, we created a [complete Stable Diffusion XL Core ML pipeli..."
          ],
          [
           "* [`apple/ml-stable-diffusion`](https://github.com/apple/ml-stable-diffusion), by Apple. Conversion ..."
          ],
          [
           "--\ntitle: \"An Introduction to Deep Reinforcement Learning\"\nthumbnail: /blog/assets/63_deep_rl_intro/..."
          ],
          [
           "Deep RL is a type of Machine Learning where an agent learnsÂ **how to behave**Â in an environmentÂ **by..."
          ],
          [
           "In this free course, you will:\n\n- ðŸ“– Study Deep Reinforcement Learning in **theory and practice**.\n- ..."
          ],
          [
           "If you prefer, you can watch the ðŸ“¹ video version of this chapter :\n\n<iframe width=\"560\" height=\"315\"..."
          ],
          [
           "## **What is Reinforcement Learning?**\n\nTo understand Reinforcement Learning, letâ€™s start with the b..."
          ],
          [
           "**Without any supervision**, the child will get better and better at playing the game.\n\nThatâ€™s how h..."
          ],
          [
           "- Our Agent receivesÂ **state \\\\(S_0\\\\)**Â from theÂ **Environment**Â â€” we receive the first frame of ou..."
          ],
          [
           "### **Observations/States Space**\n\nObservations/States are theÂ **information our agent gets from the..."
          ],
          [
           "> In reality, we use the term state in this course but we willÂ make the distinction in implementatio..."
          ],
          [
           "Taking this information into consideration is crucial because it willÂ **have importance when choosin..."
          ],
          [
           "As we can see in theÂ diagram,Â **itâ€™s more probable to eat the cheese near us than the cheese close t..."
          ],
          [
           "For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ..."
          ],
          [
           "Remember, the goal of our RL agent is to maximize the expected cumulative reward. However,Â **we can ..."
          ],
          [
           "- *Exploitation*: You go every day to the same one that you know is good and **take the risk to miss..."
          ],
          [
           "There are two approaches to train our agent to find this optimal policy Ï€*:\n\n- **Directly,** by teac..."
          ],
          [
           "- *Stochastic*: outputÂ **a probability distribution over actions.**\n\n<figure class=\"image table text..."
          ],
          [
           "â€œAct according to our policyâ€ just means that our policy isÂ **â€œgoing to the state with the highest v..."
          ],
          [
           "Deep Reinforcement Learning introducesÂ **deep neural networks to solve Reinforcement Learning proble..."
          ],
          [
           "That was a lot of information, if we summarize:\n\n- Reinforcement Learning is a computational approac..."
          ],
          [
           "---\nNow that you've studied the bases of Reinforcement Learning, youâ€™re ready to train your first la..."
          ],
          [
           "Naturally, during the course,Â **weâ€™re going to use and explain these terms again**, but itâ€™s better ..."
          ],
          [
           "--\ntitle: \"Scaling up BERT-like model Inference on modern CPU  - Part 2\"\nauthors:\n- user: echarlaix\n..."
          ],
          [
           "Back in April, Intel launched its [latest generation of Intel Xeon processors](https://www.intel.com..."
          ],
          [
           "In this area, Intel plays an essential role by providing software components under the oneAPI umbrel..."
          ],
          [
           "Some of these libraries, especially MKL or oneDNN, are natively included in frameworks such as PyTor..."
          ],
          [
           "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "Each pushes forward different approaches to improve aspects of the memory allocation and management ..."
          ],
          [
           "<br>\n\n<figure class=\"image table text-center m-0 w-full\">\n    <medium-zoom background=\"rgba(0,0,0,.7..."
          ],
          [
           "Finally, on top of this, one can find some domain specific libraries such as Intel's oneDNN which br..."
          ],
          [
           "We will also provide an initial baseline showing out-of-the-box results and a second baseline applyi..."
          ],
          [
           "Also, from the comments we had about the previous blog post, we wanted to change the way we present ..."
          ],
          [
           "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "</figure>\n<br>\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"r..."
          ],
          [
           "The global trend highlights the positive impact of the number of cores on the observed latencies. \nI..."
          ],
          [
           "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "</figure>\n<br>\n<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"r..."
          ],
          [
           "This is often referred to as â€œtracingâ€ the graph and, as you can see here, the results are not that ..."
          ],
          [
           "The default memory allocator strategies often rely on global memory pools which require the usage of..."
          ],
          [
           "#### Memory allocator benchmarks\n\nAgain, we first compare performance against frameworks executing i..."
          ],
          [
           "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "<figcaption>Figure 15. Google's TensorFlow with oneDNN enabled memory allocator and cores scaling la..."
          ],
          [
           "As per the graph above, you can notice that the standard library allocator (glibc) is often behind p..."
          ],
          [
           "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "<figcaption>Figure 19. Google's TensorFlow with oneDNN enabled memory allocator and cores scaling la..."
          ],
          [
           "This time, by knowing the underlying structure of the operator flows and matrix shapes involved then..."
          ],
          [
           "OpenMP exposes [many environment variables](https://www.openmp.org/spec-html/5.0/openmpch6.html) to ..."
          ],
          [
           "As stated above, tuning OpenMP is something you can start to tweak when you tried all the other, sys..."
          ],
          [
           "Fortunately, Intel's [SigOpt](https://sigopt.com/), through Bayesian optimization, allows us to make..."
          ],
          [
           "<table class=\"block mx-auto\">\n  <tr>\n    <td>\n        <figure class=\"image table text-center m-0 w-f..."
          ],
          [
           "<table>\n  <tr>\n    <td>\n        <figure class=\"image table text-center m-0 w-full\">\n            <med..."
          ],
          [
           "As expected, the number of cores is, by far, the most important parameter, but the others play a par..."
          ],
          [
           "## Conclusion - Accelerating Transformers for Production\n\nIn this post, we showed how the new Intel ..."
          ],
          [
           "--\ntitle: \"Supercharged Searching on the ðŸ¤— Hub\"\nthumbnail: /blog/assets/48_hubsearch/thumbnail.png\na..."
          ],
          [
           "```\n\n## Situating the Problem:\n\nFirst, let's imagine the scenario you are in. You'd like to find all..."
          ],
          [
           "```\n\n    Available Attributes or Keys:\n     * author\n     * dataset\n     * language\n     * library\n ..."
          ],
          [
           "```\n```\n    ModelInfo: {\n        modelId: Jiva/xlm-roberta-large-it-mnli\n        sha: c6e64469ec4aa1..."
          ],
          [
           "```\n\n## Taking it up a Notch\n\nWe saw how we could use the `ModelSearchArguments` and `DatasetSearchA..."
          ],
          [
           "```\n```\n    [ModelInfo: {\n     \tmodelId: Jiva/xlm-roberta-large-it-mnli\n     \tsha: c6e64469ec4aa17fe..."
          ],
          [
           "```\n\n\nVery quickly we see that it's a much more coordinated approach for searching through the API, ..."
          ],
          [
           "```\n\n\n```python\n>>> # As an attribute\n>>> ad.3_c\n```\n     File \"<ipython-input-6-c0fe109cf75d>\", lin..."
          ],
          [
           "--\ntitle: \"Optimization story: Bloom inference\"\nthumbnail: /blog/assets/bloom-inference-pytorch-scri..."
          ],
          [
           "# Porting to transformers\n\nBecause of the original training code, we set out to do something which w..."
          ],
          [
           "The first model (small-testing) is in `bfloat16` like the big bloom so \neverything should be very si..."
          ],
          [
           "```\nNote: Pipeline Parallelism (PP) means in this context that each GPU will own\nsome layers so each..."
          ],
          [
           "```\n\nNow we have a workable `transformers` clean version of the start\nworking on running this.\n\nBloo..."
          ],
          [
           "The number of users we can serve at the same time (throughput)\nHow long does it take for an average ..."
          ],
          [
           "```\n**Note: This is not the best nor the only load testing we used, but it was\nalways the first to b..."
          ],
          [
           "Let's do the math and we are getting `17 TFlop` for a single forward pass.\nLooking at the [specs](ht..."
          ],
          [
           "```\nNote: Tensor Parallelism (TP) means in this context that each GPU will own\npart of the weights, ..."
          ],
          [
           "```\n\nNow that we have a good understanding of where we stand it's time to get to work.\n\nWe tried man..."
          ],
          [
           "Results:\n\n  - Porting was not an easy task as some conditions and kernels were hard to\n    reproduce..."
          ],
          [
           "## Using ONNX/TRT or other compiled approaches\n  - They are supposed to handle most of the optimizat..."
          ],
          [
           "## DeepSpeed\n  - This is the technology that powered training, it seemed only fair to use\n    it for..."
          ],
          [
           "## Webserver ideas\n  - Given that we are going to run a free server where users are going to \n    se..."
          ],
          [
           "- Next chapter.\n\n\n\n# Final route: PyTorch + TP + 1 custom kernel + torch.jit.script\n\n## Writing more..."
          ],
          [
           "<img src=\"assets/bloom-inference-optimization/profiler.png\">\nWe see many  `cat` operations before `b..."
          ],
          [
           "## Low-hanging fruits\n\nNow that we had a TP implementation, we could start profiling and optimizing ..."
          ],
          [
           "```\n\nto \n\n```python\n@torch.jit.script\ndef bloom_gelu_forward(x):\n    return x * 0.5 * (1.0 + torch.t..."
          ],
          [
           "```\n\nThis transforms the operations from multiple small element-wise kernels (and hence tensor copie..."
          ],
          [
           "Another place where we had to be extra careful, was the initial forward pass (without\npast) and the ..."
          ],
          [
           "```\n\nThe first masked fill is creating a new tensor, which is here only to \nsay to the softmax opera..."
          ],
          [
           "Then we had to drop the use [generate](https://huggingface.co/docs/transformers/v4.22.2/en/main_clas..."
          ],
          [
           "## Have you tried ...?\n\nStuff we know exists and haven't used because of various reasons. It \ncould ..."
          ],
          [
           "## [OpenAI Triton](https://openai.com/blog/triton/)\n\n[Triton](https://github.com/openai/triton) is a..."
          ],
          [
           "# Acknowledgments\n\nAll this work results of the collaboration of many HF team members. In no particu..."
          ],
          [
           "--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nau..."
          ],
          [
           "<div><a class=\"text-xs block mb-3 text-gray-300\" href=\"/spacy/en_core_web_sm\"><code>spacy/en_core_we..."
          ],
          [
           "<div class=\"SVELTE_HYDRATER \" data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingfac..."
          ],
          [
           "Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.8379609817}]}},{&quot;tasks&quo..."
          ],
          [
           "Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.893607046}]}},{&quot;tasks&quot..."
          ],
          [
           "sks&quot;:{&quot;name&quot;:&quot;UNLABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classifi..."
          ],
          [
           "classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&q..."
          ],
          [
           "quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9185392711}]}},{&quot;tasks&quot;:{&quot;name&quot;:&..."
          ],
          [
           "&quot;:&quot;LABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metr..."
          ],
          [
           "uot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quo..."
          ],
          [
           "ot;,&quot;value&quot;:0.9185392711}]}}]}]},&quot;cardSource&quot;:true,&quot;id&quot;:&quot;spacy/en..."
          ],
          [
           "spacy/en_core_web_sm&quot;,&quot;pipeline_tag&quot;:&quot;token-classification&quot;,&quot;library_n..."
          ],
          [
           "ibrary_name&quot;:&quot;spacy&quot;,&quot;modelId&quot;:&quot;spacy/en_core_web_sm&quot;,&quot;priva..."
          ],
          [
           "ot;private&quot;:false,&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattributes&quot;},{&qu..."
          ],
          [
           "t;},{&quot;rfilename&quot;:&quot;LICENSE&quot;},{&quot;rfilename&quot;:&quot;LICENSES_SOURCES&quot;}..."
          ],
          [
           "S&quot;},{&quot;rfilename&quot;:&quot;README.md&quot;},{&quot;rfilename&quot;:&quot;accuracy.json&qu..."
          ],
          [
           ".json&quot;},{&quot;rfilename&quot;:&quot;config.cfg&quot;},{&quot;rfilename&quot;:&quot;en_core_web..."
          ],
          [
           "core_web_sm-any-py3-none-any.whl&quot;},{&quot;rfilename&quot;:&quot;meta.json&quot;},{&quot;rfilena..."
          ],
          [
           ";rfilename&quot;:&quot;tokenizer&quot;},{&quot;rfilename&quot;:&quot;attribute_ruler/patterns&quot;}..."
          ],
          [
           "s&quot;},{&quot;rfilename&quot;:&quot;lemmatizer/lookups/lookups.bin&quot;},{&quot;rfilename&quot;:&..."
          ],
          [
           "&quot;:&quot;ner/cfg&quot;},{&quot;rfilename&quot;:&quot;ner/model&quot;},{&quot;rfilename&quot;:&qu..."
          ],
          [
           "uot;:&quot;ner/moves&quot;},{&quot;rfilename&quot;:&quot;vocab/lookups.bin&quot;},{&quot;rfilename&q..."
          ],
          [
           "lename&quot;:&quot;vocab/strings.json&quot;},{&quot;rfilename&quot;:&quot;vocab/vectors&quot;}],&quo..."
          ],
          [
           ";}],&quot;tags&quot;:[&quot;en&quot;,&quot;spacy&quot;,&quot;token-classification&quot;,&quot;licens..."
          ],
          [
           "t;license:mit&quot;,&quot;model-index&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;token-class..."
          ],
          [
           "en-classification&quot;,&quot;label&quot;:&quot;Token..."
          ],
          [
           "Classification&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;spacy&quot;,&q..."
          ],
          [
           "name is Clara and I live in Berkeley, California.&quot;}]},&quot;shouldUpdateUrl&quot;:true}\" data-t..."
          ],
          [
           "0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\" fill=\"currentColor\"></path></svg></a></div> <di..."
          ],
          [
           "d=\"M7.8125 3.66254H8.9V4.75004H7.8125V3.66254Z\"></path><path d=\"M8.90001 12.3625H6.72501V9.09998C6.7..."
          ],
          [
           "6.43667 16.7122 6.72501 16.7125H8.90001C9.18834 16.7122 9.46478 16.5975 9.66867 16.3936C9.87255 16.1..."
          ],
          [
           "12.4813 12.6814C12.6852 12.4775 12.9617 12.3628 13.25 12.3625H15.425C15.7133 12.3628 15.9898 12.4775..."
          ],
          [
           "6.15636C9.01496 6.36024 8.9003 6.63668 8.90001 6.92502V8.01252C8.9003 8.30085 9.01496 8.5773 9.21885..."
          ],
          [
           "1.48752ZM9.98751 8.01252V6.92502H11.075V8.01252H9.98751ZM12.1625 5.83752V2.57502H15.425V5.83752H12.1..."
          ],
          [
           "5.3148 5.31867 5.51868C5.11478 5.72256 4.83834 5.83723 4.55001 5.83752V5.83752ZM2.37501 2.57502V4.75..."
          ],
          [
           "### Using existing models\n\nAll models from the Hub can be directly installed using `pip install`. \n\n..."
          ],
          [
           "```\n\n```python\n# Using spacy.load().\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Importing as..."
          ],
          [
           "```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browse..."
          ],
          [
           "--\ntitle: \"Porting fairseq wmt19 translation system to transformers\"\nthumbnail: /blog/assets/07_port..."
          ],
          [
           "Also, as I did the initial porting with the `en-ru`/`ru-en` models, I was totally unaware that the `..."
          ],
          [
           "```\nmkdir ~/porting\ncd ~/porting\n```\n\nWe need to install a few things for this work:\n\n```\n# install ..."
          ],
          [
           "```\n\n## Files\n\nAs a quick overview, the following files needed to be created and written:..."
          ],
          [
           "* [`src/transformers/configuration_fsmt.py`](https://github.com/huggingface/transformers/blob/129fda..."
          ],
          [
           "* [`tests/test_tokenization_fsmt.py`](https://github.com/huggingface/transformers/blob/129fdae04033f..."
          ],
          [
           "There are other files that needed to be modified as well, we will talk about those towards the end.\n..."
          ],
          [
           "```\nimport torch\ntorch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-ru', checkpoint_file='model..."
          ],
          [
           "```\nYou may have more than one entry there if you have been using the `hub` for other models.\n\nLet's..."
          ],
          [
           "```\nwe have:\n1. `model*.pt` - 4 checkpoints (pytorch `state_dict` with all the pre-trained weights, ..."
          ],
          [
           "```\n\nIf we combine the first two and the last two steps we get 3 stages:\n\n1. **Encode input**: break..."
          ],
          [
           "Let's see how this approach helps to reduce memory and computation requirements. If we have an input..."
          ],
          [
           "### fairseq's tokenizer workings\n\nLet's understand how `fairseq`'s tokenizer works.\n\n`fairseq` (*) u..."
          ],
          [
           "```\nimport torch\nsentence = \"Machine Learning is great\"\ncheckpoint_file='model4.pt'\nmodel = torch.hu..."
          ],
          [
           "```\n\nYou can see that `model.encode` does `tokenize+apply_bpe+binarize` - as we get the same output...."
          ],
          [
           "```\ne n</w> 1423551864\ne r</w> 1142368899\nth e</w> 432025210\n```\nIf the second column doesn't includ..."
          ],
          [
           "```\n$ grep -i ^mach  ~/porting/pytorch_fairseq_model/bpecodes\nmach ine</w> 463985\nMach t 376252\nMach..."
          ],
          [
           "```\n('apply_bpe: ', 'Mach@@ ine Lear@@ ning is great')\n('binarize: ', 7, tensor([10217,  1419,     3..."
          ],
          [
           "```\nfrom fairseq.data.dictionary import Dictionary\ndef rewrite_dict_keys(d):\n    # (1) remove word b..."
          ],
          [
           "```\n\nAfter running the conversion script, let's check the converted dictionary:\n\n```\n$ grep '\"Mach\"'..."
          ],
          [
           "```\n['Mach', 'ine</w>', 'Lear', 'ning</w>', 'is</w>', 'great</w>']\n```\nInstead of marking chunks tha..."
          ],
          [
           "```\nand with very few changes I had a working encoder part of the tokenizer. There was a lot of code..."
          ],
          [
           "```\n\nIf you're following along, and would like to see all the changes I did to the original `tokeniz..."
          ],
          [
           "```\nJust make sure you're checking out the repository [around the time fsmt was released](https://gi..."
          ],
          [
           "```\n\nThis was my starting point that I needed to tweak to work with the model weights provided by `f..."
          ],
          [
           "```\nFirst I looked at the model:\n```\nprint(ru2en[\"models\"][0])\n```\n```\nTransformerModel(\n  (encoder)..."
          ],
          [
           "```\nwhich looked very similar to BART's architecture, with some slight differences in a few layers -..."
          ],
          [
           "```\nargs = dict(vars(ru2en[\"args\"]))\npprint(args)\n```\n```\n 'activation_dropout': 0.0,\n 'activation_f..."
          ],
          [
           "```\n    model_conf = {\n        \"architectures\": [\"FSMTForConditionalGeneration\"],\n        \"model_typ..."
          ],
          [
           "```\nAll that remains is to save the configuration into `config.json` and create a new `state_dict` d..."
          ],
          [
           "```\n\nWe have the configuration and the model's `state_dict` ported - yay!\n\nYou will find the final c..."
          ],
          [
           "![break point group](./assets/07_porting_fsmt/pycharm-break-point-groups.png)\n\nNow that I have used ..."
          ],
          [
           "I first did this process for the simpler no-beam search, and once the outputs were 100% matching I r..."
          ],
          [
           "```\n     def convert_tokens_to_string(self, tokens):\n         \"\"\" Converts a sequence of tokens (str..."
          ],
          [
           "```\nperl -le 'for $f (@ARGV) { print qq[transformers-cli upload -y $_/$f --filename $_/$f] \\\nfor map..."
          ],
          [
           "```\ntokenizer = FSMTTokenizer.from_pretrained(\"/code/huggingface/transformers-fair-wmt/data/wmt19-en..."
          ],
          [
           "```\n\nThen the are the pipelines, which completely hide all the NLP complexities from the end user an..."
          ],
          [
           "```\n$ egrep -l \"(BartConfig|BartForConditionalGeneration|BartTokenizer)\" src/transformers/*.py \\\n| e..."
          ],
          [
           "```\nIn the `grep` search I excluded the files that also include those classes.\n\n\n## Manual testing\n\n..."
          ],
          [
           "## Porting other models\n\nI next proceeded to port the `en-de` and `de-en` models. \n\nI was surprised ..."
          ],
          [
           "Just like with the code, I started by copying `tests/test_modeling_bart.py` and converting it to use..."
          ],
          [
           "## SinusoidalPositionalEmbedding\n\n`fairseq` used a slightly different implementation of `SinusoidalP..."
          ],
          [
           "```\nexport PAIR=ru-en\nexport MODEL=facebook/wmt19-$PAIR\nexport DATA_DIR=data/$PAIR\nexport SAVE_DIR=d..."
          ],
          [
           "```\nYou can see that the BLEU score was `39.0498` and that it evaluated using 2000 test inputs, prov..."
          ],
          [
           "## Porting new models\n\nAfter uploading the 4 `fairseq` models [here](https://huggingface.co/models?f..."
          ],
          [
           "```\n# search space\nexport PAIR=ru-en\nexport DATA_DIR=data/$PAIR\nexport SAVE_DIR=data/$PAIR\nexport BS..."
          ],
          [
           "```\nYou can see that in the case of `transformers` `early_stopping=False` performs better (`fairseq`..."
          ],
          [
           "```\n---\nlanguage: \n- en\n- ru\nthumbnail:\ntags:\n- translation\n- wmt19\n- facebook\nlicense: apache-2.0\nd..."
          ],
          [
           "```\nmake docs\n```\nto test that the newly added document was building correctly. The file I needed to..."
          ],
          [
           "```\n\nThen I went to github and submitted this [PR](https://github.com/huggingface/transformers/pull/..."
          ],
          [
           "## Appreciations\n\n- Having [Sam Shleifer](https://github.com/sshleifer) mentor me through this proce..."
          ],
          [
           "```\nc = get_config()\n# Run all nodes interactively\nc.InteractiveShell.ast_node_interactivity = \"all\"..."
          ],
          [
           "--\ntitle: \"Understanding BigBird's Block Sparse Attention\"\nthumbnail: /blog/assets/18_big_bird/attn...."
          ],
          [
           "**BigBird RoBERTa-like** model is now available in ðŸ¤—Transformers. The goal of this post is to give t..."
          ],
          [
           "---\n\nIn this blog post, we will try to answer those questions.\n\n### What tokens should be attended t..."
          ],
          [
           "```\n\nNearby tokens should be important because, in a sentence (sequence of words), the current word ..."
          ],
          [
           "```\n\n* **Random tokens:** Select some tokens randomly which will transfer information by transferrin..."
          ],
          [
           "```\n\nThis way, the query token attends only to a subset of all possible tokens while yielding a good..."
          ],
          [
           "![](assets/18_big_bird/graph.gif)\n<img src=\"assets/18_big_bird/full.png\" width=230 height=230>\n\n**Bi..."
          ],
          [
           "In case, we have many global tokens, then we may not need random connections since there will be mul..."
          ],
          [
           "## BigBird block sparse attention\n\nBigBird block sparse attention is just an efficient implementatio..."
          ],
          [
           "```python\n# pseudo code\n\nQ -> Query martix (seq_length, head_dim)\nK -> Key matrix (seq_length, head_..."
          ],
          [
           "```\n\n### Sliding Attention\n\nThe sequence of key tokens is copied 2 times with each element shifted t..."
          ],
          [
           "```\n\n### Random Attention\n\nRandom attention is ensuring that each query token will attend a few rand..."
          ],
          [
           "```\n\n**Note:** The current implementation further divides sequence into blocks & each notation is de..."
          ],
          [
           "![BigBird block sparse attention](assets/18_big_bird/q1.png)\n\\\\(q_1\\\\) represents 1st block, \\\\(g_i\\..."
          ],
          [
           "![BigBird block sparse attention](assets/18_big_bird/q_middle.png)\n\n---\n\nFor calculating attention s..."
          ],
          [
           "Now, we have covered the hardest part of block sparse attention, i.e. its implementation. Hopefully,..."
          ],
          [
           "When seqlen = 4096\n=> time complexity in BERT = (8 x 512)^2\n=> **time complexity in BERT = 64 x 512^..."
          ],
          [
           "```\n\n</details>\n\n## ITC vs ETC\n\nThe BigBird model can be trained using 2 different strategies: **ITC..."
          ],
          [
           "|                                              | ITC                                   | ETC        ..."
          ],
          [
           "## Using BigBird with ðŸ¤—Transformers\n\nYou can use `BigBirdModel` just like any other ðŸ¤— model. Let's s..."
          ],
          [
           "```\n\nThere are total **3 checkpoints** available in **ðŸ¤—Hub** (at the point of writing this article):..."
          ],
          [
           "# very minimal training loop\nfor e in range(epochs):\n    for batch in dataset:\n        model.train()..."
          ],
          [
           "```\n\nIt's important to keep the following points in mind while working with big bird:\n\n* Sequence le..."
          ],
          [
           "You will soon find **BigBird Pegasus-like** model in the library for **long document summarization**..."
          ],
          [
           "--\ntitle: \"Train your first Decision Transformer\"\nthumbnail: /blog/assets/101_train-decision-transfo..."
          ],
          [
           "## What are Decision Transformers?\n\nThe Decision Transformer model was introduced byÂ **[â€œDecision Tr..."
          ],
          [
           "*Decision Transformer architecture. States, actions, and returns are fed into modality-specific line..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n    <video \n        alt=\"CheetahEd-expert\"\n     ..."
          ],
          [
           "```\n\nWhile most datasets on the hub are ready to use out of the box, sometimes we wish to perform so..."
          ],
          [
           "def __init__(self, dataset) -> None:\n        self.act_dim = len(dataset[0][\"actions\"][0])\n        se..."
          ],
          [
           "# get sequences from dataset\n            s.append(np.array(feature[\"observations\"][si : si + self.ma..."
          ],
          [
           "# padding and state + reward normalization\n            tlen = s[-1].shape[1]\n            s[-1] = np...."
          ],
          [
           "s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n        a = torch.from_numpy(np.concatenate(..."
          ],
          [
           "```\n\nThat was a lot of code, the TLDR is that we defined a class that takes our dataset, performs th..."
          ],
          [
           "```\n\nThe transformers Trainer class required a number of arguments, defined in the TrainingArguments..."
          ],
          [
           "```\n\nNow that we explained the theory behind Decision Transformer, the Trainer, and how to train it...."
          ],
          [
           "--\ntitle: Goodbye cold boot - how we made LoRA Inference 300% faster\nthumbnail: /blog/assets/171_loa..."
          ],
          [
           "## LoRA\n\nLoRA is a fine-tuning technique that belongs to the family of \"parameter-efficient\" (PEFT) ..."
          ],
          [
           "If you look, for example, inside the [Stable Diffusion XL Base 1.0 model repo](https://huggingface.c..."
          ],
          [
           "If you were requesting a LoRA that was not so popular, even if it was based on the SDXL model like t..."
          ],
          [
           "### LoRA structure\n\nIn the Hub, LoRAs can be identified with two attributes:\n\n![Hub](https://hugging..."
          ],
          [
           "We provide an example below on how one can leverage the Diffusers library to quickly load several Lo..."
          ],
          [
           "print(f\"LoRA adapter loaded and fused to main model, elapsed {elapsed:.2f} seconds\")\n\n    start = ti..."
          ],
          [
           "```\n\n## Loading figures\n\nAll numbers below are in seconds:\n\n<table>\n  <tr>\n    <th>GPU</th>\n    <td>..."
          ],
          [
           "To serve inference requests, we use [this open source community image](https://github.com/huggingfac..."
          ],
          [
           "```\n$ git clone https://github.com/huggingface/api-inference-community.git\n\n$ cd api-inference-commu..."
          ],
          [
           "```\n\n### What about batching ?\n\nRecently a really interesting [paper](https://arxiv.org/abs/2311.032..."
          ],
          [
           "--\ntitle: \"Open-Source Text Generation & LLM Ecosystem at Hugging Face\"\nthumbnail: /blog/assets/os_l..."
          ],
          [
           "![Causal LM Output](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bl..."
          ],
          [
           "On the Hugging Face Hub, you can find both causal language models and causal language models fine-tu..."
          ],
          [
           "The second type of text generation model is commonly referred to as the text-to-text generation mode..."
          ],
          [
           "### Models created with love by Hugging Face with BigScience and BigCode ðŸ’—\n\nHugging Face has co-led ..."
          ],
          [
           "- [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)\n- [XGen](https://huggingface.co/tiiuae/falc..."
          ],
          [
           "There are two code generation models, [StarCoder by BigCode](https://huggingface.co/models?sort=tren..."
          ],
          [
           "- Another popular family of models is OpenAssistant, some of which are built on Meta's Llama model u..."
          ],
          [
           "If you're looking to fine-tune a model on an existing instruction dataset, you need to know how a da..."
          ],
          [
           "| Model                                                                                    | Dataset..."
          ],
          [
           "| [MPT-30B](https://huggingface.co/mosaicml/mpt-30b)                                       | Mix of ..."
          ],
          [
           "| [FLAN-T5-XXL](https://huggingface.co/google/flan-t5-xxl)                                 | [gsm8k]..."
          ],
          [
           "| [Dolly v2](https://huggingface.co/databricks/dolly-v2-12b)                               | [Dolly]..."
          ],
          [
           "### Text Generation Inference\n\nResponse time and latency for concurrent users are a big challenge fo..."
          ],
          [
           "![HuggingChat Search](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/..."
          ],
          [
           "## Parameter Efficient Fine Tuning (PEFT)\n\nIf youâ€™d like to fine-tune one of the existing large mode..."
          ],
          [
           "--\ntitle: \"Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action\" \nthumbnail: /blog/ass..."
          ],
          [
           "1. **Define a model**: Before you can use BentoML, you need a machine learning model (or multiple mo..."
          ],
          [
           "## Table of contents\n\n- [A brief introduction to DeepFloyd IF](#a-brief-introduction-to-deepfloyd-if..."
          ],
          [
           "## Preparing the environment\n\n[This GitHub repository](https://github.com/bentoml/IF-multi-GPUs-demo..."
          ],
          [
           "```\n\nBefore building the application, letâ€™s briefly explore the key files within this directory:\n\n- ..."
          ],
          [
           "```\n\nOnce the downloads are complete, view the models in the Model store.\n\n```bash\n$ bentoml models ..."
          ],
          [
           "```\n\n## Testing the server\n\nOnce the server starts, you can visit the web UI at http://localhost:786..."
          ],
          [
           "```\n\nView the Bento in the local Bento Store.\n\n```bash\n$ bentoml list\n\nTag                          ..."
          ],
          [
           "```\n\nYou can then deploy the model on Kubernetes.\n\n## Whatâ€™s next?\n\n[BentoML](https://github.com/ben..."
          ],
          [
           "--\ntitle: \"Deep Q-Learning with Space Invaders\"\nthumbnail: /blog/assets/78_deep_rl_dqn/thumbnail.gif..."
          ],
          [
           "We got excellent results with this simple algorithm. But these environments were relatively simple b..."
          ],
          [
           "To be able to understand this unit, **you need to understand [Q-Learning](https://huggingface.co/blo..."
          ],
          [
           "The problem is that Q-Learning is aÂ *tabular method*. Aka, a problem in which the state and actions ..."
          ],
          [
           "<img src=\"assets/63_deep_rl_intro/deep.jpg\" alt=\"Deep Q Learning\"/>\n\n\nNow that we understand Deep Q-..."
          ],
          [
           "Why do we stack four frames together?\nWe stack frames together because it helps us **handle the prob..."
          ],
          [
           "<img src=\"https://huggingface.co/blog/assets/73_deep_rl_q_part2/q-ex-5.jpg\" alt=\"Q Loss\"/>\n\nIn Deep ..."
          ],
          [
           "### Experience Replay to make more efficient use of experiences\n\nWhy do we create a replay memory?\n\n..."
          ],
          [
           "### Fixed Q-Target to stabilize the training\n\nWhen we want to calculate the TD error (aka the loss),..."
          ],
          [
           "At each time step, youâ€™re trying to approach the cow, which also moves at each time step (because yo..."
          ],
          [
           "The solution is: when we compute the Q target, we use two networks to decouple the action selection ..."
          ],
          [
           "The leaderboard to compare your results with your classmates ðŸ† ðŸ‘‰ https://huggingface.co/spaces/chris..."
          ],
          [
           "--\ntitle: \"Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore\"\nthumbnail: /blog/assets..."
          ],
          [
           "<p>In 2017 a group of Google AI researchers published a paper introducing the transformer model arch..."
          ],
          [
           "<p><img src=\"https://www.graphcore.ai/hs-fs/hubfs/transformers_chrono.png?width=1024&amp;name=transf..."
          ],
          [
           "<p>A timeline showing releases of prominent transformer language models (credit: Hugging Face)</p>\n<..."
          ],
          [
           "<p><img src=\"https://www.graphcore.ai/hs-fs/hubfs/vit%20diag.png?width=1024&amp;name=vit%20diag.png\"..."
          ],
          [
           "<p>An overview of the ViT model structure as introduced in <a href=\"https://arxiv.org/abs/2010.11929..."
          ],
          [
           "<h2>ViT models â€“ a perfect fit for IPU</h2>\n<p>Graphcore IPUs are particularly well-suited to ViT mo..."
          ],
          [
           "<p>For this blog post, we will use a ViT model pre-trained on ImageNet-21k, based on the paper <a hr..."
          ],
          [
           "<li>The complexity of multi-class and multi-label problems such as pulmonary diagnosis is exponentia..."
          ],
          [
           "<p>If this is your first time using IPUs, read the <a href=\"https://docs.graphcore.ai/projects/ipu-p..."
          ],
          [
           "<p><img src=\"https://www.graphcore.ai/hs-fs/hubfs/chest%20x-ray%20examples.png?width=700&amp;name=ch..."
          ],
          [
           "<div class=\"blog-caption\" style=\"max-height: 100%; max-width: 90%; margin-left: auto; margin-right: ..."
          ],
          [
           "</ul>\n<p>The Graphcore Tutorials repository contains the step-by-step tutorial notebook and Python s..."
          ],
          [
           "<p style=\"font-weight: bold;\">Weâ€™ve even made it easier and created the HF Optimum Gradient so you c..."
          ],
          [
           "loading=\"lazy\" style=\"width: 200px; float: left;\" width=\"200\" srcset=\"https://www.graphcore.ai/hs-fs..."
          ],
          [
           "<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2>Getting the dataset</h2>\n<a id=\"getting-the-dataset\" data-hs-anchor=..."
          ],
          [
           "<div style=\"font-size: 14px; line-height: 1.3;\">\n<script src=\"https://gist.github.com/nickmaxfield/2..."
          ],
          [
           "</div>\n<p>We are going to train the Graphcore Optimum ViT model to predict diseases (defined by \"Fin..."
          ],
          [
           "</div>\n<p>When loading data using the <code>datasets.load_dataset</code> function, labels can be pro..."
          ],
          [
           "</div>\n<h2>Creating the dataset</h2>\n<p>We are now ready to create the PyTorch dataset and split it ..."
          ],
          [
           "</div>\n<p>To fine-tune a pre-trained model, the new dataset must have the same properties as the ori..."
          ],
          [
           "</div>\n<h2>Visualising the dataset</h2>\n<p>To examine the dataset, we display the first 10 rows of m..."
          ],
          [
           "<p><img src=\"https://www.graphcore.ai/hs-fs/hubfs/x-ray%20images%20transformed.jpg?width=1024&amp;na..."
          ],
          [
           "2048w, https://www.graphcore.ai/hs-fs/hubfs/x-ray%20images%20transformed.jpg?width=2560&amp;name=x-r..."
          ],
          [
           "<div class=\"blog-caption\" style=\"max-height: 100%; max-width: 90%; margin-left: auto; margin-right: ..."
          ],
          [
           "</div>\n<p>To use this model on the IPU we need to load the IPU configuration, <code>IPUConfig</code>..."
          ],
          [
           "</div>\n<h2>Implementing a custom performance metric for evaluation</h2>\n<p>The performance of multi-..."
          ],
          [
           "</div>\n<p>To train the model, we define a trainer using the <code>IPUTrainer</code> class which take..."
          ],
          [
           "<p>Now we are ready to train.</p>\n<div style=\"font-size: 14px; line-height: 1.3;\">\n<script src=\"http..."
          ],
          [
           "</div>\n<p><img src=\"https://www.graphcore.ai/hs-fs/hubfs/vit%20output.png?width=1024&amp;name=vit%20..."
          ],
          [
           "<h2>Running the evaluation</h2>\n<p>Now that we have trained the model, we can evaluate its ability t..."
          ],
          [
           "<p>In this post, we have introduced ViT models and have provided a tutorial for training a Hugging F..."
          ],
          [
           "<p><a href=\"https://console.paperspace.com/github/gradient-ai/Graphcore-HuggingFace?machine=Free-IPU..."
          ],
          [
           "300w, https://www.graphcore.ai/hs-fs/hubfs/gradient-badge-gradient-05-d-05.png?width=400&amp;name=gr..."
          ],
          [
           "<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>If youâ€™re interested in trying Hugging Face Optimum with IPUs on Pape..."
          ],
          [
           "</div>\n   </article>..."
          ],
          [
           "--\ntitle: \"NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m met..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"..."
          ],
          [
           "As shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices. The reas..."
          ],
          [
           "## How can we adapt the NystrÃ¶m method to approximate self-attention?\n\nInstead of sampling from \\\\(S..."
          ],
          [
           "This is the NystrÃ¶m approximation of the softmax matrix in the self-attention mechanism. We multiply..."
          ],
          [
           "## How is NystrÃ¶mformer implemented?\n\nThe original implementation of NystrÃ¶mformer can be found [her..."
          ],
          [
           "attention_scores = torch.matmul(q_landmarks, key_layer.transpose(-1, -2)) # \\tilde{B} before softmax..."
          ],
          [
           "```\n\n\n## Using NystrÃ¶mformer with HuggingFace\n\nNystrÃ¶mformer for Masked Language Modeling (MLM) is a..."
          ],
          [
           "```\n\n<div class=\"output stream stdout\">\n\n    Output:\n    -------------------------------------------..."
          ],
          [
           "```\n\n<div class=\"output stream stdout\">\n\n    Output:\n    -------------------------------------------..."
          ],
          [
           "--\ntitle: \"Llama 2 is here - get it on Hugging Face\" \nthumbnail: /blog/assets/llama2/thumbnail.jpg\na..."
          ],
          [
           "## Why Llama 2?\n\nThe Llama 2 release introduces a family of pretrained and fine-tuned LLMs, ranging ..."
          ],
          [
           "If youâ€™ve been waiting for an open alternative to closed-source chatbots, Llama 2-Chat is likely you..."
          ],
          [
           "| Model | License | Commercial use? | Pretraining length [tokens] | Leaderboard score |\n| --- | --- ..."
          ],
          [
           "*weâ€™re currently running evaluation of the Llama 2 70B (non chatty version). This table will be upda..."
          ],
          [
           "- training and inference scripts and examples\n- safe file format (`safetensors`)\n- integrations with..."
          ],
          [
           "```\npip install transformers\nhuggingface-cli login\n```\n\nIn the following code snippet, we show how t..."
          ],
          [
           "```\n\nAnd although the model has *only* 4k tokens of context, you can use techniques supported in `tr..."
          ],
          [
           "_Note: You might need to request a quota upgrade via email to **[api-enterprise@huggingface.co](mail..."
          ],
          [
           "First pip install `trl` and clone the script:\n```bash\npip install trl\ngit clone https://github.com/l..."
          ],
          [
           "```\n\nThen you can run the script:\n```bash\npython trl/examples/scripts/sft_trainer.py \\\n    --model_n..."
          ],
          [
           "```\n<s>[INST] <<SYS>>\n{{ system_prompt }}\n<</SYS>>\n\n{{ user_message }} [/INST]\n```\n\nThis template fo..."
          ],
          [
           "```\n\nAs you can see, the instructions between the special `<<SYS>>` tokens provide context for the m..."
          ],
          [
           "```\n\nThe model is stateless and does not \"remember\" previous fragments of the conversation, we must ..."
          ],
          [
           "## Additional Resources\n\n- [Paper Page](https://huggingface.co/papers/2307.09288)\n- [Models on the H..."
          ],
          [
           "--\ntitle: \"Introducing The World's Largest Open Multilingual Language Model: BLOOM\"\nthumbnail: /blog..."
          ],
          [
           "Researchers can [now download, run and study BLOOM](https://huggingface.co/bigscience/bloom) to inve..."
          ],
          [
           "--\ntitle: \"Optimum-NVIDIA Unlocking blazingly fast LLM inference in just 1 line of code\" \nthumbnail:..."
          ],
          [
           "```diff\n- from transformers.pipelines import pipeline\n+ from optimum.nvidia.pipelines import pipelin..."
          ],
          [
           "```\nYou can also enable FP8 quantization with a single flag, which allows you to run a bigger model ..."
          ],
          [
           "```\n\nFor more details, check out our [documentation](https://github.com/huggingface/optimum-nvidia)\n..."
          ],
          [
           "### Next steps\n\nOptimum-NVIDIA currently provides peak performance for the LLaMAForCausalLM architec..."
          ],
          [
           "--\ntitle: \"Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\" \nthumbn..."
          ],
          [
           "> We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a ..."
          ],
          [
           "## Resources\n\nThis blogpost and release come with several resources to get started with 4bit models ..."
          ],
          [
           "For more information we recommend reading the fundamentals of floating point representation through ..."
          ],
          [
           "| ![fp8_scheme](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/b..."
          ],
          [
           "`-1 * 2^(2) * (1 + 2^-1) = -1 * 4 * 1.5 = -6`\n\nFor FP4 there is no fixed format and as such one can ..."
          ],
          [
           "QLoRA tuning is shown to match 16-bit finetuning methods in a wide range of experiments. In addition..."
          ],
          [
           "```\n\n### Quickstart\n\nThe basic way to load a model in 4bit is to pass the argument `load_in_4bit=Tru..."
          ],
          [
           "```\nThat's all you need!\n\nAs a general rule, we recommend users to not manually set a device once th..."
          ],
          [
           "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)..."
          ],
          [
           "```\n\n#### Changing the compute dtype\n\nAs mentioned above, you can also change the compute dtype of t..."
          ],
          [
           "```\n\nAnd of course, as mentioned in the beginning of the section, all of these components are compos..."
          ],
          [
           "For text models, at this time of writing, this would include most used architectures such as Llama, ..."
          ],
          [
           "```\nNote that if your favorite model is not there, you can open a Pull Request or raise an issue in ..."
          ],
          [
           "We have also made some benchmarks on the impact of this quantization method on training large models..."
          ],
          [
           "| Model name                          | Half precision model size (in GB) | Hardware type / total VR..."
          ],
          [
           "| decapoda-research/llama-7b-hf       | 14GB                              | 1xNVIDIA-T4 / 16GB      ..."
          ],
          [
           "| decapoda-research/llama-13b-hf      | 27GB                              | 1xNVIDIA-T4 / 16GB      ..."
          ],
          [
           "We have used the recent `SFTTrainer` from TRL library, and the benchmarking script can be found [her..."
          ],
          [
           "--\ntitle: \"Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models\"\nthumbnail: /..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/optimum_onnxruntime-training/..."
          ],
          [
           "```\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFa..."
          ],
          [
           "```\n\n## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democr..."
          ],
          [
           "## ONNX Runtime Training\n\n[ONNX Runtime](https://onnxruntime.ai/) accelerates [large model training]..."
          ],
          [
           "## ONNX Runtime Training in Optimum\n\nOptimum provides an `ORTTrainer` API that extends the `Trainer`..."
          ],
          [
           "```diff\n-from transformers import Trainer, TrainingArguments\n+from optimum.onnxruntime import ORTTra..."
          ],
          [
           "```\n\n## Looking Forward\n\nThe Hugging Face team is working on open sourcing more large models and low..."
          ],
          [
           "## Getting Started\n\nWe invite you to check out the links below to learn more about, and get started ..."
          ],
          [
           "--\ntitle: \"SafeCoder vs. Closed-source Code Assistants\"\nthumbnail: /blog/assets/safecoder-vs-closed-..."
          ],
          [
           "In this post, we'll compare SafeCoder to closed-source services and highlight the benefits you can e..."
          ],
          [
           "Unfortunately, closed-source code assistant services don't share information about the underlying mo..."
          ],
          [
           "We also shared the [fine-tuning code](https://github.com/bigcode-project/starcoder/) on GitHub.\n \nEv..."
          ],
          [
           "Closed-source services rely on the security of the underlying cloud. Whether this works or not for y..."
          ],
          [
           "--\ntitle: \"Optimizing your LLM in production\"\nthumbnail: /blog/assets/163_optimize_llm/optimize_llm...."
          ],
          [
           "The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, e..."
          ],
          [
           "At the time of writing this post, LLMs consist of at least a couple billion parameters. Each paramet..."
          ],
          [
           "To give some examples of how much VRAM it roughly takes to load a model in bfloat16:\n\n-   **GPT3** r..."
          ],
          [
           "ðŸ¤— Transformers does not support tensor parallelism out of the box as it requires the model architect..."
          ],
          [
           "```\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretr..."
          ],
          [
           "```\n\n**Output**:\n```\nHere is a Python function that transforms bytes to Giga bytes:\\n\\n```python\\nde..."
          ],
          [
           "```\n\n**Output**:\n```bash\n29.0260648727417\n```\n\nClose enough to our back-of-the-envelope computation!..."
          ],
          [
           "```\n\nLet's call it now for the next experiment.\n\n```python\nflush()\n```\nIn the recent version of the ..."
          ],
          [
           "```\n\nNow what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be q..."
          ],
          [
           "are changed to\n\n$$ Y = X * \\text{dequantize}(W); \\text{quantize}(W) $$\n\nfor every matrix multiplicat..."
          ],
          [
           "```\n\nWe can then load models in 8-bit quantization by simply adding a `load_in_8bit=True` flag to `f..."
          ],
          [
           "```\n\n```python\nflush()\n```\n\nLet's see what peak GPU memory consumption 4-bit quantization gives. Qua..."
          ],
          [
           "```\n\n**Output**:\n```\n9.543574333190918\n```\n\nJust 9.5GB! That's really not a lot for a >15 billion pa..."
          ],
          [
           "```\n\nOverall, we saw that running OctoCoder in 8-bit precision reduced the required GPU VRAM from 32..."
          ],
          [
           "Today's top-performing LLMs share more or less the same fundamental architecture that consists of fe..."
          ],
          [
           "LLMs usually have multiple attention heads, thus doing multiple self-attention computations in paral..."
          ],
          [
           "with \\\\( s^a_{ij} \\\\) and \\\\( s^b_{ij} \\\\) being some softmax normalization statistics that need to ..."
          ],
          [
           "Let's look at a practical example.\n\nOur OctoCoder model now gets a significantly longer input prompt..."
          ],
          [
           "Question: Modify the function so that it returns all input elements when the lists have uneven lengt..."
          ],
          [
           "```\nFor demonstration purposes, we duplicate the system by ten so that the input length is long enou..."
          ],
          [
           "```\n\nWe're getting the same output as before, however this time, the model repeats the answer multip..."
          ],
          [
           "```\n\n**Output**:\n```\nGenerated in 3.0211617946624756 seconds.\n Sure. Here is a function that does th..."
          ],
          [
           "```\n\n## 3. The Science Behind LLM Architectures: Strategic Selection for Long Text Inputs and Chat\n\n..."
          ],
          [
           "A LLM based on self-attention, but without position embeddings would have great difficulties in unde..."
          ],
          [
           "Instead of using fixed position embeddings, others (such as [Devlin et al.](https://arxiv.org/abs/18..."
          ],
          [
           "Without going into too many details, *RoPE* notes that positional information can be encoded into qu..."
          ],
          [
           "As an alternative, *ALiBi* proposes a much simpler relative position encoding scheme. The relative d..."
          ],
          [
           "> Both RoPE and ALiBi are relative positional embeddings that are *not* learned during training, but..."
          ],
          [
           "Let's run a quick code snippet to show how auto-regressive works in practice. We will simply take th..."
          ],
          [
           "```\n\n**Output**:\n```\nshape of input_ids torch.Size([1, 21])\nshape of input_ids torch.Size([1, 22])\ns..."
          ],
          [
           "```\n\nAs we can see every time we increase the text input tokens by the just sampled token.\n\nWith ver..."
          ],
          [
           "for _ in range(5):\n  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_va..."
          ],
          [
           "```\n\n**Output**:\n```\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 20\nshape of inp..."
          ],
          [
           "```\nUser: How many people live in France?\nAssistant: Roughly 75 million people live in France\nUser: ..."
          ],
          [
           "```\n\nIn this chat, the LLM runs auto-regressive decoding twice:\n- 1. The first time, the key-value c..."
          ],
          [
           "There is however one catch. While the required peak memory for the \\\\( \\mathbf{QK}^T \\\\) matrix is s..."
          ],
          [
           "```\n\n**Output**:\n```\n7864320000..."
          ],
          [
           "```\n\nRoughly 8 billion float values! Storing 8 billion float values in `float16` precision requires ..."
          ],
          [
           "The important part to understand here is that reducing the number of key-value attention heads to 1 ..."
          ],
          [
           "Moreover, the authors of GQA found out that existing model checkpoints can be *uptrained* to have a ..."
          ],
          [
           "--\ntitle: \"Deploy GPT-J 6B for inference using  Hugging Face Transformers and Amazon SageMaker\"\nthum..."
          ],
          [
           "There are some hosted solutions to use `GPT-J` for production workloads, like the [Hugging Face Infe..."
          ],
          [
           "```\n\nThe caveat of this example is that it takes a very long time until the model is loaded into mem..."
          ],
          [
           "*â€œSaving a model in this way will save the entire module using Pythonâ€™sÂ [pickle](https://docs.python..."
          ],
          [
           "To create our `torch.load()` compatible model file we load `GPT-J` using Transformers and the `from_..."
          ],
          [
           "```\n\nNow we are able to load our `GPT-J` model with `torch.load()` to run predictions. \n\n```python\nf..."
          ],
          [
           "```\n\n---\n\n### Create `model.tar.gz` for the Amazon SageMaker real-time endpoint\n\nSince we can load o..."
          ],
          [
           "If you still want or need to create your own `model.tar.gz`, e.g. because of compliance guidelines, ..."
          ],
          [
           "```\n\nThe `convert_gpt.py` should print out an S3 URI similar to this. `s3://hf-sagemaker-inference/g..."
          ],
          [
           "```\n\nIf you want to use your own `model.tar.gz` just replace the `model_uri` with your S3 Uri.\n\nThe ..."
          ],
          [
           "```\n\n### Parameterized request\n\nThis is an example of a request using a custom parameter, e.g. `min_..."
          ],
          [
           "```\n\n---\n\nTo delete your endpoint you can run. \n\n```python\npredictor.delete_endpoint()\n```\n\n## Concl..."
          ],
          [
           "--\ntitle: \"Visualize proteins on Hugging Face Spaces\"\nthumbnail: /blog/assets/98_spaces_3dmoljs/thum..."
          ],
          [
           "Make sure you have the `gradio` Python package already [installed](/getting_started) and basic knowl..."
          ],
          [
           "```\n\n`update`: This is the function that does the processing of our proteins and returns an `iframe`..."
          ],
          [
           "```\nThis is a bit clunky to setup but is necessary because of the security rules in modern browsers...."
          ],
          [
           "```\nThe styles for `.mol-container` can be used to modify the size of the molecule viewer. \n\nThe `bo..."
          ],
          [
           "```\nWe use a template literal (denoted by backticks) to store our pdb file in the html document dire..."
          ],
          [
           "--\ntitle: \"Announcing our new Content Guidelines and Policy\"\nthumbnail: /blog/assets/content-guideli..."
          ],
          [
           "## Consent as a Core Value\n\nAs we prioritize respecting people's rights throughout the development a..."
          ],
          [
           "--\ntitle: \"Fine-Tune MMS Adapter Models for low-resource ASR\"\nthumbnail: /blog/assets/151_mms/mms_ma..."
          ],
          [
           "**Wav2Vec2** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [Septe..."
          ],
          [
           "In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates af..."
          ],
          [
           "You can find the pretrained-only checkpoints on the ðŸ¤— Hub for model sizes of 300 million parameters ..."
          ],
          [
           "Three **MMS** checkpoints fine-tuned for speech recognition (ASR) have been released. They include 1..."
          ],
          [
           "Adapters have a long history in speech recognition and especially **speaker recognition**. In speake..."
          ],
          [
           "Just like Wav2Vec2 or XLS-R, MMS is fine-tuned using Connectionist Temporal Classification (CTC), wh..."
          ],
          [
           "```\n\nWe strongly suggest to upload your training checkpoints directly to the [ðŸ¤— Hub](https://hugging..."
          ],
          [
           "```\n\n\n## Prepare Data, Tokenizer, Feature Extractor\n\nASR models transcribe speech to text, which mea..."
          ],
          [
           "The output size of this layer corresponds to the number of tokens in the vocabulary, which we will e..."
          ],
          [
           "```\nMany ASR datasets only provide the target text (`'sentence'`) for each audio array (`'audio'`) a..."
          ],
          [
           "```\n\n```python\nshow_random_elements(common_voice_train.remove_columns([\"path\", \"audio\"]), num_exampl..."
          ],
          [
           "```\n\nAlright! The transcriptions look fairly clean. Having translated the transcribed sentences, it ..."
          ],
          [
           "```\nLet's look at the processed text labels again.\n\n```python\nshow_random_elements(common_voice_trai..."
          ],
          [
           "```\n\nGood! This looks better. We have removed most special characters from transcriptions and normal..."
          ],
          [
           "```\n\n```python\ncommon_voice_train = common_voice_train.map(replace_hatted_characters)\ncommon_voice_t..."
          ],
          [
           "```\n\n```python\nvocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\nvocab_dict\n```\n\n```bash..."
          ],
          [
           "```\n\nCool, we see that all letters of the alphabet occur in the dataset (which is not really surpris..."
          ],
          [
           "```\n\n```bash\n    37\n```\n\nCool, now our vocabulary is complete and consists of 37 tokens, which means..."
          ],
          [
           "```\n\nLet's define an empty dictionary to which we can append the just created vocabulary\n\n```python\n..."
          ],
          [
           "```\n\nIf one wants to re-use the just created tokenizer with the fine-tuned model of this notebook, i..."
          ],
          [
           "```\n\nGreat, you can see the just created repository under `https://huggingface.co/<your-username>/wa..."
          ],
          [
           "A `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:\n\n-   `feat..."
          ],
          [
           "```\n\nGreat, MMS's feature extraction pipeline is thereby fully defined!\n\nFor improved user-friendlin..."
          ],
          [
           "```\n\nIn the example above we can see that the audio data is loaded with a sampling rate of 48kHz whe..."
          ],
          [
           "```\n\nLet's take a look at `\"audio\"` again.\n\n```python\ncommon_voice_train[0][\"audio\"]\n```\n\n\n    {'pat..."
          ],
          [
           "```\n\nGood! Everything looks fine - the data is a 1-dimensional array, the sampling rate always corre..."
          ],
          [
           "```python\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    # batched output is \"un-batche..."
          ],
          [
           "```\n\nLet's apply the data preparation function to all examples.\n\n```python\ncommon_voice_train = comm..."
          ],
          [
           "```\n\n**Note**: `datasets` automatically takes care of audio loading and resampling. If you wish to i..."
          ],
          [
           "After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it..."
          ],
          [
           "```python\nimport torch\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List,..."
          ],
          [
           "batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n          ..."
          ],
          [
           "```\n\n```python\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n```\n\nNe..."
          ],
          [
           "```\n\nThe model will return a sequence of logit vectors:\n \\\\( \\mathbf{y}_1, \\ldots, \\mathbf{y}_m \\\\) ..."
          ],
          [
           "```\n\nNow, we can load the pretrained checkpoint of [`mms-1b-all`](https://huggingface.co/facebook/mm..."
          ],
          [
           "```\n\n**Note**: It is expected that some weights are newly initialized. Those weights correspond to t..."
          ],
          [
           "```\n\nIn a final step, we define all parameters related to training.\nTo give more explanation on some..."
          ],
          [
           "```\n\nNow, all instances can be passed to Trainer and we are ready to start training!\n\n```python\nfrom..."
          ],
          [
           "```\n\n| Training Loss | Training Steps | Validation Loss | Wer    |\n|:-------------:|:----:|:--------..."
          ],
          [
           "The adapter weights will be uploaded as part of the model checkpoint, but we also want to make sure ..."
          ],
          [
           "```\n\nFinally, you can upload the result of the training to the ðŸ¤— Hub.\n\n```python\ntrainer.push_to_hub..."
          ],
          [
           "```\n\nOne of the main advantages of adapter weights training is that the \"base\" model which makes up ..."
          ],
          [
           "```\n\nLet's check that the model can correctly transcribe Turkish\n\n```python\nfrom datasets import Aud..."
          ],
          [
           "```\n\nWe again load the Swedish test set from common voice\n\n```python\ncommon_voice_test_swe = load_da..."
          ],
          [
           "```\n\n**Output**:\n\n```bash\n    Prediction:\n    jag lÃ¤mnade grovjobbet Ã¥t honom\n\n    Reference:\n    ja..."
          ],
          [
           "--\ntitle: \"Policy Gradient with PyTorch\"\nthumbnail: /blog/assets/85_policy_gradient/thumbnail.gif\nau..."
          ],
          [
           "Indeed, since the beginning of the course, we only studied value-based methods,Â **where we estimate ..."
          ],
          [
           "## What are Policy-Gradient Methods?\nPolicy-Gradient is a subclass of Policy-Based Methods, a catego..."
          ],
          [
           "But Deep Q-Learning is excellent! Why use policy gradient methods?\n\n### The Advantages of Policy-Gra..."
          ],
          [
           "Under a value-based RL algorithm, we learn a quasi-deterministic policy (\"greedy epsilon strategy\")...."
          ],
          [
           "ðŸ‘‰ If you want to go deeper on the why the advantages and disadvantages of Policy Gradients methods, ..."
          ],
          [
           "The Reinforce algorithm works like this:\nLoop: \n- Use the policy \\\\(\\pi_\\theta\\\\)  to collect an epi..."
          ],
          [
           "Now that we studied the theory behind Reinforce, **youâ€™re ready to code your Reinforce agent with Py..."
          ],
          [
           "In the next unit, weâ€™re going to learn about a combination of Policy-Based and Value-based methods c..."
          ],
          [
           "--\ntitle: \"Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\"\nthumbnail: /blog..."
          ],
          [
           "## Getting Started\n\nBefore you can connect your AWS Account with your Hugging Face account, you need..."
          ],
          [
           "![Marketplace Redirect](assets/158_aws_marketplace/03_redirect.jpg \"Marketplace Redirect\")\n\nAfter cl..."
          ],
          [
           "Pricing for Hugging Face Platform through the AWS marketplace offer is identical to the [public Hugg..."
          ],
          [
           "--\ntitle: \"Practical 3D Asset Generation: A Step-by-Step Guide\"\nthumbnail: /blog/assets/124_ml-for-g..."
          ],
          [
           "<gradio-app theme_mode=\"light\" space=\"hysts/Shap-E\"></gradio-app>\n\nEnter \"Dilapidated Shack\" as your..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-..."
          ],
          [
           "--\ntitle: \"Making LLMs lighter with AutoGPTQ and transformers\" \nthumbnail: /blog/assets/159_autogptq..."
          ],
          [
           "This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs.\n\n## Table of contents..."
          ],
          [
           "## Resources\n\nThis blogpost and release come with several resources to get started with GPTQ quantiz..."
          ],
          [
           "The benefits of this scheme are twofold:\n\n- Memory savings close to x4 for int4 quantization, as the..."
          ],
          [
           "This means that we can quantize each row independently. This is called per-channel quantization. For..."
          ],
          [
           "Since the AutoGPTQ library has a larger coverage of transformers models, we decided to provide an in..."
          ],
          [
           "```\n\nCheck out the Transformers [documentation](https://huggingface.co/docs/transformers/main/en/mai..."
          ],
          [
           "| gptq  | act_order | bits | group_size | kernel            | Load time (s) | Per-token latency (ms)..."
          ],
          [
           "Quantizing ðŸ¤—Â Transformers models with the GPTQ method can be done in a few lines:\n\n```python\nfrom tr..."
          ],
          [
           "```\n\nQuantizing a model may take a long time. Note that for a 175B model, at least 4 GPU-hours are r..."
          ],
          [
           "## **Fine-tune quantized models with PEFT**\n\nYou can not further train a quantized model using the r..."
          ],
          [
           "On the quantization side, letâ€™s emphasize again that this method only quantizes the weights. There h..."
          ],
          [
           "This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step ..."
          ],
          [
           "## Acknowledgements\n\nWe would like to thank [William](https://github.com/PanQiWei) for his support a..."
          ],
          [
           "--\ntitle: \"Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets\"\nthum..."
          ],
          [
           "## What is the ðŸ¤— Data Measurements Tool?\nThe [Data Measurements Tool (DMT)](https://huggingface.co/s..."
          ],
          [
           "A new wave of research in AI has called for a fundamental paradigm shift in how the field approaches..."
          ],
          [
           "Despite this, there are few tools openly available to the public to enable people from different dis..."
          ],
          [
           "### Descriptive Statistics\n**To look at the surface characteristics of the dataset**\n\n*This begins t..."
          ],
          [
           "You can use this to figure out whether your dataset represents language as it tends to behave in the..."
          ],
          [
           "- The [normalized pointwise mutual information (nPMI)](https://en.wikipedia.org/wiki/Pointwise_mutua..."
          ],
          [
           "--\ntitle: \"New ViT and ALIGN Models From Kakao Brain\" \nthumbnail: /blog//assets/132_vit_align/thumbn..."
          ],
          [
           "This blog will introduce the new [COYO](https://github.com/kakaobrain/coyo-dataset) dataset, Kakao B..."
          ],
          [
           "## COYO DATASET\n\n<p>\n<center>\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-im..."
          ],
          [
           "| COYO | LAION 2B| ALIGN 1.8B |\n| :----: | :----: | :----: |\n| Image-text similarity score calculate..."
          ],
          [
           "## How ViT and ALIGN work\n\nSo what do these models do? Let's breifly discuss how the ViT and ALIGN m..."
          ],
          [
           "[Google then introduced ALIGN](https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision...."
          ],
          [
           "## How to use the COYO dataset\nWe can conveniently download the `COYO` dataset with a single line of..."
          ],
          [
           "```\n\nWhile it is significantly smaller than the `LAION` dataset, the `COYO` dataset is still massive..."
          ],
          [
           "```shell\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset('kakaobrain/coyo-700m', s..."
          ],
          [
           "```\n\n## How to use ViT and ALIGN from the Hub\nLetâ€™s go ahead and experiment with the new ViT and ALI..."
          ],
          [
           "```\n\nThe rest is simple, we will forward preprocess the image and use it as input to the model to re..."
          ],
          [
           "```\n\nAnd we are done! To make things even easier and shorter, we can also use the convenient image c..."
          ],
          [
           "```\n\nIf you want to experiment more with the Kakao Brain ViT model, head over to its [Space](https:/..."
          ],
          [
           "```\n\nWe will start with zero-shot image classification first. To do this, we will suppy candidate la..."
          ],
          [
           "```\n\nAlternatively, we can use the stand-along vision and text encoders of ALIGN to retrieve multi-m..."
          ],
          [
           "```\n\nLet's do the same with `AlignVisionModel` and retrieve the multi-modal embedding of an image.\n\n..."
          ],
          [
           "```\n\nSimilar to ViT, we can use the zero-shot image classification [pipeline](https://huggingface.co..."
          ],
          [
           "```\n\n## Conclusion\n\nThere have been incredible advances in multi-modal models in recent years, with ..."
          ],
          [
           "--\ntitle: \"Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon\"\nthumbnail: /bl..."
          ],
          [
           "In a nutshell, quantization rescales model parameters to smaller value ranges. When successful, it s..."
          ],
          [
           "<kbd>\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog..."
          ],
          [
           "Now, letâ€™s see how SmoothQuant works when applied to popular LLMs.\n\n## Quantizing LLMs with SmoothQu..."
          ],
          [
           "The table below presents a summary of their findings. The second column shows the ratio of benchmark..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n    <video \n        alt=\"MPT-7B Demo\"\n        st..."
          ],
          [
           "## Next steps\n\nWeâ€™re currently working on integrating these new quantization techniques into the Hug..."
          ],
          [
           "--\ntitle: \"An overview of inference solutions on Hugging Face\"\nthumbnail: /blog/assets/116_inference..."
          ],
          [
           "Here's a sentence similarity example with the `sentence-transformers/all-MiniLM-L6-v2` [model](https..."
          ],
          [
           "```\ncurl https://api-inference.huggingface.co/models/xlm-roberta-base \\\n\t-X POST \\\n\t-d '{\"inputs\": \"..."
          ],
          [
           "```\n\nThe Inference API is the simplest way to build a prediction service that you can immediately ca..."
          ],
          [
           "<kbd>\n  <img src=\"assets/116_inference_update/endpoints.png\">\n</kbd>\n\nTo learn more about Inference ..."
          ],
          [
           "--\ntitle: \"Hugging Face's TensorFlow Philosophy\"\nthumbnail: /blog/assets/96_tensorflow_philosophy/th..."
          ],
          [
           "```\n\nThis one line will instantiate the model architecture and load the weights, giving you an exact..."
          ],
          [
           "```\n\nNow our `model` has an output head and, optionally, a loss function appropriate for its new tas..."
          ],
          [
           "# Let's load some data and tokenize it\ntest_strings = [\"This is a sentence!\", \"This is another one!\"..."
          ],
          [
           "```\n\nThis is just a taste of the library, of course - if you want more, you can check out our [noteb..."
          ],
          [
           "```\n\nAnd if you want to train that model instead, it's just:\n\n```py\nmodel.fit(my_data, my_labels)\n``..."
          ],
          [
           "```\n\n#### Philosophy #2: Loss functions are provided by default, but can be easily changed.\n\nIn Kera..."
          ],
          [
           "```\n\nBut also, and very importantly, we want to get out of your way as soon as you want to do someth..."
          ],
          [
           "```\n\nIn the past, we instead asked users to pass labels in the input dict when using the default los..."
          ],
          [
           "# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base..."
          ],
          [
           "```\n\nThis approach is great when it works, but for larger datasets you might find it starting to bec..."
          ],
          [
           "```\nWhy is [prepare_tf_dataset()](https://huggingface.co/docs/transformers/main/en/main_classes/mode..."
          ],
          [
           "```\n\nWeâ€™ve made a number of major improvements recently in this area. Most significantly, weâ€™ve upda..."
          ],
          [
           "One major obstacle in deploying NLP models, however, is that inputs will still need to be tokenized,..."
          ],
          [
           "```\n\n#### Conclusion: Weâ€™re an open-source project, and that means community is everything\n\nMade a c..."
          ],
          [
           "```\n\nI think the fact that thereâ€™s no distinction between big famous foundation models and models fi..."
          ],
          [
           "<small>(And if you can make a meme to troll the PyTorch team with after your cool new feature is mer..."
          ],
          [
           "--\ntitle: The Age of Machine Learning As Code HasÂ Arrived\nthumbnail: /blog/assets/31_age_of_ml_as_co..."
          ],
          [
           "Well, here's what I think.\n\n\n### Machine Learning For TheÂ Masses!\n\nMachine Learning is everywhere, o..."
          ],
          [
           "There's no need to reinvent the wheel either. The DevOps movement solved these problems over 10 year..."
          ],
          [
           "---\n\n### Transformers! Transformers! Transformers! ([BallmerÂ style](https://www.youtube.com/watch?v=..."
          ],
          [
           "It's a Good Thing in so many ways. State of the art is constantly advancing, and hardly anyone can k..."
          ],
          [
           "We believe in built-in best practices. \n\nWe believe in making infrastructure as transparent as possi..."
          ],
          [
           "--\ntitle: \"Introducing Storage Regions on the HF Hub\"\nthumbnail: /blog/assets/172_regions/thumbnail...."
          ],
          [
           "Any repo (model or dataset) stored in a non-default location will display its Region directly as a t..."
          ],
          [
           "--\ntitle: \"Announcing the ðŸ¤— AI Research Residency Program\"\nthumbnail: /blog/assets/57_ai_residency/r..."
          ],
          [
           "We are actively working to build a culture that values diversity, equity, and inclusivity. We are in..."
          ],
          [
           "--\ntitle: \"Hugging Face Reads, Feb. 2021 - Long-range Transformers\"\nthumbnail: /blog/assets/14_long_..."
          ],
          [
           "In particular, one issue has been at the center of the efforts: the quadratic cost in memory and tim..."
          ],
          [
           "Iz Beltagy, Matthew E. Peters, Arman Cohan\n\nLongformer addresses the memory bottleneck of transforme..."
          ],
          [
           "Longformer uses different attention patterns for autoregressive language modeling, encoder pre-train..."
          ],
          [
           "#### Main findings\n\n* The authors proposed the dilated windowed self-attention (Figure c) and showed..."
          ],
          [
           "Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap\n\n[Transformer-XL (2019)](ht..."
          ],
          [
           "A compression factor \\\\(c\\\\) (equal to 3 in the illustration) is chosen to decide the rate at which ..."
          ],
          [
           "#### Follow-up questions\n\n* Compressive Transformer requires a special optimization schedule in whic..."
          ],
          [
           "The theoretical foundations of the proposed approach are based on the Johnson-Lindenstrauss lemma. L..."
          ],
          [
           "<figure>\n  <img src=\"/blog/assets/14_long_range_transformers/Linformer.png\" alt=\"Linformer performan..."
          ],
          [
           "$$\\text{softmax}(Q * K) \\sim Qâ€™ * Kâ€™ = \\phi(Q) * \\phi(K)$$\n\n, where \\\\(phi\\\\) is a non-linear suitab..."
          ],
          [
           "## Reading group discussion\n\nThe developments in pre-trained transformer-based language models for n..."
          ],
          [
           "All these works highlight the importance of long-range inputs modeling in natural language. In the i..."
          ],
          [
           "## @Hugging Face ðŸ¤—: Long-range modeling\n\nThe Longformer implementation and the associated open-sourc..."
          ],
          [
           "--\ntitle: \"VQ-Diffusion\" \nthumbnail: /blog/assets/117_vq_diffusion/thumbnail.png\nauthors:\n- user: wi..."
          ],
          [
           "```\n\n![png](assets/117_vq_diffusion/vq_diffusion_teddy_bear_pool.png)\n\n### Architecture\n\n![svg](asse..."
          ],
          [
           "VQ-Diffusion uses a pre-trained VQ-VAE which was frozen during the diffusion training process.\n\n####..."
          ],
          [
           "The AR models section provides additional context on VQ-Diffusion's architecture in comparison to AR..."
          ],
          [
           "There is a smaller amount of literature covering discrete diffusion models than continuous diffusion..."
          ],
          [
           "AR image generative models have evolved architecturally with much work towards making transformers c..."
          ],
          [
           "[Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attentio..."
          ],
          [
           "Despite having made tremendous strides, AR models still suffer from linear decreases in inference sp..."
          ],
          [
           "[Improved Vector Quantized Diffusion Models](https://arxiv.org/abs/2205.16007) improves upon VQ-Diff..."
          ],
          [
           "--\ntitle: Zero-shot image segmentation with CLIPSeg\nthumbnail: /blog/assets/123_clipseg-zero-shot/th..."
          ],
          [
           "Image segmentation is a well-known task within the field of computer vision. It allows a computer to..."
          ],
          [
           "## CLIP: the magic model behind CLIPSeg\n\n[CLIP](https://huggingface.co/docs/transformers/main/en/mod..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"..."
          ],
          [
           "## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model tha..."
          ],
          [
           "One interesting feature of CLIPSeg is that both the query (the image we want to segment) and the pro..."
          ],
          [
           "```\n\nTo download the model, simply instantiate it.\n\n```python\nfrom transformers import CLIPSegProces..."
          ],
          [
           "```\n\nNow that we have our inputs, we can process them and input them to the\nmodel.\n\n```python\nimport..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-6/12\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "```\n\n```python\n_, ax = plt.subplots(1, 2, figsize=(6, 4))\n[a.axis('off') for a in ax.flatten()]\nax[0..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "```\n\n```python\nfrom segments import SegmentsClient\nfrom getpass import getpass\n\napi_key = getpass('E..."
          ],
          [
           "```\n\nNow we can use CLIPSeg on the image as before. This time, we\\'ll also\nscale up the outputs so t..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "```\n\nLet\\'s quickly visualize the result.\n\n```python\nplt.imshow(inds)\n```\n\n<figure class=\"image tabl..."
          ],
          [
           "```\n\nIf you take a look at the [uploaded prediction on\nSegments.ai](https://segments.ai/admin-tobias..."
          ],
          [
           "If youâ€™re interested in learning how to fine-tune a state-of-the-art segmentation model, check out o..."
          ],
          [
           "--\ntitle: \"Getting Started with Sentiment Analysis on Twitter\"\nthumbnail: /blog/assets/85_sentiment_..."
          ],
          [
           "Buckle up and enjoy the ride! ðŸ¤—\n\n## What is Sentiment Analysis?\n\nSentiment analysis uses [machine le..."
          ],
          [
           "Luckily, recent advancements in AI allowed companies to use machine learning models for sentiment an..."
          ],
          [
           "## How to do Twitter sentiment analysis with code?\n\nNowadays, getting started with sentiment analysi..."
          ],
          [
           "```\n\n2. Setting up Twitter credentials\n\nThen, you need to set up the [Twitter API credentials](https..."
          ],
          [
           "```\n\n4. Analyzing tweets with sentiment analysis\n\nNow that you have data, you are ready to analyze t..."
          ],
          [
           "```\n\nNext, you will create the API call using the `model id` and `hf_token`:\n\n```python\nAPI_URL = \"h..."
          ],
          [
           "```\n\nResults:\n\n```\n@thenotionbar @hypefury @NotionHQ Thatâ€™s genuinely smart. So basically youâ€™ve set..."
          ],
          [
           "```\n\nIt's cool to see that 50% of all tweets are positive and only 8.2% are negative:\n\n<figure class..."
          ],
          [
           "As a last step, let's create some wordclouds to see which words are the most used for each sentiment..."
          ],
          [
           "```\n\nCuriously, some of the words that stand out from the positive tweets include \"notes\", \"cron\", a..."
          ],
          [
           "## How to do Twitter sentiment analysis without coding?\n\nTo get started with sentiment analysis, you..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"..."
          ],
          [
           "Once you have your model ID and your Hugging Face token ID, go back to your Zap and follow these ins..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"..."
          ],
          [
           "Then, follow these instructions to configure this last step:\n1. Select Google Sheets as an app, and ..."
          ],
          [
           "To turn it on, just click on \"Publish\" button at the bottom of your screen:\n\n<figure class=\"image ta..."
          ],
          [
           "Luckily, tools like the [Inference API](https://huggingface.co/inference-api) makes it super easy to..."
          ],
          [
           "--\ntitle: \"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\"..."
          ],
          [
           "For example, the [IMDB dataset](https://huggingface.co/datasets/imdb) specifies `en` in the YAML met..."
          ],
          [
           "*Distribution of language tags for datasets on the hub excluding English*\n\nHowever, there is a major..."
          ],
          [
           "If we switch to the task of finding relevant machine learning models, knowing what languages were in..."
          ],
          [
           "```\n\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole datase..."
          ],
          [
           "We pass 20 examples to the model representing rows from a dataset. This results in 20 individual lan..."
          ],
          [
           "For some ISO 639-3 codes, there is no ISO 639-1 equivalent. For these cases we manually specify a ma..."
          ],
          [
           "As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic ..."
          ],
          [
           "--\ntitle: \"Jupyter X Hugging Face\" \nthumbnail: /blog/assets/135_notebooks-hub/before_after_notebook_..."
          ],
          [
           "<figure>\n  <img src=\"/blog/assets/135_notebooks-hub/before_after_notebook_rendering.png\" alt=\"A side..."
          ],
          [
           "--\ntitle: \"Gradio is joining Hugging Face!\"\nthumbnail: /blog/assets/42_gradio_joins_hf/thumbnail.png..."
          ],
          [
           "As one of the founders of Gradio, I couldn't be more excited about the next step in our journey. I s..."
          ],
          [
           "In addition to the shared mission of Gradio and Hugging Face, what delights me is the team that we a..."
          ],
          [
           "--\ntitle: \"Generating Stories: AI for Game Development #5\"\nthumbnail: /blog/assets/124_ml-for-games/..."
          ],
          [
           "### Process\n\n**Requirements:** I'm using [ChatGPT](https://openai.com/blog/chatgpt/) throughout this..."
          ],
          [
           "<div align=\"center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "> âš ï¸ **Limitation:** Using outputs from language models directly may have unintended legal, ethical,..."
          ],
          [
           "For my simple farming game, this may be an effective approach to producing all the story content for..."
          ],
          [
           "There are many other models which are not yet publicly accessible. Check out [this](https://huggingf..."
          ],
          [
           "#### In-Game Development\n\n**NPCs:** Aside from the clear uses of language models and dialog agents i..."
          ],
          [
           "--\ntitle: \"Making automatic speech recognition work on large files with Wav2Vec2 in ðŸ¤— Transformers\"\n..."
          ],
          [
           "```\n\n\n**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 202..."
          ],
          [
           "```\n\n```python\nfrom transformers import pipeline\n\n# This will work on any of the thousands of models..."
          ],
          [
           "As it turns out, CTC structure, which is used by Wav2Vec2, can be exploited\nin order to achieve very..."
          ],
          [
           "Let's note that you can choose every argument of this technique:\n\n```python\nfrom transformers import..."
          ],
          [
           "```\n\n\nChunking with stride on LM augmented models\n-------------------------------------------\n\nIn [t..."
          ],
          [
           "--\ntitle: \"Red-Teaming Large Language Models\" \nthumbnail: /blog/assets/red-teaming/thumbnail.png\naut..."
          ],
          [
           "Even recent versions of GPT3 produce similarly offensive text when attacked with prompt injection th..."
          ],
          [
           "Red-teaming can reveal model limitations that can cause upsetting user experiences or enable harm by..."
          ],
          [
           "<p align=\"center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/reso..."
          ],
          [
           "The caveat in evaluating LLMs for such malicious behaviors is that we donâ€™t know what they are capab..."
          ],
          [
           "1. Few-shot-prompted LMs with helpful, honest, and harmless behavior are *not* harder to red-team th..."
          ],
          [
           "Reach out to us (@nazneenrajani @natolambert @lewtun @TristanThrush @yjernite @thomwolf) if you're i..."
          ],
          [
           "--\ntitle: \"2023, year of open LLMs\"\nthumbnail: /blog/assets/cv_state/thumbnail.png\nauthors:\n- user: ..."
          ],
          [
           "A **tokenizer** defines how the text from the training dataset is converted to numbers (as a model i..."
          ],
          [
           "## ðŸ—ï¸ 2022, from a race for size to a race for data\nWhat open models were available to the community..."
          ],
          [
           "2. [OPT](https://huggingface.co/papers/2205.01068) (Open Pre-trained Transformer)\nThe OPT [model](ht..."
          ],
          [
           "4. Smaller or more specialized open LLM\nSome smaller open-source models were also released, mostly f..."
          ],
          [
           "This paradigm shift, while probably already known in closed labs took the open science community by ..."
          ],
          [
           "The first model family in this series was the [LLaMA](https://huggingface.co/papers/2302.13971) fami..."
          ],
          [
           "The [MPT models](https://www.mosaicml.com/blog/mpt-7b), which came out a couple of months later, rel..."
          ],
          [
           "X-Gen was a bit over-shadowed by the much visible new [LLaMA-2](https://huggingface.co/papers/2307.0..."
          ],
          [
           "In parallel, a notable event of the end of the year 2023 was the rise of performances and a number o..."
          ],
          [
           "**Chat-based fine-tuning** is a variant of supervised fine-tuning, where the annotated data is chat ..."
          ],
          [
           "Both these methods are relatively easy to implement: you just need to find or generate related datas..."
          ],
          [
           "**Direct preference optimization** (DPO) is another variation of RLHF, but does not require the trai..."
          ],
          [
           "At the beginning of 2023, a few datasets for instruction/chat finetuning were already released. For ..."
          ],
          [
           "â„ï¸ Winter 2022/2023: In January this year, the [Human ChatGPT Instruction corpus](https://huggingfac..."
          ],
          [
           "ðŸŒ± Spring: In April, BAIR (Berkeley AI Research lab) released [Koala](https://bair.berkeley.edu/blog/..."
          ],
          [
           "a fine-tune on said dataset. Microsoft then released the [GPT4-LLM](https://github.com/Instruction-T..."
          ],
          [
           "of models (Llama, Mistral, ...). In May and June, [Camel-AI](https://huggingface.co/camel-ai) releas..."
          ],
          [
           "ðŸŒ»Summer: In August, [UltraLM](https://github.com/thunlp/UltraChat) (a high-performing chat fine-tune..."
          ],
          [
           "ðŸ‚ Autumn: In October, Hugging Face released [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-..."
          ],
          [
           "*Some more specialized datasets (such as [MetaMath](https://meta-math.github.io/) or [MathInstruct](..."
          ],
          [
           "But what does it mean to merge a model?\n\n**Model merging** is a way to fuse the weights of different..."
          ],
          [
           "You might want to use what is called **parameter efficient fine-tuning** (PEFT).\nThis technique firs..."
          ],
          [
           "So, if you reduce the precision, you reduce the memory each model parameter takes in storage, theref..."
          ],
          [
           "New releases include\n- A mixture of experts:\n\t- [Mixtral](https://huggingface.co/mistralai/Mixtral-8..."
          ],
          [
           "That's it folks! \nI hope you enjoyed this year's review, learned a thing or two, and feel as enthusi..."
          ],
          [
           "--\ntitle: \"The N Implementation Details of RLHF with PPO\"\nthumbnail: /blog/assets/167_the_n_implemen..."
          ],
          [
           "- In [Matching Learning Curves](#matching-learning-curves), we show our main contribution: creating ..."
          ],
          [
           "**Here are the important links:**\n\n- ðŸ’¾Â Our reproduction codebase [*https://github.com/vwxyzjn/lm-hum..."
          ],
          [
           "- OAIâ€™s dataset was partially corrupted/lost (so we replaced them with similar HF datasets, which ma..."
          ],
          [
           "1. **The reward model and policyâ€™s value head take input as the concatenation of `query` and `respon..."
          ],
          [
           "2. So, for example, if `query = \"he was quiet for a minute, his eyes unreadable\"`., and the `respons..."
          ],
          [
           "2. **Pad with a special padding token and truncate inputs.** \n    1. OAI sets a fixed input length f..."
          ],
          [
           "1. **Note on HFâ€™s transformers â€” padding token.** According to  ([transformers#2630#issuecomment-578..."
          ],
          [
           "inputs = tokenizer.pad(\n        {\"input_ids\": tokens},\n        padding=\"max_length\",\n        max_len..."
          ],
          [
           "```\n    \n3. **Adjust position indices correspondingly for padding tokens**\n    1. When calculating t..."
          ],
          [
           "-35.36577 ]\n          [ -35.28693   -34.2875    -38.16074  ...  -41.595802  -41.082108\n            -..."
          ],
          [
           "```\n        \n    3. **Note on HFâ€™s transformers â€” `position_ids` and `padding_side`.** We can replic..."
          ],
          [
           "input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=pos..."
          ],
          [
           "```\n        \n    4. **Note on HFâ€™s transformers â€” `position_ids` during `generate`:** during generat..."
          ],
          [
           "2. **Note on HFâ€™s transformers â€” sampling could stop at `eos_token`:** in `transformers`, the genera..."
          ],
          [
           "response_length = 4\n        temperature = 0.7\n        pretrained_model = transformers.AutoModelForCa..."
          ],
          [
           "```\n        \n    3. Note that in a more recent codebase https://github.com/openai/summarize-from-fee..."
          ],
          [
           "6. **Use different seeds for different processes**\n    1. When spawning 8 GPU processes to do data p..."
          ],
          [
           "# Reward Model Implementation Details\n\nIn this section, we discuss reward-model-specific implementat..."
          ],
          [
           "1. **The reward model only outputs the value at the last token.**\n    1. Notice that the rewards obt..."
          ],
          [
           "2. Note that in a more recent codebase [*openai/summarize-from-feedback*](https://github.com/openai/..."
          ],
          [
           "2. **Reward head layer initialization**\n    1. The weight of the reward head is initialized accordin..."
          ],
          [
           "2. The bias of the reward head is set to 0 ([lm_human_preferences/language/model.py#L254](https://gi..."
          ],
          [
           "2. When performing the normalization process, the code first sets `reward_gain=1, reward_bias=0` ([l..."
          ],
          [
           "$$\\begin{aligned}g*\\mathcal{N}(\\mu_{\\mathcal{D}}, \\sigma_{\\mathcal{D}}) + b &= \\mathcal{N}(g*\\mu_{\\m..."
          ],
          [
           "5. Note that responses  \\\\( y \\sim \\rho(Â·|x) \\\\) we generated for the normalization purpose are from..."
          ],
          [
           "# Policy Training Implementation Details\n\nIn this section, we will delve into details, such as layer..."
          ],
          [
           "1. **Scale the logits by sampling temperature.** \n    1. When calculating the log probability of res..."
          ],
          [
           "2. The bias of the reward head is set to 0 ([lm_human_preferences/language/model.py#L254](https://gi..."
          ],
          [
           "2. When running `openai/lm-human-preferences`, OAIâ€™s datasets were partially corrupted/lost ([openai..."
          ],
          [
           "5. **Rejection sampling** \n    1. Ziegler et al. (2019) suggested, â€œWe use rejection sampling to ens..."
          ],
          [
           "2. **Run reward model on truncated response:** After the response has been truncated by the token tr..."
          ],
          [
           "2. Code comment:  â€œonly query humans on responses that pass that functionâ€œ\n        4. To give some e..."
          ],
          [
           "7. **Terminology of the training loop: batches and minibatches in PPO**\n    1. OAI uses the followin..."
          ],
          [
           "mini_batch_end = mini_batch_start + mini_batch_size\n                mini_batch_inds = batch_inds[min..."
          ],
          [
           "# ____â© a forward pass on [3. 2.]\n        # âª a backward pass on [6. 7. 3. 2.]\n        # ____â© a for..."
          ],
          [
           "```\n        \n8. **Per-token KL penalty**\n    - The code adds a per-token KL penalty ([lm_human_prefe..."
          ],
          [
           "- Then the `non_score_reward = beta * kl` , where `beta` is the KL penalty coefficient  \\\\(\\beta\\\\),..."
          ],
          [
           "```\n    \n    1. In each minibatch, OAI then whitens the reward `whiten(rewards, shift_mean=False)` w..."
          ],
          [
           "var = tf.Print(var, [var], 'var', summarize=100)\n            whitened = (values - mean) * tf.rsqrt(v..."
          ],
          [
           "```\n        \n        ```jsx\n        mean[1.5999999]\n        var[0.0666666627]\n        [[0.05080712 0..."
          ],
          [
           "```\n        \n10. **Clipped value function**\n    1. As done in the original PPO ([baselines/ppo2/mode..."
          ],
          [
           "def __init__(self, init_kl_coef, hparams):\n                self.value = init_kl_coef\n               ..."
          ],
          [
           "```\n        \n    - For the `sentiment` and `descriptiveness` tasks examined in this work, we have `i..."
          ],
          [
           "```python\n### pytorch adam implementation:\nbias_correction1 = 1 - beta1 ** step\nbias_correction2 = 1..."
          ],
          [
           "```\n\n- Letâ€™s compare the update equations of pytorch-style and tensorflow-style adam. Following the ..."
          ],
          [
           "$$\\begin{aligned}\\text{tensorflow adam:}\\quad \\theta_t & =\\theta_{t-1}-\\alpha_t m_t /\\left(\\sqrt{v_t..."
          ],
          [
           "- The equations above highlight that the distinction between pytorch and tensorflow implementation i..."
          ],
          [
           "- The above figure shows that, if we set the same `eps` in pytorch adam and tensorflow adam, then py..."
          ],
          [
           "| ratio_mean | 1.0051285 | 1.0105520486831665 | 1.0044583082199097 |\n    | ratio_var | 0.0007716546 ..."
          ],
          [
           "- **PyTorchâ€™s `Adam` produces a more aggressive update** for some reason. Here are some evidence:\n  ..."
          ],
          [
           "- **Larger models get affected more.** We conducted experiments comparing PyTorchâ€™s `Adam` (codename..."
          ],
          [
           "![adam_gpt2.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/rl..."
          ],
          [
           "# Conclusion\n\nIn this work, we took a deep dive into OAIâ€™s original RLHF codebase and compiled a lis..."
          ],
          [
           "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /bl..."
          ],
          [
           "Distributed training is the key to enable training such large ML models. There have been major recen..."
          ],
          [
           "In this post we will look at Data Parallelism using ZeRO and more specifically the latest PyTorch fe..."
          ],
          [
           "Sample FSDP config after running the command `accelerate config`:\n```bash\ncompute_environment: LOCAL..."
          ],
          [
           "```\n\n## Multi-GPU FSDP\n\nHere, we experiment on the Single-Node Multi-GPU setting. We compare the per..."
          ],
          [
           "```\nSample FSDP Run:\n![Sample FSDP Run](./assets/62_pytorch_fsdp/sample_fsdp_run.png)\n\n\n| Method | B..."
          ],
          [
           "### CPU Offloading to enable training humongous models that wonâ€™t fit the GPUÂ memory\n\nCommand for tr..."
          ],
          [
           "```\n\n| Method | Batch Size Max ($BS) | Num GPUs | Approx Train Time (Hours) | Notes |\n| --- | --- | ..."
          ],
          [
           "Table 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\n\nFrom Table 2, we can observe that DDP (w and w/..."
          ],
          [
           "After creating an instance of this class, users can pass it when creating the Accelerator object.\n\nF..."
          ],
          [
           "We leverage the tracking functionality support in Accelerate to log the train and evaluation peak me..."
          ],
          [
           "optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\n\n- model, optimizer, train_dataloade..."
          ],
          [
           "```\n\n- In case of a single model, if you have created optimizer with multiple parameter groups and c..."
          ],
          [
           "```\n- In case of multiple models, it is necessary to prepare the models before creating optimizers e..."
          ],
          [
           "FSDP precisely addresses this by sharding the optimizer states, gradients and model parameters acros..."
          ],
          [
           "[2] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/191..."
          ],
          [
           "[10] [Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/z..."
          ],
          [
           "--\ntitle: \"Hosting your Models and Datasets on Hugging Face Spaces using Streamlit\"\nthumbnail: /blog..."
          ],
          [
           "``` python\nimport streamlit as st\n\n# adding the text that will show in the text box as default\ndefau..."
          ],
          [
           "```\n\nThe inference code returns the generated output, you can print the output using simple ```st.wr..."
          ],
          [
           "```\n\n If you have structured data like mine, you can simply use  ```st.dataframe(df) ``` to show you..."
          ],
          [
           "--\ntitle: \"Mixture of Experts Explained\"\nthumbnail: /blog/assets/moe/thumbnail.png\nauthors:\n- user: ..."
          ],
          [
           "Letâ€™s dive in!\n\n## Table of Contents\n\n- [What is a Mixture of Experts?](#what-is-a-mixture-of-expert..."
          ],
          [
           "## TL;DR\n\nMoEs:\n- Are **pretrained much faster** vs. dense models\n- Have **faster inference** compar..."
          ],
          [
           "<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documenta..."
          ],
          [
           "Now that we have a rough idea of what a MoE is, letâ€™s take a look at the research developments that ..."
          ],
          [
           "These works led to exploring a mixture of experts in the context of NLP. Concretely, [Shazeer et al...."
          ],
          [
           "This setup introduces some challenges. For example, although large batch sizes are usually better fo..."
          ],
          [
           "$$\n    \n2. We only pick the top k\n\n$$\n\\text{KeepTopK}(v, k)_i = \\begin{cases}\nv_i & \\text{if } v_i \\..."
          ],
          [
           "GShard replaces every other FFN layer with an MoE layer using top-2 gating in both the encoder and t..."
          ],
          [
           "The GShard paper has contributions by expressing parallel computation patterns that work well for Mo..."
          ],
          [
           "- The router computation is reduced\n- The batch size of each expert can be at least halved\n- Communi..."
          ],
          [
           "This [notebook](https://colab.research.google.com/drive/1aGGVHZmtKmcNBbAwa9hbu58DDpIuB5O4?usp=sharin..."
          ],
          [
           "## What does an expert learn?\n\nThe ST-MoE authors observed that encoder experts specialize in a grou..."
          ],
          [
           "Switch Transformers observed that at a fixed pretrain perplexity, the sparse model does worse than t..."
          ],
          [
           "One last part to consider when fine-tuning sparse MoEs is that they have different fine-tuning hyper..."
          ],
          [
           "<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documenta..."
          ],
          [
           "With expert parallelism, experts are placed on different workers, and each worker takes a different ..."
          ],
          [
           "A big downside of MoEs is the large number of parameters. For local use cases, one might want to use..."
          ],
          [
           "## Open Source MoEs\n\nThere are nowadays several open source projects to train MoEs:\n\n- Megablocks: h..."
          ],
          [
           "So, TL;DR, some interesting areas to explore:\n\n* Distilling Mixtral into a dense model\n* Explore mod..."
          ],
          [
           "- [Adaptive Mixture of Local Experts (1991)](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)\n- ..."
          ],
          [
           "- [Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models (May ..."
          ],
          [
           "## Citation\n\n```bibtex\n@misc {sanseviero2023moe,\n    author       = { Omar Sanseviero and\n          ..."
          ],
          [
           "```\n\n```\nSanseviero, et al., \"Mixture of Experts Explained\", Hugging Face Blog, 2023.\n```..."
          ],
          [
           "--\ntitle: \"An Introduction to Q-Learning Part 1\"\nthumbnail: /blog/assets/70_deep_rl_q_part1/thumbnai..."
          ],
          [
           "So today, we're going toÂ **dive deeper into one of the Reinforcement Learning methods: value-based m..."
          ],
          [
           "So let's get started!\n\n- [What is RL? A short recap](#what-is-rl-a-short-recap)\n- [The two types of ..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/70_deep_rl_q_part1/policy.jpg..."
          ],
          [
           "> But what does it mean to act according to our policy? After all, we don't have a policy in value-b..."
          ],
          [
           "Consequently, whatever method you use to solve your problem,Â **you will have a policy**, but in the ..."
          ],
          [
           "For each state, the state-value function outputs the expected return if the agentÂ **starts at that s..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/70_deep_rl_q_part1/two-types...."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/70_deep_rl_q_part1/bellman2.j..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/70_deep_rl_q_part1/bellman4.j..."
          ],
          [
           "## **Monte Carlo vs Temporal Difference Learning**\n\nThe last thing we need to talk about before divi..."
          ],
          [
           "- We always start the episodeÂ **at the same starting point.**\n- **The agent takes actions using the ..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/70_deep_rl_q_part1/MC-4p.jpg\"..."
          ],
          [
           "### **Temporal Difference Learning: learning at each step**\n\n- **Temporal difference, on the other h..."
          ],
          [
           "- We just started to train our Value function, so it returns 0 value for each state.\n- Our learning ..."
          ],
          [
           "If we summarize:\n\n- With Monte Carlo, we update the value function from a complete episode, and so w..."
          ],
          [
           "---\nSo thatâ€™s all for today. Congrats on finishing this first part of the chapter! There was a lot o..."
          ],
          [
           "--\ntitle:  Introducing the Hugging Face LLM Inference Container for Amazon SageMaker\nthumbnail: /blo..."
          ],
          [
           "## What is Hugging Face LLM Inference DLC?\n\nHugging Face LLM DLC is a new purpose-built Inference Co..."
          ],
          [
           "Officially supported model architectures are currently:\n\n- [BLOOM](https://huggingface.co/bigscience..."
          ],
          [
           "Let's get started!\n\n## 1. Setup development environment\n\nWe are going to use the `sagemaker` python ..."
          ],
          [
           "```\n\nIf you are going to use Sagemaker in a local environment, you need access to an IAM Role with t..."
          ],
          [
           "```\n\n## 2. Retrieve the new Hugging Face LLM DLC\n\nCompared to deploying regular Hugging Face models,..."
          ],
          [
           "```\n\n## 3. Deploy Open Assistant 12B to Amazon SageMaker\n\n_Note: Quotas for Amazon SageMaker can var..."
          ],
          [
           "```\n\nAfter we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `de..."
          ],
          [
           "```\n\nSageMaker will now create our endpoint and deploy the model to it. This can take 5-10 minutes.\n..."
          ],
          [
           "You can find the open api specification of TGI in the [swagger documentation](https://huggingface.gi..."
          ],
          [
           "```\n<|prompter|>[Instruction]<|endoftext|>\n<|assistant|>\n```\n\nlets give it a first try and ask about..."
          ],
          [
           "```\n\n## 5. Create Gradio Chatbot backed by Amazon SageMaker\n\nWe can also create a gradio application..."
          ],
          [
           "```\n\n```python\nimport gradio as gr\n\n# hyperparameters for llm\nparameters = {\n    \"do_sample\": True,\n..."
          ],
          [
           "```\n\n![Gradio Chat application](assets/145_sagemaker-huggingface-llm/gradio.png \"Gradio Chat applica..."
          ],
          [
           "--\ntitle: \"Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face\"\nthumbnail:..."
          ],
          [
           "Throughout the project, Fetch had weekly calls with the AWS team and received support from a subject..."
          ],
          [
           "Fetch heavily relied on the ML training features of Amazon SageMaker, particularly its [training job..."
          ],
          [
           "In addition to its custom ML models, Fetch uses [AWS Deep Learning Containers ](https://aws.amazon.c..."
          ],
          [
           "Users enjoy the updates too; Fetch has grown from 10 million to 18 million monthly active users sinc..."
          ],
          [
           "--\ntitle: \"Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator\"\nthumbnail: ..."
          ],
          [
           "Such large models raise new challenges in terms of memory and speed for both [training](https://hugg..."
          ],
          [
           "Moreover, support for [HPU graphs](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inf..."
          ],
          [
           "## Benchmarks\n\nIn this section, we are going to provide an early benchmark of BLOOMZ on Gaudi2, firs..."
          ],
          [
           "Runs were performed with DeepSpeed-inference in 16-bit precision with 8 devices and using a [key-val..."
          ],
          [
           "*Update: the numbers above were updated with the releases of Optimum Habana 1.6 and SynapseAI 1.10, ..."
          ],
          [
           "### Running inference on a complete dataset\n\nThe script we wrote enables using your model to complet..."
          ],
          [
           "```\nBatch nÂ°1\nInput: ['Facebook has released a report that shows what content was most widely viewed..."
          ],
          [
           "Batch nÂ°3\nInput: ['A SpaceX Starship rocket prototype has exploded during a pressure test. It was']\n..."
          ],
          [
           "Batch nÂ°5\nInput: ['With the rise of cheap small \"Cube Satellites\", startups are now']\nOutput: ['With..."
          ],
          [
           "```\n\nIn the next section, we explain how to use the script we wrote to perform this benchmark or to ..."
          ],
          [
           "```\n\nFor multi-node inference, you can follow [this guide](https://huggingface.co/docs/optimum/haban..."
          ],
          [
           "## Conclusion\n\nWe see in this article that **Habana Gaudi2 performs BLOOMZ inference faster than Nvi..."
          ],
          [
           "---\n\nThanks for reading! If you have any questions, feel free to contact me, either through [Github]..."
          ],
          [
           "--\ntitle: Deploying ðŸ¤— ViT on Kubernetes with TF Serving\nthumbnail: /blog/assets/94_tf_serving_kubern..."
          ],
          [
           "- **Containerizing the application logic**: The application logic\n  involves a served model that can..."
          ],
          [
           "**Note**: The code snippets shown in this post can be executed on a Unix terminal\nas long as you hav..."
          ],
          [
           "```bash\n$ MODEL_TAR=model.tar.gz\n$ MODEL_NAME=hf-vit\n$ MODEL_VERSION=1\n$ MODEL_PATH=models/$MODEL_NA..."
          ],
          [
           "```\n\nBelow, we show how the `models` directory is structured in our case:\n\n```bash\n$ find /models\n/m..."
          ],
          [
           "```\n\nWe used the official Docker image of TensorFlow Serving as the base, but\nyou can use ones that ..."
          ],
          [
           "```\n\n## Running the Docker image locally\n\nLastly, you can run the newly built Docker image locally t..."
          ],
          [
           "```\n\nSince weâ€™re using GCR, you need to prefix the\nDocker image tag ([<u>note</u>](https://cloud.goo..."
          ],
          [
           "```\n\nGCP offers a variety of machine types to configure the deployment in a\nway you want. We encoura..."
          ],
          [
           "```\n\nThe `gcloud container clusters get-credentials` command takes care of\nboth connecting to the cl..."
          ],
          [
           "Next, we go through the important parts of each of these manifests.\n\n**`deployment.yaml`**:\n\n```yaml..."
          ],
          [
           "```\n\nYou can configure the names like `tfs-server`, `tfs-k8s` any way you\nwant. Under `containers`, ..."
          ],
          [
           "```\n\nWe made the service type â€˜LoadBalancerâ€™ so the endpoints are\nexposed externally to the Kubernet..."
          ],
          [
           "```\n\nHPA stands for **H**orizontal **P**od **A**utoscaler. It sets criteria\nto decide when to scale ..."
          ],
          [
           "```bash\n$ kubectl apply -f deployment.yaml\n$ kubectl apply -f service.yaml\n$ kubectl apply -f hpa.ya..."
          ],
          [
           "```\n\nWhile using `kubectl` is fine for applying each of the manifests to\nperform the deployment, it ..."
          ],
          [
           "```\n\nNote down the external IP when it becomes available.\n\nAnd that sums up all the steps you need t..."
          ],
          [
           "```\n\nIf youâ€™re interested to know how this deployment would perform if it\nmeets more traffic then we..."
          ],
          [
           "# Conclusion\n\nIn this post and the associated [repository](https://github.com/sayakpaul/deploy-hf-tf..."
          ],
          [
           "--\ntitle: 'Welcome Stable-baselines3 to the Hugging Face Hub ðŸ¤—'\nthumbnail: /blog/assets/47_sb3/thumb..."
          ],
          [
           "```\n\n### Finding Models\n\nWeâ€™re currently uploading saved models of agents playing Space Invaders, Br..."
          ],
          [
           "```\n\n### Sharing a model to the Hub\nIn just a minute, you can get your saved model in the Hub.\n\nFirs..."
          ],
          [
           "```\nTry it out and share your models with the community!\n\n### What's next?\n\nIn the coming weeks and ..."
          ],
          [
           "And we would love to hear your feedback ðŸ’–. ðŸ“§ Feel free to [reach us](mailto:thomas.simonini@huggingf..."
          ],
          [
           "--\ntitle: \"Director of Machine Learning Insights [Part 2: SaaS Edition]\"\nthumbnail: /blog/assets/67_..."
          ],
          [
           "### [Omar Rahman](https://www.linkedin.com/in/omar-rahman-4739713a/) - Director of Machine Learning ..."
          ],
          [
           "b. In most large organizations, data is often siloed and not well maintained resulting in significan..."
          ],
          [
           "Prior to Amplitude, Cao (Danica) was the Global Head of Machine Learning in the Analytics Center of ..."
          ],
          [
           "<img class=\"mx-auto\" style=\"float: left;\" padding=\"5px\" width=\"200\" src=\"/blog/assets/67_ml_director..."
          ],
          [
           "Once we transcribe a conversation we can look into the content - this is where NLP comes in and we r..."
          ],
          [
           "#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into SaaS?**\nIs my solu..."
          ],
          [
           "We are also seeing a lot of tech from NLP entering other domains like speech and vision and being ab..."
          ],
          [
           "**Fun Fact:**  The first application of ML I used was for Barbie toys. My professor at Schulich Busi..."
          ],
          [
           "#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML?**\nThe most common mist..."
          ],
          [
           "---\n\nðŸ¤—   Thank you for joining us in this second installment of ML Director Insights. Stay tuned for..."
          ],
          [
           "--\ntitle: \"From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community\" \nthumbnail: ..."
          ],
          [
           "Thanks to the concurrency and distribution support in the Erlang Virtual Machine, which Elixir runs ..."
          ],
          [
           "Several other projects were born from the Nx initiative. [Axon](https://github.com/elixir-nx/axon) b..."
          ],
          [
           "* We have also written [single-file Phoenix applications](https://github.com/elixir-nx/bumblebee/tre..."
          ],
          [
           "--\ntitle: \"ðŸ¶Safetensors audited as really safe and becoming the default\"\nthumbnail: /blog/assets/142..."
          ],
          [
           "```\n\nIt also has a number of [cool features](https://github.com/huggingface/safetensors#yet-another-..."
          ],
          [
           "```\n\nis likely to be the only thing needed to run `safetensors` files safely.\n\nGoing forward and tha..."
          ],
          [
           "Since the Hugging Face Hub is a platform where anyone can upload and share models, it is important t..."
          ],
          [
           "In the name of openness and transparency, all companies agreed to make the report\nfully public.\n\n[Fu..."
          ],
          [
           "As for `safetensors` itself, we're looking into adding more advanced features for LLM training,\nwhic..."
          ],
          [
           "--\ntitle: \"Federated Learning using Hugging Face and Flower\" \nthumbnail: /blog/assets/fl-with-flower..."
          ],
          [
           "```\n\n## Standard Hugging Face workflow\n\n### Handling the data\n\nTo fetch the IMDB dataset, we will us..."
          ],
          [
           "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n    tokenized_datasets[\"train..."
          ],
          [
           "```\n\n### Training and testing the model\n\nOnce we have a way of creating our trainloader and testload..."
          ],
          [
           "```\n\n## Federating the example\n\nThe idea behind Federated Learning is to train a model between multi..."
          ],
          [
           "```\n\nThe `get_parameters` function lets the server get the client's parameters. Inversely, the `set_..."
          ],
          [
           "```\n\nThe `weighted_average` function is there to provide a way to aggregate the metrics distributed ..."
          ],
          [
           "--\ntitle: \"Creating open machine learning datasets? Share them on the Hugging Face Hub!\"\nthumbnail: ..."
          ],
          [
           "The Hugging Face Hub can help achieve this maximum impact. \n\n## What is the Hugging Face Hub?\n\nThe [..."
          ],
          [
           "There are a growing number of tools being created which make it easier to understand datasets hosted..."
          ],
          [
           "### Community tools \n\nAlongside the datasets viewer there are a growing number of community created ..."
          ],
          [
           "### Support for large datasets\n\nThe Hub can host large datasets; it currently hosts datasets with mu..."
          ],
          [
           "The Hub also has features which allow communities to collaborate more easily. This includes a discus..."
          ],
          [
           "### How can I share my dataset on the Hugging Face Hub? \n\nHere are some resources to help you get st..."
          ],
          [
           "--\ntitle: \"Assisted Generation: a new direction toward low-latency text generation\"\nthumbnail: /blog..."
          ],
          [
           "<!-- [GIF 1 -- FWD PASS] -->\n<figure class=\"image table text-center m-0 w-full\">\n    <video\n        ..."
          ],
          [
           "From the description above, the latency bottleneck in text generation is clear: running a model forw..."
          ],
          [
           "```python\n# Example showcasing the impact of batched generation. Measurement device: RTX3090\nfrom tr..."
          ],
          [
           "```\n\nFinally, if you have multiple devices available to you, you can distribute the workload using [..."
          ],
          [
           "## Language decoder forward pass, revisited\n\nYouâ€™ve read above that each model forward pass yields t..."
          ],
          [
           "```\n\n\nThis means that you can use a model forward pass for a different purpose: in addition to feedi..."
          ],
          [
           "Obviously, there are no latency-free assistant models. Nevertheless, it is relatively easy to find a..."
          ],
          [
           "Wrapping all up, hereâ€™s our original implementation of the assisted generation loop ([code](https://..."
          ],
          [
           "Weâ€™ve designed the API in ðŸ¤— Transformers such that this process is hassle-free for you. All you need..."
          ],
          [
           "```\n\n\nIs the additional internal complexity worth it? Letâ€™s have a look at the latency numbers for t..."
          ],
          [
           "Drawing samples from a probability distribution for the next token will cause our greedy assistant t..."
          ],
          [
           "Finally, assisted generation resurfaces a crucial question in text generation. The field has been ev..."
          ],
          [
           "```\n\n\n## Acknowledgements\n\nI'd like to thank Sylvain Gugger, Nicolas Patry, and Lewis Tunstall for s..."
          ],
          [
           "--\ntitle: \"Introducing the Private Hub: A New Way to Build With Machine Learning\"\nthumbnail: /blog/a..."
          ],
          [
           "With this in mind, we launched the [Private Hub](https://huggingface.co/platform) (PH), a new way to..."
          ],
          [
           "On the Hugging Face Hub, youâ€™ll be able to create or discover the following ML assets:\n\n- [Models](h..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"..."
          ],
          [
           "Now that we have covered the basics, let's dive into the specific characteristics of models, dataset..."
          ],
          [
           "These models span 180 languages and support up to 25 ML libraries (including Transformers, Keras, sp..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"..."
          ],
          [
           "We provide flexible options for deploying your Private Hub in your private, compliant environment, i..."
          ],
          [
           "We built the Private Hub to change this. Like Git and GitHub forever changed how companies build sof..."
          ],
          [
           "For our demo example, one of the requirements for building this ML app for financial analysts is doi..."
          ],
          [
           "Now that we have a great pre-trained model for financial data, the next step is to fine-tune it usin..."
          ],
          [
           "Finally, we select the number of candidate models to train with our data. We choose 25 models and vo..."
          ],
          [
           "In less than 20 minutes, we were able to build an [interactive demo app](https://huggingface.co/spac..."
          ],
          [
           "Instead of wasting time on Docker/Kubernetes, setting up a server for running these models or optimi..."
          ],
          [
           "```\n\nWith just 12 lines of code, we are up and running in running inferences with an infrastructure ..."
          ],
          [
           "--\ntitle: \"Speculative Decoding for 2x Faster Whisper Inference\" \nthumbnail: /blog/assets/whisper-sp..."
          ],
          [
           "In this blog post, we demonstrate how Speculative Decoding can be employed to reduce the \ninference ..."
          ],
          [
           "While these candidate tokens are generated quickly, they may differ from those predicted by the main..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n    <video\n        style=\"max-width: 90%; margin..."
          ],
          [
           "The only constraint for selecting an assistant model is that it must share the same vocabulary as th..."
          ],
          [
           "## English Speech Transcription\n\n### Baseline Implementation\n\nWe start by benchmarking Whisper [larg..."
          ],
          [
           "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda...."
          ],
          [
           "```\n\nLet's load the English speech transcription dataset that we will use for benchmarking. We'll lo..."
          ],
          [
           "```\n\n**Output:**\n```\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [01:37<00:00,  1.33s/it]\n72.99542546272278\n```\n\nAlright!..."
          ],
          [
           "```\n**Output:**\n0.03507271171941831\n```\n\nOur final baseline number is 73 seconds for a WER of 3.5%.\n..."
          ],
          [
           "```\n\n------------------------------------------------------------------------\n\n\\\\({}^1\\\\) We intend ..."
          ],
          [
           "```\n\nLet's run the benchmark with speculative decoding, using Distil-Whisper as the assistant to Whi..."
          ],
          [
           "```\n**Outputs:**\n```\n0.03507271171941831\n```\n\nPerfect! 3.5% WER again, as we have identical outputs ..."
          ],
          [
           "```\n\nAn end-to-end code snippet for running speculative decoding with Whisper and Distil-Whisper can..."
          ],
          [
           "```\n\nFor our benchmarking dataset, we'll load 73 samples from the Dutch (\"nl\") split of the [VoxPopu..."
          ],
          [
           "```\n\nRight! We have our baseline time of 117 seconds and a WER of 12.8%. Let's re-run the generation..."
          ],
          [
           "```\n\nAgain, we achieve 12.8% WER, but this time in just 62 seconds of inference time, representing a..."
          ],
          [
           "#### Batch Size\n\nIt is worth noting that the largest speed gains with speculative decoding come with..."
          ],
          [
           "--\ntitle: \"Snorkel AI x Hugging Face: unlock foundation models for enterprises\"\nthumbnail: /blog/ass..."
          ],
          [
           "## Foundation models in Snorkel Flow\n\nThe Snorkel Flow development platform enables users to [adapt ..."
          ],
          [
           "Hugging Faceâ€™s service allows users to create a model API in a few clicks and begin using it immedia..."
          ],
          [
           "Clement Delangue, co-founder and CEO, Hugging Face\n\n## Conclusion\n\nTogether, Snorkel and Hugging Fac..."
          ],
          [
           "--\ntitle: \"An Introduction to Q-Learning Part 2/2\"\nthumbnail: /blog/assets/73_deep_rl_q_part2/thumbn..."
          ],
          [
           "So, in the second part, weâ€™ll **study Q-Learning**, **and implement our first RL agent from scratch*..."
          ],
          [
           "**Q-Learning is the algorithm we use to train our Q-Function**, anÂ **action-value function**Â that de..."
          ],
          [
           "Therefore, Q-function contains a Q-tableÂ **that has the value of each-state action pair.**Â And given..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/73_deep_rl_q_part2/Q-learning..."
          ],
          [
           "Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.\n\nThe idea i..."
          ],
          [
           "Therefore, our \\\\(Q(S_t, A_t)\\\\)Â **update formula goes like this:**\n\n  <figure class=\"image table te..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img src=\"assets/73_deep_rl_q_part2/off-on-1.j..."
          ],
          [
           "- You're a mouse in this tiny maze. You alwaysÂ **start at the same starting point.**\n- The goal isÂ *..."
          ],
          [
           "So, for now,Â **our Q-Table is useless**; we needÂ **to train our Q-function using the Q-Learning algo..."
          ],
          [
           "Training timestep 2:\n\n**Step 2: Choose action using Epsilon Greedy Strategy**\n\n**I take a random act..."
          ],
          [
           "---\nNow that we **studied the theory of Q-Learning**, let's **implement it from scratch**. A Q-Learn..."
          ],
          [
           "Take time to really grasp the material before continuing. \n\n  \nAnd since the best way to learn and a..."
          ],
          [
           "--\ntitle: \"Overview of natively supported quantization schemes in ðŸ¤— Transformers\" \nthumbnail: /blog/..."
          ],
          [
           "## Resources\n\n- [GPTQ blogpost](https://huggingface.co/blog/gptq-integration) â€“ gives an overview on..."
          ],
          [
           "### What are the benefits of bitsandbytes?\n**easy**: bitsandbytes still remains the easiest way to q..."
          ],
          [
           "**n-bit support**: The GPTQ algorithm makes it possible to quantize models up to 2 bits! However, th..."
          ],
          [
           "**works only for language models (for now)**: As of today, the API for quantizing a model with auto-..."
          ],
          [
           "with batch size = 1: \n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token laten..."
          ],
          [
           "with batch size = 16:\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token laten..."
          ],
          [
           "#### use_cache \nLet's test `use_cache` to better understand the impact of caching the hidden state d..."
          ],
          [
           "with a NVIDIA T4: \n\n![Benchmark T4](https://huggingface.co/datasets/huggingface/documentation-images..."
          ],
          [
           "From the benchmark above, we can conclude that GPTQ is faster than bitsandbytes independently of the..."
          ],
          [
           "with 7b model: \n\n| model_id                           | Average | ARC   | Hellaswag | MMLU  | Truthf..."
          ],
          [
           "with 13b model: \n\n| model_id                           | Average | ARC   | Hellaswag | MMLU  | Truth..."
          ],
          [
           "- (1) quantize the base model using bitsandbytes (zero-shot quantization)\n- (2) add and fine-tune th..."
          ],
          [
           "--\ntitle: 'Train and Fine-Tune Sentence Transformers Models'\nthumbnail: /blog/assets/95_training_st_..."
          ],
          [
           "This is how the Sentence Transformers models work:\n\n1. **Layer 1** â€“ The input text is passed throug..."
          ],
          [
           "```\n\nFrom the code above, you can see that Sentence Transformers models are made up of modules, that..."
          ],
          [
           "```\n\nNow for the most critical part: the dataset format.\n\n## How to prepare your dataset for trainin..."
          ],
          [
           "Most dataset configurations will take one of four forms (below you will see examples of each case):\n..."
          ],
          [
           "Note that Sentence Transformers models can be trained with human labeling (cases 1 and 3) or with la..."
          ],
          [
           "- Case 4: The [Quora Triplets dataset](https://huggingface.co/datasets/embedding-data/QQP_triplets) ..."
          ],
          [
           "```\n\nThis guide uses an unlabeled triplets dataset, the fourth case above.\n\nWith the `datasets` libr..."
          ],
          [
           "```\nYou can see that `query` (the anchor) has a single sentence, `pos` (positive) is a list of sente..."
          ],
          [
           "```\n\nThe next step is to choose a suitable loss function that can be used with the data format.\n\n## ..."
          ],
          [
           "Case 3: When your samples are triplets of the form `[anchor, positive, negative]` and you have an in..."
          ],
          [
           "The hardest part is choosing a suitable loss function conceptually. In the code, there are only two ..."
          ],
          [
           "```\nOnce the dataset is in the desired format and a suitable loss function is in place, fitting and ..."
          ],
          [
           "```\n\nThen, you can share your models by calling the `save_to_hub` method from the trained model. By ..."
          ],
          [
           "```\n\nIn the [Notebook Companion](https://colab.research.google.com/github/huggingface/blog/blob/main..."
          ],
          [
           "--\ntitle: \"SDXL in 4 steps with Latent Consistency LoRAs\"\nthumbnail: /blog/assets/lcm_sdxl/lcm_thumb..."
          ],
          [
           "## Contents\n\n- [Method Overview](#method-overview)\n- [Why does this matter](#why-does-this-matter)\n-..."
          ],
          [
           "1. Select an available teacher model from the Hub. For example, you can use [SDXL (base)](https://hu..."
          ],
          [
           "To gauge the speed difference we are talking about, generating a single 1024x1024 image on an M1 Mac..."
          ],
          [
           "```\n\nNote how the code:\n- Instantiates a standard diffusion pipeline with the SDXL 1.0 base model.\n-..."
          ],
          [
           "```\n\nThese are the 8 images displayed in a grid:\n\n<p align=\"center\">\n    <img src=\"https://huggingfa..."
          ],
          [
           "```\n\nThen we can run inference as usual for SDXL. Weâ€™ll gather results using varying number of steps..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "prompt = \"collage style kid sits looking at the night sky, full of stars\"\n\ngenerator = torch.Generat..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "## Benchmarks\n\nThis section is not meant to be exhaustive, but illustrative of the generation speed ..."
          ],
          [
           "For cards with a lot of capacity, such as A100, performance increases significantly when generating ..."
          ],
          [
           "- [`latent-consistency/lcm-sdxl`](https://huggingface.co/latent-consistency/lcm-sdxl). Full fine-tun..."
          ],
          [
           "pipe.set_adapters([\"lora\", \"toy\"], adapter_weights=[1.0, 0.8])\npipe.to(device=\"cuda\", dtype=torch.fl..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "We hope these scripts inspire the community to try their own fine-tunes. Please, do let us know if y..."
          ],
          [
           "- [LoRA the Explorer (experimental LCM version)](https://huggingface.co/spaces/latent-consistency/lc..."
          ],
          [
           "--\ntitle: \"Introducing Skops\"\nthumbnail: /blog/assets/94_skops/introducing_skops.png\nauthors:\n- user..."
          ],
          [
           "```\n\nYou can use any model filename and serialization method, like `pickle` or `joblib`. At the mome..."
          ],
          [
           "```\n\nThe repository now contains the serialized model and the configuration file. \nThe configuration..."
          ],
          [
           "You can create the model card by instantiating the `Card` class from `skops`. During model serializa..."
          ],
          [
           "```\n\nWe will now evaluate the model and add a description of the evaluation method with `add`. The m..."
          ],
          [
           "```\n\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_..."
          ],
          [
           "```\n\nYou can see the example repository pushed with above code [here](https://huggingface.co/scikit-..."
          ],
          [
           "--\ntitle: \"Run a Chatgpt-like Chatbot on a Single GPU with ROCm\" \nthumbnail: /blog/assets/chatbot-am..."
          ],
          [
           "<p align=\"center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/reso..."
          ],
          [
           "<p align=\"center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/reso..."
          ],
          [
           "- A Linux-based operating system, preferably Ubuntu 18.04 or 20.04\n\n- Conda or Docker environment\n\n-..."
          ],
          [
           "```\nsudo apt update && sudo apt upgrade -y\nwget https://repo.radeon.com/amdgpu-install/5.4.3/ubuntu/..."
          ],
          [
           "```\n**2 Model** **quantization and Model inference (Inside the docker)**\n\nYou can either download qu..."
          ],
          [
           "```\nNow that you have everything set up, it's time to run the Vicuna 13B\nmodel on your AMD GPU. Use ..."
          ],
          [
           "```\nNow the 4-bit quantized Vicuna-13B model can be fitted in RX6900XT GPU\nDDR memory, which has 16G..."
          ],
          [
           "<p align=\"center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/reso..."
          ],
          [
           "- Vicuna 13b â€“ quant (4bit/fp16): 4bits datatype parameter, fp16 Matmul\n\n<p align=\"center\">\n  <img s..."
          ],
          [
           "**Building Vicuna quantized model from the floating-point LLaMA model**\n\n**a. Download LLaMA and Vic..."
          ],
          [
           "```\ngit clone https://github.com/lm-sys/FastChat\ncd FastChat\n```\nConvert the LLaMA parameters by usi..."
          ],
          [
           "```\nNow the model is ready and saved as\n**Vicuna-13b-4bit-act-order.safetensors**.\n\n**GPTQ Dequantiz..."
          ],
          [
           "--\ntitle: \"Zero-shot image-to-text generation with BLIP-2\" \nthumbnail: /blog/assets/blip-2/thumbnail..."
          ],
          [
           "## Introduction\n\nRecent years have seen rapid advancements in computer vision and natural language p..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "In the second pre-training stage, the query embeddings now have the relevant visual information to t..."
          ],
          [
           "```\n\nNext, we'll need an input image. Every week The New Yorker runs a [cartoon captioning contest](..."
          ],
          [
           "```\n\nNotice that BLIP-2 is a rare case where you cannot load the model with Auto API (e.g. AutoModel..."
          ],
          [
           "```\n\n```\n\"two monsters sitting around a campfire\"\n```\n\n```\nprompt = \"they look like they are\"\n\ninput..."
          ],
          [
           "```\ncontext = [\n   (\"What is a dinosaur holding?\", \"a torch\"),\n   (\"Where are they?\", \"In the woods...."
          ],
          [
           "```\n\n```\nTo light a fire.\n```\n\n## Conclusion\n\nBLIP-2 is a zero-shot visual-language model that can b..."
          ],
          [
           "--\ntitle: \"Scaling-up BERT Inference on CPU (Part 1)\"\nthumbnail: /blog/assets/21_bert_cpu_scaling_pa..."
          ],
          [
           "This blog post is the first part of a series which will cover most of the hardware and software opti..."
          ],
          [
           "These two metrics will help us understand the benefits and tradeoffs along this blog post.\n\nThe benc..."
          ],
          [
           "## 3. Baselines\n\nAll the results below were run on [Amazon Web Services (AWS) c5.metal instance](htt..."
          ],
          [
           "<br>\n<figure class=\"image\">\n  <img alt=\"pytorch versus tensorflow out of the box bigger batch sizes\"..."
          ],
          [
           "For the remainder of this blog post we will focus on the latter, also known as **Multiple Inference ..."
          ],
          [
           "The figure 3. above simplifies the situation by assuming single core setup. If you want some more de..."
          ],
          [
           "```shell\nubuntu@some-ec2-machine:~$ lscpu\nArchitecture:                    x86_64\nCPU op-mode(s):   ..."
          ],
          [
           "```\n\nIn our case we have a machine with **2 sockets**, each socket providing **24 physical cores** w..."
          ],
          [
           "_Note: Setting both cores and memory affinities is important here. Having computations done on socke..."
          ],
          [
           "```\n\nThen we specify the core and memory affinity through `numactl` using all the **physical** cores..."
          ],
          [
           "```\n\n<br>\n<figure class=\"image\">\n  <img class=\"centered\" alt=\"htop CPU usage without and with numact..."
          ],
          [
           "As we are targeting just 1 thread per physical core, as explained earlier, we pick only thread 0 on ..."
          ],
          [
           "<br>\n<figure class=\"image\">\n  <img alt=\"\" src=\"assets/21_bert_cpu_scaling_part_1/imgs/core_count_sca..."
          ],
          [
           "### 7.1. How-to allocate multiple independent instances\n\nLet's start simple, if we want to spawn 2 i..."
          ],
          [
           "```\n\nStarting from here, each instance does not share any resource with the other, and everything is..."
          ],
          [
           "```\n\nThe outcomes remain the same, our 4 instances are effectively running in a truly parallel manne..."
          ],
          [
           "```\n\n\n## 8. Batch size scaling - Improving throughput and latency with multiple parallel & independe..."
          ],
          [
           "Also, it is important to notice the results might look totally different on another system _(i.e. Op..."
          ],
          [
           "<figure class=\"image\">\n  <img alt=\"Batch scaling experiment for PyTorch and Tensorflow\" src=\"assets/..."
          ],
          [
           "Last but not least, many of the knobs discussed along this blog post can be automatically tuned thro..."
          ],
          [
           "1. [Benchmarking Transformers: PyTorch and TensorFlow](https://medium.com/huggingface/benchmarking-t..."
          ],
          [
           "12. [IntelÂ® Hyper-Threading Technology - Technical User Guide](http://www.cslab.ece.ntua.gr/courses/..."
          ],
          [
           "--\ntitle: \"Welcome PaddlePaddle to the Hugging Face Hub\" \nthumbnail: /blog/assets/126_paddlepaddle/t..."
          ],
          [
           "![thumbnail](assets/126_paddlepaddle/thumbnail.jpg)\n\n**With [PaddleNLP](https://huggingface.co/docs/..."
          ],
          [
           "You are also welcome to check out the [PaddlePaddle](https://huggingface.co/PaddlePaddle) org on the..."
          ],
          [
           "```python\nfrom paddlenlp.transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTo..."
          ],
          [
           "```\n\n## Conclusion\n\nPaddlePaddle is an open source Deep Learning platform that originated from indus..."
          ],
          [
           "--\ntitle: Image Similarity with Hugging Face Datasets and Transformers\nthumbnail: /blog/assets/image..."
          ],
          [
           "Also, the approach presented in the post can potentially be extended to other modalities as well.\n\nT..."
          ],
          [
           "```py\nfrom transformers import AutoImageProcessor, AutoModel\n\n\nmodel_ckpt = \"nateraw/vit-base-beans\"..."
          ],
          [
           "```\n\nIn this case, the checkpoint was obtained by fine-tuning a [Vision Transformer based model](htt..."
          ],
          [
           "```\n\nThis is how a single sample from the training split looks like:\n\n<div align=\"center\">\n    <img ..."
          ],
          [
           "```\n\n## The process of finding similar images\n\nBelow, you can find a pictorial overview of the proce..."
          ],
          [
           "```\n\nAnd we can map `extract_embeddings()` like so:\n\n```py\ndevice = \"cuda\" if torch.cuda.is_availabl..."
          ],
          [
           "```\n\nWe'll use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to compute the s..."
          ],
          [
           "```\n\n## Perform a query\n\nGiven all the utilities, we're equipped to do a similarity search. Let's ha..."
          ],
          [
           "```\n\nSeems like our system got the right set of similar images. When visualized, we'd get:\n\n<div ali..."
          ],
          [
           "```\n\nOnce the index is built, `dataset_with_embeddings` can be used to retrieve the nearest examples..."
          ],
          [
           "```\n\nThe method returns scores and corresponding candidate examples. To know more, you can check out..."
          ],
          [
           "--\ntitle: \"Japanese Stable Diffusion\" \nthumbnail: /blog/assets/106_japanese_stable_diffusion/jsd_thu..."
          ],
          [
           "[rinna Co., Ltd](https://rinna.co.jp/). has developed a Japanese-specific text-to-image model named ..."
          ],
          [
           "<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/ma..."
          ],
          [
           "- Generate Japanese-style images\n- Understand Japanese words adapted from English\n- Understand Japan..."
          ],
          [
           "#### 1st stage: Train a Japanese-specific text encoder \nIn the 1st stage, the latent diffusion model..."
          ],
          [
           "```\n\nOn the other hand, by using our Japanese tokenizer, the prompt is split into interpretable toke..."
          ],
          [
           "```\n\nThis stage enables the model to understand Japanese prompts but does not still output Japanese-..."
          ],
          [
           "*\"ã‚µãƒ©ãƒªãƒ¼ãƒžãƒ³ æ²¹çµµ\", which means exactly \"salary man, oil painting\", from the 2nd-stage Japanese Stable Dif..."
          ],
          [
           "--\ntitle: \"Fine-Tune Wav2Vec2 for English ASR in Hugging Face with ðŸ¤— Transformers\"\nthumbnail: /blog/..."
          ],
          [
           "For the first time, it has been shown that pretraining, followed by\nfine-tuning on very little label..."
          ],
          [
           "I highly recommend reading the blog post [Sequence Modeling with CTC\n(2017)](https://distill.pub/201..."
          ],
          [
           "```\n\nNext we strongly suggest to upload your training checkpoints directly to the [Hugging Face Hub]..."
          ],
          [
           "```\n\n------------------------------------------------------------------------\n\n\\\\({}^1\\\\) Timit is u..."
          ],
          [
           "Let\\'s start by creating the tokenizer responsible for decoding the\nmodel\\'s predictions.\n\n### Creat..."
          ],
          [
           "```\n\n**Print Output:**\n```bash\n    DatasetDict({\n        train: Dataset({\n            features: ['fi..."
          ],
          [
           "```\n\nLet\\'s write a short function to display some random samples of the\ndataset and run it a couple..."
          ],
          [
           "```\n\n**Print Output:**\n\n| Idx |  Transcription     |\n|----------|:-------------:|\n|\t1  | Who took th..."
          ],
          [
           "```\n\nLet's take a look at the preprocessed transcriptions.\n\n```python\nshow_random_elements(timit[\"tr..."
          ],
          [
           "```\n\nNow, we create the union of all distinct letters in the training dataset\nand test dataset and c..."
          ],
          [
           "```\n\nCool, we see that all letters of the alphabet occur in the dataset\n(which is not really surpris..."
          ],
          [
           "```\n\nIn a final step, we use the json file to instantiate an object of the\n`Wav2Vec2CTCTokenizer` cl..."
          ],
          [
           "```\n\nGreat, you can see the just created repository under `https://huggingface.co/<your-username>/wa..."
          ],
          [
           "A Wav2Vec2 feature extractor object requires the following parameters to\nbe instantiated:\n\n-   `feat..."
          ],
          [
           "```\n\nGreat, Wav2Vec2\\'s feature extraction pipeline is thereby fully defined!\n\nTo make the usage of ..."
          ],
          [
           "```\n\n**Print Output:**\n```bash\n{'array': array([-2.1362305e-04,  6.1035156e-05,  3.0517578e-05, ...,..."
          ],
          [
           "```\n\nIt can be heard, that the speakers change along with their speaking rate, accent, etc. Overall,..."
          ],
          [
           "```\n\nGood! Everything looks fine - the data is a 1-dimensional array, the\nsampling rate always corre..."
          ],
          [
           "```\n\nLet's apply the data preparation function to all examples.\n\n```python\ntimit = timit.map(prepare..."
          ],
          [
           "```\n\n**Note**: Currently `datasets` make use of [`torchaudio`](https://pytorch.org/audio/stable/inde..."
          ],
          [
           "After having fine-tuned the model, we will correctly evaluate it on the\ntest data and verify that it..."
          ],
          [
           "```python\nimport torch\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List,..."
          ],
          [
           "processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = No..."
          ],
          [
           "```\n\nLet's initialize the data collator.\n\n```python\ndata_collator = DataCollatorCTCWithPadding(proce..."
          ],
          [
           "```\n\nThe model will return a sequence of logit vectors:\n\n$$ \\mathbf{y}_1, \\ldots, \\mathbf{y}_m $$, \n..."
          ],
          [
           "```\n\nNow, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer\\'s\n`pad_token_id` must be ..."
          ],
          [
           "```\n\nIn a final step, we define all parameters related to training. To give\nmore explanation on some..."
          ],
          [
           "```\n\n------------------------------------------------------------------------\n\n\\\\({}^1\\\\) To allow m..."
          ],
          [
           "In case you want to use this google colab to fine-tune your model, you\nshould make sure that your tr..."
          ],
          [
           "```\n\n```python\ntrainer.train()..."
          ],
          [
           "```\n\nDepending on your GPU, it might be possible that you are seeing an `\"out-of-memory\"` error here..."
          ],
          [
           "You can now upload the result of the training to the Hub, just execute this instruction:\n\n```python\n..."
          ],
          [
           "```\n\nYou can now share this model with all your friends, family, favorite pets: they can all load it..."
          ],
          [
           "```\n\nNow, we will make use of the `map(...)` function to predict the\ntranscription of every test sam..."
          ],
          [
           "```\n\n**Print Output:**\n```bash\n    Test WER: 0.221\n```\n\n22.1% WER - not bad! Our demo model would ha..."
          ],
          [
           "```\n\n| pred_str |  target_text |\n|----------|:-------------:|\n| am to balence your employe you benef..."
          ],
          [
           "pred_ids = torch.argmax(logits, dim=-1)\n\n# convert ids to tokens\n\" \".join(processor.tokenizer.conver..."
          ],
          [
           "```\n\n**Print Output:**\n```bash\n[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] t t h e e | | b b [PAD] u u n n n..."
          ],
          [
           "--\ntitle: \"Hugging Face and AWS partner to make AI more accessible\" \nthumbnail: /blog/assets/131_aws..."
          ],
          [
           "There have been significant advances in new Transformer and Diffuser machine learning models that pr..."
          ],
          [
           "## Collaborating to scale AI in the cloud\n\nThis expanded strategic partnership enables Hugging Face ..."
          ],
          [
           "--\ntitle: \"Deploy Livebook notebooks as apps to Hugging Face Spaces\"\nthumbnail: /blog/assets/120_eli..."
          ],
          [
           "If you don't know Livebook yet, it is an open-source tool for writing interactive code notebooks in ..."
          ],
          [
           "## Your turn\n\nWe hope this new integration between Livebook and Hugging Face empowers even more peop..."
          ],
          [
           "--\ntitle: \"Getting Started with Sentiment Analysis using Python\"\nthumbnail: /blog/assets/50_sentimen..."
          ],
          [
           "- *\"dear @verizonsupport your service is straight ðŸ’© in dallas.. been with yâ€™all over a decade and th..."
          ],
          [
           "There are more than [215 sentiment analysis models](https://huggingface.co/models?pipeline_tag=text-..."
          ],
          [
           "```\n\nThis code snippet uses the [pipeline class](https://huggingface.co/docs/transformers/main_class..."
          ],
          [
           "```\n\nYou can test these models with your own data using this [Colab notebook](https://colab.research..."
          ],
          [
           "Are you interested in doing sentiment analysis in languages such as Spanish, French, Italian or Germ..."
          ],
          [
           "### a. Fine-tuning model with Python\n\nIn this tutorial, you'll use the IMDB dataset to fine-tune a D..."
          ],
          [
           "```\n\nThen, install the libraries you will be using in this tutorial:\n\n```python\n!pip install dataset..."
          ],
          [
           "```\n\nNext, you will prepare the text inputs for the model for both splits of our dataset (training a..."
          ],
          [
           "```\n\nThen, let's define the metrics you will be using to evaluate how good is your fine-tuned model ..."
          ],
          [
           "```\n\nYou are almost there! Before training our model, you need to define the training arguments and ..."
          ],
          [
           "```\n\nIn our case, we got 88% accuracy and 89% f1 score. Quite good for a sentiment analysis model ju..."
          ],
          [
           "```\n\nIn the IMDB dataset, `Label 1` means positive and `Label 0` is negative. Quite good! ðŸ”¥\n\n\n### b...."
          ],
          [
           "Next, let's create a [new project on AutoNLP](https://ui.autonlp.huggingface.co/new) to train 5 cand..."
          ],
          [
           "After a few minutes, AutoNLP has trained all models, showing the performance metrics for all of them..."
          ],
          [
           "```\n!pip install -q transformers tweepy wordcloud matplotlib\n```\n\n### 2. Set up Twitter API credenti..."
          ],
          [
           "```\n\n### 3. Search for tweets using Tweepy\nAt this point, you are ready to start using the Twitter A..."
          ],
          [
           "```\n\n\n### 4. Run sentiment analysis on the tweets\nNow you can put our new skills to work and run sen..."
          ],
          [
           "```\n\nOutput:\n\n```\nTweet: @NFTGalIery Warm, exquisite and elegant palette of charming beauty Its pric..."
          ],
          [
           "```\n\nInterestingly, most of the tweets about NFTs are positive (56.1%) and almost none are negative ..."
          ],
          [
           "Finally, let's see what words stand out for each sentiment by creating a word cloud:\n\n```python\nfrom..."
          ],
          [
           "```\n\nSome of the words associated with positive tweets include Discord, Ethereum, Join, Mars4 and Sh..."
          ],
          [
           "Do you want to train a custom model for sentiment analysis with your own data? Easy peasy! You can f..."
          ],
          [
           "a href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whispe..."
          ],
          [
           "When scaled to 680,000 hours of labelled pre-training data, Whisper models \ndemonstrate a strong abi..."
          ],
          [
           "For demonstration purposes, we'll fine-tune the multilingual version of the \n[`\"small\"`](https://hug..."
          ],
          [
           "```\n\nNext, we need to update the Unix package `ffmpeg` to version 4:\n\n\n```python\n!add-apt-repository..."
          ],
          [
           "```\n\n## Load Dataset\n\nUsing ðŸ¤— Datasets, downloading and preparing data is extremely simple. \nWe can ..."
          ],
          [
           "```\n\n## Prepare Feature Extractor, Tokenizer and Data\n\nThe ASR pipeline can be de-composed into thre..."
          ],
          [
           "We'll load the feature extractor from the pre-trained checkpoint with the default values:\n\n\n```pytho..."
          ],
          [
           "```\n\n### Load WhisperTokenizer\n\nThe Whisper model outputs a sequence of _token ids_. The tokenizer m..."
          ],
          [
           "```\n\n### Prepare Data\n\nLet's print the first example of the Common Voice dataset to see \nwhat form t..."
          ],
          [
           "```\n\nRe-loading the first audio sample in the Common Voice dataset will resample \nit to the desired ..."
          ],
          [
           "```\n\n## Training and Evaluation\n\nNow that we've prepared our data, we're ready to dive into the trai..."
          ],
          [
           "The `labels` on the other hand are un-padded. We first pad the sequences\nto the maximum length in th..."
          ],
          [
           "batch[\"labels\"] = labels\n\n        return batch..."
          ],
          [
           "```\n\n###Â Load a Pre-Trained Checkpoint\n\nNow let's load the pre-trained Whisper `small` checkpoint. A..."
          ],
          [
           "```\n\n### Define the Training Configuration\n\nIn the final step, we define all the parameters related ..."
          ],
          [
           "```\n\n### Training\n\nTraining will take approximately 5-10 hours depending on your GPU or the one \nall..."
          ],
          [
           "```\n\nOur best WER is 32.0% - not bad for 8h of training data! We can submit our checkpoint to the [`..."
          ],
          [
           "```\n\nThe training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` comman..."
          ],
          [
           "--\ntitle: How to train a Language Model with Megatron-LM\nthumbnail: /blog/assets/100_megatron_traini..."
          ],
          [
           "## Why Megatron-LM?\n\nBefore getting into the training details, letâ€™s first understand what makes thi..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"assets/100_megatron_training/kernel_fusion.png\" width=\"600\" />\n</p>..."
          ],
          [
           "So after having installed Docker, you can run the container with the following command (`xx.xx` deno..."
          ],
          [
           "```\n\nYou also need to add the vocabulary file `vocab.json` and merges table `merges.txt` of your tok..."
          ],
          [
           "```\n\nThe data is then tokenized, shuffled and processed into a binary format for training using the ..."
          ],
          [
           "```\nThe `workers` and `chunk_size` options refer to the number of workers used in the preprocessing ..."
          ],
          [
           "### Training\nYou can configure the model architecture and training parameters as shown below, or put..."
          ],
          [
           "--weight-decay .1\n--adam-beta2 .999\n--fp16\n--log-interval 10\n--save-interval 2000\n--eval-interval 20..."
          ],
          [
           "```\nWith this setting, the training takes roughly 12 hours.\n\nThis setup uses Data Parallelism, but i..."
          ],
          [
           "```\nBe careful, you will need to replace the generated vocabulary file and merges table after the co..."
          ],
          [
           "```\nThis will use [accelerate](https://huggingface.co/docs/accelerate/index) library behind the scen..."
          ],
          [
           "--\ntitle: \"Accelerating Hugging Face Transformers with AWS Inferentia2\" \nthumbnail: /blog/assets/140..."
          ],
          [
           "However, for all their greatness, Transformers can be challenging to deploy in production. On top of..."
          ],
          [
           "## Introducing AWS Inferentia2\n\nAWS Inferentia2 is the next generation to Inferentia1 launched in 20..."
          ],
          [
           "Speaking of, letâ€™s show you how several Hugging Face models run on Inferentia 2. Benchmarking time!\n..."
          ],
          [
           "### Results\n\nThe benchmark confirms that the performance improvements claimed by AWS can be reproduc..."
          ],
          [
           "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "The initial benchmarking results are promising, and show that Inferentia2 delivers superior latency ..."
          ],
          [
           "--\ntitle: \"Introducing SafeCoder\" \nthumbnail: /blog/assets/159_safecoder/thumbnail.jpg\nauthors:\n- us..."
          ],
          [
           "However, relying on closed-source Code LLMs to create internal code assistants exposes companies to ..."
          ],
          [
           "Note: While StarCoder is the inspiration and model powering the initial version of SafeCoder, an imp..."
          ],
          [
           "BigCode expanded upon this work by implementing novel techniques for the code domain and building Th..."
          ],
          [
           "### Deploying SafeCoder\n\nDuring the setup phase, SafeCoder customers and Hugging Face design and pro..."
          ],
          [
           "â€œOur collaboration with Hugging Face around SafeCoder fully aligns to VMwareâ€™s goal of enabling cust..."
          ],
          [
           "--\ntitle: \"The Reformer - Pushing the limits of language modeling\"\nthumbnail: /blog/assets/03_reform..."
          ],
          [
           "Recently, long sequence modeling has experienced a surge of interest as can be seen by the many subm..."
          ],
          [
           "The memory improvements can be attributed to **4** features which the Reformer authors introduced to..."
          ],
          [
           "This blog post uses the same notation and coloring as the popular blog post [The illustrated transfo..."
          ],
          [
           "In short, a global self-attention layer projects \\\\(\\mathbf{X}\\\\) to the query, key and value matric..."
          ],
          [
           "Important to remember is that for each output vector \\\\(\\mathbf{z}_{i}\\\\), the whole input sequence ..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "$$\\mathbf{Z}^{\\text{loc}} = \\left[\\mathbf{Z}_{1:l_{c}}^{\\text{loc}}, \\ldots, \\mathbf{Z}_{(n_{c} - 1)..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "This enhanced local self-attention is better than the vanilla local self-attention architecture but ..."
          ],
          [
           "LSH self-attention relies on the LSH algorithm as presented in [Andoni et al (2015)](https://arxiv.o..."
          ],
          [
           "First, the authors of Reformer notice that sharing the query and key projections: \\\\(\\mathbf{Q} = \\m..."
          ],
          [
           "For each set of indices \\\\(C_{m}\\\\), the softmax function on the corresponding bucket of query vecto..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "All in all for all chunks \\\\( k \\in \\{1, \\ldots, n_{c}\\} \\\\), LSH self-attention can be noted down a..."
          ],
          [
           "One important feature to mention here as well is that the accuracy of LSH self-attention can be impr..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "Let's recap quickly what we have gone through above:\n\n1. We want to approximate global attention usi..."
          ],
          [
           "\\\\( {}^3 \\\\) On a side note, it is to mention the authors put a mask on the query vector \\\\( \\mathbf..."
          ],
          [
           "```\n#@title Installs and Imports\n# pip installs\n!pip -qq install git+https://github.com/huggingface/..."
          ],
          [
           "```\n\n\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1279.0, style=Progr..."
          ],
          [
           "The longer the input sequence, the more visible is the quadratic relationship \\\\( \\mathcal{O}(n^2) \\..."
          ],
          [
           "```\n  config = ReformerConfig.from_pretrained(\"google/reformer-enwik8\")\n  benchmark_args = PyTorchBe..."
          ],
          [
           "```\n\n    1 / 1\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 ..."
          ],
          [
           "As expected using local and LSH self-attention is much more memory efficient for longer input sequen..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "Let's illustrate the feed forward layers for \\\\( \\mathbf{\\overline{z}}_1, \\ldots, \\mathbf{\\overline{..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "Assuming \\\\( c_{f}=1 \\\\) for our example we can illustrate the incremental computation of the output..."
          ],
          [
           "\\\\( {}^3 \\\\) As a reminder, the output `config.num_attention_heads` is assumed to be 1 for the sake ..."
          ],
          [
           "```\n#@title Installs and Imports\n# pip installs\n!pip -qq install git+https://github.com/huggingface/..."
          ],
          [
           "```\n\n    1 / 2\n    Doesn't fit on GPU. CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 11.17 ..."
          ],
          [
           "Interesting, chunked feed forward layers do not seem to help here at all. The reason is that `config..."
          ],
          [
           "```\nconfig_no_chunk = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", chunk_size_feed_forwa..."
          ],
          [
           "```\n\n    1 / 2\n    2 / 2\n    \n    ====================      INFERENCE - MEMORY - RESULT       ======..."
          ],
          [
           "## 3. Reversible Residual Layers\n\nReversible residual layers were first introduced in [N. Gomez et a..."
          ],
          [
           "Using the same notation as before, the input of a transformer layer *i.e.* \\\\( \\mathbf{X} \\\\) is fir..."
          ],
          [
           "Let's illustrate a complete transformer layer using the example of \\\\( \\mathbf{x}_1, \\ldots, \\mathbf..."
          ],
          [
           "Here, reversible residual layers come to our help. The idea is relatively straight-forward. The resi..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "If we assume to know \\\\( \\mathbf{\\overline{Y}}^{(1)}, \\mathbf{\\overline{Y}}^{(2)} \\\\), it can easily..."
          ],
          [
           "\\\\). Alright now, \\\\( \\mathbf{Z} \\\\) and \\\\( \\mathbf{Y} \\\\) are trivial to compute via \\\\( \\mathbf{Y..."
          ],
          [
           "use of \\\\( G \\\\) and \\\\( F \\\\) during the backward pass and passing \\\\( \\mathbf{X}^{(1)} \\\\) and \\\\(..."
          ],
          [
           "**Note**: Since recently, major deep learning frameworks have released code that allows to store onl..."
          ],
          [
           "```\n#@title Installs and Imports\n# pip installs\n!pip -qq install git+https://github.com/huggingface/..."
          ],
          [
           "```\n\n\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=Progre..."
          ],
          [
           "```\nconfig_4_layers_reformer = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", num_hidden_l..."
          ],
          [
           "```\n\n    1 / 3\n    2 / 3\n    3 / 3\n    \n    ====================        TRAIN - MEMORY - RESULTS    ..."
          ],
          [
           "**Important:** *Axial Position Encodings were not explained in the official paper, but can be well u..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "Such positional encodings would use an unnecessarily large amount of memory both when loading the mo..."
          ],
          [
           "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_ben..."
          ],
          [
           "We can see that we have cut the embedding vectors into \\\\( \\mathbf{e}_\\text{down} \\\\) (*in blue*) an..."
          ],
          [
           "whereas \\\\( n_\\text{max}^1 = 7 \\\\) and \\\\( n_\\text{max}^2 = 7 \\\\) in our example.\nThese new encoding..."
          ],
          [
           "To demonstrate the drastic reduction in size, \nlet's assume we would have set `config.axial_pos_shap..."
          ],
          [
           "```\n#@title Installs and Imports\n# pip installs\n!pip -qq install git+https://github.com/huggingface/..."
          ],
          [
           "```\nconfig_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishmen..."
          ],
          [
           "```\n\n\n    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1151.0, style=Progr..."
          ],
          [
           "```\nbenchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512], batch_sizes=[8], models=[\"Ref..."
          ],
          [
           "```\n\n    1 / 2\n    2 / 2\n    \n    ====================      INFERENCE - MEMORY - RESULT       ======..."
          ],
          [
           "--\ntitle: \"Chat Templates: An End to the Silent Performance Killer\" \nthumbnail: /blog/assets/chat-te..."
          ],
          [
           "```\nBy loading the tokenizer and model from the same checkpoint, you ensure that inputs are tokenize..."
          ],
          [
           "```\nOr you could add special tokens to indicate the roles:\n```\n[USER] Hey there! [/USER]\n[ASST] Nice..."
          ],
          [
           "```\nThere are lots of ways to do this, and none of them is obviously the best or correct way to do i..."
          ],
          [
           "```\n```jinja\n{% for message in messages %}\n    {% if message['role'] == 'user' %}\n        {{ \"[USER]..."
          ],
          [
           "```\n\nIf you're unfamiliar with Jinja, I strongly recommend that you take a moment to look at these t..."
          ],
          [
           "```\n\nThere's also a second reason not to hardcode a standard format, though, beyond the proliferatio..."
          ],
          [
           "Default chat templates are also set at the class level, and tell classes like `ConversationPipeline`..."
          ],
          [
           "If a tokenizer doesn't have a `chat_template` attribute, it might still work, but it will use the de..."
          ],
          [
           "--\ntitle: \"Make your llama generation time fly with AWS Inferentia2\"\nthumbnail: /blog/assets/inferen..."
          ],
          [
           "Alternatively, you can use the [Hugging Face Neuron SDK DLC](https://github.com/aws/deep-learning-co..."
          ],
          [
           "```\n>>> from optimum.neuron import NeuronModelForCausalLM\n\n>>> compiler_args = {\"num_cores\": 24, \"au..."
          ],
          [
           "```\n\n## Generate Text using Llama 2 on AWS Inferentia2\n\nOnce your model has been exported, you can g..."
          ],
          [
           "```\n\n*Note: when passing multiple input prompts to a model, the resulting token sequences must be pa..."
          ],
          [
           "```\n\n## Benchmarks\n\nBut how much efficient is text-generation on Inferentia2?  Let's figure out!\n\nWe..."
          ],
          [
           "| Model type                 | num cores | batch_size | Hugging Face Hub model                    |\n..."
          ],
          [
           "*Note: all models are compiled with a maximum sequence length of 2048.*\n\nThe `llama2 7B` \"budget\" mo..."
          ],
          [
           "Encoding time is expressed in **seconds**.\n\n|   input tokens  |   Llama2 7B-L  |   Llama2 7B-T  |   ..."
          ],
          [
           "Latency is expressed in **seconds**.\n\n|   new tokens  |   Llama2 7B-L  |   Llama2 7B-T  |   Llama2 1..."
          ],
          [
           "Throughput is expressed in **tokens/second**.\n\n|   new tokens  |   Llama2 7B-L  |   Llama2 7B-T  |  ..."
          ],
          [
           "The deployed models demonstrate very good performance in terms of encoding time, latency and through..."
          ],
          [
           "--\ntitle: \"Introduction to 3D Gaussian Splatting\"\nthumbnail: /blog/assets/124_ml-for-games/thumbnail..."
          ],
          [
           "In practice, multiple gaussians are drawn at once.\n\n![](https://huggingface.co/datasets/huggingface/..."
          ],
          [
           "### 3. Training\n\nThe training procedure uses Stochastic Gradient Descent, similar to a neural networ..."
          ],
          [
           "## Who cares?\n\nWhy has there been so much attention on 3D Gaussian Splatting? The obvious answer is ..."
          ],
          [
           "So far, the original CUDA implementation has not been adapted to production rendering pipelines, lik..."
          ],
          [
           "--\ntitle: \"Train your ControlNet with diffusers\"\nthumbnail: /blog/assets/136_train-your-controlnet/t..."
          ],
          [
           "2. **Building your dataset**: Once a condition is decided, it is time to build your dataset. For tha..."
          ],
          [
           "For this project, we decided to go with the `FaceSynthetics` dataset by Microsoft: it is a dataset t..."
          ],
          [
           "So we decided to follow another path:\n- Use the ground truths `image` of faces of the `FaceSynthetic..."
          ],
          [
           "![New dataset](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/13..."
          ],
          [
           "<iframe src=\"https://wandb.ai/apolinario/controlnet/reports/ControlNet-Uncanny-Faces-Training--Vmlld..."
          ],
          [
           "```\n\nAnd then run the [train_controlnet.py](https://github.com/huggingface/diffusers/blob/main/examp..."
          ],
          [
           "Let's break down some of the settings, and also let's go over some optimisation tips for going as lo..."
          ],
          [
           "- `validation_prompt`: A prompt to be ran togehter with your validation image. Can be anything that ..."
          ],
          [
           "### Fitting on a 16GB VRAM GPU\n```shell \npip install bitsandbytes\n\n--train_batch_size=1 \\\n--gradient..."
          ],
          [
           "```\n\nThe combination of a batch size of 1 with 4 gradient accumulation steps is equivalent to using ..."
          ],
          [
           "--\ntitle: \"Spread Your Wings: Falcon 180B is here\" \nthumbnail: /blog/assets/162_falcon_180b/thumbnai..."
          ],
          [
           "In this blog post, we explore what makes Falcon 180B so good by looking at some evaluation results a..."
          ],
          [
           "The released [chat model](https://huggingface.co/tiiuae/falcon-180B-chat) is fine-tuned on chat and ..."
          ],
          [
           "With 68.74 on the Hugging Face Leaderboard, Falcon 180B is the highest-scoring openly released pre-t..."
          ],
          [
           "You can easily try the Big Falcon Model (180 billion parameters!) in [this Space](https://huggingfac..."
          ],
          [
           "```bash\nSystem: Add an optional system prompt here\nUser: This is the user input\nFalcon: This is what..."
          ],
          [
           "```\n\n### Transformers\n\nWith the release of Transformers 4.33, you can use Falcon 180B and leverage a..."
          ],
          [
           "```\n\nThis could produce an output such as:\n\n```\nMy name is Pedro, I live in Portugal and I am 25 yea..."
          ],
          [
           "```\n\nAs you can see, interactions from the user and responses by the model are preceded by `User: ` ..."
          ],
          [
           "## Acknowledgments\n\nReleasing such a model with support and evaluations in the ecosystem would not b..."
          ],
          [
           "--\ntitle: Swift ðŸ§¨Diffusers - Fast Stable Diffusion for Mac\nthumbnail: /blog/assets/fast-mac-diffuser..."
          ],
          [
           "## What exactly is ðŸ§¨Diffusers for Mac anyway?\n\nThe Diffusers app ([App Store](https://apps.apple.com..."
          ],
          [
           "Why would you want to run a native Mac app then? There are many reasons:\n- It uses Core ML models, i..."
          ],
          [
           "Come check out our benchmarks. All the combinations use the CPU in addition to either the GPU or the..."
          ],
          [
           "We found that the amount of memory does not seem to play a big factor on performance, but the number..."
          ],
          [
           "## Other Improvements in Version 1.1\n\nIn addition to the performance optimization and fixing a few b..."
          ],
          [
           "--\ntitle: \"Director of Machine Learning Insights\"\nthumbnail: /blog/assets/61_ml_director_insights/th..."
          ],
          [
           "ðŸš€  Letâ€™s meet some top Machine Learning Directors and hear what they have to say about Machine Learn..."
          ],
          [
           "_Tightened testing:_ In a capital intensive media venture, there is a need to shorten the time betwe..."
          ],
          [
           "**Background:** Li is an AI/ML veteran with 15+ years of experience leading high-profile Data Scienc..."
          ],
          [
           "#### **1. How has ML made a positive impact on Pharmaceuticals?**\nAI/ML applications have exploded i..."
          ],
          [
           "<img class=\"mx-auto\" style=\"float: left;\" padding=\"5px\" width=\"200\" src=\"/blog/assets/61_ml_director..."
          ],
          [
           "**Machine Learning & Sensing Laboratory:** A University of Florida laboratory that develops machine ..."
          ],
          [
           "#### **4. What excites you most about the future of ML?**\nThere are a lot of really exciting directi..."
          ],
          [
           "For example, when you see a semi-truck driving on the road, there is currently a 20% chance that the..."
          ],
          [
           "#### **4. What excites you the most about the future of ML?**\nI think the thing that excites me most..."
          ],
          [
           "#### **1. How has ML made a positive impact on Marketing?**\nIn so many ways! Itâ€™s completely changin..."
          ],
          [
           "Understanding what makes a creator/influencer successful over time is really hard. There is a lot of..."
          ],
          [
           "Much like in the beginning of the internet, software developers were few and far between and you nee..."
          ],
          [
           "<img class=\"mx-auto\" style=\"float: left;\" padding=\"5px\" width=\"200\" src=\"/blog/assets/61_ml_director..."
          ],
          [
           "**E Source:** Provides independent market intelligence, consulting, and predictive data science to u..."
          ],
          [
           "---\n\nðŸ¤—   Thank you for joining us in this first installment of ML Director Insights. Stay tuned for ..."
          ],
          [
           "--\ntitle: \"My Journey to a serverless transformers pipeline on Google Cloud\"\nthumbnail: /blog/assets..."
          ],
          [
           "Below is the [official example](https://github.com/huggingface/transformers#quick-tour) from the Tra..."
          ],
          [
           "```\n\n\n## Deploy transformers to Google Cloud\n> GCP is chosen as it is the cloud environment I am usi..."
          ],
          [
           "### Step 4 - Test on Cloud Run\n\nLastly, I moved to [Cloud Run](https://cloud.google.com/run) with a ..."
          ],
          [
           "The content on the `main.py` is really simple. The idea is to receive a `GET` request containing two..."
          ],
          [
           "```\n\nThen the `DockerFile` which will be used to create a docker image of the service. We specify th..."
          ],
          [
           "```\n\n\n## Deployment instructions\n\nFirst, you will need to meet some requirements such as having a pr..."
          ],
          [
           "```\n\nAfter a few minutes, you will also need to upgrade the memory allocated to your Cloud Run insta..."
          ],
          [
           "For my micro-service, I am planning to near 1,000 requests per month, optimistically. 500 may more l..."
          ],
          [
           "--\ntitle: \"Machine Learning Experts - Margaret Mitchell\"\nthumbnail: /blog/assets/57_meg_mitchell_int..."
          ],
          [
           "### Could you share a little bit about your background and what brought you to Hugging Face?\n\n**Dr. ..."
          ],
          [
           "So I began to run into issues where white people would be described as â€˜peopleâ€™ and black people wou..."
          ],
          [
           "### How can ML teams be more aware of harmful bias?\n\n**Meg:** A primary issue is that these concepts..."
          ],
          [
           "**Meg:** Diversity is when you have a lot of races, ethnicities, genders, abilities, statuses at the..."
          ],
          [
           "Just how you want to have a Gaussian approach over different start states, so too do you want that a..."
          ],
          [
           "Timnitâ€™s paper was called [â€˜Data Sheets for Datasetsâ€™](https://arxiv.org/abs/1803.09010). So we call..."
          ],
          [
           "### Decision thresholds & model transparency\n\n**Meg:** When Amazon first started putting out facial ..."
          ],
          [
           "### What are you working on at Hugging Face?\n\n- Working on a few different tools designed for engine..."
          ],
          [
           "### Rapid Fire Questions:\n\n### Best piece of advice for someone looking to get into AI?\n\n**Meg:** De..."
          ],
          [
           "### Should people be afraid of AI taking over the world?\n\n**Meg:** There are a lot of things to be a..."
          ],
          [
           "Hopefully, we can focus on the things that are most beneficial and continue heading in that directio..."
          ],
          [
           "We have a lot of people developing technology, which is great, but we donâ€™t have a lot of people in ..."
          ],
          [
           "**Honorable mentions + links:**\n- [Emily Bender](https://twitter.com/emilymbender?lang=en)\n- [Ehud R..."
          ],
          [
           "--\ntitle: \"Accelerating PyTorch distributed fine-tuning with Intel technologies\"\nthumbnail: /blog/as..."
          ],
          [
           "Running a text classification job, we will fine-tune a [BERT](https://huggingface.co/bert-base-cased..."
          ],
          [
           "All three major cloud providers offer virtual machines powered by Intel Ice Lake CPUs:\n\n- Amazon Web..."
          ],
          [
           "When it comes to distributed training, the main performance bottleneck is often networking. Indeed, ..."
          ],
          [
           "From a networking perspective, we will need the following setup: \n\n* Open port 22 for ```ssh``` acce..."
          ],
          [
           "It looks like a lot, but there's nothing complicated. Here we go!\n\n__Installing Intel toolkits__\n\nFi..."
          ],
          [
           "```\nwget https://registrationcenter-download.intel.com/akdlm/irc_nas/18236/l_BaseKit_p_2021.4.0.3422..."
          ],
          [
           "```\n\n__Compiling and installing oneCCL__\n\nThen, we install some native dependencies required to comp..."
          ],
          [
           "```\npython run_glue.py \\\n--model_name_or_path bert-base-cased --task_name mrpc \\\n--do_train --do_eva..."
          ],
          [
           "```\nfor nic in eth0 eib0 hib0 enp94s0f0; do\n  master_addr=$(ifconfig $nic 2>/dev/null | grep netmask..."
          ],
          [
           "```\n+import torch_ccl\n+\n import datasets\n import numpy as np\n from datasets import load_dataset, loa..."
          ],
          [
           "```\n\nSetup is now complete. Let's scale our training job to 2 nodes and 4 nodes.\n\n### Running a dist..."
          ],
          [
           "```\n\nWithin seconds, a job starts on the first two nodes. The job completes in __4 minutes and 39 se..."
          ],
          [
           "--\ntitle: \"Introducing new audio and vision documentation in ðŸ¤— Datasets\"\nthumbnail: /blog/assets/87_..."
          ],
          [
           "<div class=\"hidden xl:block\">\n<div style=\"display: flex; flex-direction: column; align-items: center..."
          ],
          [
           "Also new in the Quickstart is the `to_tf_dataset` function which takes care of converting a dataset ..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n  <img style=\"border:none;\" alt=\"An overview of ..."
          ],
          [
           "```\n\n<figure class=\"image table text-center m-0 w-full\">\n  <img style=\"border:none;\" alt=\"A table of..."
          ],
          [
           "dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\", split=\"train\")\ndataset[0][\"objects..."
          ],
          [
           "```\n\nYou can use `ImageFolder` to load an image dataset for nearly any type of image task if you hav..."
          ],
          [
           "--\ntitle: \"Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face\"\nthumbnail: /blog/assets/mixt..."
          ],
          [
           "## Table of Contents\n\n- [What is Mixtral 8x7b](#what-is-mixtral-8x7b)\n  - [About the name](#about-th..."
          ],
          [
           "**Mixtral release TL;DR;**\n\n- Release of base and Instruct versions\n- Supports a context length of 3..."
          ],
          [
           "| Model                                                                             | License       ..."
          ],
          [
           "For instruct and chat models, evaluating on benchmarks like MT-Bench or AlpacaEval is better. Below,..."
          ],
          [
           "| Model                                                                                             ..."
          ],
          [
           "| [meta-llama/Llama-2-70b-chat-hf](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)           ..."
          ],
          [
           "Impressively, Mixtral Instruct outperforms all other open-access models on MT-Bench and is the first..."
          ],
          [
           "```\n\nThis format has to be exactly reproduced for effective use. Weâ€™ll show later how easy it is to ..."
          ],
          [
           "### Using ðŸ¤—Â Transformers\n\nWith transformersÂ [release 4.36](https://github.com/huggingface/transforme..."
          ],
          [
           "```\n\nIn the following code snippet, we show how to run inference with ðŸ¤— Transformers and 4-bit quant..."
          ],
          [
           "```\n\n> \\<s>[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST] A\nMixture of ..."
          ],
          [
           "*Note: You might need to request a quota upgrade via email toÂ **[api-enterprise@huggingface.co](mail..."
          ],
          [
           "```\n\n## Fine-tuning with ðŸ¤—Â TRL\n\nTraining LLMs can be technically and computationally challenging. In..."
          ],
          [
           "```\n\nThis takes about 48 hours to train on a single A100, but can be easily parallelised by tweaking..."
          ],
          [
           "prompt = \"[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]\"\ninputs = toke..."
          ],
          [
           "```\n\nThis 4-bit quantization technique was introduced in the [QLoRA paper](https://huggingface.co/pa..."
          ],
          [
           "```\n\nYou also need to install transformers from source:\n\n```bash\npip install -U git+https://github.c..."
          ],
          [
           "```\n\nNote that for both QLoRA and GPTQ you need at least 30 GB of GPU VRAM to fit the model. You can..."
          ],
          [
           "--\ntitle: Hyperparameter Search with Transformers and Ray Tune\nthumbnail: /blog/assets/06_ray_tune/r..."
          ],
          [
           "<table>\n  <tr>\n   <td><strong>Algorithm</strong>\n   </td>\n   <td><strong>Best Val Acc.</strong>\n   <..."
          ],
          [
           "![alt_text](/blog/assets/06_ray_tune/ray-hf.jpg \"image_tooltip\")\n\n\nIn the Transformers 3.1 release, ..."
          ],
          [
           "def encode(examples):\n    outputs = tokenizer(\n        examples['sentence1'], examples['sentence2'],..."
          ],
          [
           "```\n\nBy default, each trial will utilize 1 CPU, and optionally 1 GPU if available.\nYou can leverage ..."
          ],
          [
           "```\n\n\nIt also works with [Weights and Biases](https://wandb.ai/) out of the box!\n\n![alt_text](/blog/..."
          ],
          [
           "--\ntitle: \"Accelerate Large Model Training using DeepSpeed\"\nthumbnail: /blog/assets/83_accelerate_de..."
          ],
          [
           "c. **Stage 3**: Shards optimizer states + gradients + model parameters across data parallel workers/..."
          ],
          [
           "The code is available here [run_cls_no_trainer.py](https://github.com/pacman100/accelerate-deepspeed..."
          ],
          [
           "```\n\nNow, run below command for training:\n```bash\naccelerate launch run_cls_no_trainer.py \\\n  --mode..."
          ],
          [
           "```\n\nIn our Single-Node Multi-GPU setup, the maximum batch size that DDP supports without OOM error ..."
          ],
          [
           "# Accelerate ðŸš€:  Leverage a DeepSpeed Config file to tweak more options\n\nFirst, We will look at the ..."
          ],
          [
           "We will leverage the DeepSpeed Zero Stage-2 config [zero2_config_accelerate.json](https://github.com..."
          ],
          [
           "```\n\nTo enable DeepSpeed ZeRO Stage-2 with above config, please run `accelerate config` and provide ..."
          ],
          [
           "Now, run below command for training:\n```bash\naccelerate launch run_seq2seq_no_trainer.py \\\n    --dat..."
          ],
          [
           "```\n\nWhen using DeepSpeed config, if user has specified `optimizer` and `scheduler` in config, the u..."
          ],
          [
           "```\n\n---\n| Method | Batch Size Max | Eval Size Max | Train time per epoch (seconds) | Eval time  per..."
          ],
          [
           "---\n## CPU/Disk Offloading to enable training humongous models that wonâ€™t fit the GPU memory\n\nOn a s..."
          ],
          [
           "We will leverage the DeepSpeed Zero Stage-3 CPU offload config [zero3_offload_config_accelerate.json..."
          ],
          [
           "\"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"p..."
          ],
          [
           "```\n\n**ZeRO Stage-3 CPU Offload DeepSpeed Config File Example**\n```bash\ncompute_environment: LOCAL_M..."
          ],
          [
           "```\n\n---\n| Method | Batch Size Max | Train time per epoch (seconds) | Notes |\n| --- | --- | --- | --..."
          ],
          [
           "--\ntitle: \"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\"\nthumbnail: /..."
          ],
          [
           "The main bottleneck is the latency of predictions which can make large deployments expensive to run ..."
          ],
          [
           "An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learnin..."
          ],
          [
           "In addition to superior performance for machine learning workloads, the Intel Ice Lake C6i instances..."
          ],
          [
           "In each experiment, we collect numbers for:\n* Throughput (requests per second)\n* Latency (min, max, ..."
          ],
          [
           "```\n\n### Throughput\n\nBelow you can find the throughput comparison for running infinity on 2 physical..."
          ],
          [
           "<br>\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" ..."
          ],
          [
           "If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-..."
          ],
          [
           "--\ntitle: \"Introducing âš”ï¸ AI vs. AI âš”ï¸ a deep reinforcement learning multi-agents competition system..."
          ],
          [
           "Letâ€™s see how it works with our first competition host: SoccerTwos Challenge.\n\n<div align=\"center\"> ..."
          ],
          [
           "If you want to learn more about ELO and see some calculation example, we wrote an explanation in our..."
          ],
          [
           "To run this matchmaking process continuously, we use **free Hugging Face Spaces hardware with a Sche..."
          ],
          [
           "## Our first AI vs. AI challenge experimentation: SoccerTwos Challenge âš½\n\nThis challenge is Unit 7 o..."
          ],
          [
           "We also [created a discord channel called ai-vs-ai-competition](http://hf.co/discord/join) so that p..."
          ],
          [
           "```\n@article{cochet-simonini2023,\n  author = {Cochet, Carl and Simonini, Thomas},\n  title = {Introdu..."
          ],
          [
           "--\ntitle: \"SetFit: Efficient Few-Shot Learning Without Prompts\"\nthumbnail: /blog/assets/103_setfit/i..."
          ],
          [
           "<p>ðŸŽ <strong>Fast to train</strong>: SetFit doesn't require large-scale models like T0 or GPT-3 to a..."
          ],
          [
           "SetFit takes advantage of Sentence Transformersâ€™ ability to generate dense embeddings based on paire..."
          ],
          [
           "| Rank | Method | Accuracy | Model Size | \n| :------: | ------ | :------: | :------: | \n| 2 | T-Few ..."
          ],
          [
           "Since SetFit achieves high accuracy with relatively small models, it's blazing fast to train and at ..."
          ],
          [
           "```\nNext, we import `SetFitModel` and `SetFitTrainer`, two core classes that streamline the SetFit t..."
          ],
          [
           "```\nThe last step is to train and evaluate the model:\n```python\n# Train and evaluate!\ntrainer.train(..."
          ],
          [
           "--\ntitle: \"Creating a Coding Assistant with StarCoder\"\nthumbnail: /blog/assets/starchat_alpha/thumbn..."
          ],
          [
           "In this blog post, weâ€™ll show how StarCoder can be fine-tuned for chat to create a personalised codi..."
          ],
          [
           "To get started, letâ€™s take a look at how language models can be turned into conversational agents wi..."
          ],
          [
           "```\nBelow are a series of dialogues between various people and an AI assistant.\nThe AI tries to be h..."
          ],
          [
           "```\n\nAs we can see, the first part of the prompt â€œBelow are a series...â€ corresponds to the system m..."
          ],
          [
           "```\nBelow are a series of dialogues between various people and an AI technical assistant.\nThe assist..."
          ],
          [
           "Human: Modify the function so that it returns all input elements when the lists have uneven length. ..."
          ],
          [
           "```\n\nHere we can see how a well crafted prompt can induce coding behaviour similar to that observed ..."
          ],
          [
           "Letâ€™s start by downloading the processed dataset from the Hub:\n\n```python\nfrom datasets import load_..."
          ],
          [
           "```\n\n```\nDatasetDict({\n    train: Dataset({\n        features: ['messages'],\n        num_rows: 19034\n..."
          ],
          [
           "```\n{\n    \"messages\": [\n        {\n            \"content\": \"Is it possible to imagine a society withou..."
          ],
          [
           "\"role\": \"user\",\n        },\n        {\n            \"content\": \"You are correct that there are other fa..."
          ],
          [
           "```\n\nOK, this looks like an interesting dialogue about moral philosophy, with each turn involving a ..."
          ],
          [
           "```\n\nAlthough this works fine for training, it isnâ€™t ideal for inference because the model will natu..."
          ],
          [
           "```\n\n```\n<|system|>\nBelow is a dialogue between a human and AI assistant called StarChat.\n<|end|>\n<|..."
          ],
          [
           "```\n\n```\n{\"input_ids\": [49153], \"attention_mask\": [1]}\n```\n\nGreat, it works!\n\n### Masking user label..."
          ],
          [
           "```\n\nOK, we can see that all the user input IDs have been masked in the labels as desired. These spe..."
          ],
          [
           "```\n\nNext, create a Python virtual environment using e.g. Conda:\n\n```shell\nconda create -n starchat ..."
          ],
          [
           "```\n\nHere the `config.yaml` file specifies all the parameters associated with the dataset, model, an..."
          ],
          [
           "```\n\nResponse:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt..."
          ],
          [
           "```\nDraw me a map of the world using geopandas. Make it so that only Germany and Spain are colored r..."
          ],
          [
           "```\nThere was a basketball game with the following stats. player, points, rebounds and assists: J. H..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-imag..."
          ],
          [
           "The results are shown in the table below, where we can see the fine-tuned model has improved, but no..."
          ],
          [
           "```\nGenerate a bunch of instructions for coding questions in python (in the format of {\"prompt\": ins..."
          ],
          [
           "```\nWrite a Python function called reverse_string that takes a string as its argument and returns th..."
          ],
          [
           "```\n\nBase-model completion (Assistant 1):\n\n```\n\"Sure thing! Let's start by writing out the docstring..."
          ],
          [
           "```\n\nWe can compare this to ChatGPTâ€™s response, which seems to miss the fact that the Assistant 1 do..."
          ],
          [
           "```\n\nThis shows us that while there is extremely valuable signal in AI evaluations, we have a lot to..."
          ],
          [
           "## Links\n\n- Code: [https://github.com/bigcode-project/starcoder/tree/main/chat](https://github.com/b..."
          ],
          [
           "```\n@article{Tunstall2023starchat-alpha,\n  author = {Tunstall, Lewis and Lambert, Nathan and Rajani,..."
          ],
          [
           "--\ntitle: \"AI Policy @ðŸ¤—: Response to the U.S. NTIA's Request for Comment on AI Accountability\"\nthumb..."
          ],
          [
           "Hugging Faceâ€™s mission is to [â€œdemocratize good machine learningâ€](https://huggingface.co/about). We..."
          ],
          [
           "Concretely, we make the following recommendations for accountability mechanisms:\n\n* Accountability m..."
          ],
          [
           "--\ntitle: \"Creating Privacy Preserving AI with Substra\" \nthumbnail: /blog/assets/139_owkin-substra/t..."
          ],
          [
           "As the data never leaves its source, federated learning is naturally a privacy-first approach. Not o..."
          ],
          [
           "![Substra diagram](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blo..."
          ],
          [
           "--\ntitle:  Deploy Embedding Models with Hugging Face Inference Endpoints\nthumbnail: /blog/assets/168..."
          ],
          [
           "Before we start, let's refresh our knowledge about Inference Endpoints.\n\n## 1. What is Hugging Face ..."
          ],
          [
           "You can get started with Inference Endpoints at:Â https://ui.endpoints.huggingface.co/\n\n## 2. What is..."
          ],
          [
           "Those feature enabled industry-leading performance on throughput and cost. In a benchmark forÂ [BAAI/..."
          ],
          [
           "*Note: If the instance type cannot be selected, you need toÂ [contact us](mailto:api-enterprise@huggi..."
          ],
          [
           "```python\nimport requests\n\nAPI_URL = \"https://l2skjfwp9punv393.us-east-1.aws.endpoints.huggingface.c..."
          ],
          [
           "```\n\n## Conclusion\n\nTEI on Hugging Face Inference Endpoints enables blazing fast and ultra cost-effi..."
          ],
          [
           "--\ntitle: \"What Makes a Dialog Agent Useful?\" \nthumbnail: /blog/assets/dialog-agents/thumbnail.png\na..."
          ],
          [
           "The following table compares these AI chatbots based on the details of their public access, training..."
          ],
          [
           "| &nbsp;| LaMDA | BlenderBot 3 |Sparrow | ChatGPT/ InstructGPT | Assistant|\n| --- | --- | --- | --- ..."
          ],
          [
           "| **RLHF** | âœ–ï¸ | âœ–ï¸ | âœ” | âœ” | âœ” |\n| **Hand written rules for safety** | âœ” | âœ–ï¸ | âœ” | âœ–ï¸ | âœ” |\n| **E..."
          ],
          [
           "We observe that albeit there are many differences in the training data, model, and fine-tuning, ther..."
          ],
          [
           "![Instruction and instance example](assets/dialog-agents/ift.png)\n\nData for IFT is usually a collect..."
          ],
          [
           "![IFT spectrum](assets/dialog-agents/ift-spectrum.png)\n\nOn one end is the purely model-generated IFT..."
          ],
          [
           "### Safely following instructions\n\nInstruction fine-tuned LMs, however, may not always generate resp..."
          ],
          [
           "### Fine-tuning the models\n\nOn the other hand, Open AIâ€™s InstructGPT, DeepMindâ€™s Sparrow, and Anthro..."
          ],
          [
           "![Illustration of CoT](assets/dialog-agents/cot.png)\n\nModels fine-tuned with CoT have shown to perfo..."
          ],
          [
           "## Next steps for dialogue agents\n\nThis blog summarizes many of the existing work on what makes a di..."
          ],
          [
           "```\n@article{rajani2023ift,\n  author = {Rajani, Nazneen and Lambert, Nathan and Sanh, Victor and Wol..."
          ],
          [
           "--\ntitle: \"AI for Game Development: Creating a Farming Game in 5 Days. Part 1\"\nthumbnail: /blog/asse..."
          ],
          [
           "### Setting up Stable Diffusion\n\nThere are a couple options for running Stable Diffusion: *locally* ..."
          ],
          [
           "```\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git..."
          ],
          [
           "```\n4. Download the [Stable Diffusion 1.5 weights](https://huggingface.co/runwayml/stable-diffusion-..."
          ],
          [
           "*Note:* Parts of this series will use advanced features such as image2image, which may not be availa..."
          ],
          [
           "I settled on the prompt: *isometric render of a farm by a river, simple, solid shapes, james gillear..."
          ],
          [
           "4. Set up your [Lighting](https://docs.unity3d.com/Manual/Lighting.html). I'm using a warm sun (#FFE..."
          ],
          [
           "<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documenta..."
          ],
          [
           "--\ntitle: \"From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease\"\nthu..."
          ],
          [
           "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as o..."
          ],
          [
           "```\n\nWe define the training device (`cuda`):\n\n```python\ndevice = \"cuda\"\n```\n\nBuild some PyTorch Data..."
          ],
          [
           "```\n\nTypically from here, one could either throw all of this into a python script or run it on a Jup..."
          ],
          [
           "```\n\nThe last piece of the puzzle is *how do I send my data and model to another GPU?*\n\nThis is wher..."
          ],
          [
           "```\n\nThe above will run the training script on two GPUs that live on a single machine and this is th..."
          ],
          [
           "# Build optimizer\n    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\n\n    # Train for a si..."
          ],
          [
           "```\n\nNext let's talk about how Accelerate can help. There's a few issues with the above code:\n\n1. Th..."
          ],
          [
           "# Send everything through `accelerator.prepare`\n    train_loader, test_loader, model, optimizer = ac..."
          ],
          [
           "```\n\nWith this your PyTorch training loop is now setup to be ran on any distributed setup thanks to ..."
          ],
          [
           "```\n\nOr:\n\n```python\nnotebook_launcher(train_ddp_accelerate, args=(), num_processes=2)\n```\n\n## Using ..."
          ],
          [
           "```\n\n```python\ntrainer.train()\n```\n\n```python out\n    ***** Running training *****\n      Num example..."
          ],
          [
           "```\n\n## Resources\n\nTo learn more about PyTorch Distributed Data Parallelism, check out the documenta..."
          ],
          [
           "--\ntitle: \"Probabilistic Time Series Forecasting with ðŸ¤— Transformers\"\nthumbnail: /blog/assets/118_ti..."
          ],
          [
           "Some classical methods are point-valued (meaning, they just output a single value per time step) and..."
          ],
          [
           "To begin with, the use of an Encoder-Decoder architecture is helpful at inference time where typical..."
          ],
          [
           "A drawback of the Transformer architecture is the limit to the sizes of the context and prediction w..."
          ],
          [
           "```\n\n## Load Dataset\n\nIn this blog post, we'll use the `tourism_monthly` dataset, which is available..."
          ],
          [
           "```\n\nThe `start` simply indicates the start of the time series (as a datetime), and the `target` con..."
          ],
          [
           "```\n\nHowever, this example has `prediction_length=24` additional values compared to the training exa..."
          ],
          [
           "```\n\nWe now use `datasets`' [`set_transform`](https://huggingface.co/docs/datasets/v2.7.0/en/package..."
          ],
          [
           "```\n\n## Define the Model\n\nNext, let's instantiate a model. The model will be trained from scratch, h..."
          ],
          [
           "Let's use the default lags provided by GluonTS for the given frequency (\"monthly\"):\n\n\n```python\nfrom..."
          ],
          [
           "```\n\n\nThis means that we'll look back up to 37 months for each time step, as additional features.\n\nL..."
          ],
          [
           "```\n\nNote that, similar to other models in the ðŸ¤— Transformers library, [`TimeSeriesTransformerModel`..."
          ],
          [
           "```\n\nThe transformations below are annotated with comments, to explain what they do. At a high level..."
          ],
          [
           "# a bit like torchvision.transforms.Compose\n    return Chain(\n        # step 1: remove static/dynami..."
          ],
          [
           "output_field=FieldName.OBSERVED_VALUES,\n            ),\n            # step 4: add temporal features b..."
          ],
          [
           "FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n                    FieldName.FEAT_TIME: \"time_f..."
          ],
          [
           "```\n\n## Define `InstanceSplitter`\n\nFor training/validation/testing we next create an `InstanceSplitt..."
          ],
          [
           "# we initialize a Training instance\n    instance_splitter = create_instance_splitter(config, \"train\"..."
          ],
          [
           "```\n\n\n```python\ndef create_backtest_dataloader(\n    config: PretrainedConfig,\n    freq,\n    data,\n  ..."
          ],
          [
           "```\n\nWe have a test dataloader helper for completion, even though we will not use it here. This is u..."
          ],
          [
           "```\n\nLet's check the first batch:\n\n\n```python\nbatch = next(iter(train_dataloader))\nfor k, v in batch..."
          ],
          [
           "```\n\n\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the..."
          ],
          [
           "```\n\nNote that the model is returning a loss. This is possible as the decoder automatically shifts t..."
          ],
          [
           "model, optimizer, train_dataloader = accelerator.prepare(\n    model,\n    optimizer,\n    train_datalo..."
          ],
          [
           "```\n\n\n## Inference\n\nAt inference time, it's recommended to use the `generate()` method for autoregre..."
          ],
          [
           "```\n\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values pr..."
          ],
          [
           "```\n\nWe can also plot the individual metrics of each time series in the dataset and observe that a h..."
          ],
          [
           "```\n\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/time-..."
          ],
          [
           "```\n\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/time-..."
          ],
          [
           "Of course, we need to be careful with just claiming state-of-the-art results on time series with neu..."
          ],
          [
           "Another thing on the roadmap is time series classification. This entails adding a time series model ..."
          ],
          [
           "--\ntitle: Image Classification with AutoTrain \nthumbnail: /blog/assets/105_autotrain-image-classific..."
          ],
          [
           "[Image Classification](https://huggingface.co/tasks/image-classification) models learn to *categoriz..."
          ],
          [
           "Once AutoTrain creates your project, you just need to connect your data. If you have the data locall..."
          ],
          [
           "<div class=\"grid grid-cols-2 gap-4\">\n  <figure class=\"image table text-center m-0 w-full\">\n    <medi..."
          ],
          [
           "In the screenshots above you can see that my project started 5 different models, which each reached ..."
          ],
          [
           "--\ntitle: \"Fine-Tune XLSR-Wav2Vec2 for low-resource ASR with ðŸ¤— Transformers\"\nthumbnail: /blog/assets..."
          ],
          [
           "XLSR\\'s successor, simply called **XLS-R** (refering to the\n[*\\'\\'XLM-R*](https://ai.facebook.com/bl..."
          ],
          [
           "![wav2vec2\\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/x..."
          ],
          [
           "```python\n!pip install datasets==1.18.3\n!pip install transformers==4.11.3\n!pip install huggingface_h..."
          ],
          [
           "```\n\nWe strongly suggest to upload your training checkpoints directly to the\n[Hugging Face Hub](http..."
          ],
          [
           "```\n\n------------------------------------------------------------------------\n\n\\\\( {}^1 \\\\) In the [..."
          ],
          [
           "Let\\'s start by creating the tokenizer to decode the predicted output\nclasses to the output transcri..."
          ],
          [
           "For each language-specific dataset, you can find a language code\ncorresponding to your chosen langua..."
          ],
          [
           "```\n\nMany ASR datasets only provide the target text, `'sentence'` for each\naudio array `'audio'` and..."
          ],
          [
           "```\n\n**Print Output:**\n\n| Idx |  Sentence |\n|----------|:-------------:|\n|\t1  | Jonuz, kÄ±sa sÃ¼reli g..."
          ],
          [
           "Let\\'s simply remove all characters that don\\'t contribute to the\nmeaning of a word and cannot reall..."
          ],
          [
           "```\n\n```python\ncommon_voice_train = common_voice_train.map(remove_special_characters)\ncommon_voice_t..."
          ],
          [
           "```\n\n**Print Output:**\n\n| Idx |  Transcription     |\n|----------|:-------------:|\n| 1   | birisi bey..."
          ],
          [
           "Let\\'s write another short mapping function to further simplify the text\nlabels. Remember, the simpl..."
          ],
          [
           "```\n\n```python\ncommon_voice_train = common_voice_train.map(replace_hatted_characters)\ncommon_voice_t..."
          ],
          [
           "```\n\n```python\nvocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\nvocab_dict\n```\n\n**Print..."
          ],
          [
           "```\n\nCool, we see that all letters of the alphabet occur in the dataset\n(which is not really surpris..."
          ],
          [
           "```\n\nCool, now our vocabulary is complete and consists of 39 tokens, which\nmeans that the linear lay..."
          ],
          [
           "```\n\nGreat, you can see the just created repository under\n`https://huggingface.co/<your-username>/wa..."
          ],
          [
           "A `Wav2Vec2FeatureExtractor` object requires the following parameters to\nbe instantiated:\n\n-   `feat..."
          ],
          [
           "```\n\nGreat, XLS-R\\'s feature extraction pipeline is thereby fully defined!\n\nFor improved user-friend..."
          ],
          [
           "```\n\nGreat, we can see that the audio file has automatically been loaded.\nThis is thanks to the new ..."
          ],
          [
           "```\n\nThis seemed to have worked! Let\\'s listen to a couple of audio files to\nbetter understand the d..."
          ],
          [
           "```\n\nGood! Everything looks fine - the data is a 1-dimensional array, the\nsampling rate always corre..."
          ],
          [
           "```python\ndef prepare_dataset(batch):\n    audio = batch[\"audio\"]\n\n    # batched output is \"un-batche..."
          ],
          [
           "```\n\nLet\\'s apply the data preparation function to all examples.\n\n```python\ncommon_voice_train = com..."
          ],
          [
           "```\n\nAwesome, now we are ready to start training!\n\nTraining\n--------\n\nThe data is processed so that ..."
          ],
          [
           "Without going into too many details, in contrast to the common data\ncollators, this data collator tr..."
          ],
          [
           "processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n\n    def __call__(self, features: ..."
          ],
          [
           "```\n\n```python\ndata_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n```\n\nNe..."
          ],
          [
           "```\n\nThe model will return a sequence of logit vectors:\n\\\\( \\mathbf{y}_1, \\ldots, \\mathbf{y}_m \\\\) w..."
          ],
          [
           "```\n\nNow, we can load the pretrained checkpoint of\n[Wav2Vec2-XLS-R-300M](https://huggingface.co/face..."
          ],
          [
           "```\n\nThe first component of XLS-R consists of a stack of CNN layers that are\nused to extract acousti..."
          ],
          [
           "```\n\nIn a final step, we define all parameters related to training. To give\nmore explanation on some..."
          ],
          [
           "```\n\n------------------------------------------------------------------------\n\n\\\\( {}^1 \\\\) To allow..."
          ],
          [
           "```\n\n**Print Output:**\n\n| Training Loss | Epoch | Step | Validation Loss | Wer    |\n|:-------------:..."
          ],
          [
           "```\n\nFor more examples of how XLS-R can be fine-tuned, please take a look at the official \n[ðŸ¤— Transf..."
          ],
          [
           "```\n\nFinally, we can decode the example.\n\n```python\nprint(\"Prediction:\")\nprint(processor.decode(pred..."
          ],
          [
           "--\ntitle: \"Advantage Actor Critic (A2C)\"\nthumbnail: /blog/assets/89_deep_rl_a2c/thumbnail.gif\nauthor..."
          ],
          [
           "[In Unit 5](https://huggingface.co/blog/deep-rl-pg), we learned about our first Policy-Based algorit..."
          ],
          [
           "Sounds exciting? Let's get started!\n  \n- [The Problem of Variance in Reinforce](https://huggingface...."
          ],
          [
           "This return \\\\(R(\\tau)\\\\) is calculated using a *Monte-Carlo sampling*. Indeed, we collect a traject..."
          ],
          [
           "However, increasing the batch size significantly **reduces sample efficiency**. So we need to find a..."
          ],
          [
           "On the other hand, your friend (Critic) will also update their way to provide feedback so it can be ..."
          ],
          [
           "Let's see the training process to understand how Actor and Critic are optimized:\n- At each timestep,..."
          ],
          [
           "The idea is that the Advantage function calculates **how better taking that action at a state is com..."
          ],
          [
           "Start the tutorial here ðŸ‘‰Â [https://colab.research.google.com/github/huggingface/deep-rl-class/blob/m..."
          ],
          [
           "In the next unit, we will learn to improve Actor-Critic Methods with Proximal Policy Optimization.\n\n..."
          ],
          [
           "--\ntitle: \"Faster Training and Inference: Habana GaudiÂ®2 vs Nvidia A100 80GB\"\nthumbnail: /blog/asset..."
          ],
          [
           "One of the easy, cost-efficient ways that Intel and Habana have made Gaudi2 available is on the Inte..."
          ],
          [
           "7. Go to the [Intel Developer Cloud management console](https://scheduler.cloud.intel.com/#/systems)..."
          ],
          [
           "Since Gaudi2 has roughly 3 times more memory per device compared to first-gen Gaudi, it is possible ..."
          ],
          [
           "The following table displays the throughputs we got for first-gen Gaudi, Gaudi2 and Nvidia A100 80GB..."
          ],
          [
           "[This script](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion) was..."
          ],
          [
           "### Fine-tuning T5-3B\n\nWith 96 GB of memory per device, Gaudi2 enables running much bigger models. F..."
          ],
          [
           "*BS* is the batch size per device. Gaudi2 and A100 runs were performed in fp32 with gradient checkpo..."
          ],
          [
           "---\n\nThanks for reading! If you have any questions, feel free to contact me, either through [Github]..."
          ],
          [
           "--\ntitle: \"Parameter-Efficient Fine-Tuning using ðŸ¤— PEFT\"\nthumbnail: /blog/assets/130_peft/thumbnail...."
          ],
          [
           "PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parame..."
          ],
          [
           "1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n..."
          ],
          [
           "2. Taking the previous example a notch up by enabling INT8 tuning of the `OPT-6.7b` model (6.7 Billi..."
          ],
          [
           "1. Let's get the necessary imports\n\n```diff\n  from transformers import AutoModelForSeq2SeqLM\n+ from ..."
          ],
          [
           "```\n\n2. Creating config corresponding to the PEFT method\n```py\npeft_config = LoraConfig(\n    task_ty..."
          ],
          [
           "```\n\nThis will only save the incremental PEFT weights that were trained. For example, you can find t..."
          ],
          [
           "with torch.no_grad():\n      outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_n..."
          ],
          [
           "```\n\n## Next steps\nWe've released PEFT as an efficient way of tuning large LLMs on downstream tasks ..."
          ],
          [
           "--\ntitle: \"MTEB: Massive Text Embedding Benchmark\" \nthumbnail: /blog/assets/110_mteb/thumbnail.png\na..."
          ],
          [
           "## Why Text Embeddings?\n\nText Embeddings are vector representations of text that encode semantic inf..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"assets/110_mteb/mteb_diagram_white_background.png\" alt=\"MTEB Taxono..."
          ],
          [
           "**ðŸ’ª Maximum performance** Multi-billion parameter models like [ST5-XXL](https://huggingface.co/sente..."
          ],
          [
           "```\n\nNext, benchmark a model on a dataset, for example [komninos word embeddings](https://huggingfac..."
          ],
          [
           "```\n\nNow add the metadata to the top of a `README.md` of any model on the Hub, like this [SGPT-5.8B-..."
          ],
          [
           "--\ntitle: Deploying ðŸ¤— ViT on Vertex AI\nthumbnail: /blog/assets/97_vertex_ai/image1.png\nauthors:\n- us..."
          ],
          [
           "> Vertex AI provides tools to support your entire ML workflow, across\ndifferent model types and vary..."
          ],
          [
           "- Vertex AI\n\n- Cloud Storage\n\n# Revisiting the Serving Model\n\nYouâ€™ll use the same [<u>ViT B/16 model..."
          ],
          [
           "```\n\nThe model will accept [<u>base64 encoded</u>](https://www.base64encode.org/) strings of images,..."
          ],
          [
           "The currently supported model types include `SavedModel` from\nTensorFlow, scikit-learn, and XGBoost...."
          ],
          [
           "**1.** The first step in the workflow is to upload the `SavedModel` to\nVertex AIâ€™s model registry:\n\n..."
          ],
          [
           "```\n\nLetâ€™s unpack the code piece by piece:\n\n- `GCS_BUCKET` denotes the path of your GCS bucket where..."
          ],
          [
           "```\n\nHere youâ€™re using an `endpoint_service_client` which is an\n[`EndpointServiceClient`](https://cl..."
          ],
          [
           "```\n\nHere, youâ€™re chaining together the model you uploaded to the Vertex AI\nModel Registry and the E..."
          ],
          [
           "```\n\nNotice how youâ€™re defining the traffic split for the model. If you had\nmultiple versions of the..."
          ],
          [
           "```\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\n\ndef predic..."
          ],
          [
           "```\n\nNote, however, this is not the only way to obtain predictions using a\nVertex AI Endpoint. If yo..."
          ],
          [
           "# Local Load Testing\n\nWe conducted a local load test to better understand the limits of the\nEndpoint..."
          ],
          [
           "<div align=\"center\">\n\n| **Machine Type**            | **Hourly Pricing (USD)** |\n|:-----------------..."
          ],
          [
           "# Conclusion\n\nIn this post, you learned how to deploy a Vision Transformer model with\nthe Vertex AI ..."
          ],
          [
           "--\ntitle: 'Few-shot learning in practice: GPT-Neo and the ðŸ¤— Accelerated Inference API'\n# thumbnail: ..."
          ],
          [
           "Few-Shot NLP examples consist of three main components: \n\n- **Task Description**: A short descriptio..."
          ],
          [
           "---\n\n## What is GPT-Neo?\n\nGPTâ -â Neo is a family of transformer-based language models from [EleutherA..."
          ],
          [
           "```python\nimport json\nimport requests\n\nAPI_TOKEN = \"\"\n\ndef query(payload='',parameters=None,options=..."
          ],
          [
           "```\n\n---\n## Practical Insights\n\nHere are some practical insights, which help you get started using `..."
          ],
          [
           "> ###  \n> Tweet: \"I'm a disabled happy person\"  \n> Sentiment: Negative  \n\nWhat could go wrong? Imagi..."
          ],
          [
           "--\ntitle: \"Director of Machine Learning Insights [Part 3: Finance Edition]\"\nthumbnail: /blog/assets/..."
          ],
          [
           "### [Ioannis Bakagiannis](https://www.linkedin.com/in/bakagiannisioannis//) - Director of Machine Le..."
          ],
          [
           "#### **2. What are the biggest ML challenges within finance?**\nI canâ€™t speak for companies but estab..."
          ],
          [
           "#### **4. What excites you most about the future of ML?**\nIt is difficult not to get excited with ev..."
          ],
          [
           "#### **1. How has ML made a positive impact on finance?**\nMachine learning (ML) has made a significa..."
          ],
          [
           "2. Gap between ML in basic research and education and ML in finance - Due to the regulated nature of..."
          ],
          [
           "#### **3. Whatâ€™s a common mistake you see people make trying to integrate ML into financial applicat..."
          ],
          [
           "Previously, he held the position of Fellows Leader & Senior Fellow, while working at Honeywell Inter..."
          ],
          [
           "#### **2. What are the biggest ML challenges within finance?**\nThe finance and banking industry brin..."
          ],
          [
           "#### **4. What excites you most about the future of ML?**\nNow is a great time to be in applied ML an..."
          ],
          [
           "--\ntitle: \"Instruction-tuning Stable Diffusion with InstructPix2Pix\" \nthumbnail: assets/instruction_..."
          ],
          [
           "Our code, pre-trained models, and datasets can be found [here](https://github.com/huggingface/instru..."
          ],
          [
           "With this approach, one can create exemplars covering many different tasks, which makes instruction-..."
          ],
          [
           "| ![cartoonization_results](https://huggingface.co/datasets/huggingface/documentation-images/resolve..."
          ],
          [
           "As hinted in the previous section, we wanted to benefit from both worlds:\n\n**(1)** training methodol..."
          ],
          [
           "Our final dataset for cartoonization can be found [here](https://huggingface.co/datasets/instruction..."
          ],
          [
           "We took different number of samples from the following datasets for each task and constructed a sing..."
          ],
          [
           "Overall, this setup helps draw parallels from the FLAN setup, where we create a mixture of different..."
          ],
          [
           "In our experiments, we found out that the first option helps us adapt to our datasets faster (in ter..."
          ],
          [
           "More comparative results are available [here](https://wandb.ai/sayakpaul/instruction-tuning-sd/runs/..."
          ],
          [
           "However, for low-light image enhancement, it leaves a lot to be desired: \n\n| ![image_enhancement_res..."
          ],
          [
           "<gradio-app theme_mode=\"light\" src=\"https://instruction-tuning-sd-instruction-tuned-sd.hf.space\"></g..."
          ],
          [
           "## Open questions\n\nWe acknowledge that our experiments are preliminary. We did not go deep into abla..."
          ],
          [
           "## Conclusion\n\nIn this post, we presented our exploration of â€œinstruction-tuningâ€ of Stable Diffusio..."
          ],
          [
           "## Citation\n\nTo cite this work, please use the following citation:\n\n```bibtex\n@article{\n  Paul2023in..."
          ],
          [
           "--\ntitle: \"Deploying the AI Comic Factory using the Inference API\"\nthumbnail: /blog/assets/165_ai_co..."
          ],
          [
           "## Duplicating the Space\n\nTo duplicate the AI Comic Factory, go to the Space and [click on \"Duplicat..."
          ],
          [
           "You can find more information about alternative engines and vendors in the project's [README](https:..."
          ],
          [
           "--\ntitle: What's new in Diffusers? ðŸŽ¨\nthumbnail: /blog/assets/102_diffusers_2nd_month/inpainting.png\n..."
          ],
          [
           "## Image to Image pipeline\n\nOne of the most requested features was to have image to image generation..."
          ],
          [
           "```\n\nDon't have time for code? No worries, we also created a [Space demo](https://huggingface.co/spa..."
          ],
          [
           "## Experimental inpainting pipeline\n\nInpainting allows to provide an image, then select an area in t..."
          ],
          [
           "```\n\nPlease note this is experimental, so there is room for improvement.\n\n## Optimizations for small..."
          ],
          [
           "```\n\n## Experimental ONNX exporter and pipeline\n\nThe new experimental pipeline allows users to run S..."
          ],
          [
           "```\n\n## New docs\n\nAll of the previous features are very cool. As maintainers of open-source librarie..."
          ],
          [
           "```\n\n\n### Diffusers Interpret\n\n[Diffusers interpret](https://github.com/JoaoLages/diffusers-interpre..."
          ],
          [
           "```\n\n### Japanese Stable Diffusion\n\nThe name says it all! The goal of JSD was to train a model that ..."
          ],
          [
           "## Thanks for reading!\n\nI hope you enjoy reading this! Remember to give a Star in our [GitHub Reposi..."
          ],
          [
           "--\ntitle: \"Announcing the Hugging Face Fellowship Program\"\nthumbnail: /blog/assets/62_fellowship/fel..."
          ],
          [
           "- **MarÃ­a Grandury** - Created the [largest Spanish-speaking NLP community](https://somosnlp.org/) a..."
          ],
          [
           "- **Christopher Akiki** - Contributed to sprints, workshops, [Big Science](https://t.co/oIRne5fZYb),..."
          ],
          [
           "Additionally, there are strategic areas where Hugging Face is looking for open-source contributions...."
          ],
          [
           "* **Where and how can I contribute?**\n  \nIt depends on your interests. Here are some ideas of areas ..."
          ],
          [
           "Please share in the #looking-for-contributors channel on the [Hugging Face Discord](https://hf.co/jo..."
          ],
          [
           "--\ntitle: \"How ðŸ¤— Accelerate runs very large models thanks to PyTorch\"\nthumbnail: /blog/assets/104_ac..."
          ],
          [
           "```\n\nWe'll explain what each of those arguments do in a moment, but first just consider the traditio..."
          ],
          [
           "## Creating an empty model\n\nPyTorch 1.9 introduced a new kind of device called the *meta* device. Th..."
          ],
          [
           "```\n\nas this large tensor requires `4 * 10**10` bytes (the default precision is FP32, so each elemen..."
          ],
          [
           "```\n\nThis works on any model, but you get back a shell you can't use directly: some operations are i..."
          ],
          [
           "```\n\nThis will return a dictionary mapping modules or weights to a device. On a machine with one Tit..."
          ],
          [
           "```\n\nAccelerate evaluated that the embeddings and the decoder up until the 9th block could all fit o..."
          ],
          [
           "```\n\nNow, each layer is always on the same device.\n\nIn Transformers, when using `device_map` in the ..."
          ],
          [
           "```\n\nIn this precision, we can fit the model up to layer 21 on the GPU:\n\n```python out\n\n\n{'model.dec..."
          ],
          [
           "```\n\nThis works pretty well for models with less than 1 billion parameters, but for larger models, t..."
          ],
          [
           "To load such a sharded checkpoint into a model, we just need to loop over the various shards. Accele..."
          ],
          [
           "```\n\nIf the device map computed automatically requires some weights to be offloaded on disk because ..."
          ],
          [
           "```\n\nThis will fit in Colab, but will be so close to using all the RAM available that it will go out..."
          ],
          [
           "```\n\n## Running a model split on several devices\n\nOne last part we haven't touched is how Accelerate..."
          ],
          [
           "This way, your model can be loaded and run even if you don't have enough GPU RAM and CPU RAM. The on..."
          ],
          [
           "--\ntitle: 'The Partnership: Amazon SageMaker and Hugging Face'\nthumbnail: /blog/assets/17_the_partne..."
          ],
          [
           "---\n\n## **Features & Benefits ðŸ”¥**\n\n## One Command is All you Need\n\nWith the new Hugging Face Deep Le..."
          ],
          [
           "Below you can find all the important resources to all published blog posts, videos, documentation, a..."
          ],
          [
           "## Documentation\n\n- [Hugging Face documentation for Amazon SageMaker](https://huggingface.co/docs/sa..."
          ],
          [
           "- [all Notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n- [Getting Started..."
          ],
          [
           "- [Image Classification with Vision Transformer](https://github.com/huggingface/notebooks/blob/maste..."
          ],
          [
           "---\n\n## **Getting started: End-to-End Text Classification ðŸ§­**\n\nIn this getting started guide, we wil..."
          ],
          [
           "```\n\nTo run training on SageMaker we need to create a sagemaker Session and provide an IAM role with..."
          ],
          [
           "```\n\n## Create the training script `train.py`\n\nIn a SageMaker `TrainingJob` we are executing a pytho..."
          ],
          [
           "# Data, model, and output directories\n    parser.add_argument(\"--output-data-dir\", type=str, default..."
          ],
          [
           "# compute metrics function for binary classification\n    def compute_metrics(pred):\n        labels =..."
          ],
          [
           "# Saves the model to s3; default is /opt/ml/model which SageMaker sends to S3\n    trainer.save_model..."
          ],
          [
           "```\n\n## Preprocess our data and upload it to s3\n\nWe use the `datasets` library to download and prepr..."
          ],
          [
           "# set format for pytorch\ntrain_dataset = train_dataset.rename_column(\"label\", \"labels\")\ntrain_datase..."
          ],
          [
           "```\n\n## Create a HuggingFace Estimator and train our model\n\nIn order to create a SageMaker `Training..."
          ],
          [
           "```\n\nTo start our training we call the .fit() method and pass our S3 uri as input.\n\n```python\n# star..."
          ],
          [
           "```\n\nThe \"Getting started: End-to-End Text Classification ðŸ§­\" example can be used for distributed tra..."
          ],
          [
           "distribution={\n    \"smdistributed\": {\"modelparallel\": smp_options},\n    \"mpi\": mpi_options\n}\n\n # cre..."
          ],
          [
           "```\n\n## Spot instances\n\nWith the creation of HuggingFace Framework extension for the SageMaker Pytho..."
          ],
          [
           "huggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',..."
          ],
          [
           "```\n\n## Git Repositories\n\nWhen you create an `HuggingFace` Estimator, you can specify a [training sc..."
          ],
          [
           "```\n\n## SageMaker Metrics\n\n[SageMaker Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/train..."
          ],
          [
           "```\n\n---\n\n## **FAQ ðŸŽ¯**\n\nYou can find the complete [Frequently Asked Questions](https://huggingface.c..."
          ],
          [
           "A: The DLCs are fully tested, maintained, optimized deep learning environments that require no insta..."
          ],
          [
           "_Q: How is my data and code secured by Amazon SageMaker?_\n\nA: Amazon SageMaker provides numerous sec..."
          ],
          [
           "A: No - the Hugging Face DLCs are open source and licensed under Apache 2.0.\n\n_Q: How can I run infe..."
          ],
          [
           "_Q: I use Hugging Face with Azure Machine Learning or Google Cloud Platform, what does this partners..."
          ],
          [
           "--\ntitle: \"Introducing Optimum: The Optimization Toolkit for Transformers at Scale\"\nauthors:\n- user:..."
          ],
          [
           "### ðŸ­ Optimum puts Transformers to work\n\nTo get optimal performance training and serving models, the..."
          ],
          [
           "However, putting transformer-based models into production can be tricky and expensive as they need a..."
          ],
          [
           "### ðŸ’¡ How Intel is solving quantization and more with Neural Compressor\n\nIntelÂ® [Neural Compressor](..."
          ],
          [
           "### ðŸŒŸ A journey of collaboration: join us, follow our progress\n\nEvery journey starts with a first st..."
          ]
         ],
         "hovertemplate": "source=blog<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "blog, circle",
         "marker": {
          "color": "#B6E880",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "blog, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -6.2562127,
          -3.031613,
          -4.97027,
          -5.455268,
          -5.408508,
          -4.529347,
          -6.2403603,
          -5.619998,
          -4.0333424,
          -2.6227973,
          -2.781243,
          4.6053543,
          4.834727,
          4.748618,
          -6.687653,
          3.6532753,
          3.5655026,
          3.424481,
          -7.0822296,
          0.17252122,
          10.678165,
          8.701055,
          9.4252,
          8.399855,
          8.872768,
          0.7873409,
          -0.033283394,
          -0.09201282,
          0.5897336,
          0.06939578,
          0.46307772,
          0.040724214,
          0.28025323,
          -7.5121026,
          -8.195596,
          -8.521995,
          -8.666612,
          -7.353945,
          -8.794579,
          14.103306,
          14.155615,
          -8.737661,
          -9.123018,
          -9.221976,
          -8.765903,
          -8.261809,
          -8.136628,
          -7.798984,
          -1.0708725,
          -1.0668621,
          -1.6797858,
          -0.33122393,
          -8.059649,
          -9.094024,
          0.48859677,
          0.39677432,
          0.43105024,
          0.091248125,
          0.04372863,
          0.56848353,
          0.33082134,
          0.5914887,
          0.22095737,
          0.29656774,
          0.2058927,
          4.314038,
          0.4511048,
          0.34551948,
          0.04675913,
          0.19599622,
          0.3321895,
          0.26262662,
          0.022886489,
          -1.7971983,
          -0.9286368,
          -0.06729471,
          0.6621881,
          -0.05873766,
          0.17720774,
          0.019583628,
          0.20870651,
          0.12441178,
          -9.158524,
          -5.6924124,
          -6.441169,
          -5.8423066,
          -5.7599382,
          -5.6988444,
          -5.8370976,
          -5.7423763,
          -5.8380256,
          -5.717122,
          -5.365873,
          -5.6783147,
          -4.1461325,
          -6.890574,
          -3.4233916,
          -7.2637706,
          -4.2343245,
          -4.047325,
          -3.4610574,
          -0.1678285,
          -7.2629013,
          -7.57235,
          -4.62275,
          -4.4809904,
          -3.1867514,
          6.8222857,
          6.786519,
          -2.911413,
          6.7694836,
          6.7973585,
          -3.3195536,
          6.71267,
          6.827557,
          -3.621933,
          -8.733636,
          -3.4762893,
          -3.3365817,
          -8.904291,
          -4.4037766,
          -5.471005,
          -5.953833,
          -2.046397,
          -4.092423,
          -3.0814843,
          -3.2689404,
          -2.9762952,
          -2.9766204,
          -3.4435303,
          -4.0547924,
          -7.5558324,
          -6.522996,
          -6.4331417,
          -4.5400414,
          -7.4301243,
          -6.5909896,
          -7.7169485,
          -6.8470626,
          -7.828104,
          -5.7067285,
          -7.8296523,
          -7.923743,
          -7.863681,
          -7.655385,
          -7.779461,
          -7.7935185,
          -6.7828093,
          -7.0501933,
          -7.0561624,
          3.5892785,
          3.5933225,
          1.8545531,
          3.7419066,
          2.618032,
          2.4998288,
          1.6273341,
          1.1524122,
          -1.9405508,
          3.6945083,
          -5.955389,
          -5.1703434,
          -4.6822042,
          -3.3708215,
          -2.0795739,
          -0.1344562,
          -5.948205,
          0.5096442,
          -5.062394,
          -6.5017595,
          -0.90090376,
          -0.48868695,
          -6.452059,
          -1.2002007,
          -2.790919,
          -7.1247125,
          -6.1712956,
          -5.3762326,
          -5.34452,
          4.7271004,
          4.744317,
          4.7190804,
          4.8880763,
          -2.851792,
          5.3086042,
          4.8972864,
          5.3022738,
          5.540989,
          -7.380294,
          0.33188593,
          0.5793779,
          0.8664279,
          0.46378595,
          -6.608833,
          -5.119357,
          1.4554805,
          2.6210663,
          -4.565764,
          4.041539,
          -5.170161,
          -5.306356,
          -2.4390087,
          3.57955,
          1.080182,
          0.54838085,
          -2.6958842,
          -2.477103,
          0.027335389,
          -2.1873984,
          -2.1972063,
          -0.4537409,
          -0.23611784,
          -0.089348204,
          0.051702406,
          6.7130485,
          6.5480413,
          0.25946555,
          -0.39765733,
          -0.30281118,
          -0.7090099,
          -1.5219442,
          -0.405804,
          1.9875302,
          -3.8515708,
          4.772521,
          -7.775975,
          -8.150429,
          -8.365867,
          -8.335334,
          -8.387541,
          -8.230716,
          -7.8998237,
          -8.157074,
          -8.761955,
          -8.114657,
          -8.368164,
          6.8249516,
          -8.321742,
          -6.94991,
          -8.005647,
          -6.5466766,
          6.6976576,
          6.7627716,
          6.883067,
          -0.99250245,
          8.116454,
          -6.200084,
          -4.12344,
          -3.489399,
          -5.8070774,
          -7.211841,
          -6.6165385,
          -6.744155,
          -5.264625,
          -5.2504582,
          -1.502231,
          -3.810717,
          6.7620883,
          -3.9951508,
          -3.6820633,
          -2.9533255,
          -3.230642,
          6.8363853,
          -4.0851765,
          -3.2988245,
          6.7304296,
          -3.210072,
          6.8326826,
          -3.468393,
          -2.5979252,
          6.7740436,
          -3.3454313,
          -3.1693602,
          -2.9582784,
          6.921213,
          -4.9937987,
          -4.758289,
          -3.7289853,
          -3.906085,
          -2.5874603,
          0.6574413,
          -2.6987398,
          -2.0801692,
          -3.2956157,
          -3.2418983,
          -4.8470936,
          -3.7129564,
          -3.8531883,
          -2.45476,
          -3.4772642,
          -3.782792,
          -2.248775,
          -3.7315114,
          -3.7537613,
          -3.6832466,
          -3.3721468,
          -3.6037326,
          -2.3416972,
          -1.3362602,
          -3.8508592,
          6.7440157,
          6.4275727,
          -4.7930846,
          4.9524255,
          4.9576406,
          5.3005314,
          7.576994,
          7.513211,
          0.5678084,
          7.418752,
          7.266823,
          7.6114364,
          -6.537014,
          -5.524615,
          -5.3783197,
          -5.541869,
          -5.725568,
          -5.1440334,
          6.395262,
          11.482881,
          11.559671,
          -6.4457035,
          -5.5004706,
          -4.5945654,
          -1.6083784,
          -0.53170997,
          -1.7816786,
          -2.4645078,
          -1.9974763,
          -0.8687835,
          -1.3085623,
          -1.8583486,
          -1.5286409,
          -0.8269293,
          -1.5664153,
          -1.3331695,
          -4.380798,
          -3.7032592,
          -7.6303563,
          -7.0017676,
          -7.3656254,
          -7.1537814,
          -7.433177,
          -6.1036115,
          -6.6192183,
          -3.0993483,
          -2.5280955,
          -3.1127567,
          -5.7587223,
          6.6823206,
          -6.571439,
          -1.2059877,
          -0.94091445,
          -1.4707488,
          -1.8299757,
          -1.2713515,
          -1.35807,
          -1.626436,
          -0.68753386,
          -6.2090287,
          -5.9380007,
          -5.990328,
          -5.8420606,
          -5.9368777,
          -5.916424,
          -5.9031067,
          -5.9392395,
          6.4895663,
          -5.9897585,
          -5.791828,
          6.4902873,
          6.4810805,
          6.551772,
          -5.9958167,
          -5.862754,
          -6.018689,
          -5.8391895,
          -5.2038245,
          4.019635,
          5.679675,
          -7.498955,
          -6.6573725,
          -3.6454196,
          -3.2643058,
          -4.2686563,
          -4.6700625,
          -3.3156433,
          -1.9471753,
          -2.015131,
          -1.972125,
          -1.0451425,
          0.08499716,
          -2.321862,
          -8.794787,
          -8.883584,
          -8.980882,
          -8.867644,
          -8.880792,
          -8.891571,
          -8.955312,
          -8.8348465,
          -8.971683,
          -8.815156,
          -8.920339,
          -8.82188,
          -8.890647,
          -8.814005,
          -8.517316,
          -8.686914,
          -7.6003,
          -7.9595337,
          -8.883423,
          -9.041791,
          -0.070174105,
          -9.306265,
          -8.959584,
          -8.788355,
          -8.901098,
          -8.719838,
          -6.549921,
          -6.93614,
          -5.0942936,
          -4.5565944,
          -5.3340864,
          -1.3617043,
          -5.989181,
          -5.7956843,
          -1.5563201,
          1.7220157,
          0.79741335,
          -2.0503392,
          -1.895668,
          -0.83009154,
          -2.3573833,
          -2.5369513,
          0.21327661,
          -1.928342,
          -1.9307576,
          -0.3351626,
          -0.7077645,
          -0.18461683,
          -0.47965005,
          -2.3468144,
          -3.612048,
          -5.3931,
          -4.981353,
          -6.6313744,
          4.518438,
          -3.3522723,
          -2.352502,
          -1.8409909,
          3.8028855,
          2.0489554,
          5.085942,
          0.52466124,
          -0.60382485,
          0.18903914,
          -2.835745,
          -5.4375014,
          -0.893763,
          -1.3946096,
          -1.1977715,
          -0.8947533,
          -0.7688389,
          -1.0600908,
          2.4205096,
          -1.051368,
          1.0760572,
          1.6751446,
          -1.2367331,
          -1.2049054,
          -1.7916497,
          -1.4172498,
          0.5847518,
          -0.4953195,
          -1.4138153,
          -1.3097264,
          -1.3283889,
          0.210144,
          -6.7205973,
          -5.4667497,
          -1.2944745,
          -2.2199073,
          -2.4769042,
          -0.09259011,
          -3.516494,
          -7.357086,
          -6.932882,
          5.131911,
          6.533711,
          7.95805,
          6.224998,
          7.054801,
          8.026442,
          -7.5975995,
          -6.848096,
          4.7409897,
          -5.7815123,
          -5.057275,
          -6.816836,
          -7.605816,
          -6.4060764,
          -5.2965593,
          -2.2637753,
          -2.743635,
          6.2374873,
          -1.2246553,
          -2.8274353,
          4.684353,
          5.081952,
          5.0110188,
          3.504771,
          7.298072,
          5.0979304,
          4.934203,
          -5.747423,
          7.611441,
          -4.118929,
          -3.7323246,
          -3.8454096,
          -4.151108,
          -4.4181666,
          -0.03639522,
          0.023908723,
          -6.494301,
          -5.2567782,
          -5.455351,
          -2.285259,
          -5.342304,
          -2.9273164,
          -2.6598053,
          -2.7077727,
          -2.7849264,
          -2.7086484,
          -2.6114395,
          -2.5001686,
          -2.7728171,
          -2.3296282,
          -5.172044,
          -5.4749413,
          -5.406335,
          -1.7197787,
          -5.3845577,
          -5.6175833,
          -7.40106,
          -7.008379,
          -7.0047355,
          -7.24963,
          -7.088508,
          -6.65648,
          -7.0616918,
          -7.1580076,
          -0.7968047,
          -0.39587262,
          -1.334078,
          -0.6195113,
          -0.66182685,
          0.52176225,
          0.89632076,
          -0.31892517,
          -0.75322187,
          -7.164973,
          3.277799,
          1.1027594,
          3.4083326,
          0.30376688,
          1.5197337,
          -0.033551253,
          -0.45708263,
          -0.1859736,
          -0.009366466,
          -9.204747,
          -9.685186,
          3.8446767,
          -9.41066,
          -9.412498,
          -9.536955,
          2.0834846,
          1.7082762,
          1.8396465,
          2.3155015,
          0.7802406,
          -8.27655,
          0.30569243,
          -0.071158335,
          -0.20501454,
          0.053632997,
          3.009611,
          -0.29086807,
          0.5402106,
          -9.1633005,
          -0.15903606,
          -0.20753917,
          0.13826616,
          1.4894117,
          1.7909621,
          1.9102458,
          0.3968468,
          -1.8641893,
          -7.191355,
          -6.84542,
          -6.9105706,
          -7.189169,
          -8.019994,
          -7.3630958,
          -7.1876097,
          -7.1486096,
          -7.7646246,
          -7.669507,
          -4.582437,
          -8.173425,
          -7.57325,
          -6.9816647,
          -0.65328676,
          7.31621,
          -6.612204,
          -6.946335,
          -7.278272,
          -6.5296497,
          -7.0818195,
          -7.4162626,
          -7.475251,
          -7.4051485,
          -7.3546352,
          -7.418001,
          -6.531988,
          -7.155857,
          -7.40003,
          -7.13184,
          -7.621614,
          -7.02924,
          -7.5721803,
          -7.446789,
          -7.47751,
          -7.4506364,
          -6.795904,
          -7.3312387,
          -7.471768,
          -3.8158126,
          5.869031,
          -4.363819,
          -7.549645,
          -7.53277,
          -7.6611276,
          -7.681902,
          -7.369336,
          -7.7309194,
          -8.461404,
          -7.8354435,
          -7.882906,
          -7.822542,
          -7.6740165,
          -7.6725206,
          -7.142803,
          -1.8655277,
          -2.4792287,
          -7.4444437,
          -6.5365252,
          -7.4406404,
          -7.0035906,
          -7.304182,
          -7.5968256,
          -7.379406,
          -7.809672,
          -8.135754,
          -7.665796,
          -7.711528,
          -7.5992756,
          -7.774778,
          -7.834757,
          -7.453656,
          -7.694279,
          -7.3179283,
          -1.5656089,
          -2.034074,
          -1.6071211,
          -7.2301807,
          -6.281927,
          -1.6661056,
          -1.9943885,
          -4.921593,
          -5.9665327,
          -3.9049547,
          -2.1112323,
          -2.0223303,
          -9.153548,
          -9.226747,
          -9.206953,
          -9.095249,
          -8.837033,
          -0.49709,
          -8.719773,
          -0.51423985,
          -0.47651225,
          -0.58668065,
          -0.89919096,
          -0.50428337,
          -0.52156097,
          -9.126617,
          -9.056126,
          -9.096809,
          0.5174785,
          0.34802857,
          0.30081317,
          0.1880499,
          0.20580761,
          0.30082086,
          0.060480565,
          0.2161223,
          0.2547056,
          0.23796363,
          -0.17992966,
          0.11379614,
          -9.0573635,
          -9.249011,
          6.6684136,
          -8.108997,
          -0.89719135,
          1.9785599,
          7.6643844,
          7.6546617,
          7.6049137,
          7.669096,
          7.6459455,
          7.597162,
          7.506055,
          7.696812,
          7.677767,
          -8.085038,
          1.151134,
          1.7965548,
          1.782647,
          3.4020255,
          3.2874897,
          -0.33623955,
          -0.1652703,
          0.53031844,
          0.37217915,
          -8.05401,
          -9.022267,
          -6.9122863,
          -6.648993,
          -7.0404596,
          -7.3605113,
          -7.620618,
          -7.5575533,
          -0.71607614,
          -2.2838595,
          -6.468045,
          -6.055393,
          -6.4535866,
          -7.6231503,
          -4.113041,
          -3.6548235,
          11.610233,
          11.481633,
          -4.797237,
          8.217719,
          -6.535065,
          -6.820968,
          -7.486099,
          -6.627109,
          -7.4140487,
          -7.2845497,
          -7.7368445,
          -7.2240715,
          -6.991597,
          5.7614503,
          5.4227805,
          -7.033731,
          -6.9334135,
          -5.95825,
          -5.3726897,
          -2.6123042,
          -2.5798848,
          -2.560573,
          -5.7384686,
          -3.9608896,
          -3.9134557,
          -3.979167,
          -4.9142585,
          -6.681065,
          -3.4248586,
          -5.1756983,
          -5.2443213,
          -0.7701453,
          -2.1780968,
          -5.4969864,
          -6.185266,
          -2.592542,
          -2.5741649,
          0.59519285,
          -2.9440913,
          -2.1624987,
          -2.8590534,
          -2.0858502,
          -2.0807095,
          -1.8639834,
          -2.107624,
          -1.4322581,
          -2.368005,
          -7.0671916,
          7.979027,
          -0.9144018,
          -6.0555825,
          -6.332634,
          -7.0106783,
          -6.2450595,
          -5.3007293,
          -9.057537,
          -6.4539433,
          -6.8879933,
          3.3386323,
          -5.7496862,
          -4.140127,
          -5.5051208,
          -5.486236,
          -6.14807,
          -5.5783696,
          -5.994239,
          3.342572,
          3.4709718,
          3.5469558,
          3.4013865,
          3.5476258,
          -6.604502,
          -8.3003025,
          -8.144951,
          -7.6032505,
          -7.8115044,
          -8.214973,
          -3.9560878,
          -2.6788607,
          -3.1026442,
          1.887047,
          -6.2204003,
          -6.265771,
          -0.29170132,
          -0.07830542,
          0.800454,
          2.1928997,
          0.6844666,
          1.0136476,
          2.085283,
          -0.5525618,
          -0.5256097,
          -0.52957565,
          -0.45493177,
          -0.39958224,
          -0.05550454,
          0.75279135,
          -2.5710468,
          0.60304993,
          0.29445848,
          -0.23117203,
          -3.9498315,
          -3.1670532,
          -3.329431,
          -0.9607251,
          -2.4996567,
          -1.8835084,
          -0.510382,
          -0.8169623,
          3.2453175,
          -3.0669718,
          -4.0897493,
          5.831471,
          5.187411,
          -6.4987884,
          -3.3621278,
          -2.1432917,
          3.054699,
          -6.8530455,
          4.684539,
          6.9540596,
          4.1789308,
          -8.635292,
          -8.794412,
          -8.900096,
          -7.063725,
          -9.300681,
          -9.200104,
          -9.229441,
          -9.162133,
          -9.264451,
          -9.27062,
          -9.323867,
          -9.200267,
          -7.699272,
          -7.784318,
          6.7395344,
          -6.356636,
          -5.7596583,
          -6.9514694,
          -5.9936776,
          -6.909181,
          -0.57006526,
          -1.2137629,
          -1.6087354,
          -1.4149624,
          -0.48889986,
          -0.2104491,
          -6.837044,
          -1.2016308,
          -5.074494,
          -3.8297935,
          -2.303116,
          -2.4848244,
          -3.363399,
          -3.618024,
          -1.5781089,
          -1.8324882,
          -0.9307766,
          -2.2827153,
          -1.7939541,
          -2.7247207,
          -0.23590465,
          -1.8031768,
          -1.6478138,
          -3.7169163,
          -4.7270384,
          -7.0523677,
          -9.029308,
          -9.130494,
          -9.331324,
          -8.923108,
          -1.7033817,
          -9.359953,
          -9.416214,
          -8.996183,
          -10.212511,
          -0.1316294,
          -9.451562,
          -9.555564,
          -0.33615398,
          -9.565304,
          -9.5182495,
          -9.631291,
          -9.573359,
          -9.466042,
          -9.489204,
          -9.647971,
          -9.528239,
          -9.192898,
          -9.102664,
          6.7899237,
          -5.784562,
          -5.3186684,
          -5.36951,
          -4.701857,
          -5.4607944,
          -5.4795012,
          -5.5135546,
          -5.7889476,
          -5.2520876,
          1.6583567,
          5.9508047,
          -2.3481941,
          -5.2071238,
          -4.8112607,
          1.1678759,
          1.141067,
          0.6900767,
          1.2811677,
          1.1394005,
          1.2313448,
          -5.078491,
          1.2623972,
          1.7674587,
          -4.202602,
          -4.9042172,
          -4.5790524,
          -5.2263303,
          -4.7712708,
          -5.609864,
          -5.5881476,
          -5.574421,
          -6.4496527,
          -5.711691,
          -4.971853,
          3.5301263,
          4.235241,
          -0.13319011,
          3.2781126,
          3.2863624,
          2.809331,
          -4.741993,
          -0.34009504,
          -5.2206903,
          -6.2930174,
          4.2652316,
          4.470387,
          4.601347,
          4.1020346,
          -3.69597,
          -3.3925447,
          -5.778092,
          3.8468018,
          3.9490366,
          3.2687087,
          -2.651323,
          -2.335008,
          -2.4253647,
          0.81937176,
          0.075980306,
          -2.3240056,
          -2.4194357,
          -1.667029,
          -4.839081,
          -4.802739,
          4.7283306,
          -4.4484515,
          -4.791094,
          -4.6126976,
          -4.579487,
          -5.1438684,
          0.20537025,
          2.1228456,
          2.109599,
          1.4651337,
          -5.4512267,
          -5.7113066,
          -0.42126116,
          -0.8091637,
          -0.5762248,
          -0.48508486,
          -1.2851601,
          -0.90399164,
          -1.1541466,
          -0.13242066,
          0.82553226,
          -1.3870629,
          1.5411941,
          -3.2873173,
          0.34481597,
          -4.20892,
          -1.3943567,
          -1.0777333,
          0.31985757,
          0.6319843,
          -0.98569584,
          -1.3085591,
          -1.1044675,
          -1.2078729,
          -1.0240972,
          -1.288644,
          -1.1005908,
          -6.0928535,
          -5.180356,
          -5.432419,
          -2.0825775,
          -1.806484,
          -3.3864307,
          -0.96807927,
          -3.8062499,
          -2.4509964,
          -4.9417677,
          -5.42828,
          -3.8979428,
          -2.4815366,
          -5.4719534,
          -3.8137074,
          -0.8224636,
          -4.4281135,
          -4.02011,
          -1.5999062,
          -2.363954,
          -0.3473186,
          -1.3351355,
          -0.60484946,
          -5.7815433,
          -5.175394,
          7.905975,
          7.604253,
          2.220337,
          4.6968064,
          4.897759,
          -4.708358,
          -4.6702566,
          -4.7858925,
          -4.144828,
          -3.9375732,
          -3.7330449,
          -3.9297276,
          -3.9838488,
          -3.901667,
          -4.1751914,
          -7.1970406,
          -7.0490184,
          -7.230691,
          -7.30611,
          -7.1025596,
          -7.3193383,
          -7.144967,
          -7.3276343,
          -6.557187,
          -6.409587,
          -6.615833,
          -6.250842,
          -6.4860854,
          -6.041365,
          -6.6227717,
          -6.9191856,
          -7.1380835,
          -6.663945,
          -6.157359,
          -6.600998,
          -6.6449122,
          -6.7150087,
          -6.753399,
          -7.1496015,
          -6.889897,
          -6.8018994,
          -6.837637,
          -6.9649706,
          -6.797752,
          -6.788044,
          -6.8992333,
          -7.6933336,
          -7.1631594,
          -6.4028444,
          -4.491628,
          -6.143211,
          -3.3717418,
          -5.808185,
          -4.51274,
          -4.4588675,
          -2.9012845,
          -6.2357903,
          -6.332088,
          -5.4939847,
          -6.1788054,
          -6.3700414,
          -6.3025174,
          -6.4245515,
          -6.317379,
          -6.5225472,
          -6.368859,
          -4.993667,
          0.45005497,
          -0.65514535,
          5.0544024,
          5.055473,
          5.053405,
          5.0545945,
          5.054563,
          5.053946,
          5.054901,
          5.0547614,
          5.054426,
          5.0548396,
          5.053985,
          5.0541387,
          5.0534825,
          5.0546727,
          5.056399,
          5.054977,
          5.0524473,
          5.053053,
          5.0533586,
          -2.996875,
          -2.2924178,
          -0.47812238,
          -3.249862,
          -2.9343114,
          -1.0095537,
          -1.5402861,
          -2.623966,
          -2.3876274,
          -3.2029455,
          -3.8998425,
          -3.8762727,
          -3.889711,
          -3.8120406,
          -3.904138,
          -3.8979132,
          -3.8916981,
          -3.4720232,
          -2.7828434,
          -2.348678,
          -3.8878245,
          -3.6678462,
          -3.8175178,
          -3.838837,
          -3.6812625,
          -3.6763668,
          -2.0400927,
          -2.1318295,
          -3.2314675,
          -4.1386085,
          -3.6276708,
          -2.492271,
          -2.7875319,
          -4.329906,
          -2.262579,
          -0.45112026,
          -3.3932514,
          -3.0413244,
          -1.3879578,
          -7.5280232,
          -6.536816,
          -6.640304,
          -6.4144564,
          -6.371318,
          -6.2453256,
          -7.3191504,
          -5.5491757,
          -5.450149,
          -5.3884125,
          -5.522916,
          -6.2041163,
          4.656642,
          -4.8178086,
          -5.4172997,
          -5.168567,
          4.7791877,
          5.6061406,
          4.9250937,
          -4.042902,
          -3.2711713,
          1.4640507,
          -4.4634576,
          -3.4476001,
          0.36913595,
          5.4515624,
          4.692409,
          -5.9231143,
          -5.429837,
          -7.6487193,
          -3.9179664,
          -7.3116636,
          -7.609391,
          -1.9863695,
          -4.491841,
          -5.8515234,
          -5.9142222,
          -5.0846996,
          -0.7108804,
          -0.61290795,
          2.1913676,
          -1.3347797,
          -0.7846663,
          -6.9058547,
          4.286724,
          7.9273963,
          3.416938,
          3.4487846,
          3.8101351,
          3.7964501,
          4.3981633,
          -6.240778,
          4.359396,
          5.395598,
          -7.582893,
          -7.371298,
          -7.5261383,
          -5.6940565,
          -2.8657887,
          -5.5710773,
          -5.728959,
          -5.7103405,
          -5.8364587,
          -7.7248154,
          -7.1406336,
          -7.628559,
          -7.709296,
          -7.5036077,
          -7.3500333,
          -6.4361644,
          -7.4705954,
          -7.424719,
          -7.5728025,
          -7.400166,
          -7.1685624,
          -7.3363705,
          -7.2745237,
          -7.188941,
          -7.0071054,
          -6.1513777,
          -2.5729024,
          -3.0791233,
          -3.6590447,
          -2.7976694,
          -1.6611077,
          -2.280119,
          -1.630174,
          -2.6930614,
          -7.580999,
          -7.120431,
          -7.402828,
          0.89488477,
          0.78431314,
          1.6033714,
          1.0268854,
          -7.128267,
          -6.4317727,
          -5.7995515,
          -4.99762,
          3.6403265,
          2.475682,
          5.1511893,
          1.9862081,
          3.1976252,
          2.1347811,
          2.4988115,
          1.6550581,
          1.2538604,
          0.93451333,
          1.8028446,
          1.1033572,
          2.6064272,
          -5.27418,
          -1.7121624,
          0.836434,
          0.71027684,
          -0.5886228,
          -1.059001,
          -0.002587958,
          -1.1510507,
          -4.4958267,
          -4.783781,
          -5.5171714,
          -7.434944,
          -6.5527306,
          -6.0281467,
          -6.175527,
          -5.919912,
          -5.3943925,
          -3.82623,
          -3.4771025,
          -3.8008497,
          -4.9412785,
          -1.2981973,
          -1.7394587,
          -1.3080922,
          -0.18179755,
          -1.7862991,
          -5.1405063,
          -2.6795983,
          -4.653901,
          -6.428733,
          11.2654705,
          -6.0154805,
          -4.898547,
          -4.217361,
          -7.6186333,
          -7.8093233,
          0.037387777,
          -7.7835193,
          -7.572009,
          -7.2617183,
          1.6310073,
          -7.092935,
          -7.306262,
          -7.583265,
          -0.37511078,
          -7.230171,
          -0.6765903,
          -0.21364057,
          0.18008009,
          -6.8992705,
          -7.4755936,
          -7.122479,
          -1.0821238,
          -0.9741867,
          -7.279594,
          -1.0583085,
          -0.013076697,
          0.060018115,
          -1.1534103,
          -0.41553208,
          -1.493871,
          -1.4195559,
          0.1267348,
          -0.4107299,
          -1.0969986,
          -7.505686,
          3.2803466,
          2.937142,
          1.2418568,
          1.4388741,
          1.1751308,
          -1.3646033,
          -6.7607417,
          -7.2113066,
          -7.186852,
          -1.2287716,
          -7.275522,
          -7.0726314,
          -7.2343154,
          2.5406122,
          2.6796293,
          -7.357216,
          -7.262861,
          -7.4280677,
          -7.482441,
          -7.2007704,
          -7.385107,
          -7.3452973,
          -7.2039976,
          -7.2602873,
          -7.281816,
          -7.4449015,
          -7.376367,
          -0.79439324,
          -0.5998136,
          1.7651757,
          -0.68060565,
          -1.0333054,
          -1.1246961,
          -1.122191,
          -1.3540604,
          -7.3564024,
          -1.3215663,
          -1.4559743,
          -0.8685621,
          -9.259624,
          -0.94922286,
          -0.4236157,
          -1.427544,
          -12.855245,
          -0.26998237,
          -8.151209,
          -0.52120787,
          -0.64281774,
          -7.044425,
          -13.4965725,
          -0.41042933,
          -0.2719187,
          -0.27611673,
          -0.21036452,
          -0.295904,
          -0.30983168,
          0.5038488,
          0.51781887,
          0.14572163,
          1.925903,
          0.7043329,
          0.7969934,
          0.85367465,
          0.090182334,
          1.2147143,
          1.4773781,
          0.51017344,
          0.18514375,
          -0.1872327,
          1.5981115,
          0.6415416,
          -0.8864231,
          -0.9124752,
          -0.79413265,
          -7.549113,
          -6.2289343,
          -6.5548882,
          -6.4760084,
          -6.2161756,
          1.9877975,
          2.3164372,
          -4.9106593,
          -5.3804646,
          -7.592874,
          -7.5101266,
          -7.459378,
          -6.1536126,
          -4.908674,
          -4.7767744,
          -5.3137617,
          -5.5012217,
          -3.9492016,
          -4.478873,
          -5.9916577,
          -6.2030306,
          -5.7311196,
          -5.52852,
          -5.7703414,
          -6.0692983,
          -5.22884,
          7.5029263,
          5.38936,
          5.2086926,
          4.133568,
          -3.741872,
          -5.1352158,
          4.8437896,
          -1.6330574,
          4.7872424,
          5.100714,
          -4.2162724,
          5.297262,
          5.102789,
          5.1848826,
          4.4601874,
          4.810703,
          5.322128,
          5.279229,
          5.0674396,
          6.9480414,
          -5.039024,
          4.6111774,
          -4.4954686,
          5.1617303,
          5.3411107,
          7.8901443,
          7.3981223,
          7.8687177,
          7.454,
          7.5815015,
          14.295412,
          7.67216,
          -5.189063,
          6.8812876,
          7.826316,
          5.2450914,
          5.2890053,
          7.783985,
          7.896491,
          7.944747,
          14.327245,
          7.5382357,
          7.7536645,
          7.863966,
          7.580423,
          4.6451864,
          -4.120582,
          -5.087194,
          -0.37206402,
          -0.8733674,
          -0.391888,
          4.6647434,
          4.181192,
          -3.9144804,
          -2.8285072,
          -0.6797972,
          -0.89450926,
          -0.9760121,
          -1.088941,
          2.0392787,
          -0.6871015,
          0.5991164,
          0.20990366,
          1.5074933,
          1.1990427,
          0.829287,
          -0.12844065,
          1.7426562,
          1.0230273,
          1.543616,
          0.9064163,
          1.7281063,
          1.3008205,
          1.7882884,
          -5.0963016,
          -7.294017,
          -6.1171184,
          -6.156697,
          -6.722263,
          -6.7610917,
          -8.782005,
          0.5810439,
          1.155369,
          -2.7527108,
          -8.34518,
          3.1626444,
          -0.01132804,
          -0.30082503,
          10.077438,
          -8.647117,
          -8.741425,
          -0.26983067,
          0.25881088,
          -0.3388034,
          -9.549416,
          -8.949108,
          -5.706983,
          -6.017052,
          -5.910112,
          -0.67761177,
          -1.7165726,
          5.948787,
          4.7575846,
          5.5854554,
          0.89535034,
          0.23087966,
          5.2729015,
          0.5647491,
          0.008464076,
          0.1407334,
          0.22697388,
          -1.4824892,
          -0.51907265,
          -3.0433192,
          12.524527,
          12.320163,
          12.123678,
          12.27022,
          12.316431,
          11.817501,
          12.221496,
          12.545987,
          12.587099,
          -5.7999425,
          -5.553396,
          -5.5202365,
          -5.4416513,
          -5.5687857,
          7.8395114,
          7.167505,
          5.3346424,
          4.7327056,
          5.507248,
          4.4577045,
          5.2219105,
          5.128341,
          -2.2907567,
          1.606196,
          6.9514465,
          5.0877066,
          5.158704,
          5.4295764,
          5.296939,
          5.180088,
          3.7572002,
          4.6812673,
          4.9837255,
          5.3830924,
          5.3574257,
          3.728288,
          6.934983,
          5.940274,
          4.824959,
          5.063898,
          5.17906,
          0.00026401356,
          5.2965546,
          5.321189,
          5.255009,
          5.3202214,
          3.9033504,
          5.0341363,
          4.7473793,
          5.4159307,
          5.628415,
          5.5086823,
          5.4038568,
          5.25936,
          5.2136393,
          5.558431,
          6.256452,
          7.805725,
          7.835266,
          14.306781,
          14.304772,
          7.731557,
          7.3921146,
          7.8164377,
          7.842812,
          -5.7682643,
          -6.7655616,
          -6.716283,
          -6.732723,
          -5.136565,
          -5.548889,
          -5.266488,
          -4.9600306,
          -3.6522968,
          -3.4813545,
          -0.7294939,
          -0.52768224,
          -5.119492,
          -4.517591,
          -4.326586,
          -4.947362,
          -5.3211727,
          -7.4533935,
          -6.5890017,
          -6.4215937,
          -4.5169725,
          -3.9502323,
          -1.403358,
          -4.5277123,
          7.259025,
          5.3240914,
          5.2953286,
          4.870875,
          5.4728007,
          5.288612,
          5.40228,
          5.9112244,
          5.7009654,
          5.2962484,
          5.311176,
          5.2267528,
          5.100855,
          5.515243,
          5.1276374,
          5.115468,
          5.149543,
          7.9287567,
          14.282259,
          14.269627,
          6.8915644,
          7.8158875,
          7.709876,
          -1.720844,
          7.3057547,
          8.008921,
          8.103128,
          -4.2509627,
          -4.736276,
          -4.4955726,
          -7.3472114,
          -2.1004965,
          -7.7420645,
          -7.6347637,
          -7.6840906,
          -7.878797,
          -7.627489,
          -6.9432726,
          -7.844793,
          -7.7450376,
          -7.070217,
          -5.373597,
          -5.0534816,
          -4.990421,
          0.59282607,
          1.6509182,
          -5.138563,
          -5.2521777,
          -4.796922,
          -7.59681,
          -6.806969,
          -6.7874637,
          -3.6569486,
          -3.4792411,
          0.056705587,
          -0.3619114,
          -3.3157876,
          -3.788991,
          -7.380245,
          -3.6783297,
          -3.517001,
          -3.7238104,
          -7.241105,
          2.6665907,
          0.3004317,
          2.2987497,
          -6.3597074,
          -5.901143,
          -5.820736,
          -5.8648496,
          6.483632,
          -5.422277,
          -5.127822,
          6.5128593,
          6.6358557,
          -5.9471564,
          -6.0033193,
          -5.8335342,
          -5.9983816,
          -5.693463,
          -5.800617,
          -5.816924,
          -5.920932,
          -6.050389,
          -6.245907,
          -2.8662577,
          -2.336804,
          -1.9708222,
          -1.529671,
          -1.7643993,
          -1.8888991,
          -1.7721319,
          -1.751826,
          -1.576023,
          -1.7043642,
          -1.9972534,
          -2.0288818,
          -2.0111964,
          -1.8257437,
          -1.3350314,
          -1.3802421,
          -7.7090383,
          -1.9854608,
          -3.7053533,
          -5.103628,
          -2.4681509,
          6.6663475,
          -0.6425256,
          -0.8467409,
          0.83397496,
          -0.114674985,
          -0.16255306,
          0.020362074,
          -0.6678258,
          -0.08568273,
          -0.65809894,
          -0.17465638,
          -6.752052,
          -0.13074142,
          -0.259814,
          -0.27327648,
          -0.6163573,
          -0.2437312,
          -0.019546274,
          0.008306871,
          -0.20853613,
          -0.18218628,
          -6.3880796,
          -3.2804816,
          1.9899553,
          -3.7988927,
          -2.4179864,
          -4.216893,
          -4.8606677,
          -4.767732,
          5.0836997,
          4.8847413,
          -0.9717359,
          -1.2477318,
          -8.77937,
          -0.83261335,
          -0.096116364,
          2.1205928,
          -1.8790892,
          -0.9257106,
          -2.3755372,
          -0.9790153,
          -7.393478,
          -7.3842773,
          -7.235907,
          -7.138436,
          -6.60147,
          14.087042,
          -7.3426223,
          2.8390045,
          -7.210839,
          -7.026749,
          -6.0236707,
          -7.434105,
          -7.7478247,
          -7.497392,
          -6.83003,
          -6.2823734,
          -6.3891263,
          -6.403214,
          -7.2753377,
          -7.0656075,
          -7.2660975,
          -7.204591,
          -6.8797174,
          -1.4338833,
          -0.5185755,
          -6.5484056,
          0.23970023,
          -0.38693157,
          0.40663034,
          -2.0254881,
          -7.143006,
          1.709259,
          -7.22512,
          -7.3560348,
          -7.671289,
          -7.8334937,
          -7.614763,
          -7.0634785,
          -7.7235746,
          -7.845197,
          -7.834237,
          -7.6150646,
          -7.775948,
          -7.8815694,
          -7.2928114,
          -7.4978943,
          -7.7705984,
          -7.858421,
          1.9349188,
          1.8206111,
          1.6486261,
          1.6424527,
          -7.9836807,
          -7.843171,
          -8.118169,
          -1.7376782,
          -6.803534,
          -7.9654922,
          2.3756464,
          -7.8750634,
          6.812152,
          -7.4304867,
          -4.656717,
          -3.5068583,
          -1.0842261,
          -4.103771,
          0.7810666,
          -1.9075255,
          -0.35233566,
          -0.7801502,
          0.6666903,
          -0.9499128,
          1.1155312,
          0.31402653,
          -3.8774662,
          0.6278024,
          3.8309731,
          4.4663167,
          -6.669618,
          -5.3526936,
          -5.097255,
          -4.8276453,
          -4.3961105,
          -5.03101,
          -5.1621346,
          6.0079927,
          -5.448835,
          -2.3389885,
          2.5376432,
          -4.6921554,
          -0.113775395,
          -0.6271744,
          -5.580716,
          -7.50462,
          -7.525271,
          -7.618165,
          2.3670511,
          -7.076836,
          -6.5890303,
          -6.960425,
          -5.8514175,
          -6.9416957,
          -7.4831133,
          -7.321635,
          -6.572072,
          -3.6007707,
          3.9942749,
          -7.109201,
          -7.18578,
          2.8002527,
          -5.6090503,
          -0.2905187,
          1.0556513,
          -2.989036,
          0.29039258,
          -1.5563953,
          -0.17747296,
          -2.1639924,
          -0.16579899,
          -5.772304,
          1.70518,
          2.2497306,
          -1.115961,
          7.5201883,
          5.222783,
          1.9816655,
          -0.85604465,
          -0.62633765,
          -6.41384,
          -6.2616124,
          -6.200253,
          -6.409489,
          -6.250722,
          -6.480278,
          -6.3741317,
          -6.5812464,
          -6.7589917,
          -6.637772,
          -6.776303,
          -6.1400537,
          -6.5192246,
          -6.3916945,
          -6.6291566,
          -7.0181255,
          -6.3856907,
          -6.3207407,
          -5.33685,
          -6.2271,
          -7.450941,
          0.088110834,
          2.7585306,
          2.720029,
          0.5525572,
          0.4971462,
          0.95019877,
          0.59128785,
          1.7933605,
          0.5916999,
          0.85323066,
          -4.6863775,
          -3.8828175,
          -5.8977337,
          -4.5773745,
          -4.046302,
          -3.6353576,
          -3.105146,
          -1.8666673,
          -1.3675655,
          -2.0156062,
          -0.009008493,
          3.7397113,
          1.0472072,
          -3.8374374,
          -3.1142864,
          -4.8940315,
          -7.736157,
          -7.0385256,
          -7.823001,
          -7.192106,
          -7.895154,
          -7.816284,
          -7.640363,
          -7.854866,
          -7.8465867,
          -7.459256,
          -7.944184,
          -7.944107,
          -7.532849,
          -7.9793305,
          -7.7504807,
          -7.220386,
          -7.826829,
          -7.672226,
          -7.4727435,
          -7.8511243,
          -7.3624496,
          -7.9382377,
          -7.6879377,
          -7.7366447,
          -7.7325754,
          -7.9722757,
          -7.87391,
          -7.9683285,
          -7.6695633,
          -7.428475,
          -7.5065217,
          -6.984841,
          -3.8907943,
          -4.816869,
          -2.800646,
          -2.725185,
          -3.5494962,
          -3.9514384,
          -2.3707385,
          -2.469375,
          -4.621477,
          -1.1477591,
          -0.31771028,
          -0.2136823,
          -7.618905,
          -7.800845,
          0.040378727,
          0.051995493,
          -6.839372,
          -6.6239886,
          -2.8217082,
          -0.8328787,
          -0.6206429,
          -1.1736134,
          -0.6919836,
          -3.572652,
          -1.0329585,
          -1.6388714,
          -2.3246005,
          -0.16454545,
          -0.7343666,
          -4.0591173,
          -3.5562649,
          -4.027746,
          -2.1887333,
          -3.688391,
          -3.7378457,
          -3.8196416,
          -3.9056349,
          -3.8911028,
          -3.1405547,
          -2.8018181,
          -3.4650545,
          -3.3122277,
          -1.4998826,
          -2.1880116,
          -0.6128692,
          -2.0255492,
          -1.1677128,
          -1.1392947,
          -2.4908159,
          -1.8505299,
          -1.3778058,
          -2.8722093,
          -3.111452,
          -6.696243,
          -7.472652,
          -7.4547153,
          2.722441,
          -6.959236,
          -7.0859666,
          -6.9330273,
          -7.210776,
          -7.2537327,
          -6.994854,
          -6.740358,
          -7.408775,
          -7.4120426,
          -6.7220225,
          -6.7365165,
          -5.778832,
          -5.432318,
          -5.1084814,
          -5.044447,
          -2.8120944,
          -2.3953352,
          -0.57727855,
          -3.4242935,
          -3.4203124,
          -2.072706,
          -2.4458413,
          -2.426561,
          -4.9385676,
          -5.410863,
          -5.0089326,
          12.707622,
          -5.656254,
          -6.365482,
          -3.9712272,
          -4.416034,
          -4.6505284,
          -3.6668775,
          -7.2061257,
          9.414624,
          9.12009,
          9.383268,
          8.230965,
          9.362016,
          8.96877,
          -7.0853624,
          -6.739978,
          -5.440428,
          -5.6158295,
          -5.5152245,
          -2.118825,
          3.0799906,
          -0.70215285,
          2.0649695,
          11.428818,
          11.620597,
          -6.3698006,
          -6.3246975,
          -8.9654875,
          -2.9882104,
          -2.5808308,
          -2.41176,
          -2.7029884,
          -2.5248652,
          -2.1419687,
          -1.8456975,
          0.600323,
          -2.8351636,
          2.0462463,
          -2.1392882,
          -3.2991846,
          -0.80349225,
          -0.5246574,
          -0.4552703,
          0.23985729,
          -0.7457597,
          0.27810997,
          0.08940476,
          0.3933266,
          3.1369908,
          -7.6041613,
          2.5339632,
          2.594669,
          0.21869522,
          0.6875171,
          3.2604237,
          3.1592398,
          -6.5787287,
          4.6871276,
          4.8169208,
          4.1778636,
          5.928628,
          -3.9667945,
          -0.5781569,
          -2.463406,
          -2.7406723,
          0.20403345,
          -1.383437,
          0.105071776,
          -0.45105827,
          0.17246568,
          -0.9394832,
          -0.25647303,
          0.25662947,
          0.4601678,
          -3.1084554,
          -0.12065711,
          2.1821723,
          -2.583402,
          -4.4624686,
          -6.3912287,
          -7.3490973,
          -7.1843314,
          -7.074153,
          -1.5816214,
          -6.925499,
          -7.725762,
          -2.1061287,
          -0.21666256,
          -7.152333,
          -7.245098,
          -7.2019615,
          -7.1861706,
          -7.4485946,
          -2.5302055,
          -2.5969026,
          -2.4407957,
          -7.618652,
          -7.608762,
          -7.7897873,
          -7.759671,
          -8.060348,
          -7.9150133,
          -7.7498727,
          -8.127923,
          -7.772454,
          -7.6451464,
          -6.506608,
          5.3873224,
          6.5919313,
          -1.0742091,
          -0.958508,
          -0.79433155,
          -0.83520615,
          -6.592436,
          -4.6433516,
          -5.931469,
          -6.5894265,
          -1.5317273,
          -1.4033493,
          -3.3758516,
          0.14728987,
          -0.0322554,
          -3.6497588,
          -5.146302,
          -6.1000032,
          -2.0067751,
          -2.9820952,
          4.4766674,
          -8.000074,
          -8.442317,
          -8.706108,
          -8.2463045,
          -0.92812735,
          -0.9251863,
          -0.5415281,
          -0.23259565,
          -7.7920117,
          -0.20715432,
          -8.467232,
          -6.2065024,
          -6.371731,
          -6.2825117,
          -7.59045,
          -6.440294,
          -5.684978,
          6.758847,
          -6.194543,
          -6.2146707,
          -6.3060355,
          -6.738952,
          -5.778112,
          -3.3340065,
          -2.7437103,
          -3.541453,
          -0.28166905,
          -2.5334055,
          -1.9346482,
          -2.5807264,
          -2.223851,
          -2.8028252,
          -2.5392592,
          -2.3677173,
          -1.913951,
          -3.8971605,
          -0.39408556,
          -0.23887263,
          -4.5416856,
          -0.30564567,
          -2.8487532,
          -3.4621594,
          -2.689334,
          -2.9852076,
          -7.6116033,
          -7.0819244,
          -0.8716805,
          -0.6075674,
          -1.3680317,
          0.012230936,
          -0.02992497,
          -1.3180332,
          -0.9958485,
          -0.6427833,
          -0.8723388,
          1.0459868,
          -0.95553523,
          -0.7792902,
          -0.5612294,
          -0.43122065,
          -0.22824511,
          1.0152487,
          -0.19389245,
          -1.176706,
          -1.682776,
          -8.905556,
          -8.021545,
          2.3058288,
          1.6666819,
          1.6707941,
          1.0731272,
          0.6167573,
          0.86072904,
          -0.23620881,
          -0.2196907,
          -0.94275236,
          0.7192719,
          -3.6788328,
          -3.370387,
          -7.2442718,
          -2.92731,
          -2.6367025,
          -3.3845553,
          -0.82336175,
          -0.92958117,
          1.7765628,
          -0.78432804,
          -0.5706081,
          -0.6899093,
          -0.51498556,
          -0.7469907,
          -4.1678896,
          -2.5336783,
          -1.2120839,
          -1.3487586,
          -1.3424692,
          -1.4329206,
          -1.1608963,
          -2.3703532,
          4.4912925,
          -2.0041866,
          -1.769474,
          -1.2118094,
          -1.6186146,
          -7.057705,
          -9.713254,
          -9.918236,
          -9.812034,
          -9.607608,
          -9.671265,
          -9.662071,
          -9.606603,
          -9.311714,
          -9.0515,
          -9.567959,
          -9.1470175,
          -9.6825485,
          -9.687105,
          -9.775899,
          -9.701312,
          0.009453397,
          -9.640132,
          -9.466838,
          -8.521699,
          -9.816617,
          -9.8004265,
          -9.871503,
          -5.52098,
          -4.7088537,
          -5.940215,
          -4.993888,
          -5.093522,
          -1.7999996,
          -2.8104348,
          -2.7189066,
          -4.7348275,
          -5.8897853,
          -5.5692096,
          -4.1539493,
          -4.4570737,
          -3.3591642,
          -4.9413157,
          13.862456,
          11.671409,
          10.855368,
          11.789536,
          -6.941645,
          -5.1402473,
          -4.675995,
          -4.6807137,
          -1.4148942,
          -1.4624139,
          -4.5314465,
          -7.067142,
          -3.8082304,
          -3.683671,
          -3.8564823,
          -3.705714,
          -4.7162447,
          -4.7919965,
          -1.901495,
          -4.302912,
          -1.2640421,
          -4.6916018,
          -7.0842423,
          -6.7106524,
          -9.73453,
          -2.6840787,
          -2.759494,
          0.2560569,
          8.420829,
          -7.6646194,
          -7.2793565,
          -6.187621,
          3.5779397,
          -7.2529488,
          -7.2159348,
          3.132619,
          -1.0821952,
          3.3264127,
          3.7090814,
          -7.285787,
          -7.219937,
          -7.2827945,
          -7.321701,
          -7.4252687,
          -7.1675363,
          -7.306236,
          6.8647795,
          -3.2840908,
          -3.3809774,
          -3.4049408,
          -3.3425925,
          -2.7958505,
          -2.542069,
          -3.0043983,
          -3.3746395,
          -3.2094615,
          -0.3816548,
          -1.529836,
          -0.27801427,
          0.3546977,
          0.43539485,
          0.9626445,
          1.4416506,
          -2.2003715,
          0.50860655,
          -6.2579026,
          -3.921752,
          -3.745463,
          1.6101813,
          -2.9609857,
          -1.1584707,
          -2.4624372,
          -0.45116898,
          -0.29067335,
          -0.887161,
          -3.5735683,
          -4.568658,
          -3.9188597,
          -6.6375885,
          -7.735144,
          -1.041372,
          -5.3556633,
          11.732494,
          -6.5860705,
          -5.2746615,
          -7.497583,
          -5.403422,
          -5.0356402,
          -5.429007,
          -5.2942953,
          4.6898885,
          -7.251306,
          -0.00965964,
          1.0789453,
          -0.36746654,
          -0.641769,
          -8.122986,
          -0.119161025,
          0.47900742,
          1.2290154,
          1.2518508,
          0.256018,
          -0.20466147,
          1.9506006,
          1.0737873,
          0.5319908,
          1.7059821,
          1.307273,
          1.775136,
          1.4816418,
          0.28404373,
          -0.10079899,
          1.0824691,
          -9.981044,
          -9.874226,
          -9.900399,
          -10.086249,
          -9.744081,
          0.36882144,
          0.7138122,
          1.321897,
          -9.791382,
          -10.066563,
          -0.62504876,
          -0.8396676,
          -0.739843,
          -0.8154039,
          -0.77025896,
          -0.7606693,
          -0.7128127,
          -7.044494,
          11.434127,
          2.2532053,
          -0.55999553,
          -7.231412,
          -0.8934979,
          -1.0700155,
          -0.98815197,
          -3.1829364,
          -3.8269918,
          -3.7899697,
          -0.039969306,
          -0.8914386,
          -0.9506091,
          -0.91868335,
          -0.89501894,
          -1.0541377,
          -8.13282,
          -8.039702,
          -7.854173,
          -8.495187,
          -8.527627,
          -8.605423,
          -8.618875,
          -8.083002,
          -8.026217,
          -8.583313,
          -8.617229,
          -8.391827,
          -8.477563,
          -8.589742,
          -8.772421,
          -8.811843,
          -8.717986,
          -8.463048,
          -8.563254,
          -7.8558483,
          -8.061539,
          -4.3134465,
          -2.4111304,
          -2.0922196,
          -2.2573647,
          -1.9706434,
          -2.1510737,
          -2.2736855,
          -2.5350647,
          -2.5364454,
          -1.9329686,
          -2.9156325,
          -2.6860754,
          -2.0938857,
          -2.6760569,
          -2.6351082,
          -1.8192036,
          -2.3113353,
          -2.1842372,
          -2.6413515,
          -2.586223,
          -2.569989,
          -2.784781,
          -2.5810773,
          -2.0144851,
          -2.2090757,
          -2.145188,
          -2.7924275,
          -3.1075718,
          -2.8257294,
          -2.5116913,
          -3.8088017,
          4.09376,
          3.585595,
          -0.536648,
          -0.92858726,
          -0.73893404,
          -1.4411697,
          3.2582629,
          3.1697233,
          -6.1156073,
          -3.213497,
          -2.4044554,
          -1.522764,
          -2.528612,
          -1.9901154,
          -2.62095,
          -2.0282001,
          -1.3763202,
          -1.1938574,
          -2.5098784,
          -1.9004614,
          -2.9501033,
          -1.9622885,
          -1.2550181,
          -2.9165013,
          -1.7907175,
          -0.5871991,
          -1.4190516,
          -1.394041,
          -1.4518402,
          -2.2737746,
          -1.488996,
          -1.6827583,
          -6.560424,
          -5.9667854,
          7.7920294,
          5.40693,
          0.0691242,
          0.14892653,
          5.108948,
          -0.50965357,
          5.3158717,
          4.68589,
          0.4167467,
          5.8916807,
          4.711108,
          4.672185,
          5.4222064,
          5.241903,
          5.2218328,
          5.132866,
          4.8404355,
          5.1748524,
          5.121989,
          4.8206134,
          5.052477,
          5.212494,
          8.267594,
          5.281526,
          -2.8507214,
          4.968871,
          7.563508,
          7.9143972,
          14.310082,
          14.311441,
          14.30191,
          14.287612,
          14.306917,
          6.3679137,
          3.166415,
          2.8788717,
          3.7745142,
          -6.565211,
          -3.2583325,
          2.7500765,
          5.865771,
          -1.25122,
          -2.74024,
          -1.0165907,
          0.82989717,
          2.0261142,
          -3.5706348,
          -3.7776563,
          -3.9804926,
          -3.711905,
          -1.2238439,
          -3.7081409,
          -3.474664,
          -3.026259,
          -1.2394669,
          0.73792064,
          0.46166897,
          -3.55645,
          -3.4523435,
          -2.9660525,
          -2.876187,
          -0.75358844,
          -1.2026013,
          -0.93523896,
          -1.0930268,
          -0.83377135,
          -0.1159369,
          -0.7649632,
          -0.93055594,
          -3.7960606,
          -3.110304,
          1.124358,
          -0.97835827,
          -2.9382644,
          -1.076426,
          0.10222729,
          -1.2066045,
          -1.4696887,
          -1.3847101,
          0.42295614,
          -0.99311566,
          -4.2875676,
          0.3094163,
          -1.7716861,
          3.0220833,
          4.5532627,
          -3.7549248,
          0.44833937,
          2.1848545,
          -7.4706397,
          -7.1623125,
          -3.421696,
          -3.4049194,
          -2.0841706,
          -7.817141,
          -8.004911,
          -8.031217,
          -7.7507844,
          -2.0034437,
          -7.648355,
          -2.1608791,
          -7.9728017,
          -8.0481415,
          -7.973958,
          -7.76265,
          -6.99766,
          -6.477777,
          -8.155309,
          -3.2972593,
          -2.679292,
          -0.4739445,
          -7.3310313,
          -7.4573507,
          -8.180096,
          -8.646576,
          -8.63319,
          -0.7384229,
          0.15892023,
          0.2117341,
          0.12987603,
          -0.31791678,
          -0.36837676,
          -0.915505,
          -0.4585436,
          -8.573569,
          -4.6213098,
          -0.4623489,
          -1.6463126,
          -2.3373835,
          0.42902464,
          1.03665,
          0.16261907,
          -2.4566662,
          0.83838093,
          0.6103642,
          -2.5689676,
          -6.0900846,
          -5.488211,
          -5.098492,
          -5.6135426,
          -5.7328625,
          6.4353113,
          -5.558729,
          -5.4819956,
          2.1782334,
          4.5184145,
          -6.037073,
          -6.0141554,
          -5.852822,
          -4.9568973,
          -4.9487834,
          -1.213542,
          -5.8589306,
          -1.9760071,
          -0.58914393,
          -0.19141038,
          2.6111374,
          1.3311415,
          6.22795,
          4.410643,
          -4.1075234,
          -8.001523,
          -8.270632,
          -8.573281,
          -8.466133,
          -8.54879,
          -8.529272,
          -8.638569,
          -8.601665,
          -8.632518,
          -8.581133,
          -8.539719,
          -7.7995534,
          -6.528592,
          -6.637877,
          7.654176,
          -9.538082,
          7.586864,
          -10.0652485,
          -3.194832,
          -9.808343,
          -8.96042,
          -3.253597,
          7.2317543,
          4.612162,
          2.3141255,
          -2.717801,
          7.620157,
          2.5803344,
          6.936804,
          -8.792369,
          2.4977753,
          2.4005327,
          1.5220628,
          2.296767,
          7.247316,
          7.2806854,
          -3.443779,
          -3.215305,
          -0.7260252,
          -3.3397937,
          -1.6293675,
          -1.7362809,
          -8.8312645,
          -3.6289575,
          -2.2459984,
          7.418368,
          -4.1978927,
          7.834947,
          -8.16768,
          7.459424,
          -8.092436,
          -8.257963,
          -8.36172,
          -8.040247,
          -0.77828693,
          -5.3553247,
          6.5755367,
          -8.262954,
          -5.232409,
          -5.477204,
          -5.470721,
          12.705019,
          -5.0011163,
          -5.2813234,
          1.0349962,
          -4.3577175,
          -3.6619306,
          2.8496459,
          0.7618885,
          -4.393982,
          -3.6745393,
          -4.9788656,
          -5.3330135,
          -6.5442243,
          -5.918043,
          -4.6819553,
          -0.41163066,
          -3.0995848,
          -3.2149484,
          -3.7702656,
          -5.025927,
          -3.6793818,
          -4.7769136,
          -3.8826234,
          -4.015429,
          -3.59522,
          -3.8729875,
          -0.8930122,
          -2.631766,
          -1.1541456,
          -2.598375,
          -3.4427893,
          -4.603284,
          -3.7281888,
          -3.5239792,
          -3.314605,
          6.0095277,
          6.0257397,
          -4.200129,
          -3.5478082,
          -2.2533886,
          -0.87182784,
          -4.6589127,
          -3.0288079,
          -2.7877042,
          -1.6076169,
          -3.4904308,
          -3.7508302,
          -6.6353054,
          -5.7502694,
          -5.798634,
          -5.6416597,
          5.873868,
          -5.5011864,
          -6.978939,
          -2.7195446,
          -2.5343604,
          -2.1261842,
          -1.8819417,
          -1.36309,
          -1.5193942,
          0.8679348,
          -3.652803,
          -3.7648914,
          -2.2696736,
          -1.8151425,
          -2.649547,
          -3.5670152,
          -7.6074524,
          -7.6011944,
          -7.8431277,
          -2.7779005,
          -1.0690305,
          -1.7354951,
          -1.6358752,
          -1.6644835,
          -6.5457587,
          -7.522243,
          -7.736222,
          -7.796875,
          -7.6416373,
          -7.724921,
          -1.285293,
          -0.31183133,
          -7.059671,
          -0.84203684,
          -2.339856,
          6.606403,
          -5.388025,
          -7.4850144,
          -0.08186675,
          -7.5655003,
          -7.5877323,
          -5.0246477,
          -6.3041,
          -2.1237972,
          -0.89729017,
          -1.2642928,
          -0.41310245,
          -1.0027896,
          -0.9416997,
          0.6313551,
          -0.5130115,
          -0.6585189,
          -2.6436386,
          -1.4157534,
          -6.5916786,
          12.124625,
          11.232262,
          8.324245,
          7.9177027,
          11.9117,
          -7.579198,
          -7.1701508,
          -7.4805446,
          -7.590687,
          -7.3268194,
          -7.3778195,
          -7.313868,
          -7.435815,
          -7.341919,
          2.807105,
          -7.4030643,
          -7.1336737,
          -6.213221,
          -5.6573224,
          -5.8470964,
          -5.581931,
          -5.1357484,
          -5.8787694,
          -1.140422,
          -5.6467896,
          -5.2141705,
          -5.0358634,
          -3.2818203,
          -7.446576,
          -7.4214144,
          -7.3587117,
          -7.2147164,
          -6.8116145,
          -7.272354,
          -0.16047607,
          -6.282961,
          -7.219321,
          -6.866646,
          -1.2834009,
          -1.3069298,
          -0.36006537,
          -1.0515343,
          -6.485911,
          -2.1961267,
          -0.46534365,
          -6.6527185,
          -2.5114996,
          0.09324931,
          1.8587707,
          -6.747765,
          -6.601645,
          -6.5784616,
          -6.8963733,
          -8.422333,
          -8.81322,
          -8.852638,
          -8.72376,
          -8.8011265,
          -8.891582,
          -8.887014,
          -7.8769946,
          -8.112014,
          -6.016031,
          5.4594674,
          5.2179446,
          5.4690843,
          0.8373345,
          2.1281388,
          1.601616,
          1.5417771,
          -3.9764535,
          -3.7048013,
          -3.8903866,
          -3.726293,
          -3.7644403,
          -3.3988872,
          -3.859941,
          -3.6549065,
          -2.6703308,
          -3.6703446,
          -3.2712398,
          -3.7405226,
          -3.7963524,
          -3.7614174,
          2.7366152,
          1.736078,
          2.1059449,
          1.8793986,
          1.8337502,
          -1.4829605,
          1.0413725,
          -7.3991165,
          -8.118284,
          -7.389755,
          -8.234129,
          -9.640633,
          -8.693696,
          3.2241018,
          2.5864108,
          1.3615892,
          -8.748289,
          0.0456389,
          0.97833866,
          -8.18348,
          -8.4863,
          -8.75788,
          0.09564728,
          -8.727317,
          -8.273201,
          -4.2844467,
          -3.6268914,
          -3.9697511,
          -4.0223875,
          -4.130803,
          -4.0955973,
          -3.8360703,
          -5.3181148,
          -4.8296456,
          5.4341083,
          4.836179,
          4.8919444,
          -3.4366667,
          -2.426442,
          -2.8339121,
          -1.8983936,
          -2.6418748,
          -1.7260066,
          -1.9359667,
          -1.4420347,
          -1.3932861,
          -2.9408073,
          -0.8782536,
          -1.1452149,
          -3.344367,
          -3.2793314,
          0.90278816,
          -6.24667,
          6.897507,
          -7.176599,
          -6.375138,
          -6.758677,
          -7.447688,
          -5.9412513,
          -6.401087,
          4.9639664,
          4.5047865,
          -7.623621,
          -6.577229,
          -6.3607726,
          -7.541925,
          -7.954347,
          -7.5586944,
          -7.5803595,
          -7.4899807,
          -7.586999,
          -7.8596826,
          -8.232203,
          -8.17378,
          -8.272482,
          -7.788787,
          -7.58114,
          -4.5450544,
          -0.29324803,
          -0.69728166,
          -0.96809745,
          -0.8206171,
          -0.90996563,
          -9.875221,
          -9.799348,
          -0.82604736,
          -0.9425674,
          -9.296958,
          -9.544839,
          -9.04023,
          -9.020144,
          -9.398666,
          -9.271465,
          0.51206917,
          -0.043362316,
          1.346882,
          1.0786853,
          1.1579766,
          1.0477462,
          -9.446956,
          1.7135482,
          0.5763241,
          1.2069453,
          1.5131806,
          -9.288784,
          -8.608407,
          -5.0067453,
          -4.846484,
          -4.957196,
          -4.7730093,
          4.5443263,
          -4.8004837,
          -4.712231,
          -4.6721644,
          -4.913657,
          -4.4551387,
          -4.798826,
          -4.8727803,
          -4.9789243,
          -5.0064116,
          -4.6833987,
          5.7799373,
          -4.8119135,
          -5.0143723,
          -6.429763,
          -5.7553024,
          -5.7693067,
          -5.687169,
          -5.8102107,
          -5.8134236,
          -5.9025307,
          3.993051,
          -6.185836,
          4.7557936,
          -7.476974,
          -6.393534,
          -6.4213185,
          -7.3924546,
          -6.095705,
          6.1697197,
          -6.1728454,
          -6.2762213,
          -6.3382516,
          -7.3091917,
          -7.4762115,
          -7.5866475,
          -7.1330957,
          -7.40369,
          0.7762777,
          -3.9282694,
          -6.518389,
          -6.745519,
          -6.8379726,
          -7.2154183,
          -7.100463,
          -7.0590224,
          6.5219107,
          -6.122025,
          -3.7743661,
          -5.377645,
          -6.7982144,
          -4.7570057,
          -5.4408555,
          -4.4834986,
          -5.298563,
          -5.269152,
          -5.4299836,
          -5.092577,
          -6.1309724,
          -5.9885917,
          -5.6289606,
          -5.6019654,
          -5.4783463,
          -5.1130457,
          -5.1970224,
          -5.528794,
          -6.0609174,
          -4.8416615,
          -1.8523235,
          -2.892573,
          -3.5908132,
          -6.2342515,
          -6.6061673,
          -6.501052,
          -6.2205634,
          -6.3023496,
          -3.6398678,
          -6.783469,
          -6.7718635,
          -6.729042,
          -3.0285482,
          -1.8240896,
          -1.7419107,
          14.334452,
          -1.7572975,
          -0.9163855,
          -4.411725,
          -1.9417766,
          -2.016737,
          -6.6122074,
          -1.3973079,
          -7.1146727,
          -6.8112726,
          -6.757849,
          -6.8045025,
          -6.7909503,
          -6.649094,
          -6.6572185,
          -6.628087,
          -7.5819,
          -6.6603904,
          -6.7041636,
          -6.284921,
          -6.7250776,
          -6.698943,
          -6.4793096,
          -0.94279236,
          -0.27156252,
          -0.3793986,
          -6.720477,
          -0.46444994,
          -6.589819,
          -0.18473387,
          -0.08797016,
          -8.523918,
          0.15300417,
          -2.7562945,
          -1.4300592,
          -2.6022232,
          -2.0753775,
          -1.9430596,
          -2.296126,
          14.161311,
          -2.5661812,
          -3.145592,
          -4.7746935,
          -5.840744,
          -2.583071,
          -2.1780622,
          -2.064366,
          -0.2052162,
          -1.9160767,
          -1.8730041,
          -0.9752325,
          -1.0000793,
          -1.6858283,
          -0.8166524,
          -0.8992192,
          0.07764964,
          -1.4250178,
          -1.5154366,
          -1.6645831,
          -2.5344703,
          -1.9779007,
          -5.429447,
          10.200533,
          1.1134375,
          1.0970181,
          -6.4203,
          -4.352447,
          -4.587273,
          -4.465616,
          -5.529443,
          -5.8292103,
          -4.4864073,
          -4.961089,
          -4.4542494,
          -4.736303,
          -4.4298224,
          -4.749778,
          -4.642706,
          -4.413541,
          -3.532077,
          -3.7944098,
          -4.345558,
          -4.5647144,
          -4.76748,
          -4.2199764,
          -5.145587,
          -5.2454863,
          -6.9000287,
          -7.138875,
          -8.083591,
          -8.220278,
          -8.652779,
          -8.726171,
          -8.772534,
          -8.830133,
          -8.749723,
          -8.735641,
          -8.749784,
          -8.707632,
          -8.596062,
          -8.467588,
          -8.67967,
          -8.575424,
          -8.595833,
          -8.689326,
          -8.05264,
          -4.4673004,
          -4.935739,
          -4.633679,
          1.732722,
          0.47617924,
          -3.6289637,
          -0.012141664,
          -1.1881291,
          -1.073571,
          -3.6507978,
          -2.3381555,
          12.898473,
          12.427554,
          -4.9779935,
          -5.456318,
          -4.8524604,
          -4.288451,
          -4.813731,
          -4.907394,
          -6.297422,
          -2.7727811,
          -3.012165,
          -2.8809397,
          -2.4555805,
          -3.291575,
          -4.8924036,
          6.524259,
          5.314479,
          6.39173,
          -3.3797045,
          -3.2384963,
          -3.1824079,
          -8.148317,
          -2.00431,
          3.7466142,
          -1.4672629,
          2.4732025,
          -1.2430396,
          -1.401995,
          4.487033,
          3.505969,
          4.4853544,
          3.639411,
          3.0709612,
          2.433188,
          3.4216628,
          3.008996,
          3.317557,
          3.3353558,
          -1.4474095,
          -1.8087556,
          -2.4105532,
          -6.5613685,
          2.7610211,
          3.2092102,
          -7.682157,
          4.7414436,
          -7.5720873,
          -7.680665,
          -8.011733,
          -7.76733,
          -6.8154387,
          -7.067567,
          -8.023003,
          -6.729962,
          -7.3575516,
          -8.017419,
          -7.2146597,
          -6.6610146,
          -1.8827795,
          -6.113137,
          4.66242,
          -5.7136817,
          2.3003526,
          1.426952,
          0.3047604,
          -5.657195,
          -5.4793477,
          -5.6601443,
          1.4122709,
          -0.6931517,
          -0.19561179,
          -0.2796644,
          2.1549366,
          -0.7229559,
          -7.152932,
          4.216611,
          3.3808231,
          3.3013577,
          3.7894385,
          4.1149836,
          3.5264854,
          -5.3611784,
          -5.3793225,
          -2.5695894,
          -1.4391705,
          -2.360279,
          -3.936554,
          -5.43601,
          -5.58435,
          -4.6208105,
          -1.8520755,
          -5.1027503,
          -5.196302,
          -5.3689117,
          6.916539,
          -7.4048486,
          4.431379,
          4.2216587,
          4.5712905,
          3.6934311,
          3.521582,
          2.1353595,
          7.3760457,
          4.557558,
          4.738023,
          -7.1037874,
          -5.357696,
          -5.275467,
          -5.2705374,
          -6.143881,
          4.625703,
          -6.404298,
          -6.609161,
          -6.4339247,
          -5.4080777,
          -6.389571,
          -6.2573643,
          -6.9606986,
          -0.47524112,
          -6.8297606,
          -0.51623994,
          -6.3864837,
          -5.966286,
          -6.4723854,
          -6.799683,
          -6.778622,
          -6.717925,
          -6.210565,
          -6.500549,
          -6.304015,
          -7.5655212,
          -5.5162487,
          -5.8329268,
          -5.9096594,
          -8.278775,
          -8.309385,
          -8.600378,
          -8.614496,
          -8.640022,
          -8.7362795,
          -8.746609,
          -8.711258,
          -8.563572,
          -8.644188,
          -8.609245,
          -8.115666,
          -8.027692,
          -3.8395488,
          -4.040565,
          -3.974868,
          -3.8604007,
          -3.7301922,
          -3.417193,
          -3.1614635,
          -3.257291,
          -3.562183,
          -3.6737695,
          -5.3008666,
          -4.098876,
          -4.1370907,
          -5.5252547,
          -4.967781,
          -5.4129834,
          -4.8955636,
          -3.9798174,
          -4.951102,
          -3.1452537,
          0.5070684,
          -1.5549395,
          -4.174964,
          -2.6812441,
          -1.2673689,
          -4.1884027,
          2.1436074,
          -5.1472645,
          -0.38882512,
          -0.54615057,
          -0.40777567,
          -0.35743594,
          0.41048795,
          0.32333648,
          1.537727,
          -0.065712996,
          2.5621028,
          -0.33303186,
          -2.5835683,
          -2.156999,
          -0.2251403,
          2.4492345,
          -0.5181683,
          -0.5756571,
          -0.55917764,
          -6.203138,
          2.2706256,
          3.4511569,
          3.2456958,
          0.37807176,
          3.4965582,
          3.2985857,
          -5.677292,
          -3.795749,
          -3.476033,
          0.49313185,
          1.2817978,
          -3.143843,
          0.0136680165,
          -3.6426716,
          -3.6785164,
          -4.3327165,
          -4.3464627,
          -4.754157,
          -3.2936325,
          -8.280309,
          -8.819334,
          -8.429207,
          -8.287377,
          1.093537,
          -0.4893844,
          -2.7336261,
          -2.0504198,
          -8.719124,
          -5.2132072,
          -3.814104,
          -3.4759142,
          -2.8339477,
          -2.3099008,
          -2.1970606,
          -2.2169998,
          -1.5892234,
          -2.1169353,
          -1.6287072,
          -1.683953,
          -2.1777465,
          -2.0609398,
          -2.0730667,
          -0.99477434,
          -1.434941,
          -1.4353718,
          -2.0130687,
          -2.1232328,
          -2.038157,
          -1.5152886,
          -5.2596984,
          -2.1935422,
          -7.150675,
          3.697219,
          3.421732,
          -2.2896652,
          -5.924442,
          -7.9584885,
          -8.433376,
          -0.7106809,
          -1.4402876,
          1.5973339,
          0.12735654,
          0.26147547,
          -0.4162004,
          -0.009881202,
          -8.049352,
          -0.78817075,
          -8.252421,
          -1.12001,
          -0.9395927,
          -0.7907614,
          -1.0250075,
          -5.453811,
          -3.2732713,
          -0.9854814,
          -1.194895,
          -7.492253,
          -7.474684,
          -7.1195974,
          3.2295516,
          -7.453289,
          -7.2270985,
          -5.869754,
          -5.917371,
          -4.61048,
          -5.7647147,
          -1.5969305,
          -5.2603607,
          -3.3144665,
          -7.592064,
          -7.379038,
          -7.3974624,
          -6.9328856,
          -6.7598658,
          -7.3107433,
          -0.12299652,
          -7.369847,
          -7.0049314,
          -1.4872572,
          -1.3662317,
          -0.24497609,
          -0.93355554,
          -7.1569905,
          -0.1848907,
          -7.024229,
          0.58673686,
          -0.0040175566,
          -1.286821,
          1.8284762,
          -0.6704401,
          -0.62039244,
          -0.5363512,
          -3.0690212,
          -0.8874423,
          -6.6501713,
          -7.292049,
          -5.665008,
          -5.1965337,
          -6.30786,
          -5.7663107,
          -5.9912767,
          -4.8693404,
          -5.033612,
          -3.8878508,
          -4.0138707,
          -5.16544,
          -4.935058,
          -5.191709,
          1.9764482,
          -2.3136992,
          -0.28795415,
          -1.2981378,
          -4.3558316,
          -4.937178,
          -4.4783792,
          -5.011473,
          2.666674,
          -0.15528636,
          -4.5247855,
          -4.850004,
          -4.931241,
          -4.598581,
          -4.8630514,
          -5.0738997,
          -7.382771,
          -6.7265596,
          -7.3679633,
          2.3268714,
          -7.256113,
          -7.515235,
          -1.4480072,
          -6.710412,
          -7.1493697,
          -7.2150807,
          -7.007922,
          -1.3929523,
          -0.579198,
          -4.575858,
          0.19441594,
          -1.2195544,
          -7.1660213,
          -7.2213416,
          -3.8643491,
          -1.9368486,
          -1.444763,
          1.344287,
          -2.740537,
          -2.8179083,
          2.107063,
          -0.2922826,
          -0.7576692,
          -1.6516472,
          -2.5769906,
          -2.8162537,
          -6.428785,
          -3.8056846,
          -3.8086412,
          -3.6244795,
          -3.377104,
          -3.6281836,
          -3.307223,
          -6.5717845,
          -5.5966334,
          -5.862594,
          -5.704919,
          -5.30986,
          -5.780062,
          -6.432729,
          -6.9154763,
          -8.05762,
          -8.193292,
          -8.200578,
          -7.9893937,
          -8.237779,
          -8.218171,
          -8.201032,
          -7.802369,
          -8.404979,
          -7.8403225,
          -8.2626095,
          -8.245934,
          -8.266422,
          -8.329609,
          -8.313255,
          -8.111475,
          -8.325811,
          -8.010767,
          -1.47346,
          -1.008847,
          -7.97183,
          -1.382048,
          -1.1876353,
          -8.132874,
          -7.882752,
          -7.9750953,
          -7.7846293,
          -7.985204,
          -8.198891,
          -1.2103529,
          -1.0061514,
          -7.9597874,
          -1.3821026,
          -8.020427,
          -7.9551125,
          -7.893903,
          -7.9486814,
          -7.8846307,
          -7.912134,
          -7.5617657,
          -7.8606873,
          -7.940732,
          -7.9321604,
          -1.8593193,
          -7.8178453,
          -1.2518774,
          -7.918702,
          -7.893968,
          -7.9579597,
          -7.9243283,
          -7.718384,
          -7.5404778,
          -7.9162292,
          -7.9899516,
          -7.9581537,
          -0.78823996,
          -7.9112663,
          -1.3759519,
          -7.969898,
          -3.7442389,
          -3.3375268,
          -3.229758,
          -3.649878,
          -3.120321,
          -3.320411,
          -3.745087,
          -3.8085368,
          -3.647368,
          -4.456132,
          -3.5124643,
          -1.8533269,
          -3.681727,
          -4.5044627,
          -4.4152875,
          -4.294484,
          -4.2403455,
          -4.188183,
          -3.909302,
          -4.022831,
          -3.3561323,
          1.5657583,
          1.1198186,
          -9.181278,
          0.8488241,
          0.47610155,
          -0.37513766,
          -0.94820076,
          -7.7697864,
          -7.8026843,
          -1.464219,
          -1.6054038,
          0.81635267,
          -0.8408176,
          -0.45729792,
          -1.0398324,
          -2.0078201,
          -5.395348,
          -4.0105143,
          -5.1763353,
          -4.8215156,
          -4.6418304,
          2.112987,
          -1.886864,
          -3.2320318,
          -5.14617,
          -5.2684636,
          -0.6900982,
          -0.4028597,
          -1.4731688,
          -2.3438938,
          -2.5108223,
          -0.2516436,
          -7.6078005,
          -7.6570454,
          -7.836208,
          -7.3804207,
          -8.048819,
          -7.191527,
          -7.678488,
          -7.998555,
          -8.09689,
          -7.971831,
          -8.047556,
          -8.005596,
          -6.795999,
          -7.3683414,
          -7.9169545,
          -6.9155416,
          -6.51889,
          -3.6970901,
          -1.5351188,
          1.3125734,
          2.8202786,
          3.4257276,
          4.153211,
          2.9206386,
          -2.8021846,
          -7.6168633,
          -6.844872,
          -7.519356,
          -7.9012127,
          -7.703752,
          3.2444735,
          3.3600633,
          -7.466051,
          -6.997905,
          -7.8368406,
          -7.8772054,
          -7.8112173,
          -7.8699603,
          -7.2782197,
          -4.3722353,
          -4.591571,
          -2.4982114,
          -2.142811,
          4.395956,
          -1.8025367,
          1.9709247,
          1.7766101,
          -0.0134814745,
          0.47077122,
          1.6023482,
          -0.04942789,
          -2.5298922,
          -6.722113,
          11.315458,
          -0.08660196,
          2.8450425,
          2.6112123,
          2.076942,
          2.9961214,
          -6.1065884,
          -4.624999,
          -5.088816,
          -5.0951047,
          -5.3457093,
          -5.1168804,
          12.700971,
          -4.6049557,
          -4.621877,
          -4.263381,
          -2.4245698,
          -4.7960014,
          -4.5493054,
          -3.7323322,
          -2.4139967,
          -2.277454,
          -3.8131847,
          -1.2188878,
          -3.81978,
          -5.630267,
          -3.1126215,
          -3.8813057,
          -1.7772036,
          -1.1217246,
          -6.727115,
          -3.376522,
          -1.8519131,
          -0.5044915,
          0.22405663,
          -1.9407716,
          -4.6088796,
          -0.72819823,
          -0.45187378,
          0.20742743,
          -0.45781285,
          -2.5618281,
          -1.4061441,
          -0.6415533,
          -0.9550706,
          -0.18180762,
          -1.9679146,
          -4.1105757,
          -3.9448874,
          -3.826523,
          -3.5053651,
          -2.8459868,
          -2.7448373,
          -3.5058715,
          -6.0925903,
          -7.7745786,
          -7.711934,
          -7.5379486,
          -7.569607,
          -7.758706,
          -7.7342467,
          -7.8085947,
          -5.425334,
          -5.424941,
          -5.4851494,
          -4.141055,
          -1.8231267,
          -2.6296816,
          -3.3903193,
          -6.00173,
          -5.693859,
          -5.551662,
          -6.39438,
          -4.4828362,
          -3.31977,
          -0.14965005,
          -5.565047,
          3.3067474,
          0.61158407,
          6.5248604,
          7.1046042,
          -6.068616,
          -3.3431919,
          -3.233492,
          -3.0175605,
          -3.6872358,
          1.8485367,
          1.7047735,
          9.839325,
          9.826456,
          9.6610775,
          -5.7455106,
          -5.8196697,
          -3.664218,
          -1.2411684,
          -1.348832,
          -1.3194537,
          -5.7427573,
          -5.8560686,
          -5.8537993,
          -7.75836,
          -6.9099483,
          -7.2722406,
          -7.428936,
          -6.128542,
          -6.572251,
          4.714095,
          4.74872,
          -4.859103,
          -3.2572439,
          4.709872,
          3.9213138,
          4.692297,
          -6.205212,
          -5.769692,
          -5.5414653,
          -1.1896707,
          -5.544311,
          -4.9287376,
          -5.369958,
          -5.2331696,
          -6.444869,
          -5.45055,
          -6.2163925,
          6.4346437,
          -7.3560767,
          0.05773868,
          4.5502806,
          -0.219447,
          0.5290139,
          1.580739,
          1.9243413,
          -7.350559,
          -0.99564195,
          -0.29075226,
          -0.21476407,
          -0.11313826,
          -0.7678613,
          -0.6122642,
          -0.10537244,
          -0.6122988,
          0.08627386,
          -0.4588821,
          -0.17422186,
          -0.17084022,
          -0.44984585,
          -9.283088,
          -9.211643,
          -8.758246,
          -9.135685,
          0.4509105,
          0.23084736,
          0.31506985,
          0.6994428,
          -8.97728,
          0.28792602,
          0.3373519,
          0.046486456,
          0.30453622,
          0.41416994,
          0.2751013,
          3.9590871,
          0.297891,
          0.30466798,
          0.16327848,
          0.17840853,
          -0.26951674,
          -1.9770222,
          -0.5014192,
          -0.053032517,
          -0.018661764,
          0.040531296,
          0.24079221,
          0.4115367,
          -9.069545,
          -9.202648,
          -9.183263,
          -6.767403,
          -9.097809,
          -4.018276,
          -4.012277,
          -4.454495,
          -7.4689384,
          -7.578411,
          -7.40969,
          2.0672598,
          3.0663977,
          -7.238911,
          -7.4672465,
          -6.6128798,
          -6.2373576,
          -5.400693,
          -4.320418,
          -5.9729047,
          -4.1271553,
          -1.9632326,
          -5.957289,
          -0.91326267,
          -5.604086,
          -4.7451124,
          -7.454525,
          -7.4190135,
          -7.5005956,
          -7.2224884,
          -6.692389,
          -7.3442597,
          -0.0453565,
          -6.85694,
          -7.297218,
          -1.4241786,
          -1.4031813,
          -0.37342447,
          -0.9633467,
          -6.746732,
          -7.146134,
          -7.3157535,
          -6.6240444,
          -0.25143132,
          -6.761955,
          -6.0087357,
          -8.170775,
          -8.781753,
          -8.733676,
          -8.724619,
          -8.547941,
          -8.5481415,
          -8.7246065,
          -8.696243,
          -7.8536706,
          -8.361875,
          -3.7011902,
          4.3289423,
          -3.1662595,
          -2.7952378,
          -2.6502738,
          -2.8695104,
          -3.107639,
          -3.1830907,
          6.5309553,
          -4.900841,
          -1.1529014,
          -1.5762413,
          -0.9036478,
          -1.0594009,
          0.4620259,
          0.44122824,
          -1.2326009,
          -1.4077419,
          -6.4893475,
          -5.8464427,
          -5.799603,
          -4.820522,
          -0.26066917,
          4.207136,
          -4.428982,
          -1.8309623,
          -1.3033216,
          -1.3506808,
          -1.5494542,
          -1.3106854,
          -1.3942815,
          -1.4878348,
          -1.5065501,
          -1.4391171,
          -1.0671158,
          -1.637114,
          -2.7459197,
          -2.718235,
          -1.9607835,
          -6.184691,
          -5.328791,
          -5.343039,
          3.9287324,
          -5.122584,
          -5.736082,
          -7.670888,
          -7.8001256,
          -8.108488,
          -7.9956737,
          -7.9062233,
          -8.010888,
          -8.069737,
          -7.399378,
          -8.02104,
          -7.0840397,
          -0.5108565,
          -5.0004134,
          -5.2969975,
          -0.7849571,
          -0.73006266,
          -0.75307685,
          -0.7323054,
          -0.47796702,
          -0.7894077,
          -0.8465666,
          -0.68302906,
          -0.68133104,
          -0.7715884,
          -0.57757986,
          -0.6272757,
          -5.200805,
          7.8026967,
          -4.380877,
          -0.4108409,
          1.8713702,
          -0.17960599,
          1.1237429,
          -0.40435153,
          0.7858973,
          0.1357398,
          0.4983765,
          -0.8455288,
          -0.0027288685,
          -7.556811,
          -6.3905487,
          -6.5787935,
          -6.7498035,
          -6.3707256,
          -6.6534777,
          -3.3385375,
          -1.3418974,
          -0.97012687,
          -1.2341884,
          -1.3545599,
          -1.1983125,
          -1.2615069,
          -1.3812071,
          -0.89544946,
          -1.4493941,
          0.022817355,
          -0.70804346,
          -0.9743639,
          -0.79297245,
          -2.0971286,
          -5.991475,
          -3.791916,
          -6.0200024,
          -3.775932,
          -2.6312382,
          -3.1164749,
          -4.3695,
          0.48048013,
          0.12628269,
          0.9055013,
          -0.24743025,
          -0.07959958,
          2.1536407,
          0.18089555,
          -1.0081357,
          -1.0347873,
          -2.0834513,
          0.88189334,
          -1.0666649,
          -0.8056843,
          1.9917996,
          0.5952575,
          -3.856135,
          -4.242399,
          5.822821,
          -3.9573367,
          -5.9834414,
          -6.40298,
          -3.561721,
          -3.5566308,
          -3.6864152,
          -3.9564779
         ],
         "xaxis": "x",
         "y": [
          1.5830882,
          2.3582528,
          0.20782381,
          -1.5530207,
          1.9958382,
          1.59844,
          2.1021857,
          2.084734,
          2.1470697,
          3.2357237,
          3.2565835,
          0.67377394,
          0.46942717,
          1.0192381,
          1.3679559,
          -4.2421184,
          -4.2746105,
          -4.388167,
          3.0362587,
          6.43318,
          -3.595439,
          -3.5602293,
          -3.8945706,
          -3.6615155,
          -3.8889294,
          5.729493,
          6.60753,
          6.634641,
          6.3135614,
          6.2387137,
          6.215808,
          6.629744,
          6.0151606,
          0.5983836,
          0.030755488,
          -0.22025025,
          -0.36804757,
          0.16390017,
          -0.3677148,
          8.021361,
          8.06167,
          -0.48182842,
          -2.789475,
          -3.2414112,
          -3.1959538,
          -2.790714,
          -2.8735795,
          -3.1298497,
          -2.6530986,
          -2.4293396,
          -2.543578,
          -1.6732708,
          -3.1401963,
          -3.1342788,
          -4.4349427,
          -4.669891,
          -4.318843,
          -4.440213,
          -4.178343,
          -4.6457496,
          -4.4770775,
          -3.9524763,
          -4.1569223,
          -4.162632,
          -4.3114214,
          -3.5625029,
          -3.271502,
          -3.9993742,
          -3.0988126,
          -3.3204455,
          -3.394109,
          -3.256794,
          -2.7198162,
          -2.390525,
          -1.5465268,
          0.8204762,
          -3.3950136,
          -4.7408195,
          -4.471903,
          -5.921879,
          -5.664626,
          -5.162766,
          -3.1480992,
          0.36667737,
          0.24384798,
          -5.1212835,
          -5.01388,
          -4.8761725,
          -4.898285,
          -4.89425,
          -4.799399,
          -4.632444,
          -2.35254,
          -1.9040487,
          -3.7340548,
          2.5858808,
          -3.567919,
          2.8220177,
          -3.8797925,
          -3.8904567,
          -3.4888217,
          -2.8596354,
          3.6890705,
          4.811,
          -3.4987774,
          -3.4559212,
          -3.6136584,
          -4.4670687,
          -4.192472,
          -3.3779812,
          -4.2383037,
          -4.335859,
          -3.5682566,
          -4.469488,
          -4.3539333,
          -3.3581219,
          0.010703981,
          -3.367486,
          -3.61059,
          0.23434816,
          -3.4829562,
          -2.7769334,
          2.065144,
          -2.4786782,
          2.0032523,
          2.8339279,
          2.6415923,
          2.616651,
          2.8986862,
          2.5935783,
          2.2260063,
          2.5712204,
          2.14533,
          2.1067977,
          -0.042246252,
          2.580319,
          2.1687148,
          2.7224762,
          1.6418005,
          2.6430073,
          1.1245617,
          2.7638988,
          2.8304358,
          2.77036,
          2.649114,
          2.732236,
          2.6697183,
          2.5775006,
          2.6801805,
          1.9927073,
          -0.4207836,
          -0.26053512,
          -0.06881106,
          -0.45038235,
          -0.111404166,
          0.5358558,
          -0.51904684,
          -1.197495,
          -4.5399275,
          -0.54413944,
          2.0678604,
          1.1281953,
          -1.7266061,
          2.3214884,
          2.94141,
          2.0826392,
          1.6392953,
          1.633647,
          1.3563062,
          3.1585236,
          -1.251403,
          3.609277,
          3.313311,
          -3.5080817,
          2.4614477,
          4.30973,
          2.845159,
          1.3080851,
          -1.3770285,
          1.0688913,
          1.165977,
          0.75373316,
          0.3742818,
          -3.0947266,
          -1.5158173,
          0.047246303,
          -0.30534035,
          -1.4537115,
          4.211308,
          6.9467936,
          6.437825,
          6.389979,
          6.238232,
          0.7966051,
          -1.8930203,
          -4.918677,
          -3.2715268,
          -1.9852647,
          -1.2418054,
          -0.5675237,
          -0.4655882,
          0.059524115,
          -0.2327649,
          -4.409361,
          -4.540557,
          -3.3958833,
          -3.9747891,
          -4.3910713,
          -2.3255267,
          -1.0659138,
          -6.202609,
          -5.8373632,
          0.41990194,
          -3.074916,
          -3.8425686,
          -3.912128,
          -0.04105684,
          -6.0370855,
          -4.0871773,
          -6.08509,
          -4.807517,
          -6.2105384,
          -0.034982853,
          2.0356703,
          1.2702745,
          -0.5570098,
          -0.865471,
          -0.7282197,
          -0.65215445,
          -0.655143,
          -0.684397,
          -0.5800075,
          -0.7131944,
          -0.53684956,
          -1.0354663,
          -0.9578407,
          -0.13089997,
          -0.6741873,
          -0.30060926,
          -0.6558688,
          -0.2991546,
          -0.5838814,
          -0.73416007,
          -0.4135883,
          -6.778442,
          -3.1523585,
          0.2386901,
          1.7976038,
          2.2536938,
          2.3553698,
          2.1147785,
          2.568959,
          2.7562714,
          -1.9625256,
          -3.5083556,
          -2.2921095,
          -3.709374,
          -4.2365737,
          -4.1197114,
          -4.1809645,
          -4.912622,
          -4.2583423,
          -4.3933163,
          -4.0111833,
          -3.543675,
          -4.2641387,
          -4.069807,
          -4.3177257,
          -4.1781816,
          -3.755977,
          -4.3357253,
          -4.7142386,
          -4.001763,
          -4.1711054,
          -4.4627366,
          -3.1246245,
          -1.5128859,
          2.1836884,
          2.558025,
          1.5759556,
          -3.0081313,
          1.5681306,
          0.8534126,
          2.528515,
          2.8461287,
          -2.6432955,
          -4.4415197,
          -4.2954626,
          -2.7928474,
          -3.6517458,
          -4.4622393,
          -3.605243,
          -4.570156,
          -4.3991785,
          -4.3413653,
          -4.3689957,
          -4.007262,
          -4.4989123,
          -3.214234,
          -4.1406302,
          -0.10811539,
          -2.3405383,
          1.7786742,
          -0.6120929,
          0.25901031,
          -0.27463663,
          -3.237324,
          -3.192209,
          7.0097337,
          -3.1079683,
          -3.1629694,
          -3.0328913,
          0.8932986,
          -2.6102288,
          -2.2392642,
          -1.5852766,
          -1.7913938,
          -1.4855164,
          -3.679754,
          -4.998871,
          -4.2912674,
          1.3079304,
          -1.868256,
          2.415309,
          2.4952705,
          0.8078283,
          2.7944946,
          3.3544726,
          2.9710476,
          2.2329211,
          2.003383,
          2.9550126,
          2.768728,
          2.4457753,
          3.0045419,
          2.4006007,
          2.3731194,
          1.6172501,
          2.166181,
          2.93782,
          2.6611822,
          2.7683597,
          2.6546912,
          1.4183849,
          1.185795,
          -5.234199,
          -4.4972396,
          -4.7638803,
          1.5246542,
          -3.7353175,
          1.5829375,
          -6.2211485,
          -3.8235347,
          -4.9947295,
          -5.597243,
          -4.9831047,
          -6.332013,
          -6.38662,
          -6.135633,
          1.6356609,
          1.2464472,
          1.4151257,
          0.7930182,
          1.6511825,
          1.9958215,
          2.101562,
          2.178613,
          -4.5800376,
          1.7408229,
          1.5427866,
          -4.3990016,
          -4.370109,
          -4.470745,
          1.9416963,
          1.7040931,
          1.3375226,
          1.214929,
          -0.3223817,
          -3.3114052,
          -2.934943,
          1.9562556,
          2.910348,
          -0.16062489,
          -0.012349727,
          -0.58659905,
          -0.88711166,
          -0.6145183,
          -1.2661978,
          0.26985192,
          -1.4878132,
          -0.48672658,
          -5.540174,
          1.9200742,
          -0.74749833,
          -0.861739,
          -0.7881843,
          -0.73812693,
          -0.71823907,
          -0.77420276,
          -0.7861485,
          -0.9434877,
          -0.8430265,
          -0.7540067,
          -0.8415693,
          -0.6736136,
          -0.7406808,
          -0.76211643,
          -0.8594789,
          -0.7981657,
          -0.2636238,
          -0.25808045,
          -0.8395674,
          -0.21322374,
          -2.536301,
          -0.24076113,
          -0.5999148,
          -0.75405645,
          -0.6659323,
          -0.55915064,
          1.113459,
          1.5930719,
          0.20358418,
          1.1120332,
          -0.036145177,
          3.7907107,
          -2.04116,
          0.123891,
          -0.62371886,
          -3.3092911,
          -4.7562633,
          -4.126611,
          -4.142808,
          -3.9356565,
          -2.3861735,
          -1.8269671,
          2.6939049,
          -0.79998237,
          -1.428386,
          -6.1445,
          -0.21596175,
          -0.02741341,
          0.2958014,
          1.7504156,
          1.0695536,
          0.6400487,
          1.0568326,
          1.4419352,
          -2.0623853,
          2.701956,
          2.8996184,
          2.7126544,
          -0.736749,
          0.78006595,
          -1.6113931,
          1.692169,
          2.2112365,
          1.8342477,
          2.9607754,
          2.305132,
          7.109823,
          6.1624374,
          6.633308,
          7.045808,
          7.2407746,
          6.776451,
          -1.1988527,
          6.9734473,
          6.7845917,
          6.857854,
          6.7047896,
          6.891719,
          0.12432543,
          -1.1345636,
          -2.4020317,
          -3.0370038,
          -0.34545872,
          -0.57464933,
          -0.6415138,
          -1.2657274,
          0.9664509,
          -1.5818396,
          0.5692543,
          2.7847261,
          2.0241797,
          0.19280374,
          1.6493955,
          0.48210764,
          2.1020079,
          -1.0861984,
          -2.3800147,
          -3.3213282,
          -2.3087497,
          -3.0119562,
          -3.4248757,
          1.9724677,
          -0.1089904,
          0.9728681,
          -0.8254466,
          -0.88034135,
          0.612777,
          0.50651366,
          2.1027308,
          2.1833036,
          2.5630975,
          0.75596184,
          -4.4728293,
          0.63245165,
          2.807351,
          1.1129129,
          0.42163408,
          0.38409838,
          -3.3168633,
          -3.4827378,
          0.10300559,
          0.38761577,
          1.1851647,
          -2.3412392,
          1.2063923,
          1.1555636,
          1.4061221,
          1.0368537,
          0.99309003,
          8.868902,
          8.659566,
          0.714336,
          -1.2982422,
          -1.0534525,
          -1.7181116,
          -0.9122524,
          -6.422119,
          -6.739312,
          -6.8579216,
          -6.478139,
          -6.781907,
          -6.77764,
          -6.9486947,
          -6.732657,
          -6.814313,
          -0.7428777,
          -1.1917744,
          -1.071229,
          -3.7543578,
          -1.2246233,
          -0.95607716,
          2.1584709,
          2.2085814,
          2.351872,
          2.4531107,
          2.4232168,
          2.1180694,
          2.3269138,
          2.502917,
          5.5635934,
          6.871415,
          4.3898835,
          4.504969,
          5.342533,
          2.4783394,
          5.4251246,
          5.839829,
          5.470533,
          -1.1237795,
          -3.0571795,
          -4.3747377,
          -4.0390005,
          -5.3515506,
          -3.98353,
          -3.3378031,
          -2.9929857,
          0.025505956,
          0.41771436,
          0.11201651,
          -0.079658456,
          -0.8375311,
          0.040325668,
          0.022052268,
          0.030694347,
          -2.7651024,
          -3.4861856,
          -3.0371172,
          -2.6947749,
          -2.9036365,
          0.41139036,
          0.48699823,
          -6.2224975,
          -6.406702,
          0.34942302,
          -0.15231694,
          -1.2899212,
          -2.612941,
          0.12880847,
          6.6098623,
          7.1142125,
          6.974852,
          6.820059,
          6.918662,
          5.9049044,
          6.8245,
          2.395139,
          0.9599358,
          -0.1475484,
          -0.4341755,
          -1.3026073,
          -0.7607859,
          -1.0483441,
          -1.0541896,
          -1.3214885,
          -1.266581,
          -1.343695,
          1.1822555,
          -2.0049794,
          -1.2781483,
          0.06288821,
          -5.3516345,
          -3.4809694,
          1.0306227,
          -3.401694,
          -3.0956938,
          -2.813828,
          -3.4345639,
          -3.7716134,
          -3.6779623,
          -3.7682238,
          -3.8247015,
          -3.7021563,
          -3.7493227,
          -3.6098125,
          -3.7840762,
          -3.7197113,
          -2.976838,
          -3.5325468,
          -3.50526,
          -3.7737513,
          -3.5647247,
          -3.7194,
          -3.5308013,
          -3.802477,
          -3.7296124,
          -3.5064056,
          -3.703761,
          -3.582558,
          -3.4529982,
          -3.2457314,
          -3.2897017,
          -3.3563156,
          -3.4552321,
          -3.1775126,
          -2.5182092,
          -3.4071755,
          -3.3526793,
          -3.188567,
          -3.4492316,
          -3.4804552,
          -2.95011,
          -2.9097214,
          -2.9224217,
          -3.3127084,
          -2.8525214,
          -3.8133204,
          -3.3360972,
          -3.72944,
          -3.5201597,
          -3.7173793,
          -3.3195431,
          -3.063331,
          -3.4360442,
          -3.4363441,
          -3.5231504,
          -3.4599512,
          -3.5138607,
          -3.4991865,
          -3.4492795,
          -2.9444146,
          -2.7999444,
          -3.1309164,
          -3.004431,
          -3.2561834,
          -2.5158396,
          -2.945215,
          -3.2924273,
          -3.747047,
          1.6163081,
          2.0305855,
          2.380103,
          2.8608212,
          -2.815207,
          -3.1602812,
          -3.1851096,
          -3.169845,
          -3.2025537,
          -1.986407,
          -2.7709765,
          -2.714449,
          -2.1371694,
          -2.3045733,
          -2.362241,
          -2.3962784,
          -2.1568203,
          -3.0648599,
          -3.2179904,
          -2.8121374,
          -4.7230196,
          -4.1258507,
          -3.4473615,
          -3.1492715,
          -3.2634537,
          -3.4093065,
          -3.996444,
          -4.9427104,
          -4.9709787,
          -4.8164682,
          -3.8587763,
          -4.80358,
          -3.0012808,
          -3.2578132,
          -3.3394148,
          0.066431075,
          -3.4745991,
          -3.6957273,
          -3.7874093,
          -3.806578,
          -3.8874183,
          -3.8366923,
          -3.718482,
          -3.7958055,
          -3.8484967,
          -3.7745876,
          -3.769654,
          0.30811906,
          -3.5842094,
          -3.8318303,
          -3.8939526,
          -3.0647767,
          -2.922738,
          -2.3640456,
          -3.5643346,
          -3.4466627,
          -3.1770103,
          0.18921143,
          -0.25609657,
          1.4622809,
          0.8402814,
          1.6131876,
          0.5801549,
          2.455092,
          2.135758,
          -1.3029743,
          -3.2047012,
          1.471255,
          -2.054287,
          -1.8151907,
          -5.7763143,
          2.201585,
          2.1590922,
          -4.1926413,
          -4.250715,
          -0.7068675,
          -2.71927,
          2.1256604,
          2.2360463,
          2.0824342,
          2.8423772,
          4.015208,
          4.374899,
          2.2494113,
          2.6215725,
          2.924346,
          -1.777673,
          -1.93407,
          2.7983603,
          2.8679745,
          1.238575,
          0.7502561,
          2.8080611,
          2.676808,
          2.7222428,
          -1.5466292,
          -5.409164,
          -5.4995584,
          -5.5442433,
          -1.3936081,
          3.2599149,
          2.3962178,
          -2.1090372,
          -0.58230484,
          -0.5180115,
          -2.421747,
          -0.42443448,
          2.3185718,
          2.899923,
          3.225396,
          -3.4168742,
          2.9896,
          2.9984796,
          3.283358,
          2.509822,
          2.958091,
          2.7909594,
          2.8840966,
          2.7377815,
          3.1550508,
          1.9172827,
          -2.5718133,
          -5.67943,
          -0.6452806,
          1.8800503,
          0.6629256,
          1.1718618,
          0.2646103,
          -0.14617664,
          -0.886686,
          -0.54815984,
          -3.1253712,
          0.24058944,
          1.8549001,
          0.342964,
          0.55614394,
          1.7179166,
          0.20804961,
          0.1893103,
          -1.9623287,
          -2.0157528,
          -1.8968664,
          -1.8433152,
          -1.8806548,
          1.1082978,
          -1.7499288,
          -2.7560914,
          -2.4822412,
          -2.350656,
          -1.3816364,
          0.08847213,
          -3.1651194,
          -2.442058,
          0.5915631,
          -2.2892387,
          1.9084101,
          7.527965,
          8.037051,
          7.1102037,
          6.0060053,
          6.66986,
          6.71181,
          6.489365,
          7.5417,
          7.7233596,
          7.5641947,
          7.6574373,
          7.923083,
          -0.95535517,
          6.7151213,
          -2.6510808,
          6.6682506,
          7.2486157,
          8.061801,
          1.4580626,
          1.1877362,
          1.2215405,
          0.6047239,
          1.1465878,
          1.0169845,
          0.75288445,
          0.70703346,
          -0.51267177,
          1.3164356,
          1.6543237,
          -2.4973686,
          -0.13976009,
          1.722976,
          1.1221663,
          -1.4190937,
          -2.555633,
          3.4350333,
          0.1461727,
          -3.9658906,
          -0.4102342,
          6.232454,
          6.2174935,
          6.162424,
          2.6297588,
          6.245918,
          6.2234054,
          6.2709794,
          6.251447,
          6.312465,
          6.2581587,
          6.215306,
          6.280053,
          5.8237796,
          5.726721,
          -3.3016884,
          0.76432127,
          -0.3033746,
          1.9381568,
          0.22734809,
          1.548797,
          7.2326536,
          4.3291698,
          3.3302524,
          4.5098267,
          5.431845,
          6.458453,
          -1.0471736,
          4.403882,
          -1.2458932,
          -2.642476,
          -3.04281,
          -2.869627,
          -3.5209522,
          -2.2258816,
          -1.1624329,
          -1.8102287,
          -2.9415886,
          -2.256618,
          1.0869899,
          -3.121851,
          -3.9963806,
          -2.8333569,
          -3.1077344,
          -2.033495,
          -1.1993767,
          -0.46400645,
          -1.1112939,
          -1.9585806,
          -1.9979683,
          -2.028516,
          -2.0516078,
          -1.9581321,
          -1.829648,
          -1.843916,
          -2.1728516,
          -1.948078,
          -1.8556341,
          -1.6091803,
          -1.588932,
          -1.8194466,
          -1.7452525,
          -1.3374414,
          -1.7596412,
          -1.8811476,
          -1.9522794,
          -1.8068277,
          -1.5917814,
          -1.7118088,
          -1.8026803,
          -3.4472878,
          -1.9230745,
          -2.9275742,
          -2.9108093,
          -0.20243783,
          -2.0435736,
          -2.1055064,
          -1.9356099,
          0.24990034,
          0.45700824,
          -4.445385,
          -3.509906,
          -6.058847,
          -0.11064551,
          0.058898903,
          -4.668232,
          -4.9491444,
          -4.469752,
          -4.535462,
          -4.792604,
          -4.724592,
          -0.5364693,
          -4.749469,
          -4.5759735,
          0.6865559,
          0.78785586,
          -0.6666965,
          -1.4731374,
          -1.1246144,
          -0.5475911,
          -0.6233871,
          -1.2317475,
          -1.484036,
          -2.015038,
          -2.211204,
          -0.3864383,
          0.5266522,
          -4.467704,
          -3.364579,
          -3.4196022,
          -3.1838803,
          -2.6432054,
          -4.6438646,
          -2.4329116,
          1.236478,
          -1.2892864,
          -1.1336975,
          -1.3211693,
          -1.0792758,
          0.40830636,
          2.3931239,
          1.0540138,
          -2.4019413,
          -2.1960883,
          -3.377866,
          2.542401,
          3.110979,
          3.0435445,
          1.0674655,
          2.8959043,
          2.629132,
          2.333889,
          3.396363,
          0.045170825,
          0.7731335,
          1.1715899,
          1.028526,
          0.45584765,
          -0.47673604,
          0.4921411,
          0.79384494,
          2.1419377,
          -0.37744084,
          -0.079276554,
          -0.8541259,
          0.27398407,
          1.3076622,
          0.7262451,
          1.88751,
          1.9701705,
          0.424239,
          1.9085091,
          1.0178992,
          -0.046087142,
          -5.517905,
          -5.294325,
          1.0547621,
          0.3476738,
          0.9090211,
          1.2450106,
          0.98363626,
          -0.68760175,
          -1.1588696,
          -2.4623413,
          -2.5785172,
          -1.5749974,
          -1.3994418,
          -1.1120749,
          -0.7952633,
          -1.5056432,
          -0.93209726,
          -1.5279815,
          2.3931289,
          2.2478867,
          0.27552158,
          2.6845431,
          2.9011238,
          2.8231883,
          3.8350408,
          2.4319673,
          2.5249157,
          -0.999627,
          -0.8611086,
          1.6821859,
          1.4348931,
          -0.6895496,
          -2.8866768,
          -6.319642,
          -2.3807905,
          -2.9112926,
          -1.64098,
          -2.812975,
          -7.25131,
          -3.1661124,
          -2.6667607,
          0.44412398,
          -0.70438194,
          -2.5529504,
          -2.3779361,
          4.833759,
          -1.1398003,
          -0.7209192,
          1.9767289,
          1.7957989,
          1.6887691,
          2.6524763,
          2.9486701,
          2.950361,
          3.0097163,
          2.8988662,
          2.849363,
          2.5890741,
          1.5955501,
          1.3509485,
          1.3758616,
          1.297248,
          1.3947542,
          1.4739449,
          1.4670775,
          1.324701,
          1.1280998,
          -3.29937,
          -3.2329798,
          -2.2191257,
          -2.7283905,
          -2.5156913,
          -2.959346,
          -3.4708521,
          -3.4783316,
          -2.9727912,
          -2.827277,
          -3.1581373,
          -2.9692328,
          -3.2870986,
          -3.522728,
          -3.5832458,
          -3.3486423,
          -3.3145137,
          -3.3384817,
          -3.1042204,
          -3.4161832,
          -3.3087535,
          -3.310392,
          -3.5024996,
          -3.322676,
          -3.036543,
          -1.15812,
          -2.813797,
          -4.9601264,
          -2.1086814,
          -2.820466,
          -3.5289822,
          -5.583026,
          -3.0807421,
          -2.9964602,
          0.2817552,
          -2.940421,
          -2.630256,
          -2.8645575,
          -3.0883532,
          -3.2035606,
          -0.90225005,
          -2.8269737,
          -2.4331808,
          -4.057468,
          -4.2413063,
          22.051502,
          22.052399,
          22.051674,
          22.051638,
          22.051111,
          22.050821,
          22.053091,
          22.05149,
          22.052708,
          22.053257,
          22.052101,
          22.052101,
          22.053993,
          22.052397,
          22.053001,
          22.053104,
          22.052359,
          22.053595,
          22.05304,
          -4.684885,
          -4.322836,
          -5.232994,
          -4.201026,
          -3.4242516,
          -2.9988143,
          -3.0457492,
          -1.4255881,
          -1.193455,
          -1.4172801,
          -1.8313104,
          -1.838593,
          -1.8570137,
          -1.7649065,
          -1.8088578,
          -1.8306446,
          -1.8687263,
          -0.5515262,
          -1.3868375,
          -3.1667588,
          -1.6135671,
          -1.9642589,
          -1.6943538,
          -1.7973782,
          -1.729226,
          -1.911249,
          -1.0740838,
          -4.0854173,
          -2.713898,
          -2.0771883,
          -2.973539,
          -1.4039602,
          -3.377291,
          -2.0730839,
          -1.4176927,
          -6.736883,
          -1.1589936,
          -1.9920658,
          -6.012769,
          2.125045,
          2.5377653,
          2.8398085,
          2.355655,
          2.153259,
          2.6433794,
          1.7199888,
          -1.3792989,
          -1.7605611,
          -1.2392629,
          -0.3393999,
          2.1352026,
          1.04998,
          1.9484953,
          1.0782675,
          1.3798332,
          0.7378632,
          -2.2142675,
          -0.7362672,
          -1.3805327,
          -4.232206,
          6.644404,
          -1.7512271,
          -3.202377,
          6.6313605,
          -1.4241512,
          0.99675155,
          0.33833295,
          0.9486923,
          -1.6356356,
          2.7574492,
          -2.2319896,
          -2.7106464,
          0.7709351,
          1.8595918,
          0.004608037,
          0.5326755,
          0.5275104,
          6.781167,
          7.153917,
          6.765874,
          4.187706,
          4.5296264,
          1.655116,
          -0.913909,
          -2.592455,
          -1.1926464,
          -3.1498373,
          -2.8877447,
          -2.915912,
          -1.3513026,
          1.2174988,
          -1.4021657,
          -0.72448224,
          2.347179,
          2.7237499,
          2.509315,
          0.91843075,
          -1.2402148,
          1.0677657,
          1.1347927,
          1.095636,
          1.3275782,
          2.2121112,
          2.8799765,
          2.6741073,
          2.6482375,
          2.6442463,
          2.5438051,
          1.8326076,
          2.6270626,
          2.5409148,
          2.7037485,
          2.5697415,
          2.800847,
          2.4895148,
          2.5371335,
          2.8360608,
          2.7960913,
          1.207673,
          1.3136479,
          1.5624876,
          0.9890772,
          0.23361585,
          0.306484,
          0.39793992,
          0.28450114,
          1.1576097,
          2.1426854,
          2.6495678,
          4.2026706,
          6.032354,
          5.9736176,
          5.259179,
          5.624421,
          3.3065898,
          0.35129318,
          -0.23434709,
          -1.9172441,
          -3.6759734,
          -3.384415,
          -2.216731,
          -4.2812314,
          -3.7465143,
          -4.499104,
          -4.515599,
          -4.0533876,
          -4.466733,
          -4.3496156,
          -4.253011,
          -4.4741707,
          -2.732825,
          -0.8161683,
          0.5459055,
          -3.6969023,
          -4.3140235,
          -1.248996,
          -4.168331,
          -3.8691828,
          -0.04463346,
          -2.1383512,
          -2.234632,
          -0.59951407,
          4.361697,
          1.8213501,
          0.7228523,
          0.9829828,
          -0.63674176,
          -1.732945,
          -5.165292,
          -5.0763264,
          -5.0792766,
          -1.7785507,
          -3.2913733,
          -3.6242175,
          1.5171014,
          -5.1670184,
          -4.9014363,
          -2.0667925,
          -3.8686888,
          -0.3444057,
          2.0538335,
          -4.192367,
          1.9457492,
          1.7270856,
          1.0752237,
          -6.521315,
          -6.5549283,
          7.687307,
          -6.3344426,
          -6.5605335,
          -6.736074,
          7.4206896,
          -7.0337424,
          -6.9540496,
          -6.6423273,
          5.8151164,
          -6.943192,
          5.6783996,
          -4.4111505,
          8.141957,
          -6.8760166,
          -6.7369757,
          -6.966676,
          2.4877682,
          2.460471,
          -6.757896,
          6.6672816,
          7.675648,
          5.8143167,
          4.400756,
          5.300842,
          4.6354256,
          2.7505035,
          2.827094,
          5.6286297,
          5.316135,
          -6.4605255,
          -3.7381065,
          -4.2428827,
          -4.4983807,
          -4.641618,
          -4.517344,
          -4.8056636,
          -6.489353,
          -7.4965153,
          -7.621087,
          -4.822964,
          -7.376929,
          -7.173008,
          -6.96215,
          -4.7092967,
          -4.7903824,
          -6.2709613,
          -5.788672,
          -6.0218806,
          -6.163258,
          -6.073149,
          -6.125222,
          -5.9012547,
          -5.9605007,
          -5.8564215,
          -5.6271567,
          -5.831336,
          -6.27809,
          7.262042,
          7.6073723,
          6.444484,
          7.6822424,
          7.7424974,
          7.921138,
          7.6410084,
          7.721874,
          -3.6747682,
          7.7200027,
          7.818788,
          7.6835876,
          -0.8878232,
          -1.2483388,
          -2.0757494,
          -1.5478677,
          0.44032767,
          -1.9258965,
          -2.4613476,
          -2.1343768,
          -2.2272482,
          -2.048435,
          0.4351837,
          -1.9230682,
          -1.9033536,
          -2.1753151,
          7.3277073,
          -1.8621391,
          -2.3031871,
          6.8489776,
          -2.6867259,
          7.114646,
          6.1190643,
          6.6695337,
          6.4749312,
          -3.4320533,
          7.0872283,
          6.3801785,
          5.993971,
          6.838695,
          0.42835188,
          -3.931145,
          5.4917216,
          6.700897,
          7.594887,
          7.5530515,
          7.6626797,
          2.200088,
          1.1856955,
          2.5459054,
          2.4422956,
          1.2216337,
          -4.546538,
          -4.6394806,
          1.8842373,
          2.1166472,
          4.6340985,
          5.086196,
          4.8526926,
          0.16531561,
          -0.960333,
          -2.4673831,
          -1.598171,
          -0.1444939,
          -5.2481246,
          -4.076326,
          0.26733539,
          0.16873182,
          -0.34901446,
          -0.7660859,
          0.8355638,
          -0.39065808,
          -1.6100006,
          -3.651073,
          -3.8290703,
          -3.7197187,
          -2.9136944,
          -2.8080566,
          -2.1701353,
          -3.1813815,
          -1.480433,
          -3.8174243,
          -3.7730353,
          -2.7557628,
          -3.7908132,
          -3.7920241,
          -3.7839684,
          -3.2841632,
          -3.815252,
          -3.949377,
          -3.8766136,
          -3.8187058,
          -3.7118533,
          -2.2579315,
          -3.7534087,
          -3.1462264,
          -3.723531,
          -3.9448261,
          -4.4166446,
          -3.438956,
          -4.4965434,
          -3.8727267,
          -3.6134305,
          8.136179,
          -4.0244308,
          -2.8533547,
          -3.2223363,
          -3.823163,
          -4.0639524,
          -3.8347971,
          -3.8719678,
          -4.339772,
          -4.5228,
          8.195911,
          -3.9242878,
          -3.8649247,
          -4.4360075,
          -4.0168743,
          0.28001,
          -1.2777153,
          -1.0956573,
          6.9265556,
          7.5016084,
          6.3185835,
          0.6264274,
          -0.12883952,
          2.342462,
          2.7093995,
          5.360465,
          2.657754,
          2.7592294,
          2.64983,
          0.91891986,
          6.3556647,
          7.013793,
          4.2773004,
          6.570369,
          6.718702,
          6.9595785,
          7.20445,
          6.27202,
          6.824464,
          6.7425804,
          7.0183234,
          6.931764,
          6.8946743,
          6.6772656,
          0.076539785,
          2.082912,
          2.2093177,
          2.1187317,
          2.5379813,
          1.4702795,
          -0.21241641,
          -1.6229154,
          -2.846543,
          0.32858008,
          -0.14159857,
          -3.5116682,
          7.652407,
          7.9418216,
          -3.4909499,
          0.19396394,
          -0.4416069,
          -2.821981,
          -2.5993304,
          -4.4904513,
          -0.57619625,
          -0.3344349,
          1.7229598,
          2.0782716,
          2.1924648,
          6.454355,
          3.0596845,
          -3.1624203,
          -2.9197614,
          -2.9607859,
          1.4316549,
          5.909564,
          -2.1264212,
          2.189073,
          2.0511754,
          3.7874534,
          4.9748173,
          0.3924523,
          6.5110135,
          2.6622076,
          -4.2419915,
          -3.986856,
          -3.981456,
          -4.504521,
          -4.5026402,
          -5.112365,
          -4.2004647,
          -4.166234,
          -4.338793,
          -1.804539,
          -1.6155635,
          -1.3358092,
          -2.0754542,
          -2.138801,
          -3.6988375,
          -3.6350806,
          -3.9272306,
          -3.4558244,
          -3.9500873,
          -3.5611722,
          -3.389925,
          -3.4506285,
          -1.3243517,
          7.922315,
          -1.4396006,
          -3.2874806,
          -3.6228304,
          -3.403893,
          -3.8967927,
          -3.818155,
          -2.7328289,
          -3.2094522,
          -3.7995129,
          -3.8927712,
          -3.8468149,
          -3.932069,
          -1.4377513,
          -2.992823,
          -3.9591782,
          -3.8969934,
          -3.9438496,
          -2.7386644,
          -4.0959177,
          -4.032428,
          -3.8416662,
          -4.0843134,
          -4.101409,
          -4.0846024,
          -4.0806212,
          -3.944443,
          -3.86103,
          -3.6764388,
          -4.0014963,
          -4.085316,
          -4.1027994,
          -3.7936218,
          -3.2163515,
          -4.0448456,
          -4.438748,
          8.17673,
          8.175103,
          -4.02586,
          -3.4856074,
          -4.3949738,
          -3.8281631,
          -2.0765266,
          -1.4506041,
          -2.3719554,
          -1.7673299,
          -0.94157326,
          -0.9441814,
          -0.9780129,
          -1.0396229,
          -0.0151122995,
          -3.5734403,
          -5.427375,
          -5.5838523,
          -1.459306,
          -1.0591834,
          -0.8786902,
          -1.6256814,
          -1.510732,
          2.140152,
          2.6017118,
          2.9415572,
          -2.2774575,
          -2.7374392,
          -3.8215563,
          -0.6138927,
          -3.7406414,
          -3.8302007,
          -3.5587695,
          -3.6205866,
          -3.827198,
          -4.094298,
          -4.0231113,
          -3.9356234,
          -3.9848201,
          -4.0501595,
          -4.06582,
          -4.080771,
          -4.0425706,
          -4.1392193,
          -3.8990235,
          -3.7985606,
          -0.36944693,
          -4.46287,
          8.157342,
          8.145795,
          -4.170504,
          -4.1399736,
          -3.796744,
          -3.6116667,
          -3.8836246,
          -3.8864143,
          -3.846746,
          -2.2860682,
          -2.031346,
          -2.2993848,
          1.9391588,
          -6.3519034,
          2.4349198,
          2.2429326,
          2.1517107,
          2.4650123,
          2.6036322,
          1.8832221,
          2.4823015,
          2.4640875,
          2.7379658,
          -0.6474708,
          -1.6552229,
          -1.5362393,
          -3.8583455,
          -4.2825246,
          -1.3870265,
          -0.6730845,
          -0.07953358,
          2.1707268,
          2.493251,
          1.8160701,
          2.3357913,
          2.1492684,
          1.4267864,
          -5.4056582,
          2.2948406,
          3.082333,
          -1.1983459,
          2.5660899,
          2.866319,
          2.009473,
          2.1299148,
          -0.48899043,
          -1.5445262,
          -1.611479,
          1.3515744,
          1.4779073,
          1.4445381,
          1.3460108,
          -4.4785743,
          -1.1443154,
          0.6857276,
          -4.5318766,
          -4.5243225,
          1.3785185,
          1.2142513,
          1.5598251,
          0.846485,
          1.7319202,
          1.4300387,
          1.4424013,
          1.5172296,
          1.7934713,
          2.4844675,
          2.871975,
          2.7466762,
          2.9820967,
          2.8019428,
          2.7694533,
          3.0182626,
          2.8342493,
          2.9468696,
          2.8698647,
          2.8891518,
          2.907986,
          2.9983957,
          2.7189946,
          2.2835243,
          2.7655745,
          2.7669742,
          -2.4964721,
          2.273527,
          2.0353286,
          -0.04385797,
          2.8123124,
          -3.956264,
          7.650926,
          7.3437204,
          6.932275,
          6.8019357,
          7.4132156,
          7.7265906,
          5.8875947,
          8.293881,
          7.3777976,
          7.7401915,
          2.3266098,
          7.042467,
          7.307283,
          6.9918156,
          7.0701146,
          6.9031415,
          8.446748,
          8.5866785,
          8.277298,
          8.276911,
          1.4423287,
          2.2498062,
          0.56068605,
          -0.3339505,
          0.29095644,
          -0.5195866,
          -1.1235642,
          -0.7250025,
          -0.7936111,
          -0.8036496,
          7.356349,
          7.347936,
          6.134828,
          6.876977,
          5.8524723,
          0.8765829,
          2.3624575,
          4.9415474,
          -4.960217,
          7.4179816,
          -5.7770996,
          -5.689366,
          -5.626019,
          -5.2539554,
          -0.08789308,
          8.007206,
          -6.043988,
          0.3954933,
          -6.4032454,
          -6.526287,
          -6.748147,
          -6.3930874,
          -6.165508,
          -6.52185,
          -5.9310822,
          -6.3820615,
          -6.5701923,
          -6.892811,
          -7.630342,
          -7.4717,
          -7.4485393,
          -6.4494467,
          -6.385007,
          -3.9228265,
          -6.080648,
          -5.958272,
          0.5918677,
          0.37516263,
          0.4296561,
          2.0407662,
          -6.1546884,
          0.11491428,
          -6.147346,
          -6.0997224,
          2.3234308,
          2.498413,
          2.4911065,
          2.8788292,
          2.585451,
          2.534015,
          2.532931,
          2.509158,
          2.5121799,
          2.5349958,
          2.591505,
          2.3033392,
          2.453595,
          2.5652432,
          -4.8482,
          -4.9803724,
          -5.1014714,
          -5.0540824,
          2.4549646,
          2.4187949,
          2.2314007,
          -6.314627,
          1.2202191,
          2.441239,
          -4.4031034,
          2.3669105,
          -3.266987,
          2.6716695,
          0.8783039,
          1.1779268,
          1.1516438,
          0.6041027,
          0.80297476,
          -1.2717588,
          1.2235131,
          0.9005348,
          1.7826134,
          1.4391444,
          -2.001884,
          -2.6086364,
          1.2292458,
          -3.2323816,
          -0.3238026,
          1.0496107,
          0.86899287,
          -1.0255365,
          -1.6690346,
          -2.1353004,
          -1.697675,
          -1.5394739,
          -1.3760326,
          -4.176873,
          -0.10145041,
          2.629356,
          -0.18453433,
          -1.521798,
          -3.2986872,
          -0.25476184,
          -0.9915729,
          -6.048324,
          -6.2176614,
          -6.085089,
          0.44196558,
          -6.8486524,
          -6.46949,
          -6.391948,
          -5.616095,
          -6.058116,
          -6.126521,
          -6.035718,
          -5.792913,
          -3.9110036,
          -0.100437455,
          -5.6778502,
          -5.9805164,
          -3.2323315,
          -5.5642204,
          -4.527989,
          -5.0014596,
          3.0419068,
          -4.517967,
          3.1479,
          -4.3213134,
          -4.6990657,
          -4.5515833,
          -5.6647506,
          -2.0536108,
          -2.0309262,
          -4.1930203,
          -2.4782436,
          -2.8950002,
          -2.956918,
          -4.3743143,
          -3.8917527,
          1.5297109,
          1.3683134,
          2.3120668,
          3.0673344,
          2.640833,
          3.1682515,
          2.9838104,
          3.4082637,
          3.7139359,
          3.5745792,
          3.6610885,
          3.0490952,
          3.1746233,
          2.8858755,
          3.498159,
          4.0579305,
          2.9515905,
          2.5134068,
          1.4595909,
          2.6183276,
          4.790072,
          5.5665517,
          0.3181629,
          0.23518564,
          7.9274406,
          4.8104444,
          4.9731355,
          4.8782806,
          6.507802,
          5.162804,
          4.855938,
          -0.99929476,
          1.378799,
          -2.1902452,
          -1.4354113,
          -1.7320188,
          -3.2937038,
          -3.987868,
          -4.224857,
          0.5477437,
          1.0157995,
          1.2547507,
          -1.93415,
          1.1025304,
          0.056629658,
          2.1432807,
          -1.2915189,
          2.5591176,
          2.7128656,
          2.7526917,
          2.2471015,
          2.7875218,
          2.643088,
          2.6507578,
          2.7809734,
          2.7399535,
          2.510537,
          2.7550814,
          2.815443,
          2.5744562,
          2.7881334,
          2.682366,
          2.6163955,
          2.7642481,
          2.6749969,
          2.6461132,
          2.7856882,
          2.5902636,
          2.776821,
          2.7149374,
          2.7296767,
          2.7441783,
          2.770535,
          2.799266,
          2.7437885,
          2.6991816,
          2.3813972,
          2.6372435,
          1.8714612,
          -0.09506047,
          -2.2888052,
          -0.5568143,
          -0.89422363,
          1.1520619,
          -0.14790013,
          2.558372,
          2.6996877,
          0.67065525,
          2.2870798,
          2.5289655,
          2.510531,
          -2.0919473,
          -2.3286903,
          2.3469691,
          2.758625,
          1.2230079,
          1.7131246,
          2.7180653,
          6.622287,
          6.3257565,
          6.8852234,
          6.061459,
          3.2675593,
          5.2624907,
          4.4628086,
          4.3336716,
          4.362274,
          6.8918886,
          2.3957968,
          2.6003766,
          3.4703074,
          2.4949498,
          3.115474,
          3.1916025,
          3.129055,
          2.5932002,
          3.1880858,
          3.0338454,
          3.2069137,
          2.6679902,
          3.3643365,
          0.4875637,
          1.4902099,
          -2.329219,
          0.8335359,
          0.9782212,
          -0.07428394,
          1.5542725,
          1.6363593,
          1.4048307,
          2.759615,
          2.520162,
          0.87832546,
          -5.517458,
          -5.7251534,
          0.31347457,
          -6.6678233,
          -6.753081,
          -6.8326807,
          -6.290892,
          -5.9620204,
          -6.903906,
          -6.4424214,
          -6.0918202,
          -5.536436,
          -6.636772,
          -6.085752,
          1.5046555,
          0.4959571,
          1.0464525,
          1.0940368,
          1.3491461,
          -3.070818,
          -4.467477,
          -2.6448817,
          -4.954594,
          -2.7386878,
          -2.9325767,
          1.9000021,
          1.2705437,
          0.8577531,
          1.921272,
          15.789954,
          1.2975507,
          1.8883569,
          2.0522537,
          2.3253496,
          2.0719213,
          1.9904104,
          3.9523404,
          -4.6345997,
          -4.4284353,
          -4.514289,
          -3.92395,
          -4.6258574,
          -4.359786,
          3.6390457,
          1.7787192,
          -1.7015556,
          -2.2625077,
          -2.6326375,
          -4.5777693,
          -3.4671056,
          -4.5931134,
          -3.6573427,
          -4.7194037,
          -4.3365645,
          0.63539517,
          1.592489,
          -0.9553831,
          3.1219716,
          2.6498702,
          3.381551,
          3.2479253,
          3.3291824,
          3.0211856,
          3.939842,
          4.9903855,
          2.9311996,
          0.62068677,
          2.4802437,
          2.797125,
          2.055495,
          1.646613,
          1.9279472,
          1.7697511,
          1.8704934,
          0.23825222,
          2.0832002,
          2.0369859,
          -3.433889,
          -6.0403824,
          -3.6239278,
          -2.8951776,
          -2.5474281,
          -3.1511188,
          -2.9686754,
          -3.3711233,
          1.8711474,
          1.0317189,
          1.0358554,
          -0.35987547,
          -1.8492041,
          -0.08869793,
          -3.6102343,
          -4.495848,
          -0.7440373,
          0.7509952,
          -1.0190864,
          -4.2991724,
          -3.9305406,
          -4.3910136,
          1.1182493,
          0.36319292,
          0.8216938,
          0.28899628,
          -0.9673407,
          -7.6035376,
          -4.378261,
          -1.1773394,
          -0.5990417,
          -0.95031106,
          -5.7087936,
          -5.9622803,
          -6.7121606,
          2.296094,
          -7.082213,
          -6.3040714,
          3.5657144,
          -4.044275,
          -7.081705,
          -7.080946,
          -7.0783777,
          -6.943612,
          -6.9167385,
          3.2754226,
          3.3317294,
          3.2138433,
          2.5340562,
          2.802809,
          2.8296134,
          2.9100406,
          2.9212155,
          2.863067,
          2.8380036,
          2.9747689,
          2.853755,
          2.7643135,
          1.7450203,
          -1.7254772,
          -3.2723672,
          7.222538,
          7.2299547,
          7.300408,
          7.292351,
          1.5713909,
          2.3129876,
          2.277989,
          3.5076933,
          -4.162265,
          -0.42862767,
          1.843618,
          2.6475565,
          0.6581493,
          2.3009317,
          1.1069849,
          -0.5179377,
          -2.3203645,
          -2.8946958,
          -0.62231195,
          5.1449604,
          5.495117,
          5.235122,
          4.9310303,
          0.1275116,
          -3.5829625,
          -1.8015466,
          -1.7264816,
          5.5261602,
          -1.4795843,
          5.1736197,
          0.70399183,
          1.1549466,
          2.2158263,
          2.5691974,
          1.9359262,
          1.5013579,
          -3.2851338,
          1.7631329,
          2.3814468,
          2.4686458,
          2.5319707,
          0.4812669,
          1.9261448,
          1.004358,
          0.38081652,
          0.96222484,
          1.1630304,
          0.24942827,
          0.7075797,
          0.70552635,
          1.5056723,
          1.69077,
          0.75608224,
          0.4674518,
          -0.6632214,
          -5.3225293,
          -7.6563554,
          -1.7459313,
          -3.470484,
          2.8979445,
          1.1053122,
          1.5310065,
          1.9856948,
          2.0510163,
          2.8607953,
          7.120281,
          7.1589003,
          6.2866564,
          6.5970087,
          5.474917,
          6.1517754,
          6.449377,
          6.397408,
          5.2264385,
          5.843751,
          4.6958923,
          4.678657,
          4.682338,
          5.120786,
          5.0260262,
          5.636904,
          5.091515,
          4.501168,
          4.4300623,
          -0.35249865,
          -1.0755206,
          -3.6129966,
          -3.7133515,
          -3.277445,
          -2.045508,
          -2.649732,
          -3.1792161,
          -5.8396163,
          -1.5912,
          0.7338349,
          0.679499,
          -0.38850158,
          -0.14415883,
          -2.2894404,
          -0.2915286,
          -0.9368151,
          -0.1833651,
          7.1684256,
          6.058091,
          6.2015495,
          5.1448617,
          6.2867007,
          5.15212,
          5.4411063,
          5.0234847,
          1.3335358,
          0.9504228,
          -0.85572803,
          -1.0060283,
          -2.0992458,
          -1.8176802,
          -1.4568988,
          -1.118263,
          -1.3772467,
          -1.6457479,
          -2.6346178,
          -2.159571,
          -0.9496732,
          1.3403987,
          0.092616715,
          -0.2870736,
          0.08782464,
          0.14384791,
          0.25830525,
          0.24661218,
          0.19093259,
          0.19513816,
          0.39725426,
          0.29124323,
          0.44818103,
          0.15836464,
          0.14589347,
          -0.0018323995,
          0.17050743,
          -2.426674,
          -0.1795716,
          -0.42705557,
          0.4497011,
          0.042460617,
          0.060698975,
          -0.00068115373,
          1.2877142,
          1.7232422,
          -0.23392282,
          1.9814255,
          0.82326007,
          1.2860268,
          -3.3401217,
          2.6629593,
          1.4006449,
          0.77358884,
          1.9660105,
          1.9064212,
          1.6683582,
          2.0705986,
          1.1318161,
          -3.8580794,
          -4.659873,
          -4.8232765,
          -4.206789,
          2.1311324,
          0.59999824,
          0.86296284,
          0.20488054,
          5.9968276,
          -0.20033184,
          0.8099417,
          -1.9661629,
          -4.363363,
          -5.576224,
          -4.900103,
          -2.6053894,
          -0.45894617,
          0.80476445,
          0.2054027,
          0.5059476,
          -0.8853596,
          0.12577035,
          4.021433,
          2.5437438,
          -0.3550037,
          0.3940634,
          0.53735894,
          -0.7328601,
          -3.4275024,
          5.1450624,
          4.475281,
          1.4852339,
          0.032841664,
          -6.9459853,
          -7.2162604,
          -1.3779236,
          -2.7871776,
          -0.39459863,
          -0.50475395,
          3.6777778,
          4.112263,
          4.391545,
          4.5519853,
          4.9004607,
          4.526026,
          4.4099894,
          -3.2523797,
          -3.0774076,
          -3.3864684,
          -3.2833936,
          -3.559955,
          -1.8908368,
          -2.7405784,
          -3.187613,
          -3.2676508,
          -3.5692194,
          6.312504,
          3.7173324,
          4.6306605,
          4.7635837,
          5.5399733,
          4.9344306,
          5.386561,
          3.6714485,
          6.1641407,
          0.9031927,
          1.6111009,
          0.9718888,
          0.7776333,
          -1.5862302,
          -0.6128533,
          0.6694396,
          -0.13846748,
          0.82253194,
          0.84859127,
          1.0558814,
          -0.047979254,
          2.0691202,
          1.1958477,
          -0.35458896,
          -0.6570393,
          -0.01241758,
          -4.280177,
          0.39488307,
          -0.46276262,
          2.2678223,
          0.22011332,
          2.0011153,
          1.9132351,
          1.8163712,
          1.164246,
          2.257564,
          7.547903,
          6.4669847,
          7.5285997,
          7.11386,
          0.28715983,
          7.3959975,
          7.7505465,
          6.5711412,
          7.078556,
          7.6349125,
          5.4652796,
          6.348751,
          6.7504644,
          5.4541416,
          5.767427,
          6.376057,
          7.1032224,
          7.120755,
          7.548507,
          7.1287465,
          6.400891,
          -0.26584494,
          -0.23224147,
          -0.19196227,
          -0.30902544,
          -0.782095,
          -2.5380332,
          -2.32796,
          -3.0385785,
          -0.2576189,
          -0.41165683,
          7.124782,
          7.212001,
          7.1572638,
          7.012712,
          7.1752315,
          7.208029,
          7.11945,
          0.9806214,
          -4.027212,
          5.681826,
          7.0652666,
          1.1345456,
          7.320241,
          6.9999733,
          7.0004263,
          3.5638874,
          3.1680555,
          2.9292262,
          6.5662584,
          7.063702,
          7.0372653,
          7.093395,
          7.11288,
          7.0073457,
          5.9638405,
          5.9592037,
          5.855585,
          6.3525424,
          6.397278,
          6.4894648,
          6.4612694,
          6.1379924,
          6.102213,
          6.4895177,
          6.607745,
          6.2769213,
          6.3416367,
          6.4639807,
          6.7466755,
          6.6288667,
          6.659187,
          6.5623484,
          6.501924,
          5.9313955,
          5.9237304,
          1.1017421,
          2.7913017,
          3.0046275,
          2.8499105,
          2.8738468,
          3.1866596,
          2.8989952,
          2.4516659,
          3.169744,
          2.3087907,
          2.7333226,
          2.6478076,
          3.103842,
          2.7170053,
          2.7366107,
          2.9678051,
          3.3242702,
          3.1347837,
          2.8014207,
          2.5818343,
          3.1773925,
          2.8002472,
          2.6320815,
          2.9352853,
          3.0475392,
          3.1864395,
          3.0159075,
          2.317687,
          2.5666316,
          3.0853512,
          2.2204597,
          -1.6439478,
          -1.0681134,
          -2.5394347,
          -1.3700116,
          -2.6266332,
          -1.229527,
          -3.4533708,
          -1.9410421,
          1.9742227,
          0.26704672,
          2.07429,
          2.8079708,
          2.602,
          2.7460501,
          3.2004955,
          2.9181983,
          2.668783,
          3.721977,
          3.2579844,
          2.4897652,
          2.564479,
          2.8543313,
          2.3981836,
          2.9528189,
          3.1093266,
          -2.377685,
          2.715383,
          2.8131342,
          2.432745,
          2.6703398,
          2.4362037,
          2.1552298,
          2.4893045,
          -0.3379232,
          -3.41476,
          -4.0702925,
          -6.195834,
          -5.9996905,
          -3.9347577,
          -6.9505243,
          -4.0976315,
          -4.1511207,
          -6.0584593,
          -3.9476345,
          -3.6173382,
          -2.7934525,
          -3.6361153,
          -3.8982825,
          -3.8529449,
          -3.7515821,
          -3.275856,
          -3.9029834,
          -3.750292,
          -3.5159566,
          -3.771467,
          -3.8313587,
          -1.3788013,
          -3.949986,
          -4.3169084,
          -3.8123765,
          -3.8028102,
          -4.510324,
          8.176524,
          8.183033,
          8.17494,
          8.1642475,
          8.174619,
          -4.1620326,
          -0.4720142,
          -0.39519954,
          -0.44324794,
          1.1306388,
          -0.58142966,
          0.24786547,
          -2.4317706,
          -0.6581813,
          -3.5106862,
          -0.3405231,
          -0.42558035,
          -1.3935717,
          -4.449522,
          -5.028621,
          -5.542854,
          -5.319465,
          -2.8459022,
          -5.106172,
          -4.975417,
          -4.8194275,
          -2.5375478,
          -3.7195213,
          -3.2901306,
          -4.8170376,
          -5.200017,
          -4.6422586,
          -3.0402389,
          -0.081349865,
          -1.3628252,
          -0.52340275,
          -1.209716,
          -0.91694504,
          -0.9604987,
          -0.3164425,
          -1.1719518,
          -4.37831,
          -3.484098,
          -1.1526065,
          -0.9315937,
          -2.5608153,
          -1.8038993,
          -2.779005,
          -0.72706884,
          -0.9135247,
          -0.750984,
          -3.4656785,
          -6.5698733,
          1.1301068,
          -3.7492185,
          -0.4928216,
          -1.1770256,
          -0.842703,
          -0.059476916,
          -1.2670276,
          -0.8934056,
          -2.5920196,
          -2.3976562,
          -4.512867,
          -4.21309,
          -4.266768,
          -2.4637413,
          -2.5469913,
          -2.5992792,
          -2.5706894,
          -4.4884286,
          -2.6476564,
          -4.2423353,
          -2.6183472,
          -2.6311646,
          -2.5958045,
          -2.6058445,
          -2.1833224,
          -2.0807724,
          -2.662976,
          -1.9489249,
          -0.9802928,
          0.20974538,
          -2.3691916,
          -2.4346209,
          4.652403,
          5.28567,
          5.1272635,
          0.1894998,
          -3.9794278,
          -4.2794333,
          -4.2547436,
          -3.018157,
          -1.9810125,
          -1.6072559,
          0.5394118,
          5.151577,
          2.617051,
          4.0149245,
          4.3433833,
          3.5143807,
          4.9412036,
          5.4925013,
          -0.8975471,
          3.356932,
          5.265188,
          5.802072,
          3.8009655,
          0.91154504,
          0.2723309,
          1.278358,
          -1.2960743,
          1.091904,
          -2.3037837,
          0.82822543,
          1.2370856,
          -3.636439,
          -3.0517507,
          -0.0827868,
          -0.15901135,
          0.6959324,
          0.97009116,
          1.1330588,
          3.6053452,
          1.7917669,
          -0.10999289,
          7.099354,
          2.1219985,
          -0.20288713,
          -0.061591446,
          -2.7036817,
          -1.3552936,
          1.4906135,
          5.8147206,
          6.5777707,
          6.862753,
          6.7957087,
          6.876954,
          6.853301,
          7.0683107,
          6.8186355,
          6.786548,
          6.915981,
          6.959865,
          5.8424172,
          1.4886212,
          -1.2100102,
          -0.30692562,
          -0.84916836,
          -3.881335,
          -0.44693583,
          2.2745366,
          -0.42407453,
          0.22871216,
          2.3522215,
          -3.6767728,
          -1.8947421,
          0.14384142,
          2.5950847,
          -4.0172544,
          -3.8415453,
          -3.4675617,
          -0.14305505,
          -3.8849552,
          -3.1416087,
          -2.4438806,
          -3.8430958,
          -3.514478,
          -3.768018,
          0.8513561,
          1.9712385,
          -6.7992315,
          1.3923204,
          0.9692038,
          0.15943852,
          0.27212837,
          2.176129,
          2.413446,
          -3.7879984,
          2.0364141,
          -2.9490612,
          -2.5616927,
          -3.6817253,
          -2.931926,
          -2.9084632,
          -2.671837,
          -2.1494055,
          -2.2641566,
          -2.5087178,
          -2.5710204,
          -2.4564629,
          1.5575266,
          1.3743317,
          1.2712896,
          15.782305,
          1.463609,
          0.6476961,
          -0.2946913,
          1.6240474,
          2.223645,
          0.029140756,
          -1.1555171,
          -1.0641177,
          -1.8568555,
          -0.45321277,
          1.3433756,
          1.1745172,
          1.0707539,
          1.7227384,
          -0.54845285,
          2.1474566,
          2.9314587,
          2.4110518,
          1.8026783,
          2.5051732,
          2.0239408,
          3.579906,
          3.7051373,
          2.8115978,
          2.380572,
          -0.3954644,
          2.14232,
          -0.9367186,
          1.7803051,
          2.7841012,
          -0.8465644,
          2.1086996,
          2.9314225,
          3.2104855,
          -3.3964646,
          -3.2718263,
          2.2311673,
          2.1068666,
          1.4077494,
          2.2686157,
          2.0325086,
          1.8987637,
          1.792671,
          0.7156503,
          2.2843657,
          1.9586203,
          1.9138448,
          1.179464,
          1.0371749,
          1.4568317,
          -1.9782888,
          -0.10682238,
          -1.6640223,
          2.6995609,
          2.7981353,
          1.9972901,
          1.9009919,
          2.0858932,
          2.3473911,
          -0.2418747,
          3.0219676,
          3.0284262,
          1.9800216,
          2.28523,
          2.313554,
          2.8391888,
          -2.3907511,
          -2.3201885,
          -2.3253682,
          -2.033596,
          -4.419018,
          2.257673,
          2.5962358,
          2.4551587,
          -1.3369974,
          -3.0887618,
          -3.0053504,
          -3.0200841,
          -2.901836,
          -2.8945155,
          -2.5400858,
          -3.4972167,
          -2.6354,
          -2.9044564,
          -2.821602,
          -4.591562,
          -1.5999335,
          -2.1782196,
          -4.400241,
          -2.1124146,
          -2.0929918,
          1.1212778,
          1.1397527,
          1.9513731,
          0.5897828,
          -0.38962755,
          -0.9541207,
          -1.5709366,
          0.90343946,
          0.31217384,
          0.87085027,
          -0.016762892,
          -3.101686,
          0.7745245,
          1.0463587,
          -4.1015406,
          -4.6513624,
          -3.3608768,
          -3.2866735,
          -4.455194,
          2.0642,
          2.7986803,
          -5.7171,
          -5.88581,
          -5.945418,
          -5.854901,
          -6.128978,
          -5.9770184,
          -6.508953,
          0.36167192,
          -6.245204,
          -6.3032126,
          -6.664179,
          -6.5581474,
          -6.3934374,
          -6.7316422,
          -5.989799,
          -6.4478045,
          -5.0565443,
          -6.2320356,
          -5.512745,
          -5.324146,
          -4.8369174,
          -6.6116924,
          -6.446987,
          -6.861123,
          -7.643475,
          -7.1813707,
          -7.1803374,
          -4.439907,
          -6.7899976,
          -6.397886,
          -6.629977,
          -3.9214666,
          -3.8097312,
          -6.4508505,
          -4.6740665,
          -6.17027,
          -1.0455356,
          0.99145645,
          -6.1309457,
          1.8691038,
          -0.22015244,
          0.14875522,
          -6.0191803,
          -6.340119,
          -6.820661,
          -5.956627,
          6.454039,
          6.6565456,
          6.6734276,
          6.6924534,
          6.6693697,
          6.5459757,
          6.480824,
          5.865934,
          5.8063793,
          1.8705485,
          -0.20555349,
          0.08663519,
          0.1224762,
          5.906361,
          5.023206,
          5.4586535,
          5.3981175,
          2.7514563,
          2.8497992,
          2.8457193,
          3.0814712,
          2.8512793,
          2.2517142,
          2.8765757,
          2.710775,
          1.1939857,
          2.8126762,
          2.756836,
          2.8464046,
          2.7811618,
          2.7192357,
          -3.9235954,
          -4.755402,
          -4.0123,
          -4.422745,
          -4.5240803,
          -5.7840095,
          -5.3368206,
          0.64906746,
          0.3186001,
          0.5206459,
          0.22628759,
          -0.47937396,
          -0.4878886,
          -3.140451,
          -4.565112,
          -4.416255,
          0.0822611,
          -2.6577964,
          -1.3409592,
          0.025471946,
          -0.38575268,
          -1.0119188,
          -2.8497887,
          -0.20142056,
          -0.1095332,
          1.8690178,
          2.8165529,
          2.8947973,
          2.5085123,
          2.4171648,
          2.430951,
          2.411155,
          2.1143556,
          -1.247027,
          -1.5578316,
          0.8277053,
          0.97105044,
          0.3156871,
          -0.8880746,
          -1.0392203,
          -3.4988697,
          -0.6148169,
          -1.3134799,
          -1.4048481,
          -1.5632966,
          -2.243766,
          -1.3866539,
          -3.8766768,
          -1.8458382,
          -1.5102097,
          -3.356636,
          -0.06570528,
          2.1997352,
          -3.4546556,
          1.8004491,
          1.9606634,
          2.1869164,
          -1.0609063,
          2.1098318,
          2.4131393,
          -1.289653,
          -1.6686069,
          2.0922034,
          2.9647717,
          1.2811462,
          -2.7123125,
          -2.7279713,
          -2.6739008,
          -2.6378503,
          -2.7270856,
          -2.780736,
          -2.8548393,
          -2.815732,
          -2.6574204,
          -2.6517909,
          -2.8269293,
          -2.7650604,
          1.7227138,
          7.4685454,
          7.532681,
          7.8217807,
          7.8025336,
          7.7751923,
          -0.8338542,
          -0.82588446,
          7.5882792,
          7.5449266,
          -0.055157132,
          -0.040887278,
          -0.33161515,
          -0.23431253,
          -0.1514894,
          -0.20823404,
          -1.7947247,
          -2.5836196,
          -2.3432264,
          -2.2545438,
          -2.099551,
          -3.1238458,
          -0.026176479,
          -3.1889102,
          -2.6980634,
          -2.5071914,
          -2.8698773,
          -0.095078774,
          0.20977773,
          -2.6119423,
          -2.6156335,
          -2.5979798,
          -2.681503,
          -2.8001165,
          -2.1659255,
          -2.659133,
          -2.9343033,
          -2.8858447,
          -3.0297928,
          -2.784426,
          -2.4853165,
          -2.1255407,
          -1.7328572,
          -2.9570363,
          -2.919721,
          -2.8439822,
          -1.9216374,
          0.33932477,
          -5.0549083,
          -5.0407014,
          -4.934204,
          -4.920837,
          -5.008432,
          -4.7754416,
          -1.2995691,
          1.5459745,
          -1.275139,
          1.9694687,
          2.6668108,
          2.8737593,
          4.06288,
          1.1878515,
          -2.5486474,
          0.7465214,
          1.4018447,
          1.6022382,
          4.3538914,
          -6.2724695,
          -6.1311035,
          -6.410194,
          -6.2824283,
          -2.8524122,
          1.8548927,
          1.335564,
          1.459095,
          1.6722218,
          2.2269888,
          2.1790485,
          2.007214,
          -3.2053556,
          0.8341453,
          -4.948301,
          1.1693974,
          -1.0850497,
          1.5076618,
          1.0191903,
          2.0057092,
          0.81259304,
          1.5170788,
          0.8994344,
          0.14177081,
          2.2213378,
          2.253301,
          1.163259,
          1.3364406,
          1.2925292,
          1.3125823,
          1.2460166,
          1.3486981,
          1.4508473,
          0.74784136,
          -0.06501606,
          2.5639713,
          2.8513594,
          1.3663441,
          2.6570976,
          3.278204,
          3.2525775,
          2.7970889,
          2.2674594,
          3.8734696,
          3.9548783,
          3.7764764,
          -3.7280324,
          -3.545492,
          -3.1792998,
          8.204822,
          -2.8075716,
          -2.4113283,
          -2.7048197,
          -2.066997,
          -2.5462346,
          3.6832213,
          2.5660918,
          4.5024457,
          4.0412254,
          3.9707913,
          4.078234,
          4.0629725,
          3.9947958,
          3.926019,
          3.7848022,
          5.04721,
          3.8997705,
          3.7970314,
          2.5188267,
          3.8713179,
          3.792939,
          2.9932952,
          0.91016334,
          0.32036823,
          -1.1603869,
          3.9521463,
          -2.3763337,
          3.9249394,
          -3.599617,
          -4.0242057,
          5.7958536,
          -1.023354,
          0.34172815,
          -0.4773026,
          0.6816212,
          -0.32282668,
          0.60019135,
          0.47200802,
          8.070277,
          0.79756147,
          1.2253433,
          2.236934,
          3.16644,
          2.5175679,
          3.004234,
          2.6435366,
          2.6244898,
          2.9535646,
          2.721219,
          2.3623023,
          2.4274657,
          2.7372525,
          2.0283375,
          2.4400117,
          1.29911,
          1.1991947,
          2.2898345,
          2.6339378,
          2.761508,
          3.0919826,
          0.41971314,
          -4.850764,
          -4.5674524,
          -4.72812,
          1.6379157,
          1.855373,
          1.684477,
          1.7439111,
          1.6882348,
          -1.0915834,
          1.8212245,
          1.4381399,
          1.5876064,
          1.631287,
          1.7785125,
          0.96827334,
          0.90311855,
          1.4568206,
          1.8177419,
          2.0411894,
          1.7103744,
          1.8318046,
          1.600744,
          2.03976,
          1.2437506,
          0.5566054,
          2.1973298,
          2.6777372,
          5.9736147,
          6.3239856,
          6.5559177,
          6.7177515,
          6.720207,
          6.833483,
          6.708018,
          6.752737,
          6.8642483,
          6.7275376,
          6.494564,
          6.400776,
          6.7000504,
          6.675366,
          6.647973,
          6.743522,
          5.9756,
          1.793802,
          1.4212362,
          1.7803735,
          0.32646912,
          0.71900445,
          1.5025465,
          0.70570225,
          0.87056625,
          0.45802,
          -2.169273,
          -2.4736066,
          -5.3871994,
          -5.398745,
          1.7146782,
          2.1705904,
          1.9305837,
          1.9144051,
          1.938725,
          2.011245,
          1.4206594,
          2.8111944,
          2.520861,
          3.0610554,
          2.856585,
          2.7590263,
          -1.0372688,
          -4.416992,
          -3.640529,
          -4.365049,
          2.6159728,
          1.2179859,
          2.8603277,
          -0.47535855,
          0.12313959,
          -1.0730169,
          -0.3817197,
          -0.88881403,
          -0.44763178,
          -0.3154904,
          -1.3946776,
          -0.9838846,
          -1.5843151,
          -1.174978,
          -0.90853655,
          -0.9987127,
          -1.3166633,
          -1.2400645,
          -0.85255325,
          -1.0522127,
          -0.23542029,
          0.56027955,
          0.5983686,
          2.3405206,
          0.4437843,
          0.14886574,
          5.6924214,
          -0.70025545,
          2.432103,
          2.805842,
          2.9145777,
          2.894789,
          1.9884249,
          1.6572744,
          2.8693738,
          2.1462703,
          2.088942,
          2.9241917,
          2.7376142,
          1.9721006,
          2.572804,
          1.975886,
          -1.9009601,
          1.5440995,
          -0.22265491,
          -0.77092016,
          -0.40187114,
          1.3530747,
          1.4073168,
          -0.30137432,
          -3.335455,
          -4.1102123,
          0.54344994,
          0.25119585,
          -1.9771631,
          -6.3755765,
          1.7989936,
          -1.3331363,
          -3.3315551,
          -3.4783235,
          -2.5988123,
          -2.0999496,
          -3.0878954,
          -1.6976599,
          -1.9902442,
          2.6041965,
          -2.430175,
          2.887555,
          -3.5378098,
          -2.1934385,
          -1.9981093,
          -2.769169,
          -1.8839629,
          -1.0332922,
          -1.6697488,
          -2.4810781,
          -3.5350804,
          2.1995556,
          -1.0342879,
          -0.9344385,
          -1.2020118,
          -0.8840051,
          -1.701311,
          -4.2743387,
          -2.1601171,
          -1.1981051,
          -1.1711563,
          1.2361612,
          -0.57885736,
          -0.6171444,
          -0.59458715,
          1.2376398,
          0.9368504,
          2.3311508,
          -5.294415,
          -5.1208816,
          -2.1710174,
          -4.717728,
          -4.6801863,
          -6.167444,
          -0.31160823,
          -6.8244123,
          -4.2144427,
          -5.4962325,
          -5.054311,
          -5.7058125,
          -6.4052863,
          -5.545026,
          -6.6240706,
          -6.0859823,
          -5.043453,
          -5.144544,
          2.1329331,
          1.7035166,
          1.9365182,
          2.1372306,
          6.1792727,
          6.462439,
          6.8989606,
          6.9706826,
          6.8574004,
          6.773824,
          6.7690954,
          6.75139,
          6.668622,
          6.86067,
          6.7354364,
          6.248987,
          5.98904,
          2.5181024,
          2.7074122,
          2.7156148,
          2.7738924,
          2.6657345,
          3.6408162,
          3.2373145,
          3.0065522,
          3.0549915,
          2.872156,
          2.1305227,
          2.8233976,
          2.303257,
          -1.6518838,
          -1.9486231,
          -2.1954286,
          -2.3565824,
          -2.9101613,
          -2.3870513,
          -2.9507031,
          -4.5311313,
          -3.9802034,
          -2.8691425,
          -3.2460313,
          -1.6839695,
          -2.038317,
          0.07212084,
          -1.9789406,
          6.212261,
          5.5100346,
          6.0717416,
          5.560086,
          6.3866234,
          6.422033,
          5.262087,
          6.3596725,
          6.2004414,
          6.569899,
          3.400893,
          3.9360504,
          6.2037735,
          6.3065014,
          6.1101623,
          6.8332977,
          6.3684716,
          1.1186242,
          -0.5171885,
          -1.7246622,
          -1.9535191,
          -2.4242902,
          -0.10909156,
          -1.6633683,
          1.3163333,
          2.69049,
          2.6767328,
          1.2296402,
          1.0107919,
          1.9493548,
          0.7245866,
          2.751216,
          2.5682745,
          2.1864245,
          1.7921883,
          1.3210456,
          2.447999,
          -0.6330702,
          -0.85730106,
          -0.85114574,
          -0.8224284,
          5.4370604,
          -1.830981,
          -2.5126586,
          -3.2148676,
          -0.86177576,
          0.4038386,
          1.6717533,
          2.3689384,
          2.800496,
          2.30034,
          3.1214423,
          3.1882973,
          2.8951464,
          3.2096918,
          2.6643019,
          2.5135868,
          3.0315688,
          2.9274886,
          2.8808079,
          2.4743068,
          2.3971426,
          2.5501592,
          2.8304827,
          2.7548637,
          2.4257665,
          2.5674477,
          21.096975,
          3.1503613,
          2.1079147,
          -0.6804155,
          -0.73158914,
          -2.6729815,
          1.6799003,
          -0.08976523,
          -0.064864404,
          -1.5215278,
          -0.44855523,
          -2.9750462,
          -2.7938557,
          -2.782673,
          -3.8132987,
          -4.416848,
          0.32477608,
          -4.0152044,
          0.2469471,
          7.142985,
          7.478436,
          7.459586,
          7.2732344,
          -2.7876651,
          -4.5591063,
          7.3337016,
          7.3387103,
          -5.8029785,
          -6.2590795,
          -6.5903482,
          0.26669565,
          -6.234709,
          -5.8929505,
          -6.3186445,
          -6.7371335,
          -5.832239,
          -6.558987,
          -5.3583007,
          -5.9567714,
          -4.405644,
          -6.497919,
          -6.430914,
          -6.6785154,
          -7.3538957,
          -6.952109,
          -7.1847258,
          -4.077592,
          -6.7657847,
          -6.5200357,
          -3.807699,
          -3.8406436,
          -6.4125676,
          -4.6321063,
          -6.415535,
          0.6302713,
          -6.1824293,
          0.3460535,
          0.27143672,
          2.2031448,
          0.1401051,
          -0.68452096,
          -3.65413,
          -4.5827627,
          -4.21326,
          -3.1025732,
          -6.2103744,
          2.0414011,
          2.1405404,
          2.1079717,
          1.7704539,
          1.3019555,
          1.748773,
          -2.4802196,
          -2.2795494,
          -2.8854797,
          -3.0575593,
          -1.1726964,
          -1.3724921,
          -0.9792647,
          -0.6812687,
          -3.4348235,
          -5.8807178,
          -0.5743748,
          -2.4044285,
          -2.200076,
          0.5498752,
          -2.041427,
          0.063731045,
          -4.479318,
          -2.693596,
          -2.843919,
          -2.7320337,
          -2.827108,
          -2.6290903,
          -0.7368284,
          -6.0267296,
          -4.443232,
          -6.251293,
          0.24915674,
          -6.3106885,
          -6.3906226,
          -0.08349537,
          -6.4275594,
          -7.6374593,
          -7.4563866,
          -6.5150013,
          -3.9172432,
          -4.1691732,
          -4.4098654,
          0.6209874,
          2.0731347,
          -6.3916955,
          -6.2642317,
          0.077628955,
          2.7746663,
          2.6848848,
          0.96220773,
          -3.5819736,
          -4.5029373,
          -4.227996,
          2.710997,
          0.28086567,
          2.3377614,
          -2.5102663,
          1.0958018,
          1.1972153,
          1.990396,
          2.3372805,
          2.5546443,
          2.8418508,
          2.381057,
          2.7004948,
          1.8138748,
          1.1788396,
          1.510878,
          1.2385002,
          1.5009419,
          1.7239221,
          1.1550372,
          -2.6474543,
          -2.8447635,
          -2.7540371,
          -2.8439548,
          -2.804208,
          -2.741203,
          -2.9029477,
          -2.6614869,
          -2.6368494,
          -2.9215636,
          -2.779829,
          -2.9952457,
          -2.860385,
          -2.6639657,
          -2.8141384,
          -2.8414629,
          -2.6840107,
          -2.9671257,
          -2.6308813,
          1.4457802,
          2.3251994,
          -2.51629,
          1.4139425,
          2.4861588,
          -2.7600713,
          -3.2081726,
          -3.1412148,
          -3.340539,
          -3.1632533,
          -2.497811,
          1.0593634,
          2.4441028,
          -2.4921424,
          1.5642455,
          -2.521919,
          -3.210952,
          -3.2385044,
          -3.2082968,
          -3.296861,
          -3.2799916,
          -3.4373944,
          -3.282466,
          -3.4560611,
          -2.925101,
          0.7412921,
          -2.5160713,
          1.4562441,
          -2.7530112,
          -2.9569304,
          -3.020865,
          -3.1002226,
          -3.3665364,
          -3.1976972,
          -3.15231,
          -3.0612657,
          -2.8692276,
          -1.7601857,
          -2.9655333,
          1.6395065,
          -2.7780066,
          -2.0482922,
          -2.4191923,
          -2.4884446,
          -2.2010849,
          -2.8534045,
          -2.4971836,
          -2.1610456,
          -1.9923421,
          -2.1205673,
          1.6448368,
          1.4769547,
          0.5893065,
          -2.0341663,
          -1.7044306,
          2.1130974,
          2.197369,
          2.5605457,
          2.6220276,
          2.8882532,
          2.5962765,
          2.912178,
          5.0110517,
          5.1308246,
          -0.50804365,
          5.441852,
          4.751909,
          7.407092,
          5.1632605,
          0.41531944,
          0.39096987,
          4.6933618,
          4.1693087,
          3.495072,
          4.568516,
          3.2046,
          2.868593,
          4.071487,
          1.1725719,
          2.1361015,
          1.5119991,
          1.9179806,
          1.1889445,
          -1.1023121,
          0.16851844,
          -2.392643,
          -0.21144854,
          1.3809397,
          7.3131876,
          7.4657197,
          4.7100434,
          3.3074055,
          3.2464786,
          7.132842,
          2.5027845,
          2.714883,
          2.7324796,
          2.798077,
          2.9316785,
          2.5632982,
          2.752425,
          2.9050417,
          2.8799655,
          2.8864062,
          2.9640467,
          2.9447372,
          2.3462312,
          2.7653465,
          2.9330163,
          2.6447766,
          1.4118397,
          -2.9416394,
          -0.24896426,
          -0.21821296,
          -2.3992174,
          -1.3690196,
          -1.3505199,
          -1.1425544,
          1.7382069,
          2.5934327,
          2.7349446,
          2.5691388,
          2.7405329,
          2.6458251,
          -1.9399064,
          -1.9802585,
          2.5592194,
          2.6058586,
          2.748668,
          2.7495618,
          2.7461941,
          2.8360968,
          2.4811642,
          2.3624191,
          -0.002110417,
          2.9864416,
          2.747779,
          -1.1395265,
          2.8816667,
          0.615465,
          0.7156993,
          1.5978285,
          1.3436196,
          0.5116369,
          1.8394836,
          2.7059414,
          0.78692025,
          -4.1755233,
          -2.7415051,
          -3.7111735,
          -3.518927,
          -4.076824,
          -3.9498923,
          1.6752958,
          1.550311,
          1.6321481,
          1.9088091,
          1.090134,
          1.873198,
          15.820066,
          1.6825424,
          1.4391809,
          0.32282707,
          1.9507121,
          0.69043666,
          1.3272288,
          2.1414404,
          2.6355972,
          -3.4858305,
          2.7472472,
          -1.0790892,
          2.7479706,
          0.78521985,
          2.2970922,
          0.24875562,
          -3.5992453,
          2.0701435,
          -0.635409,
          2.694747,
          2.8252048,
          2.4725835,
          2.5282369,
          3.0297735,
          1.1053202,
          2.8491962,
          2.6614916,
          2.3988588,
          2.5886147,
          2.7998726,
          2.8785717,
          2.821857,
          2.4062707,
          2.5983706,
          2.9423637,
          2.2980494,
          2.293678,
          2.409877,
          2.1616962,
          3.1171577,
          3.0891693,
          2.639924,
          1.1352617,
          5.1505756,
          5.1517396,
          4.995956,
          4.9439254,
          5.3880057,
          5.249911,
          5.3594027,
          -0.71950793,
          -1.1932497,
          -1.2748291,
          1.3502176,
          2.524292,
          -1.8965842,
          -1.7933204,
          1.2925887,
          0.95483655,
          0.5843304,
          1.6373515,
          -1.0083628,
          -1.7820271,
          -4.7311654,
          0.39583048,
          -3.4337304,
          -4.5468507,
          -3.566574,
          -2.842715,
          1.4271548,
          -2.4760804,
          -4.5386257,
          -2.6889176,
          0.45429257,
          0.39721122,
          -3.4767988,
          -5.3918414,
          -5.4182315,
          -5.436025,
          0.5494865,
          1.0233542,
          -1.2645851,
          -4.07469,
          -2.7272456,
          -3.9896882,
          0.94814134,
          0.63088924,
          1.25162,
          2.1007285,
          2.750982,
          2.5460765,
          2.0682352,
          1.7563728,
          2.6067615,
          1.306099,
          1.1465279,
          0.4035571,
          2.842644,
          -0.014637018,
          -3.1059642,
          1.1646191,
          1.5093439,
          0.980978,
          0.83221155,
          -6.548464,
          0.5768533,
          -0.94151753,
          -0.066727735,
          0.38424972,
          3.2440238,
          0.7741462,
          1.9282025,
          -2.610615,
          4.308294,
          6.9486957,
          -0.6192574,
          7.6724224,
          6.6036577,
          5.5367427,
          5.248263,
          4.7245936,
          1.9708409,
          -2.0385442,
          1.0968026,
          2.004202,
          2.096934,
          2.3108,
          0.88060737,
          2.1867733,
          0.7865212,
          2.015264,
          0.65522355,
          0.7981676,
          1.6150584,
          -3.177423,
          -3.3190918,
          -3.2463431,
          -3.1333444,
          -4.6497087,
          -4.467611,
          -4.6850533,
          -3.9723241,
          -3.2064462,
          -4.5530066,
          -4.602381,
          -4.277442,
          -4.153389,
          -4.01233,
          -4.3590183,
          -3.5691137,
          -3.4782557,
          -3.1819603,
          -3.3331678,
          -3.2366266,
          -2.819687,
          -2.5865374,
          -0.47598824,
          0.83179647,
          -4.5611963,
          -5.8859773,
          -5.0409718,
          -5.00055,
          -3.015663,
          -3.1586385,
          -3.2070184,
          0.8524145,
          0.13675307,
          0.8701204,
          0.9144667,
          0.37787592,
          -5.9956493,
          -5.983143,
          -6.1859035,
          0.38778856,
          0.38153893,
          -6.259209,
          -6.013973,
          -5.9862976,
          -6.610305,
          -6.085135,
          -5.6202006,
          -6.6274643,
          -5.342518,
          -4.1153865,
          -6.499859,
          -5.1066513,
          -6.1694555,
          -5.4274592,
          -6.6212826,
          -6.4802275,
          -6.713331,
          -7.6713142,
          -7.1224594,
          -7.1331024,
          -4.418727,
          -6.9346223,
          -6.371575,
          -4.018608,
          -3.9344645,
          -6.6847715,
          -4.818594,
          -6.2326474,
          -6.2413898,
          -6.229266,
          -6.1853395,
          -1.8781774,
          -6.5230064,
          -6.124676,
          5.810819,
          6.5294595,
          6.3027096,
          6.1781716,
          6.2177,
          6.2221365,
          6.5287952,
          6.4087634,
          5.7903256,
          5.9802384,
          2.5613928,
          -1.1406986,
          2.200394,
          2.5704584,
          3.464154,
          3.2936792,
          2.6291773,
          2.784823,
          -0.9398789,
          0.6468727,
          3.8826528,
          3.5320294,
          4.9543467,
          -0.5210839,
          1.9768715,
          1.9418834,
          -2.8691707,
          3.831457,
          0.8121662,
          -1.8406416,
          -0.35343155,
          1.1314496,
          -4.5289526,
          -1.9122566,
          1.1257995,
          0.07225672,
          -0.91150916,
          -0.36555788,
          -0.392526,
          -0.58990544,
          -0.41765222,
          -0.39802682,
          -0.41216245,
          -0.5810784,
          -1.0492384,
          -0.13491876,
          3.0756054,
          2.084084,
          0.17691466,
          -0.12122059,
          -0.83658016,
          -0.5521986,
          -2.8753755,
          -1.455431,
          -0.7333968,
          2.49714,
          2.7825432,
          2.9979596,
          2.912491,
          2.8016555,
          2.9015746,
          2.8991635,
          2.7384503,
          2.9390554,
          1.9484609,
          6.5818257,
          0.12660475,
          -0.93100256,
          5.932584,
          6.062448,
          5.918937,
          5.9646063,
          6.085967,
          5.7361927,
          5.560097,
          6.0169187,
          6.9368715,
          6.1145816,
          6.456765,
          6.95039,
          2.0945485,
          -2.4420304,
          1.512416,
          7.5074706,
          6.819584,
          7.2841926,
          7.4875574,
          5.706961,
          7.10284,
          7.7413974,
          7.5858335,
          7.3549585,
          8.431832,
          2.0503159,
          1.6837474,
          1.965355,
          2.8600729,
          2.4621644,
          3.0176787,
          2.336867,
          1.366202,
          0.48676872,
          1.1998762,
          1.335652,
          -0.65588695,
          2.0119014,
          1.7492821,
          0.044041365,
          1.3539025,
          0.12332793,
          0.63116264,
          0.65614617,
          1.573926,
          2.450056,
          1.9856601,
          1.3652277,
          0.57980627,
          1.3783591,
          1.009261,
          1.2355136,
          0.40103927,
          0.68468183,
          0.42602208,
          -0.53935313,
          -6.2076597,
          0.3891412,
          -4.352255,
          -3.145048,
          0.8876798,
          1.011418,
          0.6890746,
          0.12317153,
          1.1271442,
          1.0033051,
          -0.20637228,
          -0.7055333,
          1.4760164,
          1.7080575,
          -2.8461099,
          1.5473747,
          2.2426794,
          1.2390308,
          2.2215827,
          2.620079,
          2.4461582,
          2.193282
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "hat is dynamic padding? In the \"Batching Inputs together\" video, we have seen that to be able to gro..."
          ],
          [
           "we can apply. The most obvious one is to pad all the elements of the dataset to the same length: the..."
          ],
          [
           "sentence inside the batch. This way batches composed of short inputs will be smaller than the batch ..."
          ],
          [
           "and tokenizer, we applied the tokenization to all the dataset with padding and truncation to make al..."
          ],
          [
           "(usually 512) get truncated to that length. Then we pad our samples dynamically by using a data coll..."
          ],
          [
           "on CPUs and GPUs, so you should apply it if you can. Remember to switch back to fixed padding howeve..."
          ],
          [
           "ow to slice and dice a dataset. Most of the time, the data you work with wonâ€™t be perfectly prepared..."
          ],
          [
           "shuffling. It is generally a good idea to apply shuffling to the training set so that your model doe..."
          ],
          [
           "if you have to create your own test splits from raw data. To do this, you just apply the train_test_..."
          ],
          [
           "most common way to do this is with the select method. This method expects a list or generator of the..."
          ],
          [
           "checks whether each rows fulfills some condition or not. For example, here we've created a small lam..."
          ],
          [
           "method to delete them. You can see examples of both these method here. Some datasets have nested col..."
          ],
          [
           "in the dataset. For example,here we first define a lowercase_title function that simply lowercases t..."
          ],
          [
           "he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library t..."
          ],
          [
           "determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which ..."
          ],
          [
           "even if your dataset is huge you won't get out of RAM: only the elements you request are loaded in m..."
          ],
          [
           "and names for the labels. 0 stands for not equivalent and 1 for equivalent. To preprocess all the el..."
          ],
          [
           "directly apply to all the splits in our dataset with the map method. As long as the function returns..."
          ],
          [
           "not need to change for this. You can also use multiprocessing with the map method, check out its doc..."
          ],
          [
           "efore diving in character-based tokenization, understanding why this kind of tokenization is interes..."
          ],
          [
           "estimated 170,000 different words, we would need a very large vocabulary to encompass all words. Wit..."
          ],
          [
           "in a language, even words unseen during the tokenizer training can still be tokenized, so out-of-voc..."
          ],
          [
           "of information held in single characters, but for others like roman-based languages, the model will ..."
          ],
          [
           "This tokenization, while it has some issues, has seen some very good results in the past and should ..."
          ],
          [
           "Normalization and pre-tokenization[[normalization-and-pre-tokenization]]\n\n<CourseFloatingBanner chap..."
          ],
          [
           "## Normalization[[normalization]]\n\n<Youtube id=\"4IIC2jI9CaU\"/>\n\nThe normalization step involves some..."
          ],
          [
           "```\n\n```python out\n<class 'tokenizers.Tokenizer'>\n```\n\nThe `normalizer` attribute of the `tokenizer`..."
          ],
          [
           "```\n\n```python out\n[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (..."
          ],
          [
           "```\n\nAlso note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.\n\nFor..."
          ],
          [
           "```\n\nLike the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), ..."
          ],
          [
           "## Algorithm overview[[algorithm-overview]]\n\nIn the following sections, we'll dive into the three ma..."
          ],
          [
           "ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them to..."
          ],
          [
           "duplicates; in the second, they are not. Another pair classification problem is when we want to know..."
          ],
          [
           "an academic benchmark for text classification), 8 of the 10 datasets are focused on tasks using pair..."
          ],
          [
           "deal with pairs of sentences: you just have to pass them as two arguments to the tokenizer. On top o..."
          ],
          [
           "also added special tokens so we have a CLS token, the tokens from the first sentence, a SEP token, t..."
          ],
          [
           "outputs the same length, and properly dealt with token type IDS and attention masks for the two sent..."
          ],
          [
           "The Hugging Face Course\n\nThis repo contains the content that's used to create the **[Hugging Face co..."
          ],
          [
           "| Language                                                                      | Source            ..."
          ],
          [
           "|:------------------------------------------------------------------------------|:------------------..."
          ],
          [
           "-------------------------------------------------------------------------|:-------------------------..."
          ],
          [
           "----------------------------------------------------------------------------------------------------..."
          ],
          [
           "----------------------------------------------------|..."
          ],
          [
           "| [English](https://huggingface.co/course/en/chapter1/1)                        | [`chapters/en`](ht..."
          ],
          [
           "| [Spanish](https://huggingface.co/course/es/chapter1/1) (WIP)                  | [`chapters/es`](ht..."
          ],
          [
           "| [Hebrew](https://huggingface.co/course/he/chapter1/1) (WIP)                   | [`chapters/he`](ht..."
          ],
          [
           "| [Japanese](https://huggingface.co/course/ja/chapter1/1) (WIP)                 | [`chapters/ja`](ht..."
          ],
          [
           "| [Russian](https://huggingface.co/course/ru/chapter1/1) (WIP)                  | [`chapters/ru`](ht..."
          ],
          [
           "| [Vietnamese](https://huggingface.co/course/vi/chapter1/1)               | [`chapters/vi`](https://..."
          ],
          [
           "### Translating the course into your language\n\nAs part of our mission to democratise machine learnin..."
          ],
          [
           "```\n\n**ðŸ“‹ Copy-paste the English files with a new language code**\n\nThe course files are organised und..."
          ],
          [
           "```\n\n> ðŸš¨ Make sure the `_toctree.yml` file only contains the sections that have been translated! Oth..."
          ],
          [
           "```\npip install -r requirements.txt\nmake style\n```\n\nOnce that's run, commit any changes, open a pull..."
          ],
          [
           "```\n\nThen run the following script:\n\n```bash\npython utils/generate_notebooks.py --output_dir nbs\n```..."
          ],
          [
           "What to do when you get an error[[what-to-do-when-you-get-an-error]]\n\n<CourseFloatingBanner chapter=..."
          ],
          [
           "```\n\nor the following in your favorite terminal:\n\n```bash\nhuggingface-cli login\n```\n\nThis will promp..."
          ],
          [
           "```\n\nNow when you call `copy_repository_template()`, it will create a copy of the template repositor..."
          ],
          [
           "```\n\nOh no, something seems to have gone wrong! If you're new to programming, these kind of errors c..."
          ],
          [
           "```python out\n\"\"\"\nMake sure that:\n\n- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ If you encounter an error message that is difficult to understand, just copy and paste..."
          ],
          [
           "Okay, this got a hit. Now let's try to download the model again with the correct model ID:\n\n```pytho..."
          ],
          [
           "```\n\n```python out\n\"\"\"\nOSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squa..."
          ],
          [
           "```\n\nInteresting -- there doesn't seem to be a *config.json* file in the repository! No wonder our `..."
          ],
          [
           "```\n\nNow we can test if this worked by loading the model from the latest commit on the `main` branch..."
          ],
          [
           "```\n\nWoohoo, it worked! Let's recap what you've just learned:\n\n- The error messages in Python are kn..."
          ],
          [
           "```\n\nNext we need a question, so let's see if our favorite frameworks are supported:\n\n```python\nques..."
          ],
          [
           "```\n\n```python out\n\"\"\"\n---------------------------------------------------------------------------\nA..."
          ],
          [
           "~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_di..."
          ],
          [
           "AttributeError: 'list' object has no attribute 'size'\n\"\"\"..."
          ],
          [
           "```\n\nOh dear, it looks like we have a bug in our code! But we're not afraid of a little debugging. Y..."
          ],
          [
           "```\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modelin..."
          ],
          [
           "```\n\nIt looks like our code tried to call `input_ids.size()`, but this clearly won't work for a Pyth..."
          ],
          [
           "```\n\n```python out\n\"\"\"\nQuestion: Which frameworks can I use?\nAnswer: pytorch, tensorflow, and jax\n\"\"..."
          ],
          [
           "hat is transfer learning? The idea of Transfer Learning is to leverage the knowledge acquired by a m..."
          ],
          [
           "B. When training from scratch, all the modelâ€™s weight are initialized randomly. In this example, we ..."
          ],
          [
           "86% easily. This is because pretrained models are usually trained on large amounts of data that prov..."
          ],
          [
           "key difference with ImageNet is that the pretraining is usually self-supervised, which means it does..."
          ],
          [
           "have done in school. BERT was pretrained this way using the English Wikipedia and 11,000 unpublished..."
          ],
          [
           "labels. To be as efficient as possible, the pretrained model used should be as similar as possible t..."
          ],
          [
           "from these countries. OpenAI also studied the bias in the predictions of its GPT-3 model (which was ..."
          ],
          [
           "ow to batch inputs together? In this video, we will see how to batch input sequences together. In ge..."
          ],
          [
           "error, because all arrays and tensors should be rectangular. One way to overcome this limit is to ma..."
          ],
          [
           "used to pad the second sentence should not be picked randomly: the model has been pretrained with a ..."
          ],
          [
           "should not come as a total surprise: when computing the contextual representation of each token, the..."
          ],
          [
           "input IDs, with zeros and ones. Ones indicate the tokens the attention layers should consider in the..."
          ],
          [
           "upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on whi..."
          ],
          [
           "or another and to learn a new API. All those setups are handled by the Trainer API, and there are se..."
          ],
          [
           "training loop (here shown on the code of the training loop from the \"Raw training loop\" video), Acce..."
          ],
          [
           "main method to remember. Accelerate handles device placement, so you don't need to put your batch on..."
          ],
          [
           "this: pass along the evaluation dataloader to the accelerator.prepare method, like for training. The..."
          ],
          [
           "easy API to configure your setup and launch your training script. In a terminal, run accelerate conf..."
          ],
          [
           "Building your first demo[[building-your-first-demo]]\n\n<CourseFloatingBanner chapter={9}\n  classNames..."
          ],
          [
           "```\n\nLet's walk through the code above:\n\n- First, we define a function called `greet()`. In this cas..."
          ],
          [
           "Try using this GUI right now with your own name or some other input!\n\nYou'll notice that in this GUI..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-hello-world-custom.hf.space\" frameBorder=\"0\" height=\"300\" tit..."
          ],
          [
           "```\n\nThis function completes prompts that you provide, and you can run it with your own input prompt..."
          ],
          [
           "Natural Language Processing[[natural-language-processing]]\n\n<CourseFloatingBanner\n    chapter={1}\n  ..."
          ],
          [
           "## Why is it challenging?[[why-is-it-challenging]]\n\nComputers don't process information in the same ..."
          ],
          [
           "Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={8}\n    classNames=\"absolute z-10 ri..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Sharing pretrained models[[sharing-pretrained-models]]\n\n{#if fw ===..."
          ],
          [
           "<Youtube id=\"9yY3RB_GSPM\"/>\n\nWe encourage all users that train models to contribute by sharing them ..."
          ],
          [
           "```\n\nIn a terminal, you can run:\n\n```bash\nhuggingface-cli login\n```\n\nIn both cases, you should be pr..."
          ],
          [
           "```\n\nWhen you call `trainer.train()`, the `Trainer` will then upload your model to the Hub each time..."
          ],
          [
           "```\n\nThen you should add `callbacks=[callback]` in your call to `model.fit()`. The callback will the..."
          ],
          [
           "```\n\nThis will create the new repository `dummy-model` in your profile, and populate it with your mo..."
          ],
          [
           "```\n\nNow head to the Model Hub to find your newly uploaded model: *https://huggingface.co/user-or-or..."
          ],
          [
           "The `push_to_hub()` method is backed by the [`huggingface_hub`](https://github.com/huggingface/huggi..."
          ],
          [
           "```\n\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpo..."
          ],
          [
           "```\n\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belo..."
          ],
          [
           "After creating your model repository, you should see a page like this:\n\n<div class=\"flex justify-cen..."
          ],
          [
           "We'll take a look at how to add some new files next.\n\n## Uploading the model files[[uploading-the-mo..."
          ],
          [
           "```\n\nThis will upload the file `config.json` available at `<path_to_file>` to the root of the reposi..."
          ],
          [
           "```\n\nAnd others! We recommend taking a look at the `Repository` documentation available [here](https..."
          ],
          [
           "```\n\n```bash\nUpdated git hooks.\nGit LFS initialized.\n```\n\nOnce that's done, the first step is to clo..."
          ],
          [
           "```\n\n```bash\nREADME.md\n```\n\nIf you just created your repository using Hugging Face Hub's `create_rep..."
          ],
          [
           "```\n{/if}\n\nNow that we've saved some model and tokenizer artifacts, let's take another look at the *..."
          ],
          [
           "```\n\nWe can then have a look at the files that are currently staged:\n\n```bash\ngit status\n```\n\n{#if f..."
          ],
          [
           "```\n\nWe can see that all files have `Git` as a handler, except *pytorch_model.bin* and *sentencepiec..."
          ],
          [
           "```\n{/if}\n\nPushing can take a bit of time, depending on the speed of your internet connection and th..."
          ],
          [
           "```\n\n{#if fw === 'pt'}\nIf we take a look at the model repository when this is finished, we can see a..."
          ],
          [
           "n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the..."
          ],
          [
           "one, so we recommend you look at the videos in the following order: Word-based, Character-based, and..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning a masked language model[[fine-tuning-a-masked-language-..."
          ],
          [
           "However, there are a few cases where you'll want to first fine-tune the language models on your data..."
          ],
          [
           "<iframe src=\"https://course-demos-distilbert-base-uncased-finetuned-imdb.hf.space\" frameBorder=\"0\" h..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img src=\"https://huggingface.co/datasets/huggingface-course/docum..."
          ],
          [
           "```\n\nWe can see how many parameters this model has by calling the `num_parameters()` method:\n\n```pyt..."
          ],
          [
           "```python out\nModel: \"tf_distil_bert_for_masked_lm\"\n________________________________________________..."
          ],
          [
           "vocab_projector (TFDistilBer multiple                  23866170  \n==================================..."
          ],
          [
           "```\n\n{/if}\n\nWith around 67 million parameters, DistilBERT is approximately two times smaller than th..."
          ],
          [
           "```\n\nWith a tokenizer and a model, we can now pass our text example to the model, extract the logits..."
          ],
          [
           "```\n\n{/if}\n\n```python out\n'>>> This is a great deal.'\n'>>> This is a great success.'\n'>>> This is a ..."
          ],
          [
           "```\n\nWe can see that the `train` and `test` splits each consist of 25,000 reviews, while there is an..."
          ],
          [
           "```\n\n```python out\n\n'>>> Review: This is your typical Priyadarshan movie--a bunch of loony character..."
          ],
          [
           "'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, an..."
          ],
          [
           "```\n\nYep, these are certainly movie reviews, and if you're old enough you may even understand the co..."
          ],
          [
           "So to get started, we'll first tokenize our corpus as usual, but _without_ setting the `truncation=T..."
          ],
          [
           "```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_id..."
          ],
          [
           "```\n\n```python out\n512\n```\n\nThis value is derived from the *tokenizer_config.json* file associated w..."
          ],
          [
           "```\n\n```python out\n'>>> Review 0 length: 200'\n'>>> Review 1 length: 559'\n'>>> Review 2 length: 192'\n..."
          ],
          [
           "```\n\nAs you can see in this example, the last chunk will generally be smaller than the maximum chunk..."
          ],
          [
           "```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['attention_mask', 'input_id..."
          ],
          [
           "```\n\n```python out\n\".... at.......... high. a classic line : inspector : i'm here to sack one of you..."
          ],
          [
           "```\n\nTo see how the random masking works, let's feed a few examples to the data collator. Since it e..."
          ],
          [
           "```\n\nNice, it worked! We can see that the `[MASK]` token has been randomly inserted at various locat..."
          ],
          [
           "```py\nimport collections\nimport numpy as np\n\nfrom transformers import default_data_collator\n\nwwm_pro..."
          ],
          [
           "```\n\n{:else}\n\n```py\nimport collections\nimport numpy as np\n\nfrom transformers.data.data_collator impo..."
          ],
          [
           "```\n\n```python out\n'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as so..."
          ],
          [
           "```\n\n<Tip>\n\nâœï¸ **Try it out!** Run the code snippet above several times to see the random masking ha..."
          ],
          [
           "```\n\nThis has automatically created new `train` and `test` splits, with the training set size set to..."
          ],
          [
           "```\n\nNext, we set up our training hyperparameters and compile our model. We use the `create_optimize..."
          ],
          [
           "# Train in mixed-precision float16\ntf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n\nmode..."
          ],
          [
           "```\n\nWe're now ready to run `model.fit()` -- but before doing so let's briefly look at _perplexity_,..."
          ],
          [
           "```\n\nHere we tweaked a few of the default options, including `logging_steps` to ensure we track the ..."
          ],
          [
           "```\n\nWe're now ready to run `trainer.train()` -- but before doing so let's briefly look at _perplexi..."
          ],
          [
           "```\n\n{:else}\n\nAssuming our test set consists mostly of sentences that are grammatically correct, the..."
          ],
          [
           "```\n\n{:else}\n\n```python\neval_loss = model.evaluate(tf_eval_dataset)\nprint(f\"Perplexity: {math.exp(ev..."
          ],
          [
           "```\n\n{/if}\n\n<Tip>\n\nâœï¸ **Your turn!** Run the training above after changing the data collator to the ..."
          ],
          [
           "```\n\nNext, we'll apply this function to our test set and drop the unmasked columns so we can replace..."
          ],
          [
           "```\n\nWith these objects, we can now prepare everything for training with the `Accelerator` object:\n\n..."
          ],
          [
           "```\n\nWith that done, it's just a simple matter of writing out the full training and evaluation loop:..."
          ],
          [
           "```\n\nCool, we've been able to evaluate perplexity with each epoch and ensure that multiple training ..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Using pretrained models[[using-pretrained-models]]\n\n{#if fw === 'pt..."
          ],
          [
           "Let's say we're looking for a French-based model that can perform mask filling.\n\n<div class=\"flex ju..."
          ],
          [
           "```\n\n```python out\n[\n  {'sequence': 'Le camembert est dÃ©licieux :)', 'score': 0.49091005325317383, '..."
          ],
          [
           "```\n\nAs you can see, loading a model within a pipeline is extremely simple. The only thing you need ..."
          ],
          [
           "```\n{:else}\n```py\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM\n\ntokenizer = C..."
          ],
          [
           "Understanding the Interface class[[understanding-the-interface-class]]\n\n<CourseFloatingBanner chapte..."
          ],
          [
           "For a complete list of components, [see the Gradio docs ](https://gradio.app/docs). Each pre-built c..."
          ],
          [
           "```py\nimport numpy as np\nimport gradio as gr\n\n\ndef reverse_audio(audio):\n    sr, data = audio\n    re..."
          ],
          [
           "```\n\nThe code above will produce an interface like the one below (if your browser doesn't\nask you fo..."
          ],
          [
           "The code snippet below shows how three input components line up with the three arguments of the `gen..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-generate-tone.hf.space\" frameBorder=\"0\" height=\"450\" title=\"G..."
          ],
          [
           "We'll cover the `share` parameter in a lot more detail in the next section!\n\n## âœï¸ Let's apply it![[..."
          ],
          [
           "```\n\nIf your browser doesn't ask you for microphone permissions, <a href=\"https://huggingface.co/spa..."
          ],
          [
           "ou are at the right place if you want to understand what the Byte pair Encoding subword tokenization..."
          ],
          [
           "corpus we used to train it. How is a BPE tokenizer trained? First of all, we have to get a corpus of..."
          ],
          [
           "of the following words: huggingface, hugging, hug, hugger, etc. BPE is an algorithm that starts with..."
          ],
          [
           "how to increase it. We return to our split corpus, we will go through the words one by one and count..."
          ],
          [
           "note our first merging rule and we add the new token to our vocabulary. We can then apply this mergi..."
          ],
          [
           "the vocabulary and then we merge all the pairs of tokens composed of the token \"le\" and \"a\" into our..."
          ],
          [
           "have learned our vocabulary and our merging rules, we can tokenize new texts. For example, if we wan..."
          ],
          [
           "that's it, I hope that now the BPE algorithm has no more secret for you!..."
          ],
          [
           "ome bugs in your code are very straightforward. You try running it, you get a syntax error somewhere..."
          ],
          [
           "added bonus problem that your models are often compiled before execution, which is great for perform..."
          ],
          [
           "we create our tokenizer and we tokenize the dataset. Next, we convert our datasets to TensorFlow dat..."
          ],
          [
           "- how do we even begin to debug something like that? When the error you get doesn't immediately sugg..."
          ],
          [
           "like so, by looping over the dataset for one iteration and then breaking. So what do we get when we ..."
          ],
          [
           "need to be passed in the input dictionary, where the model can see them. This internal loss is the l..."
          ],
          [
           "are, or we keep using Keras losses, but we move the labels to the place Keras expects them. For simp..."
          ],
          [
           "all the outputs nan , all the weights are nan too. Once a single nan creeps into your computations, ..."
          ],
          [
           "You can see this in more detail in the accompanying section of the course notes, but we find that if..."
          ],
          [
           "loss of nan because we got an \"impossible\" label. To fix that, we need to go back and set the model ..."
          ],
          [
           "at a fairly high value. What's going on? Well, when things are mostly working, but training is just ..."
          ],
          [
           "but in the process we invisibly got the default learning rate, which is 1e-3, or ten to the power of..."
          ],
          [
           "this to see this in more detail and to experiment with the code yourself. Good luck, and remember to..."
          ],
          [
           "How to write a good issue[[how-to-write-a-good-issue]]\n\n<CourseFloatingBanner chapter={8}\n  classNam..."
          ],
          [
           "<Tip>\n\nðŸš¨ Many issues in the ðŸ¤— Transformers repository are unsolved because the data used to reproduc..."
          ],
          [
           "```\ntransformers-cli env\n```\n\nand you should get something like this:\n\n```out\nCopy-and-paste the tex..."
          ],
          [
           "```\n```python\n```\n\nthen paste in your minimal reproducible example and type a new line with three ba..."
          ],
          [
           "oading a custom dataset. Although the Hugging Face Hub hosts over a thousand public datasets, you'll..."
          ],
          [
           "need to provide the name of the format to the load_dataset function, along with a data_files argumen..."
          ],
          [
           "data_files argument. The CSV loading script also allows you to pass several keyword arguments, so he..."
          ],
          [
           "filepath. Let's now take a look at loading raw text files. This format is quite common in NLP and yo..."
          ],
          [
           "text are also represented as a row in the dataset. For JSON files, there are two main formats to kno..."
          ],
          [
           "Summary[[summary]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 right-0 top-..."
          ],
          [
           "What if my dataset isn't on the Hub?[[what-if-my-dataset-isnt-on-the-hub]]\n\n<CourseFloatingBanner ch..."
          ],
          [
           "ðŸ¤— Datasets provides loading scripts to handle the loading of local and remote datasets. It supports ..."
          ],
          [
           "The training and test splits are hosted on GitHub, so we can download them with a simple `wget` comm..."
          ],
          [
           "```\n\nThis will download two compressed files called *SQuAD_it-train.json.gz* and *SQuAD_it-test.json..."
          ],
          [
           "```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n   ..."
          ],
          [
           "```\n\nThis is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up ..."
          ],
          [
           "```\n\nThis can be useful if you don't want to manually decompress many GZIP files. The automatic deco..."
          ],
          [
           "```\n\nThis returns the same `DatasetDict` object obtained above, but saves us the step of manually do..."
          ],
          [
           "rite your own training loop in PyTorch. In this video, we will look at how we can do the same fine-t..."
          ],
          [
           "it to the model. With the labels, we can then compute a loss. That number is not useful on its own, ..."
          ],
          [
           "on your favorite deep learning course. We will use the GLUE MRPC dataset here again, and we have see..."
          ],
          [
           "we try to grab a batch of data and inspect it. Like our dataset elements, it's a dictionary, but thi..."
          ],
          [
           "here two. Again, to be sure everything is going well, we pass the batch we grabbed to our model and ..."
          ],
          [
           "you like. Using the previous loss and computing the gradients with loss.backward(), we check that we..."
          ],
          [
           "the Transformers library is just a convenience function to easily build such a scheduler, you can ag..."
          ],
          [
           "for you! We can now put everything together! First we put our model in training mode (which will act..."
          ],
          [
           "in our scheduler for the next iteration and zero the gradients of the optimizer. Once this is finish..."
          ],
          [
           "send it those intermediate predictions. Once the evaluation loop is finished, we just have to call t..."
          ],
          [
           "atasets and DataFrames equals love. Although the processing functions of Datasets will cover most th..."
          ],
          [
           "an example, let's suppose we're analysing Supreme Court cases from Switzerland. As usual we download..."
          ],
          [
           "regions. Answering these questions with the native Arrow format isn't easy, but we can easily switch..."
          ],
          [
           "that the Datasets library changes the magic __getitem__() method of the dataset. The __getitem__() m..."
          ],
          [
           "one go. And once you have a DataFrame, you can find answers to all sorts of complex questions or mak..."
          ],
          [
           "n this video, we'll study the decoder architecture. An example of a popular decoder-only architectur..."
          ],
          [
           "use a small example, using three words. We pass them through the decoder. We retrieve a numerical re..."
          ],
          [
           "representation of the word in question. The dimension of that vector is defined by the architecture ..."
          ],
          [
           "the words on the left and right, I.e., the bidirectional context, decoders only have access to the w..."
          ],
          [
           "used in a wide variety of tasks. However, the strength of a decoder lies in the way a word has acces..."
          ],
          [
           "We use this as input for the decoder. The model outputs a vectors of dimension 768. This vector cont..."
          ],
          [
           "we are now at \"My name\". This is where the \"autoregressive\" aspect comes in. Auto-regressive models ..."
          ],
          [
           "a while; GPT-2, for example, has a maximum context size of 1024. We could eventually generate up to ..."
          ],
          [
           "It is based off of the masked self-attention layer, which allows to have word embeddings which have ..."
          ],
          [
           "\"sequence-to-sequence\" transformer (which can generally be used interchangeably). We recommend you c..."
          ],
          [
           "n our other videos, and as always, there'll be links below if you want to check those out, we showed..."
          ],
          [
           "can use the standard Keras predict() method, as shown here. You simply pass in tokenized text to thi..."
          ],
          [
           "into the modelâ€™s probability outputs, you just apply a softmax, like so. What if we want to turn tho..."
          ],
          [
           "skip the softmax step entirely, because the largest logit will always be the largest probability too..."
          ],
          [
           "data from the MRPC dataset, which is part of the GLUE benchmark. Each of the GLUE datasets, as well ..."
          ],
          [
           "compute those metrics to benchmark our model, we just pass them the modelâ€™s predictions, and the gro..."
          ],
          [
           "'metric' argument to compile(). As with things like loss and optimizer, you can specify the metrics ..."
          ],
          [
           "a bit beyond the scope of this course, I'll link to the relevant TF docs below because it can be ver..."
          ],
          [
           "Unigram tokenization[[unigram-tokenization]]\n\n<CourseFloatingBanner chapter={6}\n  classNames=\"absolu..."
          ],
          [
           "This is all a very costly operation, so we don't just remove the single symbol associated with the l..."
          ],
          [
           "```\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```\n\nand for this example, we will..."
          ],
          [
           "```\n\nSo, the sum of all frequencies is 210, and the probability of the subword `\"ug\"` is thus 20/210..."
          ],
          [
           "```\n\nSo, `\"pug\"` would be tokenized as `[\"p\", \"ug\"]` or `[\"pu\", \"g\"]`, depending on which of those s..."
          ],
          [
           "```\n\nThus `\"unhug\"` would be tokenized as `[\"un\", \"hug\"]`.\n\n<Tip>\n\nâœï¸ **Now your turn!** Determine t..."
          ],
          [
           "```\n\nNow we need to compute how removing each token affects the loss. This is rather tedious, so we'..."
          ],
          [
           "```\n\nLike for BPE and WordPiece, we begin by counting the number of occurrences of each word in the ..."
          ],
          [
           "```\n\nWe group the characters with the best subwords to arrive at an initial vocabulary of size 300:\n..."
          ],
          [
           "```\n\nNow the main function is the one that tokenizes words using the Viterbi algorithm. As we saw be..."
          ],
          [
           "Once the main loop is finished, we just start from the end and hop from one start position to the ne..."
          ],
          [
           "```\n\nWe can already try our initial model on some words:\n\n```python\nprint(encode_word(\"Hopefully\", m..."
          ],
          [
           "```\n\nWe can try it on a given token:\n\n```python\nscores = compute_scores(model)\nprint(scores[\"ll\"])\np..."
          ],
          [
           "```\n\nThen, to tokenize some text, we just need to apply the pre-tokenization and then use our `encod..."
          ],
          [
           "n this video, we're going to go over the HuggingFace Model Hub navigation. This is the huggingface.c..."
          ],
          [
           "tasks, such as question answering or text classification, but it isn't only limited to NLP. Other ta..."
          ],
          [
           "it isn't limited to it. The model Hub is used to host a lot of different frameworks' models, and we ..."
          ],
          [
           "which the model is shared. On the right, you'll find the models available on the model Hub! The mode..."
          ],
          [
           "crafted a model card is, the easier it will be for other users to leverage your model in their appli..."
          ],
          [
           "we have just seen. The \"Files & Versions tab\" displays the architecture of the repository of that mo..."
          ],
          [
           "shows how to load that model within the appropriate library. For BERT, this is transformers...."
          ],
          [
           "n this video, we're going to see how to load and fine-tune a pre-trained model. It's very quick, and..."
          ],
          [
           "To start, we pick which model we want to start with - in this case we're going to use the famous, th..."
          ],
          [
           "of interest. We load the language model with this one line of code here, using the \"from_pretrained\"..."
          ],
          [
           "probably seen this already, but if not, this is one of its core methods - you always need to \"compil..."
          ],
          [
           "for the wrong classes. Note that you can specify the loss function as a string, like we did with the..."
          ],
          [
           "In fact, if you remember absolutely nothing else from this video, remember to always check whether y..."
          ],
          [
           "rate, and to do that we'll need to import the actual optimizer rather than just calling it by string..."
          ],
          [
           "to break the data into batches and train on it.  So the first input is tokenized text - you will alm..."
          ],
          [
           "classes for our examples, and thatâ€™s it. If you're following along with the data from our datasets v..."
          ],
          [
           "everything works out, you should see a little training progress bar as your loss goes down. And whil..."
          ],
          [
           "get an even lower loss, and an even more accurate model. And what do we do with our model once it's ..."
          ],
          [
           "Part 2 Release Event[[part-2-release-event]]\n\nFor the release of part 2 of the course, we organized ..."
          ],
          [
           "<div class=\"flex justify-center\">\n<Youtube id=\"VzvG23gmcYU\"/>\n</div>\n\n<p align=\"center\">\n<img src=\"h..."
          ],
          [
           "Margaret Mitchell is a researcher working on Ethical AI, currently focused on the ins and outs of et..."
          ],
          [
           "Chen Qian is a software engineer from Keras team, with a focus on high-level modeling APIs. Chen got..."
          ],
          [
           "## Day 2: The tools to use[[day-2-the-tools-to-use]]\n\n**Lewis Tunstall:** *Simple Training with the ..."
          ],
          [
           "**Lucile Saulnier:** *Get your own tokenizer with ðŸ¤— Transformers & ðŸ¤— Tokenizers*\n\n<div class=\"flex j..."
          ],
          [
           "Abubakar Abid is the CEO of [Gradio](www.gradio.app). He received his Bachelor's of Science in Elect..."
          ],
          [
           "How do Transformers work?[[how-do-transformers-work]]\n\n<CourseFloatingBanner\n    chapter={1}\n    cla..."
          ],
          [
           "- **February 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsup..."
          ],
          [
           "We will dive into these families in more depth later on.\n\n## Transformers are language models[[trans..."
          ],
          [
           "Another example is *masked language modeling*, in which the model predicts a masked word in the sent..."
          ],
          [
           "Unfortunately, training a model, especially a large one, requires a large amount of data. This becom..."
          ],
          [
           "This is why sharing language models is paramount: sharing the trained weights and building on top of..."
          ],
          [
           "This pretraining is usually done on very large amounts of data. Therefore, it requires a very large ..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "To put this into context, consider the task of translating text from English to French. Given the in..."
          ],
          [
           "To speed things up during training (when the model has access to target sentences), the decoder is f..."
          ],
          [
           "As we dive into Transformer models in this course, you'll see mentions of *architectures* and *check..."
          ],
          [
           "n this video we'll take a look at how you upload your very own dataset to the Hub. The first you'll ..."
          ],
          [
           "specify whether it is a public or private dataset. Public datasets can be accessed by anyone, while ..."
          ],
          [
           "uploaded the files, you'll see them appear in the repository under the \"Files and versions\" tab. The..."
          ],
          [
           "First you need to create some metadata that will allow your dataset to be easily found by others on ..."
          ],
          [
           "name of your repository and a data_files argument for the files and you're good to go!..."
          ],
          [
           "n this video we take a look at the mysterious sounding metric called Perplexity. You might have enco..."
          ],
          [
           "the likelihood. We can calculate the likelihood as the product of each tokenâ€™s probability What this..."
          ],
          [
           "which also is  a classification task. Therefore, if we want to calculate the cross entropy of an exa..."
          ],
          [
           "ability to generate quality text and the same is true for perplexity. For this reason one usually al..."
          ],
          [
           "Decoder models[[decoder-models]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-1..."
          ],
          [
           "n this video, we'll study the encoder architecture. An example of a popular encoder-only architectur..."
          ],
          [
           "Let's dive in this representation. It contains one vector per word that was passed through the encod..."
          ],
          [
           "which we call the â€œcontextâ€.As in, it looks to the left context, the word on the left of the one we'..."
          ],
          [
           "positions (or different words) in a single sequence, in order to compute a representation of that se..."
          ],
          [
           "BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of r..."
          ],
          [
           "shine. First of all, Masked Language Modeling, or MLM.Â It's the task of predicting a hidden word in ..."
          ],
          [
           "little chance that BERT would have been able to identify \"name\" as the correct word. The encoder nee..."
          ],
          [
           "giving a sequence a rating from one to five stars if doing review analysis, to giving a positive or ..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Translation[[translation]]\n\n{#if fw === 'pt'}\n\n<CourseFloatingBanne..."
          ],
          [
           "- **Style transfer**: Creating a model that *translates* texts written in a certain style to another..."
          ],
          [
           "Once we're finished, we will have a model able to make predictions like this one:\n\n<iframe src=\"http..."
          ],
          [
           "As in the previous sections, you can find the actual model that we'll train and upload to the Hub us..."
          ],
          [
           "```\n\nIf you want to work with a different pair of languages, you can specify them by their codes. A ..."
          ],
          [
           "```\n\nNow let's take a look at one element of the dataset:\n\n```py\nsplit_datasets[\"train\"][1][\"transla..."
          ],
          [
           "```\n\nOur pretrained model, however, sticks with the compact and familiar English word:\n\n```py\ntransl..."
          ],
          [
           "```\n\nYou can also replace the `model_checkpoint` with any other model you prefer from the [Hub](http..."
          ],
          [
           "```\n\nAs we can see, the output contains the input IDs associated with the English sentence, while th..."
          ],
          [
           "```\n\nNote that we set the same maximum length for our inputs and outputs. Since the texts we're deal..."
          ],
          [
           "```\n\nNow that the data has been preprocessed, we are ready to fine-tune our pretrained model!\n\n{#if ..."
          ],
          [
           "```\n\n<Tip warning={false}>\n\nðŸ’¡ The `Helsinki-NLP/opus-mt-en-fr` checkpoint only has PyTorch weights, ..."
          ],
          [
           "{#if fw === 'pt'}\n\n```py\nfrom transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollat..."
          ],
          [
           "```\n\n{:else}\n\n```py\nfrom transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorFor..."
          ],
          [
           "```\n\nHere are the labels for the first and second elements in our dataset:\n\n```py\nfor i in range(1, ..."
          ],
          [
           "```\n\n{/if}\n\n\n### Metrics[[metrics]]\n\n<Youtube id=\"M05L1DhFqcw\"/>\n\n{#if fw === 'pt'}\n\nThe feature tha..."
          ],
          [
           "One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult..."
          ],
          [
           "```\n\nWe can then load it via `evaluate.load()` like we did in [Chapter 3](/course/chapter3):\n\n```py\n..."
          ],
          [
           "```\n\nThis gets a BLEU score of 46.75, which is rather good -- for reference, the original Transforme..."
          ],
          [
           "```\n\n```python out\n{'score': 0.0,\n 'counts': [2, 1, 0, 0],\n 'totals': [2, 1, 0, 0],\n 'precisions': [..."
          ],
          [
           "```\n\nThe score can go from 0 to 100, and higher is better.\n\n{#if fw === 'tf'}\n\nTo get from the model..."
          ],
          [
           "@tf.function(jit_compile=True)\ndef generate_with_xla(batch):\n    return model.generate(\n        inpu..."
          ],
          [
           "```\n\n{:else}\n\nTo get from the model outputs to texts the metric can use, we will use the `tokenizer...."
          ],
          [
           "```\n\n{#if fw === 'tf'}\n\nBefore we start, let's see what kind of results we get from our model withou..."
          ],
          [
           "```\n\nNext, we define a `PushToHubCallback` to upload our model to the Hub during training, as we saw..."
          ],
          [
           "```\n\n```\n{'bleu': 57.334066271545865}\n```\n\nAt this stage, you can use the inference widget on the Mo..."
          ],
          [
           "```\n\nApart from the usual hyperparameters (like learning rate, number of epochs, batch size, and som..."
          ],
          [
           "```\n\nBefore training, we'll first look at the score our model gets, to double-check that we're not m..."
          ],
          [
           "```\n\nThat's a nearly 14-point improvement, which is great.\n\nFinally, we use the `push_to_hub()` meth..."
          ],
          [
           "```\n\nAt this stage, you can use the inference widget on the Model Hub to test your model and share i..."
          ],
          [
           "```\n\nThen we will need an optimizer:\n\n```py\nfrom transformers import AdamW\n\noptimizer = AdamW(model...."
          ],
          [
           "```\n\nLastly, to push our model to the Hub, we will need to create a `Repository` object in a working..."
          ],
          [
           "```\n\nWe can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. ..."
          ],
          [
           "```\n\nThe training loop looks a lot like the ones in [section 2](/course/chapter7/2) and [Chapter 3](..."
          ],
          [
           "# Necessary to pad predictions and labels for being gathered\n        generated_tokens = accelerator...."
          ],
          [
           "```\n\n```python out\nepoch 0, BLEU score: 53.47\nepoch 1, BLEU score: 54.24\nepoch 2, BLEU score: 54.44\n..."
          ],
          [
           "```\n\nAnother great example of domain adaptation!\n\n<Tip>\n\nâœï¸ **Your turn!** What does the model retur..."
          ],
          [
           "Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={6}\n    classNames=\"absolute z-10 ri..."
          ],
          [
           "Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 ri..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Question answering[[question-answering]]\n\n{#if fw === 'pt'}\n\n<Cours..."
          ],
          [
           "<Youtube id=\"ajPx5LwJD-I\"/>\n\nWe will fine-tune a BERT model on the [SQuAD dataset](https://rajpurkar..."
          ],
          [
           "This is actually showcasing the model that was trained and uploaded to the Hub using the code shown ..."
          ],
          [
           "</Tip>\n\n## Preparing the data[[preparing-the-data]]\n\nThe dataset that is used the most as an academi..."
          ],
          [
           "```\n\nWe can then have a look at this object to learn more about the SQuAD dataset:\n\n```py\nraw_datase..."
          ],
          [
           "```\n\nThe `context` and `question` fields are very straightforward to use. The `answers` field is a b..."
          ],
          [
           "```\n\nWe won't dive into the evaluation script as it will all be wrapped up by a ðŸ¤— Datasets metric fo..."
          ],
          [
           "```\n\nAs mentioned previously, we'll be fine-tuning a BERT model, but you can use any other model typ..."
          ],
          [
           "```\n\nThe labels will then be the index of the tokens starting and ending the answer, and the model w..."
          ],
          [
           "```\n\n```python out\n'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [S..."
          ],
          [
           "```\n\nAs we can see, our example has been in split into four inputs, each of them containing the ques..."
          ],
          [
           "```\n\nAs we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as ..."
          ],
          [
           "```\n\nAs we can see, the first three examples (at indices 2, 3, and 4 in the training set) each gave ..."
          ],
          [
           "```py\nanswers = raw_datasets[\"train\"][2:6][\"answers\"]\nstart_positions = []\nend_positions = []\n\nfor i..."
          ],
          [
           "```\n\n```python out\n([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n [85, 53, 21,..."
          ],
          [
           "```\n\n```python out\n'Theoretical answer: a Marian place of prayer and reflection, decoded example: [C..."
          ],
          [
           "```\n\nIndeed, we don't see the answer inside the context.\n\n<Tip>\n\nâœï¸ **Your turn!** When using the XL..."
          ],
          [
           "# Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n      ..."
          ],
          [
           "```\n\nNote that we defined two constants to determine the maximum length used as well as the length o..."
          ],
          [
           "```\n\nAs we can see, the preprocessing added roughly 1,000 features. Our training set is now ready to..."
          ],
          [
           "sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"o..."
          ],
          [
           "```\n\nWe can apply this function on the whole validation dataset like before:\n\n```py\nvalidation_datas..."
          ],
          [
           "```\n\nIn this case we've only added a couple of hundred samples, so it appears the contexts in the va..."
          ],
          [
           "{:else}\n\n<Youtube id=\"VN67ZpN33Ss\"/>\n\n{/if}\n\nThe model will output logits for the start and end posi..."
          ],
          [
           "```python\nsmall_eval_set = raw_datasets[\"validation\"].select(range(100))\ntrained_checkpoint = \"disti..."
          ],
          [
           "```\n\nNow that the preprocessing is done, we change the tokenizer back to the one we originally picke..."
          ],
          [
           "```\n\n{:else}\n\n```python\nimport tensorflow as tf\nfrom transformers import TFAutoModelForQuestionAnswe..."
          ],
          [
           "```\n\nWith this in hand, we can really get to work by looping through all the examples and, for each ..."
          ],
          [
           "answers.append(\n                    {\n                        \"text\": context[offsets[start_index][0..."
          ],
          [
           "```\n\nThe final format of the predicted answers is the one that will be expected by the metric we wil..."
          ],
          [
           "```\n\nAgain, that's rather good considering that according to [its paper](https://arxiv.org/abs/1910...."
          ],
          [
           "predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n       ..."
          ],
          [
           "```\n\nWe can check it works on our predictions:\n\n```python\ncompute_metrics(start_logits, end_logits, ..."
          ],
          [
           "```\n\nIf you aren't working in a notebook, just type the following line in your terminal:\n\n```bash\nhu..."
          ],
          [
           "```\n\nWe've seen most of these before: we set some hyperparameters (like the learning rate, the numbe..."
          ],
          [
           "```\n\nNext, we set up our training hyperparameters and compile our model:\n\n```python\nfrom transformer..."
          ],
          [
           "```\n\nFinally, we're ready to train with `model.fit()`. We use a `PushToHubCallback` to upload the mo..."
          ],
          [
           "```\n\n{/if}\n\nNote that while the training happens, each time the model is saved (here, every epoch) i..."
          ],
          [
           "```\n\n{/if}\n\n```python out\n{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}\n```\n\nGreat! As..."
          ],
          [
           "```\n\nThe `Trainer` also drafts a model card with all the evaluation results and uploads it.\n\n{/if}\n\n..."
          ],
          [
           "```\n\nNext we reinstantiate our model, to make sure we're not continuing the fine-tuning from before ..."
          ],
          [
           "```\n\nTo push our model to the Hub, we will need to create a `Repository` object in a working folder...."
          ],
          [
           "```\n\nWe can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. ..."
          ],
          [
           "optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.upda..."
          ],
          [
           "```\n\nIn case this is the first time you're seeing a model saved with ðŸ¤— Accelerate, let's take a mome..."
          ],
          [
           "```\n\nThe first line is self-explanatory: it tells all the processes to wait until everyone is at tha..."
          ],
          [
           "```\n\n```python out\n{'score': 0.9979003071784973,\n 'start': 78,\n 'end': 105,\n 'answer': 'Jax, PyTorch..."
          ],
          [
           "ow to write a good issue on GitHub? GitHub is the main place for the Hugging Face open source librar..."
          ],
          [
           "it by executing this command in a notebook (remove the exclamation mark to execute it in a terminal)..."
          ],
          [
           "it calls AutoTokenizer.from_pretrained. Using the debugger, we find the values passed to that method..."
          ],
          [
           "not just this one, and that it disappears when we use use_fast=False inside our tokenizer call. The ..."
          ],
          [
           "is to properly name your issue. Don't pick a title that is too vague! Then you have to fill your env..."
          ],
          [
           "with tokenizers, we pick the maintainer associated with them. There is no point tagging more than 3 ..."
          ],
          [
           "issue at hand. With all of this, you should expect an answer to your issue pretty fast, and hopefull..."
          ],
          [
           "The Hugging Face Hub[[the-hugging-face-hub]]\n\n<CourseFloatingBanner\n    chapter={4}\n    classNames=\"..."
          ],
          [
           "The video below shows how to navigate the Hub.\n\n<Youtube id=\"XvSGPZFEjDY\"/>\n\nHaving a huggingface.co..."
          ],
          [
           "n this video, we will study together \"the Unigram Language Model subword tokenization algorithm\".\n\nT..."
          ],
          [
           "Before going further in the explanation of the training algorithm, I need to explain what is an Unig..."
          ],
          [
           "previous word. This \"assumption\" allows us to write that the probability of a text is equal to the p..."
          ],
          [
           "We are now ready to return to our explanation of the training algorithm. Let's say that we have as a..."
          ],
          [
           "possible strict substrings that's what we'll do here. We could also have used the BPE algorithm with..."
          ],
          [
           "characters to be able to tokenize any word. Let's go for it! The probability of a token is simply es..."
          ],
          [
           "of our text \"Hug\" will be the one with the highest probability of occurrence according to our Unigra..."
          ],
          [
           "need to tokenize as we just did all the remaining words in the corpus. The loss is then the sum over..."
          ],
          [
           "for example the token 'ug'. We notice that the tokenization for \"hug\" with the letter h and the tupl..."
          ],
          [
           "continue the calculation, we would notice that we could remove any token without it impacting the lo..."
          ],
          [
           "end, we obtain by removing the token composed of the letters \"h\" and \"u\" from the vocabulary a loss ..."
          ],
          [
           "could remove p % of the tokens by iteration. The second token that could be removed at this iteratio..."
          ],
          [
           "And that's it! I hope that this example has allowed you to better understand the Unigram tokenizatio..."
          ],
          [
           "Creating your own dataset[[creating-your-own-dataset]]\n\n<CourseFloatingBanner chapter={5}\n  classNam..."
          ],
          [
           "## Getting the data[[getting-the-data]]\n\nYou can find all the issues in ðŸ¤— Datasets by navigating to ..."
          ],
          [
           "```\n\nOnce the library is installed, you can make GET requests to the `Issues` endpoint by invoking t..."
          ],
          [
           "```python out\n[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n  'repositor..."
          ],
          [
           "'html_url': 'https://github.com/bhavitvyamalik',\n   'followers_url': 'https://api.github.com/users/b..."
          ],
          [
           "'locked': False,\n  'assignee': None,\n  'assignees': [],\n  'milestone': None,\n  'comments': 1,\n  'cre..."
          ],
          [
           "```\n\nWhoa, that's a lot of information! We can see useful fields like `title`, `body`, and `number` ..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nâš ï¸ Do not share a notebook with your `GITHUB_TOKEN` pasted in it. We reco..."
          ],
          [
           "if len(batch) > rate_limit and len(all_issues) < num_issues:\n            all_issues.extend(batch)\n  ..."
          ],
          [
           "```\n\nNow when we call `fetch_issues()` it will download all the issues in batches to avoid exceeding..."
          ],
          [
           "```\n\nGreat, we've created our first dataset from scratch! But why are there several thousand issues ..."
          ],
          [
           "```\n\n```python out\n>> URL: https://github.com/huggingface/datasets/pull/850\n>> Pull request: {'url':..."
          ],
          [
           "```\n\n<Tip>\n\nâœï¸ **Try it out!** Calculate the average time it takes to close issues in ðŸ¤— Datasets. Yo..."
          ],
          [
           "```py\nissue_number = 2792\nurl = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_nu..."
          ],
          [
           "```python out\n[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128'..."
          ],
          [
           "'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n   'starred_url': 'https..."
          ],
          [
           "'updated_at': '2021-08-12T12:31:17Z',\n  'author_association': 'CONTRIBUTOR',\n  'body': \"@albertvilla..."
          ],
          [
           "```\n\nWe can see that the comment is stored in the `body` field, so let's write a simple function tha..."
          ],
          [
           "```\n\nThis looks good, so let's use `Dataset.map()` to add a new `comments` column to each issue in o..."
          ],
          [
           "```\n\n```python out\nDataset({\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '..."
          ],
          [
           "```\n\nCool, we've pushed our dataset to the Hub and it's available for others to use! There's just on..."
          ],
          [
           "2. Read the [ðŸ¤— Datasets guide](https://github.com/huggingface/datasets/blob/master/templates/README_..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n<!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-..."
          ],
          [
           "### 3. Which of the following is an example of subword tokenization?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\tt..."
          ],
          [
           "{#if fw === 'pt'}\n### 5. What is an AutoModel?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"A model that aut..."
          ],
          [
           "{/if}\n\n### 6. What are the techniques to be aware of when batching sequences of different lengths to..."
          ],
          [
           "### 8. What method is most of the tokenizer API centered around?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext:..."
          ],
          [
           "```\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"A list of strings, each string being a token\",\n\t\t\texplain: ..."
          ],
          [
           "```\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"No, it seems correct.\",\n\t\t\texplain: \"Unfortunately, couplin..."
          ],
          [
           "```\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"No, it seems correct.\",\n\t\t\texplain: \"Unfortunately, couplin..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Processing the data[[processing-the-data]]\n\n{#if fw === 'pt'}\n\n<Cou..."
          ],
          [
           "```python\nimport torch\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassifica..."
          ],
          [
           "```\n{:else}\nContinuing with the example from the [previous chapter](/course/chapter2), here is how w..."
          ],
          [
           "```\n{/if}\n\nOf course, just training the model on two sentences is not going to yield very good resul..."
          ],
          [
           "```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', '..."
          ],
          [
           "```\n\nWe can see the labels are already integers, so we won't have to do any preprocessing there. To ..."
          ],
          [
           "```\n\nBehind the scenes, `label` is of type `ClassLabel`, and the mapping of integers to label name i..."
          ],
          [
           "```\n\n```python out\n{ \n  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996..."
          ],
          [
           "```\n\nSo we see the model expects the inputs to be of the form `[CLS] sentence1 [SEP] sentence2 [SEP]..."
          ],
          [
           "```\n\nAs you can see, the parts of the input corresponding to `[CLS] sentence1 [SEP]` all have a toke..."
          ],
          [
           "```\n\nThis works well, but it has the disadvantage of returning a dictionary (with our keys, `input_i..."
          ],
          [
           "```\n\nThis function takes a dictionary (like the items of our dataset) and returns a new dictionary w..."
          ],
          [
           "```\n\nThe way the ðŸ¤— Datasets library applies this processing is by adding new fields to the datasets,..."
          ],
          [
           "```\n\nYou can even use multiprocessing when applying your preprocessing function with `map()` by pass..."
          ],
          [
           "{:else}\n\nThe function that is responsible for putting together samples inside a batch is called a *c..."
          ],
          [
           "```\n{:else}\n```py\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWith..."
          ],
          [
           "```\n\n{:else}\n\n```python out\n{'attention_mask': torch.Size([8, 67]),\n 'input_ids': torch.Size([8, 67]..."
          ],
          [
           "```\n\nLooking good! Now that we've gone from raw text to batches our model can deal with, we're ready..."
          ],
          [
           "```\n\nAnd that's it! We can take those datasets forward into the next lecture, where training will be..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Models[[models]]\n\n{#if fw === 'pt'}\n\n<CourseFloatingBanner chapter=..."
          ],
          [
           "The `AutoModel` class and all of its relatives are actually simple wrappers over the wide variety of..."
          ],
          [
           "```\n{:else}\n```py\nfrom transformers import BertConfig, TFBertModel\n\n# Building the config\nconfig = B..."
          ],
          [
           "```\n{/if}\n\nThe model can be used in this state, but it will output gibberish; it needs to be trained..."
          ],
          [
           "```\n\nAs you saw earlier, we could replace `TFBertModel` with the equivalent `TFAutoModel` class. We'..."
          ],
          [
           "```\n\nThis saves two files to your disk:\n\n{#if fw === 'pt'}\n```\nls directory_on_my_computer\n\nconfig.j..."
          ],
          [
           "```\n\nThe tokenizer converts these to vocabulary indices which are typically called *input IDs*. Each..."
          ],
          [
           "ow to instantiate a Transformers model? In this video we will look at how we can create and use a mo..."
          ],
          [
           "similarly for GPT-2 or BART. Behind the scenes, this API can take the name of a checkpoint on the Hu..."
          ],
          [
           "type of the model (BERT, GPT-2 or BART for instance). Once it has the proper configuration class, it..."
          ],
          [
           "To easily load the configuration of a model from any checkpoint or a folder containing the configura..."
          ],
          [
           "instance the BERT model associated with the bert-base-cased checkpoint has 12 layers, a hidden size ..."
          ],
          [
           "Saving a model once it's trained or fine-tuned is very easy: we just have to use the save_pretrained..."
          ],
          [
           "Introduction to Gradio[[introduction-to-gradio]]\n\n<CourseFloatingBanner\n    chapter={9}\n    classNam..."
          ],
          [
           "* An extractive **question answering** model that takes in a context paragraph and a quest and outpu..."
          ],
          [
           "This chapter is broken down into sections which include both _concepts_ and _applications_. After yo..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Handling multiple sequences[[handling-multiple-sequences]]\n\n{#if fw..."
          ],
          [
           "In the previous section, we explored the simplest of use cases: doing inference on a single sequence..."
          ],
          [
           "```\n\n```python out\nIndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1..."
          ],
          [
           "```\n{:else}\n```py\ntokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\nprint(tokenized_inputs..."
          ],
          [
           "```\n{:else}\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequ..."
          ],
          [
           "```\nbatched_ids = [ids, ids]\n```\n\nThis is a batch of two identical sequences!\n\n<Tip>\n\nâœï¸ **Try it ou..."
          ],
          [
           "```\n\nThe padding token ID can be found in `tokenizer.pad_token_id`. Let's use it and send our two se..."
          ],
          [
           "```\n\n```py out\ntf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)\ntf.Tensor([[ 0.5803..."
          ],
          [
           "```\n\n```python out\ntensor([[ 1.5694, -1.3895],\n        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)..."
          ],
          [
           "```\n{/if}\n\nNow we get the same logits for the second sentence in the batch.\n\nNotice how the last val..."
          ],
          [
           "Advanced Interface features[[advanced-interface-features]]\n\n<CourseFloatingBanner chapter={9}\n  clas..."
          ],
          [
           "See the chatbot example below:\n\n```py\nimport random\n\nimport gradio as gr\n\n\ndef chat(message, history..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-Chatbot-Demo.hf.space\" frameBorder=\"0\" height=\"350\" title=\"Gr..."
          ],
          [
           "# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabel..."
          ],
          [
           "```\n\nTest the interpretation function by submitting an input then clicking Interpret under the outpu..."
          ],
          [
           "n this video we will see together what is the purpose of training a tokenizer, what are the key step..."
          ],
          [
           "legal, or uses a different style, a language from another century for instance. For example, if I ta..."
          ],
          [
           "that either a word is divided into many sub tokens or that the tokenizer does not know one of the un..."
          ],
          [
           "part of the text. In this other example, we can see that the tokenizer replaces words containing cha..."
          ],
          [
           "language model. This training consists in learning rules to divide the text into tokens and the way ..."
          ],
          [
           "completely design your tokenizer but it requires more experience and attention. Once the architectur..."
          ],
          [
           "tokenizer trained on it - to convince you of this we will see at the end the difference produced on ..."
          ],
          [
           "open source libraries on Github. It's good timing, this dataset is known by the datasets library and..."
          ],
          [
           "it on our new corpus. An argument that is common to most of the tokenization algorithms used at the ..."
          ],
          [
           "we see that all spaces are isolated and the method name \"randn\" relatively common in python code is ..."
          ],
          [
           "he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune ..."
          ],
          [
           "tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively ..."
          ],
          [
           "the model signature. The last steps before creating the Trainer are to define our model and some tra..."
          ],
          [
           "display a progress bar and after a few minutes (if you are running on a GPU) you should have the tra..."
          ],
          [
           "contains the model predictions), label_ids (which contains the labels if your dataset had them) and ..."
          ],
          [
           "it can be loaded as easily as our dataset with the load_metric function, and it returns the evaluati..."
          ],
          [
           "our TrainingArguments, we tell the Trainer to evaluate at the end of every epoch. Launching a traini..."
          ],
          [
           "he tokenizer pipeline. In this video, we'll look at how a tokenizer converts raw text to numbers tha..."
          ],
          [
           "happen in this order, but viewing it like this is better for understanding what happens. The first s..."
          ],
          [
           "algorithms\" videos linked below for more information! The ## prefix we see in front of ize is the co..."
          ],
          [
           "the vocabulary of the tokenizer.  This is why we need to download a file when we instantiate a token..."
          ],
          [
           "had a number at the beginning and at the end that are missing, those are the special tokens. The spe..."
          ],
          [
           "depending on which tokenizer you are using. The BERT tokenizer uses [CLS] and [SEP] but the roberta ..."
          ],
          [
           "Sharing demos with others[[sharing-demos-with-others]]\n\n<CourseFloatingBanner chapter={9}\n  classNam..."
          ],
          [
           "To add additional content to your demo, the `Interface` class supports some optional parameters:\n   ..."
          ],
          [
           "article = \"Check out [the original Rick and Morty Bot](https://huggingface.co/spaces/kingabzpro/Rick..."
          ],
          [
           "```\n\nUsing the options above, we end up with a more complete interface. Try the interface below:\n\n<i..."
          ],
          [
           "```\n\nThis generates a public, shareable link that you can send to anybody! When you send this link, ..."
          ],
          [
           "<Youtube id=\"LS9Y2wDVI0k\" />\n\n## âœï¸ Let's apply it![[lets-apply-it]]\n\nUsing what we just learned in ..."
          ],
          [
           "LABELS = Path(\"class_names.txt\").read_text().splitlines()\n\nmodel = nn.Sequential(\n    nn.Conv2d(1, 3..."
          ],
          [
           "```\n\nNow that we have a `predict()` function. The next step is to define and launch our gradio inter..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Fast tokenizers in the QA pipeline[[fast-tokenizers-in-the-qa-pipel..."
          ],
          [
           "{#if fw === 'pt'}\n\n<Youtube id=\"_wxyB3j3mk4\"/>\n\n{:else}\n\n<Youtube id=\"b3u8RzBCX9Y\"/>\n\n{/if}\n\n## Usin..."
          ],
          [
           "```\n\n```python out\n{'score': 0.97773,\n 'start': 78,\n 'end': 105,\n 'answer': 'Jax, PyTorch and Tensor..."
          ],
          [
           "```\n\nUnlike the other pipelines, which can't truncate and split texts that are longer than the maxim..."
          ],
          [
           "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and Tensor..."
          ],
          [
           "```\n\n```python out\n{'score': 0.97149,\n 'start': 1892,\n 'end': 1919,\n 'answer': 'Jax, PyTorch and Ten..."
          ],
          [
           "```\n\n{/if}\n\nNote that we tokenize the question and the context as a pair, with the question first.\n\n..."
          ],
          [
           "```\n\n{:else}\n\n```python out\n(1, 66) (1, 66)\n```\n\n{/if}\n\nTo convert those logits into probabilities, ..."
          ],
          [
           "```\n\n{/if}\n\nNow that we have properly masked the logits corresponding to positions we don't want to ..."
          ],
          [
           "```\n\n{/if}\n\nAt this stage, we could take the argmax of the start and end probabilities -- but we mig..."
          ],
          [
           "```\n\n{:else}\n\nThen we'll mask the values where `start_index > end_index` by setting them to `0` (the..."
          ],
          [
           "```\n\nNow we just have to format everything to get our result:\n\n```py\nresult = {\n    \"answer\": answer..."
          ],
          [
           "```\n\n```python out\n\"\"\"\n[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Tran..."
          ],
          [
           "```\n\nThis means the model will have a hard time picking the correct answer. To fix this, the `questi..."
          ],
          [
           "```\n\n```python out\ndict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n```\n\nAs ..."
          ],
          [
           "```\n\ngets us:\n\n```python out\n[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n```\n\nwhich means the first sentence i..."
          ],
          [
           "```\n\n```python out\ntorch.Size([2, 384])\n```\n\n{:else}\n\n```py\n_ = inputs.pop(\"overflow_to_sample_mappi..."
          ],
          [
           "```\n\n{:else}\n\n```py\nsequence_ids = inputs.sequence_ids()\n# Mask everything apart from the tokens of ..."
          ],
          [
           "```\n\n{/if}\n\nThe next step is similar to what we did for the small context, but we repeat it for each..."
          ],
          [
           "```\n\n{/if}\n\n```python out\n[(0, 18, 0.33867), (173, 184, 0.97149)]\n```\n\nThose two candidates correspo..."
          ],
          [
           "```\n\nIf we ignore the first result, we get the same result as our pipeline for this long context -- ..."
          ],
          [
           "he tokenization pipeline involves several steps that convert raw text into numbers. In this video, w..."
          ],
          [
           "- but the apostrophe is not a division criterion for example. We also notice that spaces have been r..."
          ],
          [
           "examples, we could observe the two main types of operations brought by the pre-tokenization: some ch..."
          ],
          [
           "method. This operation defines the largest tokens that can be produced by the tokenization or in oth..."
          ],
          [
           "et's see how to preprocess a dataset for summarization. This is the task of well summarizing a long ..."
          ],
          [
           "For once, our labels are not integers corresponding to some classes, but plain text. We will thus ne..."
          ],
          [
           "usually much shorter than the documents, you should definitely pick different maximum lengths for th..."
          ],
          [
           "and same for the targets. We pad the inputs with the pad token and the targets with the -100 index, ..."
          ],
          [
           "he post-processing step in a question answering task. When doing question answering, the processing ..."
          ],
          [
           "THE answer for a given example. For the processing step, you should refer to the video linked below...."
          ],
          [
           "back feature to the examples that they originated from. If you don't want to compute the validation ..."
          ],
          [
           "you want to evaluate. With the to_tf_dataset method, we can just sent our processed dataset to model..."
          ],
          [
           "and end logits and be done, but if our model predicts something impossible, like tokens in the quest..."
          ],
          [
           "answers or answer that are too long. As we saw in the preprocessing, the labels (0, 0) correspond to..."
          ],
          [
           "logit score in all the features the example generated. Now you know how to get answers from your mod..."
          ],
          [
           "A full training[[a-full-training]]\n\n<CourseFloatingBanner chapter={3}\n  classNames=\"absolute z-10 ri..."
          ],
          [
           "```\n\n### Prepare for training[[prepare-for-training]]\n\nBefore actually writing our training loop, we..."
          ],
          [
           "```\n\nTo quickly check there is no mistake in the data processing, we can inspect a batch like this:\n..."
          ],
          [
           "```\n\n```python out\ntensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])\n```\n\nAll ðŸ¤— Transform..."
          ],
          [
           "```\n\n```python out\n1377\n```\n\n### The training loop[[the-training-loop]]\n\nOne last thing: we will wan..."
          ],
          [
           "```\n\nYou can see that the core of the training loop looks a lot like the one in the introduction. We..."
          ],
          [
           "```\n\nAgain, your results will be slightly different because of the randomness in the model head init..."
          ],
          [
           "```\n\nAnd here are the changes:\n\n```diff\n+ from accelerate import Accelerator\n  from transformers imp..."
          ],
          [
           "```\n\nThe first line to add is the import line. The second line instantiates an `Accelerator` object ..."
          ],
          [
           "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n    train_dataloader, eval_dataloader, mo..."
          ],
          [
           "```\n\nPutting this in a `train.py` script will make that script runnable on any kind of distributed s..."
          ],
          [
           "Transformers, what can they do?[[transformers-what-can-they-do]]\n\n<CourseFloatingBanner chapter={1}\n..."
          ],
          [
           "The [ðŸ¤— Transformers library](https://github.com/huggingface/transformers) provides the functionality..."
          ],
          [
           "```\n\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437}]\n```\n\nWe can even pass severa..."
          ],
          [
           "```\n\nBy default, this pipeline selects a particular pretrained model that has been fine-tuned for se..."
          ],
          [
           "```\n\n```python out\n{'sequence': 'This is a course about the Transformers library',\n 'labels': ['educ..."
          ],
          [
           "```\n\nYou can control how many different sequences are generated with the argument `num_return_sequen..."
          ],
          [
           "```\n\nYou can refine your search for a model by clicking on the language tags, and pick a model that ..."
          ],
          [
           "```\n\nThe `top_k` argument controls how many possibilities you want to be displayed. Note that here t..."
          ],
          [
           "```\n\nHere the model correctly identified that Sylvain is a person (PER), Hugging Face an organizatio..."
          ],
          [
           "```\n\n```python out\n{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n`..."
          ],
          [
           "```\n\nLike with text generation, you can specify a `max_length` or a `min_length` for the result.\n\n\n#..."
          ],
          [
           "sing the Python debugger in a notebook. In this video, we'll learn how to use the Python debugger in..."
          ],
          [
           "then we get the following error. We use PyTorch here but you will get the same error with TensorFlow..."
          ],
          [
           "by typing %debug in any cell. When executing that cell, you go to the very bottom of the traceback w..."
          ],
          [
           "can see the value of return_tensors or batch_outputs to try to understand what triggered the error. ..."
          ],
          [
           "with them! This is because the pad method only takes care of the tokenizer outptus: input IDs, atten..."
          ],
          [
           "ow to instantiate a Transformers model? In this video we will look at how we can create and use a mo..."
          ],
          [
           "and similarly for GPT-2 or BART. Behind the scenes, this API can take the name of a checkpoint on th..."
          ],
          [
           "the type of the model (BERT, GPT-2 or BART for instance). Once it has the proper configuration class..."
          ],
          [
           "model. To easily load the configuration of a model from any checkpoint or a folder containing the co..."
          ],
          [
           "model architecture. For instance the BERT model associated with the bert-base-cased checkpoint has 1..."
          ],
          [
           "layers instead of 12. Saving a model once it's trained or fine-tuned is very easy: we just have to u..."
          ],
          [
           "n this video, we're going to understand how to manage a model repository on the HuggingFace model hu..."
          ],
          [
           "The model name is the model identifier that will then be used to identify your model on your chosen ..."
          ],
          [
           "users won't know it exists and will not be able to use it. Let's create a dummy model to play with. ..."
          ],
          [
           "you're unaware of what is a git repository, you can think of it as a folder containing files. If you..."
          ],
          [
           "first start by adding files to the repository. Files can be added through the web interface thanks t..."
          ],
          [
           "model files. First, I make sure that both git and git-lfs are correctly installed on my system. Link..."
          ],
          [
           "used for sentiment analysis. I'll simply copy over the contents to this folder. This includes the mo..."
          ],
          [
           "seen two ways of adding files to a repository, a third way is explored in the video about the push t..."
          ],
          [
           "model, ensuring reusability by fellow community members and reproducibility of results, and providin..."
          ],
          [
           "be used in downstream libraries simply by specifying your model identifier...."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Behind the pipeline[[behind-the-pipeline]]\n\n{#if fw === 'pt'}\n\n<Cou..."
          ],
          [
           "Let's start with a complete example, taking a look at what happened behind the scenes when we execut..."
          ],
          [
           "```\n\nand obtained:\n\n```python out\n[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n {'label': 'N..."
          ],
          [
           "```\n\nAs we saw in [Chapter 1](/course/chapter1), this pipeline groups together three steps: preproce..."
          ],
          [
           "- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*\n-..."
          ],
          [
           "```\n\nOnce we have the tokenizer, we can directly pass our sentences to it and we'll get back a dicti..."
          ],
          [
           "```\n{/if}\n\nDon't worry about padding and truncation just yet; we'll explain those later. The main th..."
          ],
          [
           "```\n{:else}\n\nHere's what the results look like as TensorFlow tensors:\n\n```python out\n{\n    'input_id..."
          ],
          [
           "```\n{:else}\nWe can download our pretrained model the same way we did with our tokenizer. ðŸ¤— Transform..."
          ],
          [
           "```\n\n```python out\ntorch.Size([2, 16, 768])\n```\n{:else}\n```py\noutputs = model(inputs)\nprint(outputs...."
          ],
          [
           "```\n{/if}\n\nNote that the outputs of ðŸ¤— Transformers models behave like `namedtuple`s or dictionaries...."
          ],
          [
           "There are many different architectures available in ðŸ¤— Transformers, with each one designed around ta..."
          ],
          [
           "```\n{:else}\nFor our example, we will need a model with a sequence classification head (to be able to..."
          ],
          [
           "```\n{/if}\n\nOur model predicted `[-1.5607, 1.6123]` for the first sentence and `[ 4.1692, -3.3464]` f..."
          ],
          [
           "```\n{/if}\n\nNow we can see that the model predicted `[0.0402, 0.9598]` for the first sentence and `[0..."
          ],
          [
           "ðŸ¤— Datasets, check![[datasets-check]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute..."
          ],
          [
           "n this video we will see together what is the normalizer component that we find at the beginning of ..."
          ],
          [
           "FNet model has transformed the letters with font variants or circled into their basic version and ha..."
          ],
          [
           "it is very easy to observe the normalization chosen for the currently loaded tokenizer. Indeed, each..."
          ],
          [
           "to tokenize a text. For example, if we hadn't included the albert normalizer we would have had a lot..."
          ],
          [
           "must then decode this sequence of bytes into a sequence of \"code points\". In our example the 2 bytes..."
          ],
          [
           "Latin Small Letter Cand the combining cedilla. But it's annoying because what appears to us to be a ..."
          ],
          [
           "sentence\". However, you must be aware that some normalizations can be very harmful if they are not a..."
          ],
          [
           "but I advise you to take the time to select them so that they do not make you lose important informa..."
          ],
          [
           "Building a tokenizer, block by block[[building-a-tokenizer-block-by-block]]\n\n<CourseFloatingBanner c..."
          ],
          [
           "The ðŸ¤— Tokenizers library has been built to provide several options for each of those steps, which yo..."
          ],
          [
           "<Youtube id=\"MR8tZm5ViWU\"/>\n\nMore precisely, the library is built around a central `Tokenizer` class..."
          ],
          [
           "You can find the whole list of building blocks [here](https://huggingface.co/docs/tokenizers/python/..."
          ],
          [
           "```\n\nThe function `get_training_corpus()` is a generator that will yield batches of 1,000 texts, whi..."
          ],
          [
           "```\n\nWe have to specify the `unk_token` so the model knows what to return when it encounters charact..."
          ],
          [
           "```\n\n```python out\nhello how are u?\n```\n\n<Tip>\n\n**To go further** If you test the two versions of th..."
          ],
          [
           "```\n\nIf you only want to split on whitespace, you should use the `WhitespaceSplit` pre-tokenizer ins..."
          ],
          [
           "```\n\nThe next step in the tokenization pipeline is running the inputs through the model. We already ..."
          ],
          [
           "```\n\nThe `encoding` obtained is an `Encoding`, which contains all the necessary outputs of the token..."
          ],
          [
           "```\n\nNote that we need to pass along the IDs of the special tokens, so the tokenizer can properly co..."
          ],
          [
           "```\n\nGreat! We can save our tokenizer in a single JSON file like this:\n\n```python\ntokenizer.save(\"to..."
          ],
          [
           "```\n\nYou can then use this tokenizer like any other ðŸ¤— Transformers tokenizer. You can save it with t..."
          ],
          [
           "```\n\n```python out\n[('Let', (0, 3)), (\"'s\", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (..."
          ],
          [
           "```\n\nWe apply the byte-level post-processing for the GPT-2 tokenizer as follows:\n\n```python\ntokenize..."
          ],
          [
           "```\n\nor:\n\n```python\nfrom transformers import GPT2TokenizerFast\n\nwrapped_tokenizer = GPT2TokenizerFas..."
          ],
          [
           "```\n\n```python out\n[(\"â–Let's\", (0, 5)), ('â–test', (5, 10)), ('â–the', (10, 14)), ('â–pre-tokenizer!', ..."
          ],
          [
           "```\n\n```python out\n['â–Let', \"'\", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']\n```\n\nA peculiarit..."
          ],
          [
           "```\n\n```python out\n['â–Let', \"'\", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>'..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n<!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-..."
          ],
          [
           "### 2. What part of the preprocessing for token classification differs from the other preprocessing ..."
          ],
          [
           "### 4. What does \"domain adaptation\" mean?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It's when we run a m..."
          ],
          [
           "### 6. Which of these tasks can be seen as a sequence-to-sequence problem?\n\n<Question\n\tchoices={[\n\t\t..."
          ],
          [
           "{#if fw === 'pt'}\n\n### 8. Why is there a specific subclass of `Trainer` for sequence-to-sequence pro..."
          ],
          [
           "{/if}\n\n### 10. When should you pretrain a new model?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"When there..."
          ],
          [
           "### 12. What are the main challenges when preprocessing data for a question answering task?\n\n<Questi..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n<!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-..."
          ],
          [
           "### 3. What can you do using the Hugging Face Hub web interface? \n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext..."
          ],
          [
           "{#if fw === 'pt'}\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"A tokenizer\",\n\t\t\texplain: \"Correct! All tokeni..."
          ],
          [
           "]}\n/>\n{:else}\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"A tokenizer\",\n\t\t\texplain: \"Correct! All tokenizers..."
          ],
          [
           "### 6. What is the first step when using the `push_to_hub()` method or the CLI tools?\n\n<Question\n\tch..."
          ],
          [
           "### 8. Which git operations can you do with the `Repository` class?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\tte..."
          ],
          [
           "ext embeddings and semantic search. In this video weâ€™ll explore how Transformer models represent tex..."
          ],
          [
           "Reading the text, we can see that walking the dog seems to be most similar to walking the cat, but l..."
          ],
          [
           "this example, our embedding vectors live in 3D and we can see that the orange and grey vectors are c..."
          ],
          [
           "per sentence, and each vector has 384 dimensions. But what we really want is a single embedding vect..."
          ],
          [
           "the attention mask being used here. This now gives us one 384 dimensional vector per sentence which ..."
          ],
          [
           "similarity between a question and a corpus of documents. For example, suppose we embed every post in..."
          ],
          [
           "embedding logic as before. This gives us a new column called \"embeddings\" that stores the embedding ..."
          ],
          [
           "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingB..."
          ],
          [
           "### 3. Suppose you try to run the following code, which throws an error:\n\n```py\nfrom transformers im..."
          ],
          [
           "```\n\nWhich of the following might be a good choice for the title of a forum topic to ask for help?\n\n..."
          ],
          [
           "<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"The optimization step where we compute gradients and perform bac..."
          ],
          [
           "### 6. What is the best way to get an issue on GitHub fixed?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Po..."
          ],
          [
           "### 8. Why is it a good idea to include details on your compute environment with `transformers-cli e..."
          ],
          [
           "et's study the transformer architecture. This video is the introductory video to the encoders, decod..."
          ],
          [
           "leverage only some parts of it, according to what we're trying to do. We won't dive into the specifi..."
          ],
          [
           "converts this text, these words, into numerical representations. These numerical representations can..."
          ],
          [
           "It uses a similar mechanism as the encoder, which is the masked self-attention as well. It differs f..."
          ],
          [
           "passed to the decoder. The decoder uses the encoder's output alongside other inputs, in order to gen..."
          ],
          [
           "Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\n\n<CourseFloatingBanner chapter={9}\n  ..."
          ],
          [
           "- Group together related demos as multiple tabs in one web application\n- Change the layout of your d..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-flip-text.hf.space\" frameBorder=\"0\" height=\"400\" title=\"Gradi..."
          ],
          [
           "3. You can assign events to any `Blocks` component. This will run your function when the component i..."
          ],
          [
           "Here's what you should keep in mind: any components created under a `Column` (this is also the defau..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-flip-text-image.hf.space\" frameBorder=\"0\" height=\"450\" title=..."
          ],
          [
           "- `fn`: the function to run\n- `inputs`: a (list of) component(s) whose values should supplied as the..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-blocks-gpt.hf.space\" frameBorder=\"0\" height=\"300\" title=\"Grad..."
          ],
          [
           "def text_to_sentiment(text):\n    return classifier(text)[0][\"label\"]\n\n\ndemo = gr.Blocks()\n\nwith demo..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-blocks-multi-step.hf.space\" frameBorder=\"0\" height=\"600\" titl..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-blocks-update-component-properties.hf.space\" frameBorder=\"0\" ..."
          ],
          [
           "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingB..."
          ],
          [
           "### 3. Where can you launch a Gradio demo from?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"Standard ..."
          ],
          [
           "### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?\n\n<Q..."
          ],
          [
           "### 8. Which of the following are components included in the Gradio library?\n\n<Question\n\tchoices={[\n..."
          ],
          [
           "### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spac..."
          ],
          [
           "i, this is going to be a video about the push_to_hub API for Tensorflow and Keras. So, to get starte..."
          ],
          [
           "just getting everything ready for training. So we're just going to load a dataset, we're going to to..."
          ],
          [
           "PushToHubCallback. So a callback in Keras is a function that's called regularly during training. You..."
          ],
          [
           "means you can resume from that save, so you get this automatic cloud-saving of your model, and you c..."
          ],
          [
           "are going to be saved to before they're uploaded to the Hub. The second argument is the tokenizer, a..."
          ],
          [
           "us know extremely urgently. But if you do have access to your own organization then you can use that..."
          ],
          [
           "a once-off method - it's not called regularly during training. You can just call this manually whene..."
          ],
          [
           "the way. So I'm going to run both of these cells and then I'm going to cut the video here, just beca..."
          ],
          [
           "our call to model.push_to_hub() after training. So everything's looking good! So now if we drop over..."
          ],
          [
           "the Glue CoLA dataset, and CoLA is an acronym for Corpus of Linguistic Acceptability. So what that m..."
          ],
          [
           "a couple of seconds out of this video here. Okay, we're back! The model loaded and we got an output,..."
          ],
          [
           "we can get that from the dataset we loaded, from the 'features' attribute it has. And then we can cr..."
          ],
          [
           "generate new results I'm going to use something slightly different. So let's try an incorrect senten..."
          ],
          [
           "be unacceptable than acceptable. Presumably if we tried a bit harder with training we could get a mu..."
          ],
          [
           "and a very low probability of being unacceptable. So you can use this inference API even with the ch..."
          ],
          [
           "pipeline or you can just load it with, for example, TFAutoModelForSequenceClassification and then fo..."
          ],
          [
           "you wanna do. So that was a quick overview of how, after your training or during your training, you ..."
          ],
          [
           "et's take a look at word-based tokenization. Word-based tokenization is the idea of splitting the ra..."
          ],
          [
           "single number is high as a word contains a lot of contextual and semantic information in a sentence...."
          ],
          [
           "plural form of the word dog. Another issue with this approach is that there are a lot of different w..."
          ],
          [
           "mappings requires an enormous number of weights when the vocabulary size is large. If we want our mo..."
          ],
          [
           "be converted to the out-of-vocabulary word, or the \"unknown\" word. This can rapidly become an issue:..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Putting it all together[[putting-it-all-together]]\n\n{#if fw === 'pt..."
          ],
          [
           "```py\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-..."
          ],
          [
           "```\n\nHere, the `model_inputs` variable contains everything that's necessary for a model to operate w..."
          ],
          [
           "```\n\nThe `tokenizer` object can handle the conversion to specific framework tensors, which can then ..."
          ],
          [
           "```\n\nOne token ID was added at the beginning, and one at the end. Let's decode the two sequences of ..."
          ],
          [
           "```\n{:else}\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForSequ..."
          ],
          [
           "n this video, I'm going to give you a very quick introduction to how our transformers models work to..."
          ],
          [
           "fold, I'm going to quickly introduce Keras models, and how we work with them. In other videos, which..."
          ],
          [
           "by building your model out by hand - you added one layer after another, maybe using model.add() or t..."
          ],
          [
           "all packed inside a Model, you don't need to worry about that complexity if you don't want to! You h..."
          ],
          [
           "we'll cover concrete examples of how to use those methods in other videos that I'll link below. For ..."
          ],
          [
           "Gradio, check![[gradio-check]]\n\n<CourseFloatingBanner\n    chapter={9}\n    classNames=\"absolute z-10 ..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Fast tokenizers' special powers[[fast-tokenizers-special-powers]]\n\n..."
          ],
          [
           "{/if}\n\nIn this section we will take a closer look at the capabilities of the tokenizers in ðŸ¤— Transfo..."
          ],
          [
           "</Tip>\n\n## Batch encoding[[batch-encoding]]\n\n<Youtube id=\"3umI3tm27Vw\"/>\n\nThe output of a tokenizer ..."
          ],
          [
           "```\n\nAs mentioned previously, we get a `BatchEncoding` object in the tokenizer's output:\n\n```python ..."
          ],
          [
           "```\n\nWe can see that the tokenizer's special tokens `[CLS]` and `[SEP]` are mapped to `None`, and th..."
          ],
          [
           "Lastly, we can map any word or token to characters in the original text, and vice versa, via the `wo..."
          ],
          [
           "```\n\n```python out\nSylvain..."
          ],
          [
           "```\n\nAs we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of t..."
          ],
          [
           "{/if}\n\n### Getting the base results with the pipeline[[getting-the-base-results-with-the-pipeline]]\n..."
          ],
          [
           "```\n\n```python out\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'e..."
          ],
          [
           "```\n\nThe model properly identified each token generated by \"Sylvain\" as a person, each token generat..."
          ],
          [
           "```\n\nThe `aggregation_strategy` picked will change the scores computed for each grouped entity. With..."
          ],
          [
           "```py\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\nmodel_checkpoint = \"d..."
          ],
          [
           "```\n\nSince we're using `AutoModelForTokenClassification` here, we get one set of logits for each tok..."
          ],
          [
           "```\n\n```python out\n(1, 19)\n(1, 19, 9)\n```\n\n{/if}\n\nWe have a batch with 1 sequence of 19 tokens and t..."
          ],
          [
           "```\n\nAs we saw earlier, there are 9 labels: `O` is the label for the tokens that are not in any name..."
          ],
          [
           "With this map, we are ready to reproduce (almost entirely) the results of the first pipeline -- we c..."
          ],
          [
           "```\n\n```python out\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\n {'entity': 'I..."
          ],
          [
           "```\n\n```python out\n[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22..."
          ],
          [
           "```\n\nThis is the same as what we got from the first pipeline!\n\n### Grouping entities[[grouping-entit..."
          ],
          [
           "```\n\n```python out\nHugging Face\n```\n\nTo write the code that post-processes the predictions while gro..."
          ],
          [
           "```\n\nAnd we get the same results as with our second pipeline!\n\n```python out\n[{'entity_group': 'PER'..."
          ],
          [
           "et's take a look at subword-based tokenization. Understanding why subword-based tokenization is inte..."
          ],
          [
           "very long sequences, less meaningful individual tokens for character-based tokenizers. These algorit..."
          ],
          [
           "still the word dog, with an added s while slightly changes the meaning while keeping the original id..."
          ],
          [
           "the model will now be able to make sense of token in different situations. It will understand that t..."
          ],
          [
           "completing a word. Here the ## prefix indicates that ization is part of a word rather than the begin..."
          ],
          [
           "in reducing the vocabulary sizes by sharing information across different words, having the ability t..."
          ],
          [
           "et's see together what is the training strategy of the WordPiece algorithm and how it performs the t..."
          ],
          [
           "increases this vocabulary to the desired size. To build the initial vocabulary, we divide each word ..."
          ],
          [
           "each of these pairs. As for the BPE algorithm, we will select the pair with the highest score. Takin..."
          ],
          [
           "this score will be decreased. In our example, the pair \"hu\" appears 4 times, the letter \"h\" 4 times ..."
          ],
          [
           "operations until we have the vocabulary at the desired size! Let's look at a few more steps to see t..."
          ],
          [
           "of our word. And so on until we reach the end! And that's it, huggingface is divided into 4 sub-toke..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Summarization[[summarization]]\n\n{#if fw === 'pt'}\n\n<CourseFloatingB..."
          ],
          [
           "<Youtube id=\"yHnr5Dk2zCI\"/>\n\nAlthough there already exist various fine-tuned models for summarizatio..."
          ],
          [
           "## Preparing a multilingual corpus[[preparing-a-multilingual-corpus]]\n\nWe'll use the [Multilingual A..."
          ],
          [
           "```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['review_id', 'product_id', ..."
          ],
          [
           "```\n\n```python out\n'>> Title: Worked in front position, not rear'\n'>> Review: 3 stars because these ..."
          ],
          [
           "```\n\n<Tip>\n\nâœï¸ **Try it out!** Change the random seed in the `Dataset.shuffle()` command to explore ..."
          ],
          [
           "```\n\nThe most popular products in the English dataset are about household items, clothing, and wirel..."
          ],
          [
           "```\n\n```python out\n'>> Title: I\\'m dissapointed.'\n'>> Review: I guess I had higher expectations for ..."
          ],
          [
           "```\n\nOkay, we can see that the reviews are not strictly about books and might refer to things like c..."
          ],
          [
           "```\n\nThis certainly looks like a mix of English and Spanish reviews! Now that we have a training cor..."
          ],
          [
           "```\n\nNow that we've prepared our corpus, let's take a look at a few possible Transformer models that..."
          ],
          [
           "| Transformer model | Description                                                                   ..."
          ],
          [
           "|     [mT5](https://huggingface.co/google/mt5-base)     | A multilingual version of T5, pretrained o..."
          ],
          [
           "As you can see from this table, the majority of Transformer models for summarization (and indeed mos..."
          ],
          [
           "<Tip>\n\nâœï¸ **Try it out!** Once you've worked through this section, see how well mT5 compares to mBAR..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ In the early stages of your NLP projects, a good practice is to train a class of \"smal..."
          ],
          [
           "```\n\n```python out\n['â–I', 'â–', 'loved', 'â–reading', 'â–the', 'â–Hung', 'er', 'â–Games', '</s>']\n```\n\nTh..."
          ],
          [
           "```\n\nLet's walk through this code to understand what's happening. The first thing we've done is defi..."
          ],
          [
           "```\n\nNow that the corpus has been preprocessed, let's take a look at some metrics that are commonly ..."
          ],
          [
           "```\n\nOne way to compare them could be to count the number of overlapping words, which in this case w..."
          ],
          [
           "Applying this to our verbose summary gives a precision of 6/10  = 0.6, which is considerably worse t..."
          ],
          [
           "```\n\nand then loading the ROUGE metric as follows:\n\n```python\nimport evaluate\n\nrouge_score = evaluat..."
          ],
          [
           "```\n\nWhoa, there's a lot of information in that output -- what does it all mean? First, ðŸ¤— Datasets a..."
          ],
          [
           "```\n\n```python out\nScore(precision=0.86, recall=1.0, fmeasure=0.92)\n```\n\nGreat, the precision and re..."
          ],
          [
           "```\n\nand then download the punctuation rules:\n\n```python\nimport nltk\n\nnltk.download(\"punkt\")\n```\n\nNe..."
          ],
          [
           "```\n\n```python out\n{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}\n```\n\nWe can..."
          ],
          [
           "```\n\n{/if}\n\n<Tip>\n\nðŸ’¡ If you're wondering why you don't see any warnings about fine-tuning the model ..."
          ],
          [
           "```\n\nwhich will display a widget where you can enter your credentials. Alternatively, you can run th..."
          ],
          [
           "```\n\nHere, the `predict_with_generate` argument has been set to indicate that we should generate sum..."
          ],
          [
           "```python\nimport numpy as np\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n ..."
          ],
          [
           "```\n\n{/if}\n\nNext, we need to define a data collator for our sequence-to-sequence task. Since mT5 is ..."
          ],
          [
           "```\n\n```python out\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,..."
          ],
          [
           "```\n\nThe main thing to notice here is that the first example is longer than the second one, so the `..."
          ],
          [
           "```\n\nFrom the scores we can see that our model has handily outperformed our lead-3 baseline -- nice!..."
          ],
          [
           "```\n\nThis will save the checkpoint and configuration files to `output_dir`, before uploading all the..."
          ],
          [
           "```\n\nNow, we define our training hyperparameters and compile:\n\n```python\nfrom transformers import cr..."
          ],
          [
           "```\n\nWe got some loss values during training, but really we'd like to see the ROUGE metrics we compu..."
          ],
          [
           "all_preds = []\nall_labels = []\nfor batch, labels in tqdm(tf_generate_dataset):\n    predictions = gen..."
          ],
          [
           "```\n\nOnce we have our lists of label and prediction strings, computing the ROUGE score is easy:\n\n```..."
          ],
          [
           "```\n\nWe can then instantiate the data collator and use this to define our dataloaders:\n\n```python\nfr..."
          ],
          [
           "```\n\n<Tip>\n\nðŸš¨ If you're training on a TPU, you'll need to move all the code above into a dedicated t..."
          ],
          [
           "```\n\nThis should look familiar to you if you recall how we defined the `compute_metrics()` function ..."
          ],
          [
           "```\n\nThis will allow us to push the artifacts back to the Hub by calling the `repo.push_to_hub()` me..."
          ],
          [
           "generated_tokens = accelerator.pad_across_processes(\n                generated_tokens, dim=1, pad_in..."
          ],
          [
           "# Save and upload\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model..."
          ],
          [
           "```\n\n```python out\nEpoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.50..."
          ],
          [
           "```\n\nAnd that's it! Once you run this, you'll have a model and results that are pretty similar to th..."
          ],
          [
           "```\n\nLet's take a look at one of the English examples we get:\n\n```python\nprint_summary(100)\n```\n\n```..."
          ],
          [
           "et's study how to preprocess a dataset for token classification! Token classification regroups any t..."
          ],
          [
           "to get to the same point, with one column containing words (as list of strings) and another containi..."
          ],
          [
           "location, PER, for person, ORG for organization and MISC for miscellaneous. Each label has two versi..."
          ],
          [
           "word may have been split into several tokens, our labels won't match the tokens anymore. This is whe..."
          ],
          [
           "we tell the Transformer loss functions to ignore them when computing the loss. The code is then pret..."
          ],
          [
           "a batch. Unless you changed the preprocessing function to apply some fixed padding, we will get sent..."
          ],
          [
           "are either ready to send your data and this data collator to the Trainer, or to use the to_tf_datase..."
          ],
          [
           "he fast tokenizers of the Transformers library are fast, but they also implement features that will ..."
          ],
          [
           "is thus not enough if we want to match some tokens with a span of text (something we will need to do..."
          ],
          [
           "at the beginning of a word, and T5 uses this special underscore symbol for the same purpose. Thankfu..."
          ],
          [
           "calling it on one (or several) text by adding the return_offsets_mapping=True argument. In this inst..."
          ],
          [
           "then we apply the model of the tokenizer, which is where the words are splits into tokens,() before ..."
          ],
          [
           "instead of randomly chosen tokens. This will require us to use the word IDs we saw. When doing token..."
          ],
          [
           "et's have a look inside the question answering pipeline. The question answering pipeline can extract..."
          ],
          [
           "post-processing is applied. The tokenization and model steps should be familiar. We use the auto cla..."
          ],
          [
           "SEP special tokens). The answer is a part of those tokens. So we ask the model to predict which toke..."
          ],
          [
           "an impossible answer. This is what it looks in terms of code. We use a large negative number for the..."
          ],
          [
           "we have the start and end positions of the tokens, we use the offset mappings provided by our tokeni..."
          ],
          [
           "context. If we take disjoint chunks of texts, we might end up with the answer being split between tw..."
          ],
          [
           "post-processing we saw before for each feature, we get the answer with a score for each of them, and..."
          ],
          [
           "Basic usage completed![[basic-usage-completed]]\n\n<CourseFloatingBanner\n    chapter={2}\n    className..."
          ],
          [
           "et's study how to preprocess a dataset for question answering! Question answering is the task of fin..."
          ],
          [
           "containing the questions, one column containing the contexts, one column for the index of the start ..."
          ],
          [
           "In this case, we wont have any proper labels for our model. So we should keep the truncated part as ..."
          ],
          [
           "only and the padding to the maximum length. The stride argument is where we set the number of overla..."
          ],
          [
           "contain the answer, we set the two labels to the index of the CLS token. We also do this if the cont..."
          ],
          [
           "on our previous example. Putting it all together looks like this big function, which we can apply to..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={7}..."
          ],
          [
           "Each section can be read independently.\n\n{/if}\n\n\n<Tip>\n\nIf you read the sections in sequence, you wi..."
          ],
          [
           "hat happens inside the pipeline function? In this video, we will look at what actually happens when ..."
          ],
          [
           "the model, which outputs logits. Finally, the post-processing steps transforms those logits into lab..."
          ],
          [
           "at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer match..."
          ],
          [
           "We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since th..."
          ],
          [
           "a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding ..."
          ],
          [
           "the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that..."
          ],
          [
           "to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except..."
          ],
          [
           "returns logits. To make sense of those logits, we need to dig into the third and last step of the pi..."
          ],
          [
           "the negative label, and the seconds (index 1) correspond to the positive label. This is how our clas..."
          ],
          [
           "et's see how to preprocess a dataset for translation. This is the task of well translating a sentenc..."
          ],
          [
           "be able to follow the same steps. For once, our labels are not integers corresponding to some classe..."
          ],
          [
           "as a single word. That's because our inputs have been tokenized as English. Since our model knows tw..."
          ],
          [
           "setting padding=max_length. Here we will show you how to pad dynamically as it requires one more ste..."
          ],
          [
           "library provides us with a data collator to do this all automatically. You can then pass it to the T..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning, Check![[fine-tuning-check]]\n\n<CourseFloatingBanner\n   ..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Debugging the training pipeline[[debugging-the-training-pipeline]]\n..."
          ],
          [
           "The best way to debug an error that arises in `model.fit()` is to manually go through this whole pip..."
          ],
          [
           "```\n\nIf you try to execute it, you might get some `VisibleDeprecationWarning`s when doing the datase..."
          ],
          [
           "```\n\n`break` ends the loop after one iteration, so this grabs the first batch that comes out of `tra..."
          ],
          [
           "```\n\nThis looks right, doesn't it? We're passing the `labels`, `attention_mask`, and `input_ids` to ..."
          ],
          [
           "```\n\nNow we'll use the model's internal loss, and this problem should be resolved!\n\n<Tip>\n\nâœï¸ **Your..."
          ],
          [
           "```\n\nOh no. \n\n`nan` is not a very encouraging loss value. Still, we've checked our data, and it look..."
          ],
          [
           "```\n\nWell, this is tricky. Everything is `nan`! But that's strange, isn't it? How would all our logi..."
          ],
          [
           "```\n\nWhen we run that, we get:\n\n```py out\nTFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), d..."
          ],
          [
           "```\n\n*Now* we're getting somewhere! There are no `nan` values in our logits, which is reassuring. Bu..."
          ],
          [
           "```python out\narray([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,\n        23212, ..."
          ],
          [
           "0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0],\n    ..."
          ],
          [
           "0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0..."
          ],
          [
           "0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0..."
          ],
          [
           "0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0..."
          ],
          [
           "```\n\nWell, there's a lot in here, but nothing stands out as unusual. Let's look at the labels:\n\n```p..."
          ],
          [
           "```\n\nWe're training! No more `nan`s, and our loss is declining... sort of. If you watch it for a whi..."
          ],
          [
           "Does anything stand out here? That's right -- the learning rate! When we just use the string `'adam'..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ You can also import the `create_optimizer()` function from ðŸ¤— Transformers, which will ..."
          ],
          [
           "```\n\nNow our loss is really going somewhere! Training finally looks like it's working. There's a les..."
          ],
          [
           "</Tip>\n\n### Hungry Hungry TensorFlow ðŸ¦›[[hungry-hungry-tensorflow]]\n\nOne particular quirk of TensorFl..."
          ],
          [
           "### Check your data (again!)[[check-your-data-again]]\n\nYour model will only learn something if it's ..."
          ],
          [
           "```\n\nThen you can compare it with the first label, like so:\n\n```py\nlabels = batch[\"labels\"].numpy()\n..."
          ],
          [
           "```\n\nOnce you can view your data like this, you can ask yourself the following questions:\n\n- Is the ..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ If your training data is unbalanced, make sure to build a batch of training data conta..."
          ],
          [
           "If you are tweaking the model itself, keep it simple and don't try anything you can't reasonably jus..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Debugging the training pipeline[[debugging-the-training-pipeline]]\n..."
          ],
          [
           "The best way to debug an error that arises in `trainer.train()` is to manually go through this whole..."
          ],
          [
           "```\n\nIf you try to execute it, you will be met with a rather cryptic error:\n\n```python out\n'ValueErr..."
          ],
          [
           "```\n\nDo you notice something wrong? This, in conjunction with the error message about `input_ids` mi..."
          ],
          [
           "metric = evaluate.load(\"glue\", \"mnli\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = e..."
          ],
          [
           "```\n\nThis new code will now give a different error (progress!):\n\n```python out\n'ValueError: expected..."
          ],
          [
           "```\n\n```python out\ndict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'..."
          ],
          [
           "```\n\n```python out\nTrue\n```\n\nThat's good! Lastly, let's check our label:\n\n```py\ntrainer.train_datase..."
          ],
          [
           "```\n\nThis code creates the training dataloader, then iterates through it, stopping at the first iter..."
          ],
          [
           "```\n\nSo this is the `default_data_collator`, but that's not what we want in this case. We want to pa..."
          ],
          [
           "def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    return metric.compute(predic..."
          ],
          [
           "```\n\nThe good news? We don't get the same error as before, which is definitely progress. The bad new..."
          ],
          [
           "```\n\nIf you're running this code in a notebook, you may get a CUDA error that's similar to the one w..."
          ],
          [
           "```\n\n```python out\n~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional..."
          ],
          [
           "```\n\nWith two labels, only 0s and 1s are allowed as targets, but according to the error message we g..."
          ],
          [
           "```\n\nWe aren't including the `trainer.train()` line yet, to take the time to check that everything l..."
          ],
          [
           "```\n\nAgain, if you're using the default optimizer in the `Trainer`, you shouldn't get an error at th..."
          ],
          [
           "```\n\n```python out\nTypeError: only size-1 arrays can be converted to Python scalars\n```\n\nYou will re..."
          ],
          [
           "```\n\nThis tells us that the error originates in the `datasets/metric.py` module -- so this is a prob..."
          ],
          [
           "```\n\n```python out\n{'accuracy': 0.625}\n```\n\nNow our error is fixed! This was the last one, so our sc..."
          ],
          [
           "```\n\nIn this instance, there are no more problems, and our script will fine-tune a model that should..."
          ],
          [
           "</Tip>\n\nAfter looking at your data, go through a few of the model's predictions and decode them too...."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ If your training data is unbalanced, make sure to build a batch of training data conta..."
          ],
          [
           "```\n\n100% accuracy, now this is a nice example of overfitting (meaning that if you try your model on..."
          ],
          [
           "Here are some additional resources that may prove helpful:\n\n- [\"Reproducibility as a vehicle for eng..."
          ],
          [
           "Part 1 completed![[part-1-completed]]\n\n<CourseFloatingBanner\n    chapter={4}\n    classNames=\"absolut..."
          ],
          [
           "Bias and limitations[[bias-and-limitations]]\n\n<CourseFloatingBanner chapter={1}\n  classNames=\"absolu..."
          ],
          [
           "```\n\nWhen asked to fill in the missing word in these two sentences, the model gives only one gender-..."
          ],
          [
           "n this video, we'll study the encoder-decoder architecture. An example of a popular encoder-decoder ..."
          ],
          [
           "now know that the numerical representation holds information about the meaning of the sequence. Let'..."
          ],
          [
           "that's where the encoder-decoder magic happens. The encoder accepts a sequence as input. It computes..."
          ],
          [
           "decoder is essentially decoding what the encoder has output. The \"start of sequence word\" indicates ..."
          ],
          [
           "second word. Please note that the first word is still here; as the model still outputs it. However, ..."
          ],
          [
           "That encoder output is then sent to the decoder, for it to be decoded. While we can now discard the ..."
          ],
          [
           "encoder to create a representation of theÂ EnglishÂ sentence. We cast this to the decoder and, with th..."
          ],
          [
           "We've translated the sentence! Where the encoder-decoder really shines, is that we have an encoder a..."
          ],
          [
           "other hand, we have the decoder, whose sole purpose is to decode the feature output by the encoder. ..."
          ],
          [
           "Firstly, this means that from a sequence of three words, we're able to generate a sequence of four w..."
          ],
          [
           "example a very long context for the encoder which handles the text, and a smaller context for the de..."
          ],
          [
           "tasks. This wraps things up for the encoder-decoders. Thanks for watching!..."
          ],
          [
           "et's have a look inside the token classification pipeline. In the pipeline video, we looked at the d..."
          ],
          [
           "the text classification pipeline we studied in a previous video. There are three steps: the tokeniza..."
          ],
          [
           "all the other models of the Transformers library, our model outputs logits, which we turn into predi..."
          ],
          [
           "is why we didn't see it in our results in the first slide. On top of the label and the probability, ..."
          ],
          [
           "group together tokens that correspond to the same entity.This is why we had two labels for each type..."
          ],
          [
           "cases, we can flag a new entity each time we see a new label appearing (either with the I or B prefi..."
          ],
          [
           "hat is the ROUGE metric? For many NLP tasks we can use common metrics like accuracy or F1 score, but..."
          ],
          [
           "a summary that tells us how \"good\" it is compared to one or more reference summaries. In this exampl..."
          ],
          [
           "n-grams of the references. An n-gram is just a fancy way of saying \"a chunk of n words\", so let's st..."
          ],
          [
           "unigrams. This means we just count the number of matching words in the generated and reference summa..."
          ],
          [
           "This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal wi..."
          ],
          [
           "then count how many pairs in the generated summary are present in the reference one. This gives us R..."
          ],
          [
           "instead treats each summary as a sequence of words and then looks for the longest common subsequence..."
          ],
          [
           "very simple: just use the load_metric() function, provide your model's summaries along with the refe..."
          ],
          [
           "score. We've already seen ROUGE-1, ROUGE-2 and ROUGE-L, so what is ROUGE-LSUM? Well, the â€œsumâ€ in RO..."
          ],
          [
           "WordPiece tokenization[[wordpiece-tokenization]]\n\n<CourseFloatingBanner chapter={6}\n  classNames=\"ab..."
          ],
          [
           "```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginni..."
          ],
          [
           "```\n\nso the initial vocabulary will be `[\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]` (if we forget a..."
          ],
          [
           "```\nVocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"]\nCorpus: (\"hu\" \"##g\", 10), ..."
          ],
          [
           "```\n\nand we continue like this until we reach the desired vocabulary size.\n\n<Tip>\n\nâœï¸ **Now your tur..."
          ],
          [
           "When the tokenization gets to a stage where it's not possible to find a subword in the vocabulary, t..."
          ],
          [
           "```\n\nFirst, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece toke..."
          ],
          [
           "```\n\nAs we saw before, the alphabet is the unique set composed of all the first letters of words, an..."
          ],
          [
           "```\n\nNext we need to split each word, with all the letters that are not the first prefixed by `##`:\n..."
          ],
          [
           "```\n\n```python out\n('T', '##h'): 0.125\n('##h', '##i'): 0.03409090909090909\n('##i', '##s'): 0.0272727..."
          ],
          [
           "```\n\nAnd we can have a look at the result of the first merge:\n\n```py\nsplits = merge_pair(\"a\", \"##b\",..."
          ],
          [
           "```\n\nWe can then look at the generated vocabulary:\n\n```py\nprint(vocab)\n```\n\n```python out\n['[PAD]', ..."
          ],
          [
           "```\n\nAs we can see, compared to BPE, this tokenizer learns parts of words as tokens a bit faster.\n\n<..."
          ],
          [
           "```\n\nWe can try it on any text:\n\n```python\ntokenize(\"This is the Hugging Face course!\")\n```\n\n```pyth..."
          ],
          [
           "emory mapping and streaming. In this video we'll take a look at two core features of the Datasets li..."
          ],
          [
           "To handle these large datasets, the Datasets library is built on two core features: the Apache Arrow..."
          ],
          [
           "your hard disk. For these cases, the Datasets library provides a streaming API that allows you to pr..."
          ],
          [
           "virtual memory. This allows applications to access can access segments in an extremely large file wi..."
          ],
          [
           "laptop - that's not too bad at all! Let's now take a look at how we can stream a large dataset. The ..."
          ],
          [
           "will download and access a single example from the dataset, which  means you can progressively itera..."
          ],
          [
           "methods because we can't index into the dataset. The take() method returns the first N examples in t..."
          ],
          [
           "Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={2}\n    classNames=\"absolute z-10 ri..."
          ],
          [
           "Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function..."
          ],
          [
           "Introduction[[introduction]]\n\nWelcome to the Hugging Face course! This introduction will guide you t..."
          ],
          [
           "Once you're comfortable moving around in Colab, create a new notebook and get started with the setup..."
          ],
          [
           "```\n!pip install transformers\n```\n\nYou can make sure the package was correctly installed by importin..."
          ],
          [
           "```\n!pip install transformers[sentencepiece]\n```\n\nThis will take a bit of time, but then you'll be r..."
          ],
          [
           "```\n\nFrom inside this directory, create a virtual environment using the Python `venv` module:\n\n```\np..."
          ],
          [
           "n this video we take a look at the data processing necessary to train causal language models. Causal..."
          ],
          [
           "text files. These files can webpages scraped from the internet such as the Common Crawl dataset or t..."
          ],
          [
           "length and depending on the data source it is possible that the tokenized texts are much longer than..."
          ],
          [
           "fill it. In this case we would like to remove it. With the return_length keyword we also get the len..."
          ],
          [
           "existing columns. We need to remove columns because we can create multiple samples per text and the ..."
          ],
          [
           "for short, token in between. Finally we can chunk this long sequence with the context length and we ..."
          ],
          [
           "predict is â€œformersâ€. In the next step we feed â€œTransâ€ and â€œformersâ€ to the model and the label is t..."
          ],
          [
           "the sequence ends. Letâ€™s have a look at what we need to do to create the labels for causal language ..."
          ],
          [
           "hat is the BLEU metric? For many NLP tasks we can use common metrics like accuracy or F1 score, but ..."
          ],
          [
           "to one or more reference translations. In this example we have a sentence in Spanish that has been t..."
          ],
          [
           "the n-grams of the generated translation to the n-grams of the references. An n-gram is just a fancy..."
          ],
          [
           "This means we just count the number of matching words in the generated and reference translations an..."
          ],
          [
           "times. If we just count the number of word matches, we can get really high precision scores even tho..."
          ],
          [
           "clip the numerator to one and the modified unigram precision now gives a much lower score. Another p..."
          ],
          [
           "for several different n-grams and then averages the result. For example, if we compare 4-grams, then..."
          ],
          [
           "score itself is then calculated by taking the geometric mean of the precision scores. By default, th..."
          ],
          [
           "on a benchmark. On the other hand, there are several problems with BLEU, including the fact it doesn..."
          ],
          [
           "see in this example, computing the SacreBLEU score is almost identical to the BLEU one. The main dif..."
          ],
          [
           "elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging ..."
          ],
          [
           "result with the community. The second will dive deeper into our libraries and teach you how to tackl..."
          ],
          [
           "Learning. If you don't know what a training and validation set is or what gradient descent means, yo..."
          ],
          [
           "Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 ri..."
          ],
          [
           "- Chapters 1 to 4 provide an introduction to the main concepts of the ðŸ¤— Transformers library. By the..."
          ],
          [
           "## Who are we?[[who-are-we]]\n\nAbout the authors:\n\n[**Abubakar Abid**](https://huggingface.co/abidlab..."
          ],
          [
           "[**Dawood Khan**](https://huggingface.co/dawoodkhan82) is a Machine Learning Engineer at Hugging Fac..."
          ],
          [
           "- **Does taking this course lead to a certification?**\nCurrently we do not have any certification fo..."
          ],
          [
           "The Jupyter notebooks containing all the code from the course are hosted on the [`huggingface/notebo..."
          ],
          [
           "```\n@misc{huggingfacecourse,\n  author = {Hugging Face},\n  title = {The Hugging Face Course, 2022},\n ..."
          ],
          [
           "ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks pl..."
          ],
          [
           "Question Answering video\n\nWelcome to the Hugging Face tasks series. In this video, we will take a lo..."
          ],
          [
           "Masked Language Modeling video\n\nWelcome to the Hugging Face tasks series! In this video weâ€™ll take a..."
          ],
          [
           "Translation video\n\nWelcome to the Hugging Face tasks series. In this video, we will take a look at t..."
          ],
          [
           "as recorded adlib - need to generate transcript with Whisper :)..."
          ],
          [
           "Big data? ðŸ¤— Datasets to the rescue![[big-data-datasets-to-the-rescue]]\n\n<CourseFloatingBanner chapte..."
          ],
          [
           "## What is the Pile?[[what-is-the-pile]]\n\nThe Pile is an English text corpus that was created by [El..."
          ],
          [
           "```\n\nNext, we can load the dataset using the method for remote files that we learned in [section 2](..."
          ],
          [
           "```\n\nOkay, this looks like the abstract from a medical article. Now let's see how much RAM we've use..."
          ],
          [
           "```\n\nNice -- despite it being almost 20 GB large, we're able to load and access the dataset with muc..."
          ],
          [
           "Memory-mapped files can also be shared across multiple processes, which enables methods like `Datase..."
          ],
          [
           "```\n\n```python out\n'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'\n```\n\n..."
          ],
          [
           "```\n\n```python out\n{'meta': {'pmid': 11409574, 'language': 'eng'},\n 'text': 'Epidemiology of hypoxae..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ To speed up tokenization with streaming you can pass `batched=True`, as we saw in the ..."
          ],
          [
           "```\n\n```python out\n[{'meta': {'pmid': 11409574, 'language': 'eng'},\n  'text': 'Epidemiology of hypox..."
          ],
          [
           "```\n\nLet's round out our exploration of dataset streaming with a common application: combining multi..."
          ],
          [
           "```\n\nThis dataset is large enough to stress the RAM of most laptops, yet we've been able to load and..."
          ],
          [
           "```\n\nHere we've used the `islice()` function from Python's `itertools` module to select the first tw..."
          ],
          [
           "```\n\n<Tip>\n\nâœï¸ **Try it out!** Use one of the large Common Crawl corpora like [`mc4`](https://huggin..."
          ],
          [
           "et's see how we can preprocess our data for masked language modeling. As a reminder, masked language..."
          ],
          [
           "together. The first way to make all the texts the same length is the one we used in text classificat..."
          ],
          [
           "lost. This is why a second way to generate samples of text with the same length is to chunk our text..."
          ],
          [
           "of chunking is ideal if all your texts are very long, but it won't work as nicely if you have a vari..."
          ],
          [
           "chunk it. Notice how it reduces the number of samples in our dataset here, there must have been quit..."
          ],
          [
           "Subtitles for the course videos\n\nThis folder contains all the subtitles for the course videos on You..."
          ],
          [
           "```\n\nTo upload the SRT file to YouTube, we need the subtitle in monolingual format, i.e. the above b..."
          ],
          [
           "he pipeline function. The pipeline function is the most high-level API of the Transformers library. ..."
          ],
          [
           "given input, and determines if it's positive or negative. Here, it attributed the positive label on ..."
          ],
          [
           "classification pipeline is a more general text-classification pipeline: it allows you to provide the..."
          ],
          [
           "until now, we have used the pipeline API with the default model associated to each task, but you can..."
          ],
          [
           "a lighter version of gpt2 created by the Hugging Face team. When applying the pipeline to a given pr..."
          ],
          [
           "we ask the two most likely values for the missing words (according to the model) and get mathematica..."
          ],
          [
           "text. The grouped_entities=True argument used is to make the pipeline group together the different w..."
          ],
          [
           "API is translation. Here we use a French/English model found on the model hub to get the English ver..."
          ],
          [
           "Sequence-to-sequence models[sequence-to-sequence-models]\n\n<CourseFloatingBanner\n    chapter={1}\n    ..."
          ],
          [
           "aving and reloading a dataset. In this video we'll take a look saving a dataset in various formats, ..."
          ],
          [
           "downloaded the allocine dataset from the Hugging Face Hub and you can see there are three Arrow file..."
          ],
          [
           "want to save it in either the Arrow or Parquet formats. Arrow files are great if you plan to reload ..."
          ],
          [
           "library will automatically create a directory for each split to store the Arrow table and metadata. ..."
          ],
          [
           "need to loop over the splits of the DatasetDict object and save each dataset as an individual CSV fi..."
          ],
          [
           "associated with each split. As you can see in this example, by providing all the splits and their fi..."
          ],
          [
           "or Parquet files, we can reload them again with the appropriate script in the load_dataset function,..."
          ],
          [
           "n our other videos we talked about the basics of fine-tuning a language model with Tensorflow (and a..."
          ],
          [
           "two things we want to change about the default learning rate for Adam. The first is that it's way to..."
          ],
          [
           "down to a tiny value, or even 0, over the course of training. That's what this PolynomialDecay sched..."
          ],
          [
           "is the size of the training set, divided by the batch_size to get the number of batches per epoch, a..."
          ],
          [
           "5e-5, which means 5 times ten to the minus 5, and then decays down at a constant rate until it hits ..."
          ],
          [
           "just passed it the string \"adam\". Keras recognizes the names of common optimizers and loss functions..."
          ],
          [
           "be sparse categorical crossentropy if you're following along from the fine-tuning video. And now we ..."
          ],
          [
           "Tokenizers, check![[tokenizers-check]]\n\n<CourseFloatingBanner\n    chapter={6}\n    classNames=\"absolu..."
          ],
          [
           "We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since th..."
          ],
          [
           "dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is..."
          ],
          [
           "weights. However, the TFAutoModel API will only instantiate the body of the model, that is, the part..."
          ],
          [
           "the TFAutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that..."
          ],
          [
           "To make sense of those logits, we need to dig into the third and last step of the pipeline: post-pro..."
          ],
          [
           "and the seconds (index 1) correspond to the positive label. This is how our classifier built with th..."
          ],
          [
           "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingB..."
          ],
          [
           "```\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It will return classification scores for this sentence, wit..."
          ],
          [
           "```\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"This pipeline requires that labels be given to classify thi..."
          ],
          [
           "### 6. True or false? A language model usually does not need labels for its pretraining.\n\n<Question\n..."
          ],
          [
           "### 9. Which of those types of models would you use for summarizing texts?\n\n<Question\n\tchoices={[\n\t\t..."
          ],
          [
           "### 11. What possible source can the bias observed in a model have?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\tte..."
          ],
          [
           "n this video we will see how you can create your own tokenizer from scratch! To create your own toke..."
          ],
          [
           "the creation of an attention mask but also the generation of a list of token ids. The decoding opera..."
          ],
          [
           "a tokenizer from the tokenizers library. So, to create your own transformers tokenizer you will have..."
          ],
          [
           "English. We attack here the big part: the design of our tokenizer with the tokenizers library. We st..."
          ],
          [
           "the level of spaces and the second one isolating the punctuation marks. Now, we can define the train..."
          ],
          [
           "can retrieve the ids of the special class and separation tokens because we will need them to post-pr..."
          ],
          [
           "there it ist, you have all the necessary lines of code to define your own tokenizer. Now that we hav..."
          ],
          [
           "that you are ready to navigate the tokenizers library documentation to choose the components for you..."
          ],
          [
           "Gradio Blocks Party[[gradio-blocks-party]]\n\nAlong with the release of the Gradio chapter of the cour..."
          ],
          [
           "Training a new tokenizer from an old one[[training-a-new-tokenizer-from-an-old-one]]\n\n<CourseFloatin..."
          ],
          [
           "<Youtube id=\"DJimQynXZsQ\"/>\n\n<Tip warning={true}>\n\nâš ï¸ Training a tokenizer is not the same as traini..."
          ],
          [
           "```py\nfrom datasets import load_dataset\n\n# This can take a few minutes to load, so grab a coffee or ..."
          ],
          [
           "```\n\nWe can have a look at the training split to see which columns we have access to:\n\n```py\nraw_dat..."
          ],
          [
           "```\n\nThe first thing we need to do is transform the dataset into an _iterator_ of lists of texts -- ..."
          ],
          [
           "```\n\nwe get them once and then an empty list:\n\n```python out\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[]\n```\n\n..."
          ],
          [
           "```\n\nEven though we are going to train a new tokenizer, it's a good idea to do this to avoid startin..."
          ],
          [
           "```\n\nThis tokenizer has a few special symbols, like `Ä ` and `ÄŠ`, which denote spaces and newlines, r..."
          ],
          [
           "```\n\nThis command might take a bit of time if your corpus is very large, but for this dataset of 1.6..."
          ],
          [
           "Most of the Transformer models have a fast tokenizer available (there are some exceptions that you c..."
          ],
          [
           "```\n\n```python out\n['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä \"\"\"', 'Add',..."
          ],
          [
           "```\n\n```python out\n['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self'..."
          ],
          [
           "```\n\nIn addition to the token corresponding to an indentation, here we can also see a token for a do..."
          ],
          [
           "```\n\nOnce you've logged in, you can push your tokenizer by executing the following command:\n\n```py\nt..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n<!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-..."
          ],
          [
           "### 3. How does the BERT model expect a pair of sentences to be processed?\n\n<Question\n\tchoices={[\n\t\t..."
          ],
          [
           "### 5. What does dynamic padding mean?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It's when you pad the in..."
          ],
          [
           "### 6. What is the purpose of a collate function?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It ensures al..."
          ],
          [
           "<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"Nothing, but you get a warning.\",\n\t\t\texplain: \"You do get a warn..."
          ],
          [
           "### 9. Why should you use the ðŸ¤— Accelerate library?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It provides..."
          ],
          [
           "### 5. The TensorFlow models from `transformers` are already Keras models. What benefit does this of..."
          ],
          [
           "is why we didn't see it in our results in the first slide. On top of the label and the probability, ..."
          ],
          [
           "group together tokens that correspond to the same entity. This is why we had two labels for each typ..."
          ],
          [
           "both cases, we can flag a new entity each time we see a new label appearing (either with the I or B ..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning a model with Keras[[fine-tuning-a-model-with-keras]]\n\n<..."
          ],
          [
           "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n\ndata_collator = DataCollator..."
          ],
          [
           "```\n\n### Training[[training]]\n\nTensorFlow models imported from ðŸ¤— Transformers are already Keras mode..."
          ],
          [
           "```\n\nYou will notice that unlike in [Chapter 2](/course/chapter2), you get a warning after instantia..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nNote a very common pitfall here â€” you *can* just pass the name of the los..."
          ],
          [
           "```py\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\n\nbatch_size = 8\nnum_epochs =..."
          ],
          [
           "```\n\n<Tip>\n\nThe ðŸ¤— Transformers library also has a `create_optimizer()` function that will create an ..."
          ],
          [
           "```\n\nWe can convert these logits into the model's class predictions by using `argmax` to find the hi..."
          ],
          [
           "back feature to the examples that they originated from. If you don't want to compute the validation ..."
          ],
          [
           "you want to evaluate. We will run a manual evaluation loop, so we create a PyTorch DataLoader with o..."
          ],
          [
           "just take the best index for the start and end logits and be done, but if our model predicts somethi..."
          ],
          [
           "ones. We ignore the logits that spawn impossible answers or answer that are too long. As we saw in t..."
          ],
          [
           "picking for each the answer with the best logit score in all the features the example generated. Now..."
          ],
          [
           "Asking for help on the forums[[asking-for-help-on-the-forums]]\n\n<CourseFloatingBanner chapter={8}\n  ..."
          ],
          [
           "On the lefthand side you can see all the categories that the various topics are grouped into, while ..."
          ],
          [
           "```python\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_checkpoint = \"distilbert-base-unc..."
          ],
          [
           "```\n\nNow suppose we try to embed a whole section of the [Wikipedia article](https://en.wikipedia.org..."
          ],
          [
           "The Transformers TV series began around the same time. Produced by Sunbow\nProductions and Marvel Pro..."
          ],
          [
           "```\n\n```python output\nIndexError: index out of range in self..."
          ],
          [
           "```\n\nUh-oh, we've hit a problem -- and the error message is far more cryptic than the ones we saw in..."
          ],
          [
           "Although this topic contains the error message we need help with, there are a few problems with the ..."
          ],
          [
           "### Formatting your code snippets[[formatting-your-code-snippets]]\n\nReading source code is hard enou..."
          ],
          [
           "### Including the full traceback[[including-the-full-traceback]]\n\nSince the last line of the traceba..."
          ],
          [
           "### Providing a reproducible example[[providing-a-reproducible-example]]\n\nIf you've ever tried to de..."
          ],
          [
           "n this video, we will see how to debug an error you encounter when running trainer.train(). As an ex..."
          ],
          [
           "us there is a problem there, but the problem could come from many different causes. To debug an erro..."
          ],
          [
           "a problem there as we see texts and not numbers. The error message was telling us the model did not ..."
          ],
          [
           "can confirm this by asking the Trainer to get us a batch of the training data loader, which reproduc..."
          ],
          [
           "to do that. So let's fix the issue and run again. This time we get a nasty CUDA error. They are very..."
          ],
          [
           "is because everything that happens on the GPU is done asynchronously: when you execute the model cal..."
          ],
          [
           "hasn't finished the forward pass of the model since all that took no time at all. The CPU stops movi..."
          ],
          [
           "a traceback we can trust this time. As we said before, the error happens during the forward pass of ..."
          ],
          [
           "when we create the model. Now the training script will run to completion! We did not need it yet, bu..."
          ],
          [
           "Encoder models[[encoder-models]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-1..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Token classification[[token-classification]]\n\n{#if fw === 'pt'}\n\n<C..."
          ],
          [
           "{/if}\n\nThe first application we'll explore is token classification. This generic task encompasses an..."
          ],
          [
           "<iframe src=\"https://course-demos-bert-finetuned-ner.hf.space\" frameBorder=\"0\" height=\"350\" title=\"G..."
          ],
          [
           "## Preparing the data[[preparing-the-data]]\n\nFirst things first, we need a dataset suitable for toke..."
          ],
          [
           "```\n\nThis will download and cache the dataset, like we saw in [Chapter 3](/course/chapter3) for the ..."
          ],
          [
           "```\n\n```python out\n[3, 0, 7, 0, 0, 0, 7, 0, 0]\n```\n\nThose are the labels as integers ready for train..."
          ],
          [
           "```\n\nWe already saw these labels when digging into the `token-classification` pipeline in [Chapter 6..."
          ],
          [
           "```\n\n```python out\n'EU    rejects German call to boycott British lamb .'\n'B-ORG O       B-MISC O    ..."
          ],
          [
           "```\n\nYou can replace the `model_checkpoint` with any other model you prefer from the [Hub](https://h..."
          ],
          [
           "```\n\nAs we can see, the tokenizer added the special tokens used by the model (`[CLS]` at the beginni..."
          ],
          [
           "```\n\n```python out\n[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n```\n\nWith a tiny bit of work, we can t..."
          ],
          [
           "```\n\nAs we can see, our function added the `-100` for the two special tokens at the beginning and th..."
          ],
          [
           "```\n\nNote that we haven't padded our inputs yet; we'll do that later, when creating the batches with..."
          ],
          [
           "```\n\nWe've done the hardest part! Now that the data has been preprocessed, the actual training will ..."
          ],
          [
           "```\n\n{/if}\n\nTo test this on a few samples, we can just call it on a list of examples from our tokeni..."
          ],
          [
           "```\n\n```python out\n[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n[-100, 1, 2, -100]\n```\n\n{#if fw === 'p..."
          ],
          [
           "```\n\n\n Next stop: the model itself.\n\n{/if}\n\n{#if fw === 'tf'}\n\n### Defining the model[[defining-the-..."
          ],
          [
           "```\n\n```python out\n9\n```\n\n<Tip warning={true}>\n\nâš ï¸ If you have a model with the wrong number of labe..."
          ],
          [
           "```\n\nAfter logging in, we can prepare everything we need to compile our model. ðŸ¤— Transformers provid..."
          ],
          [
           "```\n\nNote also that we don't supply a `loss` argument to `compile()`. This is because the models can..."
          ],
          [
           "```\n\nYou can specify the full name of the repository you want to push to with the `hub_model_id` arg..."
          ],
          [
           "```\n\nWe can then load it via the `evaluate.load()` function like we did in [Chapter 3](/course/chapt..."
          ],
          [
           "```\n\nNote that the metric takes a list of predictions (not just one) and a list of labels. Here's th..."
          ],
          [
           "```\n\n{#if fw === 'pt'}\n\nThis is sending back a lot of information! We get the precision, recall, and..."
          ],
          [
           "```\n\nNow that this is done, we are almost ready to define our `Trainer`. We just need a `model` to f..."
          ],
          [
           "```\n\n\n```python out\n{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},\n 'MISC'..."
          ],
          [
           "```\n\nNow we can just pass them to the `AutoModelForTokenClassification.from_pretrained()` method, an..."
          ],
          [
           "```\n\nOnce this is done, we can define our `TrainingArguments`:\n\n```python\nfrom transformers import T..."
          ],
          [
           "```\n\nYou've seen most of those before: we set some hyperparameters (like the learning rate, the numb..."
          ],
          [
           "```\n\nNote that while the training happens, each time the model is saved (here, every epoch) it is up..."
          ],
          [
           "```\n\nThe `Trainer` also drafts a model card with all the evaluation results and uploads it. At this ..."
          ],
          [
           "```\n\nOnce we have all those objects, we can send them to the `accelerator.prepare()` method:\n\n```py\n..."
          ],
          [
           "```\n\n```python out\n'sgugger/bert-finetuned-ner-accelerate'\n```\n\nThen we can clone that repository in..."
          ],
          [
           "```\n\nThen we can write the training loop. After defining a progress bar to follow how training goes,..."
          ],
          [
           "predictions = outputs.logits.argmax(dim=-1)\n        labels = batch[\"labels\"]\n\n        # Necessary to..."
          ],
          [
           "```\n\nThe first line is self-explanatory: it tells all the processes to wait until everyone is at tha..."
          ],
          [
           "```\n\n```python out\n[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end..."
          ],
          [
           "n this video we take a look at setting up a custom loss function for training. In the default loss f..."
          ],
          [
           "other rules. For each sample we get a loss value during training and we can combine that loss with a..."
          ],
          [
           "see a loss function that does exactly that for causal language modeling. It takes the models it take..."
          ],
          [
           "over the batch. With the view we unflatten the tensor to get a matrix with a row for each sample in ..."
          ],
          [
           "get the information for each keyword in a separate matrix. Only want to know how many times keywords..."
          ],
          [
           "weighted loss. Letâ€™s see how we can make use of that custom loss with Accelerate and the Trainer In ..."
          ],
          [
           "you can integrate your own awesome loss function with both the trainer and accelerates...."
          ],
          [
           "Time to slice and dice[[time-to-slice-and-dice]]\n\n<CourseFloatingBanner chapter={5}\n  classNames=\"ab..."
          ],
          [
           "First we need to download and extract the data, which can be done with the `wget` and `unzip` comman..."
          ],
          [
           "```\n\nSince TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can lo..."
          ],
          [
           "```\n\n```python out\n{'Unnamed: 0': [87571, 178045, 80482],\n 'drugName': ['Naproxen', 'Duloxetine', 'M..."
          ],
          [
           "```\n\nNote that we've fixed the seed in `Dataset.shuffle()` for reproducibility purposes. `Dataset.se..."
          ],
          [
           "```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['patient_id', 'drugName', '..."
          ],
          [
           "```\nlambda <arguments> : <expression>\n```\n\nwhere `lambda` is one of Python's special [keywords](http..."
          ],
          [
           "```\n\n```python out\n['left ventricular dysfunction', 'adhd', 'birth control']\n```\n\nIt works! Now that..."
          ],
          [
           "```\n\nAs expected, we can see a `review_length` column has been added to our training set. We can sor..."
          ],
          [
           "```\n\n```python out\n{'train': 138514, 'test': 46108}\n```\n\nAs you can see, this has removed around 15%..."
          ],
          [
           "```\n\nAs you can see, the `Dataset.map()` method is quite useful for processing data -- and we haven'..."
          ],
          [
           "```\n\nIf you're running this code in a notebook, you'll see that this command executes way faster tha..."
          ],
          [
           "```\n\nYou can also time a whole cell by putting `%%time` at the beginning of the cell. On the hardwar..."
          ],
          [
           "`Dataset.map()` also has some parallelization capabilities of its own. Since they are not backed by ..."
          ],
          [
           "```\n\nYou can experiment a little with timing to determine the optimal number of processes to use; in..."
          ],
          [
           "<Tip>\n\nðŸ’¡ In machine learning, an _example_ is usually defined as the set of _features_ that we feed ..."
          ],
          [
           "```\n\nLet's test this on one example before using `Dataset.map()` on the whole dataset:\n\n```py\nresult..."
          ],
          [
           "```\n\n```python out\nArrowInvalid: Column 1 named condition expected length 1463 but got length 1000\n`..."
          ],
          [
           "```\n\n```python out\n(206772, 138514)\n```\n\nWe mentioned that we can also deal with the mismatched leng..."
          ],
          [
           "```\n\nWe get the same number of training features as before, but here we've kept all the old fields. ..."
          ],
          [
           "```\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></..."
          ],
          [
           "Let's create a `pandas.DataFrame` for the whole training set by selecting all the elements of `drug_..."
          ],
          [
           "```\n\n<Tip>\n\nðŸš¨ Under the hood, `Dataset.set_format()` changes the return format for the dataset's `__..."
          ],
          [
           "```\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></..."
          ],
          [
           "```\n\n## Creating a validation set[[creating-a-validation-set]]\n\nAlthough we have a test set we could..."
          ],
          [
           "```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['patient_id', 'drugName', '..."
          ],
          [
           "```\n\nGreat, we've now prepared a dataset that's ready for training some models on! In [section 5](/c..."
          ],
          [
           "```\n\nwhere we can see that each split is associated with its own *dataset.arrow* table, and some met..."
          ],
          [
           "```\n\nThis saves each split in [JSON Lines format](https://jsonlines.org), where each row in the data..."
          ],
          [
           "```\n\nAnd that's it for our excursion into data wrangling with ðŸ¤— Datasets! Now that we have a cleaned..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={3}..."
          ],
          [
           "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingB..."
          ],
          [
           "```\n\nWhich of the following commands will produce a random sample of 50 elements from `dataset`?\n\n<Q..."
          ],
          [
           "<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"<code>pets_dataset.filter(lambda x : x['name'].startswith('L'))<..."
          ],
          [
           "### 5. Which of the following are the main benefits of memory mapping?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t..."
          ],
          [
           "```\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It tries to stream a dataset that's too large to fit in RAM..."
          ],
          [
           "### 7. Which of the following are the main benefits of creating a dataset card?\n\n<Question\n\tchoices=..."
          ],
          [
           "### 9. For asymmetric semantic search, you usually have:\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"A shor..."
          ],
          [
           "hy are fast tokenizers called fast? In this video we will see exactly how much faster the so-called ..."
          ],
          [
           "use_fast=False to define the slow one. In a notebook, we can time the execution of a cell with the t..."
          ],
          [
           "text at a time is like sending a cargo ship between two continents with just one container, it's ver..."
          ],
          [
           "fast. And this is only for tokenizing texts. If you ever need to train a new tokenizer, they do this..."
          ],
          [
           "Part 2 completed![[part-2-completed]]\n\n<CourseFloatingBanner\n    chapter={8}\n    classNames=\"absolut..."
          ],
          [
           "n a lot of our examples, you're going to see DataCollators popping up over and over. They're used in..."
          ],
          [
           "example, when you're doing sequence classification, all you really need from your data collator is t..."
          ],
          [
           "You'll see these approaches used in the examples and notebooks throughout this course. In both cases..."
          ],
          [
           "of the time, and so are often totally unaware that this option exists. This is a valuable lesson abo..."
          ],
          [
           "need any special processing before being ready for training. Most sequence classification tasks, for..."
          ],
          [
           "are the same length then you can use the even simpler DefaultDataCollator, but it'll give you an err..."
          ],
          [
           "labels are also a sequence of tokens that can have variable length. In both of these cases, we handl..."
          ],
          [
           "do in NLP, and secondly because it has two modes that do two very different things. You choose which..."
          ],
          [
           "When you set mlm to True, though, you get quite different behaviour! That's because masked language ..."
          ],
          [
           "some tokens with a masking token, other tokens with a random token and then keep a third set of toke..."
          ],
          [
           "Integrations with the Hugging Face Hub[[integrations-with-the-hugging-face-hub]]\n\n<CourseFloatingBan..."
          ],
          [
           "```py\nimport gradio as gr\n\ntitle = \"GPT-J-6B\"\ndescription = \"Gradio Demo for GPT-J 6B, a transformer..."
          ],
          [
           "```\n    \nThe code above will produce the interface below:\n\n<iframe src=\"https://course-demos-gpt-j-6..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-remove-bg-original.hf.space\" frameBorder=\"0\" height=\"650\" tit..."
          ],
          [
           "```\n\n<iframe src=\"https://course-demos-Remove-bg.hf.space\" frameBorder=\"0\" height=\"550\" title=\"Gradi..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Fine-tuning a model with the Trainer API[[fine-tuning-a-model-with-..."
          ],
          [
           "raw_datasets = load_dataset(\"glue\", \"mrpc\")\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokeniz..."
          ],
          [
           "```\n\n### Training[[training]]\n\nThe first step before we can define our `Trainer` is to define a `Tra..."
          ],
          [
           "```\n\nYou will notice that unlike in [Chapter 2](/course/chapter2), you get a warning after instantia..."
          ],
          [
           "```\n\nThis will start the fine-tuning (which should take a couple of minutes on a GPU) and report the..."
          ],
          [
           "```\n\n```python out\n(408, 2) (408,)\n```\n\nThe output of the `predict()` method is another named tuple ..."
          ],
          [
           "```\n\n```python out\n{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}\n```\n\nThe exact results..."
          ],
          [
           "```\n\nNote that we create a new `TrainingArguments` with its `evaluation_strategy` set to `\"epoch\"` a..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Tokenizers[[tokenizers]]\n\n{#if fw === 'pt'}\n\n<CourseFloatingBanner ..."
          ],
          [
           "```\nJim Henson was a puppeteer\n```\n\nHowever, models can only process numbers, so we need to find a w..."
          ],
          [
           "```\n\nThere are also variations of word tokenizers that have extra rules for punctuation. With this k..."
          ],
          [
           "<Youtube id=\"ssLq_EK2jLE\"/>\n\nCharacter-based tokenizers split the text into characters, rather than ..."
          ],
          [
           "## Subword tokenization[[subword-tokenization]]\n\n<Youtube id=\"zHvTiHr506c\"/>\n\nSubword tokenization a..."
          ],
          [
           "### And more![[and-more]]\n\nUnsurprisingly, there are many more techniques out there. To name a few:\n..."
          ],
          [
           "```\n\n{#if fw === 'pt'}\nSimilar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokeni..."
          ],
          [
           "```\n\nSaving a tokenizer is identical to saving a model:\n\n```py\ntokenizer.save_pretrained(\"directory_..."
          ],
          [
           "```\n\nThe output of this method is a list of strings, or tokens:\n\n```python out\n['Using', 'a', 'trans..."
          ],
          [
           "```\n\n```python out\n'Using a Transformer network is simple'\n```\n\nNote that the `decode` method not on..."
          ],
          [
           "Building a model card[[building-a-model-card]]\n\n<CourseFloatingBanner\n    chapter={4}\n    classNames..."
          ],
          [
           "Let's take a look at what each of these sections should contain.\n\n### Model description[[model-descr..."
          ],
          [
           "### Evaluation results[[evaluation-results]]\n\nFinally, provide an indication of how well the model p..."
          ],
          [
           "```\n---\nlanguage: fr\nlicense: mit\ndatasets:\n- oscar\n---\n```\n\nThis metadata is parsed by the Hugging ..."
          ],
          [
           "Byte-Pair Encoding tokenization[[byte-pair-encoding-tokenization]]\n\n<CourseFloatingBanner chapter={6..."
          ],
          [
           "```\n\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n```\n\nThe base vocabulary will then be `[\"b\", \"g\", \"h\", \"n\", \"..."
          ],
          [
           "```\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```\n\nmeaning `\"hug\"` was present 1..."
          ],
          [
           "```\n\nNow we have some pairs that result in a token longer than two characters: the pair `(\"h\", \"ug\")..."
          ],
          [
           "```\n\nAnd we continue like this until we reach the desired vocabulary size.\n\n<Tip>\n\nâœï¸ **Now your tur..."
          ],
          [
           "```\n\nNext, we need to pre-tokenize that corpus into words. Since we are replicating a BPE tokenizer ..."
          ],
          [
           "```\n\nThe next step is to compute the base vocabulary, formed by all the characters used in the corpu..."
          ],
          [
           "```\n\nLet's have a look at a part of this dictionary after the initial splits:\n\n```python\npair_freqs ..."
          ],
          [
           "```\n\nAnd we can have a look at the result of the first merge:\n\n```py\nsplits = merge_pair(\"Ä \", \"t\", s..."
          ],
          [
           "```\n\nAs a result, we've learned 19 merge rules (the initial vocabulary had a size of 31 -- 30 charac..."
          ],
          [
           "```\n\nAnd the vocabulary is composed of the special token, the initial alphabet, and all the results ..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ Using `train_new_from_iterator()` on the same corpus won't result in the exact same vo..."
          ],
          [
           "```\n\n```python out\n['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']\n```\n\n<Tip warning={true}..."
          ],
          [
           "Live sessions and workshops[[live-sessions-and-workshops]]\n\nFor the release of parts 1 and 2 of the ..."
          ],
          [
           "<div class=\"flex justify-center\">\n<Youtube id=\"Ihgk8kGLpIE\"/>\n</div>\n\nFor the second workshop, Merve..."
          ],
          [
           "sing the Python debugger in a terminal. In this video, we'll learn how to use the Python debugger in..."
          ],
          [
           "here but you will get the same error with TensorFlow. As we have seen in the \"How to debug an error?..."
          ],
          [
           "command, you are sent to the first instruction of your script. You can run just the next instruction..."
          ],
          [
           "is p, for print. It allows you to print any value you want. For instance here, we can see the value ..."
          ],
          [
           "can actually confirm by printing the sizes. No wonder the tokenizer wasn't able to create a tensor w..."
          ],
          [
           "in the script. It will interrupt the execution and launch the Python debugger at this place, and we ..."
          ],
          [
           "n this video, we will learn the first things to do when you get an error. Let's say we want to use t..."
          ],
          [
           "is that Python shows you with a clear arrow the line of code that triggered the error. So you don't ..."
          ],
          [
           "KeyError we see displayed. Note that Python tells you exactly where the functions it's executing liv..."
          ],
          [
           "it's telling us it doesn't know the question answering task, and helpfully gives us the list of supp..."
          ],
          [
           "telling us that we should check our model is a correct model identifier, so let's hop on to hf.co/mo..."
          ],
          [
           "Mastering NLP[[mastering-nlp]]\n\n<CourseFloatingBanner\n    chapter={7}\n    classNames=\"absolute z-10 ..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Semantic search with FAISS[[semantic-search-with-faiss]]\n\n{#if fw =..."
          ],
          [
           "<Youtube id=\"OATCgQtNX2o\"/>\n\n## Using embeddings for semantic search[[using-embeddings-for-semantic-..."
          ],
          [
           "```\n\n```python out\nDataset({\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '..."
          ],
          [
           "```\n\n```python out\nDataset({\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', '..."
          ],
          [
           "```\n\n```python out\nDataset({\n    features: ['html_url', 'title', 'comments', 'body'],\n    num_rows: ..."
          ],
          [
           "```\n\nIf we inspect the first row in this `DataFrame` we can see there are four comments associated w..."
          ],
          [
           "<table border=\"1\" class=\"dataframe\" style=\"table-layout: fixed; word-wrap:break-word; width: 100%;\">..."
          ],
          [
           "<td>Hello,\\r\\nI am trying to run run_glue.py and it gives me this error...</td>\n    </tr>\n    <tr>\n ..."
          ],
          [
           "Great, we can see the rows have been replicated, with the `comments` column containing the individua..."
          ],
          [
           "```\n\n```python out\nDataset({\n    features: ['html_url', 'title', 'comments', 'body'],\n    num_rows: ..."
          ],
          [
           "```\n\nHaving cleaned up our dataset a bit, let's concatenate the issue title, description, and commen..."
          ],
          [
           "```\n\nWe're finally ready to create some embeddings! Let's take a look.\n\n## Creating text embeddings[..."
          ],
          [
           "```\n\n{:else}\n\n```py\nfrom transformers import AutoTokenizer, TFAutoModel\n\nmodel_ckpt = \"sentence-tran..."
          ],
          [
           "```\n\nWe can test the function works by feeding it the first text entry in our corpus and inspecting ..."
          ],
          [
           "```\n\n```python out\nTensorShape([1, 768])\n```\n\nGreat, we've converted the first entry in our corpus i..."
          ],
          [
           "```\n\nWe can now perform queries on this index by doing a nearest neighbor lookup with the `Dataset.g..."
          ],
          [
           "```\n\nNow we can iterate over the first few rows to see how well our query matched the available comm..."
          ],
          [
           "```\n\n```python out\n\"\"\"\nCOMMENT: Requiring online connection is a deal breaker in some cases unfortun..."
          ],
          [
           "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool fea..."
          ],
          [
           "```\n>\n> import datasets\n>\n> data = datasets.load_dataset(...)\n>\n> data.save_to_disk(/YOUR/DATASET/DI..."
          ],
          [
           "```\n\nNot bad! Our second hit seems to match the query.\n\n<Tip>\n\nâœï¸ **Try it out!** Create your own qu..."
          ],
          [
           "he Push to Hub API. Let's have a look at the push_to_hub API. You will need to be logged in with you..."
          ],
          [
           "you can find it in any Transformers tutorial, or by looking at the videos linked below. What interes..."
          ],
          [
           "of the output directory as a repository name. You can pick another name by passing it to the hub_mod..."
          ],
          [
           "start playing with its inference widget while it's training! There is something wrong with the label..."
          ],
          [
           "repo. Going back to the model page, you can see the Trainer included some metadata that is interpret..."
          ],
          [
           "not using the Trainer API to fine-tune your model, you can use the push_to_hub method on the model a..."
          ],
          [
           "done and we can check on the website the model is now showing the proper labels! Now that the model ..."
          ],
          [
           "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingB..."
          ],
          [
           "### 2. What is the advantage of using a generator of lists of texts compared to a list of lists of t..."
          ],
          [
           "### 4. How does the `token-classification` pipeline handle entities that span over several tokens?\n\n..."
          ],
          [
           "### 6. What is normalization?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"It's any cleanup the tokenizer pe..."
          ],
          [
           "### 8. Select the sentences that apply to the BPE model of tokenization.\n\n<Question\n\tchoices={[\n\t\t{\n..."
          ],
          [
           "### 9. Select the sentences that apply to the WordPiece model of tokenization.\n\n<Question\n\tchoices={..."
          ],
          [
           "### 10. Select the sentences that apply to the Unigram model of tokenization.\n\n<Question\n\tchoices={[..."
          ],
          [
           "FrameworkSwitchCourse {fw} />\n\n# Training a causal language model from scratch[[training-a-causal-la..."
          ],
          [
           "{/if}\n\nUp until now, we've mostly been using pretrained models and fine-tuning them for new use case..."
          ],
          [
           "<iframe src=\"https://course-demos-codeparrot-ds.hf.space\" frameBorder=\"0\" height=\"300\" title=\"Gradio..."
          ],
          [
           "However, training on the full corpus is time- and compute-consuming, and we only need the subset of ..."
          ],
          [
           "```\n\nLet's test it on two examples:\n\n```py\nfilters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n..."
          ],
          [
           "```\n\n```python out\n3.26% of data after filtering.\n```\n\nThis leaves us with about 3% of the original ..."
          ],
          [
           "```\n\n<Tip>\n\nPretraining the language model will take a while. We suggest that you first run the trai..."
          ],
          [
           "```\n\nWe can see that the `content` field contains the code that we want our model to train on. Now t..."
          ],
          [
           "<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "```\n\n```python out\nInput IDs length: 34\nInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128..."
          ],
          [
           "```\n\nWe can see that we get 34 segments in total from those two examples. Looking at the chunk lengt..."
          ],
          [
           "```\n\n```python out\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids'],\n        num_r..."
          ],
          [
           "```\n\nWe now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion ..."
          ],
          [
           "{#if fw === 'pt'}\n\n```py\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n\nconfig..."
          ],
          [
           "```\n\nWith that configuration, we can load a new model. Note that this is the first time we don't use..."
          ],
          [
           "```\n\n```python out\n_________________________________________________________________\nLayer (type)   ..."
          ],
          [
           "```\n\n{/if}\n\nOur model has 124M parameters that we'll have to tune. Before we can start training, we ..."
          ],
          [
           "```\n\n{:else}\n\n```python out\ninput_ids shape: (5, 128)\nattention_mask shape: (5, 128)\nlabels shape: (..."
          ],
          [
           "```\n\n{#if fw === 'pt'}\n\nAll that's left to do is configure the training arguments and fire up the `T..."
          ],
          [
           "```\n\nAfter training completes, we can push the model and tokenizer to the Hub:\n\n```py\ntrainer.push_t..."
          ],
          [
           "```\n\n{/if}\n\n<Tip>\n\nâœï¸ **Try it out!** It only took us about 30 lines of code in addition to the `Tra..."
          ],
          [
           "```\n\n{:else}\n\n```py\nfrom transformers import pipeline\n\ncourse_model = TFGPT2LMHeadModel.from_pretrai..."
          ],
          [
           "```\n\nNice, that's the correct answer -- although it then inserts the column `x` again. Since the num..."
          ],
          [
           "```\n\n{#if fw === 'tf'}\n\nLooking at these few examples, it seems that the model has learned some of t..."
          ],
          [
           "<Youtube id=\"Hm8_PgVTFuc\"/>\n\nSince we are mainly interested in sensible autocompletion for the the d..."
          ],
          [
           "```\n\n```python out\n'Keyword has not single token: testtest'\n```\n\nGreat, that seems to work nicely! W..."
          ],
          [
           "```\n\nBefore we can start training with this awesome new loss function, we need to prepare a few thin..."
          ],
          [
           "```\n\nSince we want to evaluate the model regularly on the validation set during training, let's writ..."
          ],
          [
           "```\n\n<Tip>\n\nðŸš¨ If you're training on a TPU, you'll need to move all the code starting at the cell abo..."
          ],
          [
           "```\n\n```python out\n'sgugger/codeparrot-ds-accelerate'\n```\n\nThen we can clone that repository in a lo..."
          ],
          [
           "```\n\nThose are very high values for loss and perplexity, but that's not surprising as we haven't tra..."
          ],
          [
           "```py\nfrom tqdm.notebook import tqdm\n\ngradient_accumulation_steps = 8\neval_steps = 5_000\n\nmodel.trai..."
          ],
          [
           "```\n\nAnd that's it -- you now have your own custom training loop for causal language models such as ..."
          ],
          [
           "ow to ask a question on the Hugging Face forums?\n\nIf you have a general question or are looking to d..."
          ],
          [
           "For this example, we will use the following code,\n\nthat produces an error, as we saw in the \"What to..."
          ],
          [
           "hat is domain adaptation? When fine-tuning a pretrained model on a new dataset, the fine-tuned model..."
          ],
          [
           "the pretrained distilBERT model with the version fine-tuned in chapter 7 of the course (linked below..."
          ],
          [
           "is another example on a translation task. On top we use a pretrained French/English model and at the..."
          ]
         ],
         "hovertemplate": "source=course<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "course, circle",
         "marker": {
          "color": "#FF97FF",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "course, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -2.3688421,
          -2.0621235,
          -2.2888043,
          -1.5851471,
          -1.6194971,
          -1.5722153,
          1.3767625,
          1.2055178,
          1.1054924,
          1.5341334,
          1.4012436,
          0.41265717,
          -0.9529707,
          3.2224388,
          2.1998482,
          1.6657202,
          -2.3305016,
          -0.921475,
          -0.30114195,
          -3.7962296,
          -3.7339303,
          -3.8111846,
          -3.7781422,
          -3.7021048,
          -3.8943155,
          -3.9975235,
          -3.8333583,
          -3.7916448,
          -3.7643988,
          -3.9057238,
          -3.9345284,
          -2.924474,
          -4.5917196,
          -5.008612,
          -3.3446672,
          -3.5886192,
          -3.2617652,
          -5.2467833,
          6.3435054,
          9.798584,
          10.338247,
          10.7118,
          9.866923,
          -6.5597878,
          -6.729845,
          -6.58867,
          -6.8015685,
          -6.70166,
          -6.674993,
          -5.866995,
          5.1344757,
          5.188138,
          4.6051264,
          4.771285,
          2.0366735,
          4.4014854,
          2.7527702,
          2.0904043,
          1.7836939,
          5.418807,
          1.6064367,
          2.303988,
          2.1830688,
          -3.060093,
          1.832125,
          -2.258961,
          1.6035436,
          -0.6436822,
          2.244569,
          -0.35900626,
          -0.3994619,
          -0.9328908,
          -2.2141879,
          -4.068562,
          -4.4554286,
          -5.039572,
          -4.9264836,
          -4.18168,
          -4.7061477,
          -6.142317,
          -2.0262065,
          -2.4022255,
          -3.2307308,
          -3.6303728,
          -3.504953,
          -0.6597283,
          0.32868004,
          0.19509372,
          0.20813,
          0.10253689,
          0.50281394,
          11.831837,
          11.543416,
          11.816532,
          11.472405,
          11.524968,
          -5.251706,
          -4.974619,
          5.3545055,
          -3.3066218,
          3.3453093,
          3.5598626,
          1.3859824,
          1.9031901,
          3.2996235,
          3.2738817,
          3.4784396,
          4.473198,
          3.947676,
          3.8762898,
          4.4810314,
          3.9793885,
          4.3684707,
          4.1179996,
          4.0934544,
          3.9300146,
          4.3337364,
          4.1026206,
          4.255178,
          4.4939137,
          -3.9556615,
          -3.7345726,
          -3.413894,
          -5.169683,
          -6.0801444,
          -4.644169,
          -1.9037403,
          -3.3124278,
          5.4669876,
          -4.938264,
          -2.2487576,
          -4.662636,
          -0.56836,
          6.8105597,
          6.840486,
          -4.9968,
          -2.195872,
          -2.1495152,
          -2.4889026,
          -0.9622435,
          -2.0076334,
          -1.33094,
          -3.5373685,
          -3.302629,
          -4.3033986,
          -1.3137856,
          -1.3042945,
          6.7419744,
          -3.733432,
          0.80664665,
          0.08930145,
          -0.2701485,
          -0.5216368,
          -0.46540204,
          -2.7079582,
          -2.2916605,
          -0.4421903,
          -4.2328134,
          -0.6985865,
          0.94790965,
          0.079376906,
          -4.798465,
          -3.4988549,
          -0.9278631,
          -0.49770826,
          -1.2526946,
          -1.6195803,
          11.884897,
          11.110667,
          11.038763,
          10.321862,
          11.28737,
          11.396002,
          10.730143,
          10.175251,
          -3.8744555,
          -3.7957401,
          -3.6824932,
          -3.6740067,
          -3.6636684,
          -3.8223777,
          -3.850939,
          -6.976064,
          -0.39216158,
          -0.94811136,
          -1.2270899,
          -0.8590885,
          -1.4057817,
          -1.4543977,
          -1.493429,
          -1.0804236,
          -1.1435963,
          -1.3520701,
          -1.9018283,
          -1.933078,
          5.640113,
          5.0956955,
          5.138285,
          3.9695432,
          2.0136776,
          3.3819695,
          2.7603498,
          2.632248,
          2.8880765,
          2.620893,
          -5.4391203,
          3.3873303,
          2.7155962,
          1.3550617,
          2.7154453,
          1.5075629,
          2.5514045,
          2.8280041,
          3.095621,
          -0.3244949,
          -1.3222063,
          0.01741093,
          -1.1572967,
          -1.2023373,
          -1.2549794,
          -1.02825,
          -1.0846876,
          -0.36515704,
          -0.5654281,
          1.4331895,
          0.23009874,
          0.8607101,
          1.4016688,
          0.7588807,
          -7.0291924,
          -6.744001,
          -7.3188543,
          -7.3895326,
          -6.1116166,
          -6.55054,
          -6.47813,
          -7.1307707,
          -7.2660203,
          -6.925288,
          -2.1809824,
          -1.1827219,
          -1.0373889,
          -1.1502457,
          -0.58034927,
          -0.35746175,
          0.0025198017,
          -0.5225443,
          -3.8621578,
          -3.718179,
          -3.7911637,
          -3.6243029,
          -3.5383782,
          -3.8034046,
          -3.708481,
          -2.9764638,
          -2.4233494,
          -3.2776287,
          -2.2421,
          -2.0440688,
          -2.3631961,
          -3.6200235,
          3.893825,
          -1.608216,
          3.237259,
          3.32671,
          3.579635,
          4.042283,
          -2.4082448,
          -2.896962,
          -2.850542,
          -1.3897176,
          -1.9456432,
          -1.5696783,
          -2.0781994,
          -1.1751524,
          -1.763678,
          -1.1780714,
          -3.0942695,
          -2.263804,
          -6.3342967,
          -7.378241,
          -7.2449074,
          -6.255471,
          -6.3793635,
          -6.393213,
          -6.5985503,
          -6.1861167,
          -6.2587223,
          -6.3132496,
          -6.081955,
          -5.490657,
          -5.255141,
          -4.23541,
          -4.648526,
          -6.9210863,
          -6.672138,
          -6.759745,
          -3.0394204,
          3.592442,
          3.7393012,
          3.9993827,
          3.6112907,
          4.3404713,
          -2.546836,
          -3.3390365,
          -3.4291105,
          -1.9188954,
          -6.627176,
          -6.6802044,
          -6.466621,
          -7.29238,
          -6.6131005,
          -6.090368,
          -5.778329,
          -5.0040803,
          -3.5002506,
          -3.7527606,
          -5.5044866,
          -5.421111,
          -4.682972,
          1.4425502,
          -3.3010406,
          -3.4747288,
          -3.4862916,
          -2.8025072,
          -1.9514784,
          -1.1646429,
          -2.9831333,
          -1.2434303,
          -1.3599055,
          -0.63473475,
          -1.8067032,
          -1.7044982,
          -0.53297955,
          -1.2748684,
          -0.1514268,
          -1.4671472,
          -1.1223111,
          -0.72991544,
          -0.60614806,
          1.2240762,
          -0.94310915,
          0.61747885,
          -0.57699686,
          1.5342579,
          -1.1050626,
          -0.19314559,
          3.2354577,
          1.4400667,
          0.08761459,
          -0.56648654,
          -2.7313623,
          -5.2650714,
          -3.607312,
          1.8736243,
          -4.4048834,
          -5.1477337,
          -5.18204,
          -1.0977899,
          -0.94999295,
          -1.0570432,
          -1.2832288,
          -3.1776478,
          -2.248841,
          6.9123197,
          -2.1414587,
          -1.6998019,
          -2.2523768,
          -1.7199049,
          -2.06444,
          6.9032125,
          -2.2263756,
          -1.7972857,
          -0.8400486,
          -1.6198407,
          -0.9557729,
          0.7978768,
          -1.3910508,
          -2.5609016,
          -0.2680614,
          -1.1593155,
          -1.5067239,
          -0.947523,
          -0.31624645,
          -0.3138746,
          -0.3504847,
          -0.26471666,
          0.06005686,
          -0.077680476,
          -0.6122269,
          -0.65235364,
          0.9969789,
          -0.22174534,
          0.6777083,
          -1.1169645,
          -0.9833085,
          3.2369032,
          1.3352071,
          0.096590795,
          0.13325928,
          -1.7200367,
          -0.58137655,
          4.7595606,
          0.9372451,
          0.1661371,
          0.79371643,
          5.055073,
          -3.2180116,
          5.082318,
          4.1037793,
          4.827525,
          -3.9186404,
          -3.8443758,
          -3.716236,
          -3.7904327,
          -3.804246,
          -3.7652981,
          -3.6925318,
          -3.724446,
          -3.8167658,
          -3.7491965,
          -3.7325366,
          -3.7212517,
          -3.82294,
          3.1022353,
          4.2238483,
          4.341883,
          4.151601,
          5.5190763,
          4.1099453,
          5.2001724,
          4.499512,
          1.3909588,
          4.0106316,
          4.100686,
          3.9998496,
          4.091961,
          3.9936903,
          4.19974,
          5.3232927,
          2.9789445,
          3.5640378,
          3.4234068,
          1.6954211,
          3.5247679,
          3.5684302,
          -3.4488356,
          -4.1830196,
          -4.1160755,
          -2.7984693,
          -3.6082525,
          -2.9812708,
          -3.3159175,
          -3.9571483,
          -1.8811972,
          -2.222444,
          -1.7459394,
          -3.5698981,
          0.7992247,
          0.90777266,
          -2.6943772,
          -2.5492542,
          -2.3452134,
          -3.3762643,
          -0.6832705,
          -1.2970457,
          -0.11442539,
          -0.8103701,
          -0.72147703,
          -1.2373127,
          -0.8959155,
          -0.5340009,
          -4.895106,
          -3.7687361,
          -2.4137964,
          -2.50415,
          -2.42123,
          -2.3405926,
          -0.96560895,
          -2.024509,
          -2.59315,
          -1.9832709,
          -2.2679265,
          -1.9702489,
          -3.4572005,
          -2.040256,
          11.817689,
          -5.130483,
          -6.2726197,
          -2.919237,
          -2.2161522,
          -1.6460582,
          -1.6811668,
          -1.9654213,
          -2.117398,
          -1.9983034,
          -1.6561182,
          -1.3288902,
          -2.5278764,
          11.305971,
          12.532393,
          11.34106,
          10.538467,
          11.481825,
          -3.8448157,
          -3.892299,
          -3.696149,
          -3.8021894,
          -3.957104,
          -3.7842667,
          -3.071038,
          -2.1920066,
          -3.6575117,
          -3.4944751,
          -3.4721732,
          -2.265994,
          0.28822964,
          -0.30331907,
          -0.4488054,
          -0.35121822,
          0.33028108,
          -3.9394948,
          -3.954079,
          -3.877751,
          -3.351922,
          -3.8559585,
          -3.7245607,
          11.64851,
          7.985147,
          6.802149,
          11.397602,
          11.357833,
          11.0643,
          -0.23509899,
          11.38379,
          -3.986528,
          -4.1322103,
          -0.23873068,
          -4.7596703,
          -4.0209827,
          -2.2723153,
          -1.9955549,
          -1.5633278,
          -0.98353094,
          -1.0271907,
          -1.3506569,
          -2.2877824,
          -5.6905417,
          -2.4148226,
          -2.2016919,
          -2.3077245,
          -1.4885945,
          -1.5192821,
          -0.91997004,
          -1.662734,
          -3.4064827,
          -3.9133449,
          -3.88244,
          -3.6371474,
          -3.7786925,
          -1.1484153,
          -2.2916203,
          -2.472233,
          -1.0150833,
          -2.2494466,
          -2.5269008,
          -1.6947843,
          -0.7497715,
          -2.187124,
          -2.2677112,
          -1.4907088,
          -2.9229088,
          -0.7209749,
          -0.97446036,
          -0.6749153,
          0.021321021,
          -0.28463876,
          -0.54854614,
          -0.13346177,
          0.07134977,
          -0.01702537,
          0.25518242,
          -5.291533,
          -4.5479026,
          -0.88467586,
          -3.748782,
          -2.626962,
          -2.6684127,
          -4.249776,
          -4.160521,
          -4.2255483,
          -2.0737405,
          -3.8846815,
          -0.44006965,
          -0.9841135,
          0.08809454,
          -0.91332006,
          -1.2647303,
          -2.5438023,
          -1.9571402,
          -2.2495697,
          -2.1530528,
          -3.4037008,
          -2.1201465,
          4.056721,
          4.435997,
          3.4548323,
          4.3560123,
          4.3354073,
          4.2525487,
          4.5058613,
          3.6306083,
          3.1915362,
          2.7705548,
          -3.3200316,
          -3.5095062,
          -0.4094082,
          -4.120737,
          -3.3643415,
          -1.9008578,
          -1.3315719,
          -1.667221,
          -2.2739522,
          -0.7050588,
          -6.370074,
          -2.7739186,
          -1.4380492,
          -0.98099,
          -1.6212943,
          -5.027876,
          -3.9744403,
          -4.01637,
          -3.7156746,
          -3.9939818,
          -6.3821173,
          -3.879173,
          -4.0744157,
          6.300894,
          -3.921365,
          -3.8562808,
          -3.8160899,
          -3.4797735,
          -3.6030478,
          -3.9459348,
          -3.913669,
          -3.773152,
          -3.7160707,
          -3.2067335,
          -3.139195,
          -3.3837228,
          -3.6881576,
          -3.6821017,
          -3.433599,
          -3.6859372,
          -3.6727521,
          -3.3180687,
          -3.2931495,
          -4.210805,
          -3.967363,
          -4.314467,
          -4.088955,
          -3.846819,
          -4.148794,
          -4.1528716,
          4.0068474,
          4.309854,
          5.2290263,
          5.4609995,
          4.267095,
          5.354285,
          -6.149753,
          -6.594306,
          -6.1759076,
          -6.975208,
          -5.9535575,
          -5.184678,
          -5.174894,
          2.0149007,
          1.670977,
          3.5754266,
          3.9569168,
          4.69335,
          4.5358405,
          -7.2521253,
          -6.9859724,
          -7.243148,
          -7.2511735,
          -7.094604,
          11.828092,
          11.554462,
          11.4948435,
          11.52243,
          10.482638,
          11.497056,
          11.238732,
          11.330963,
          -4.3355975,
          11.481356,
          11.119138,
          12.005596,
          12.063323,
          11.083254,
          12.129301,
          8.395455,
          2.5465622,
          0.46873385,
          0.71778667,
          1.0893048,
          3.951796,
          1.9825331,
          1.0103639,
          1.4057965,
          2.492206,
          -4.38454,
          -0.99638265,
          2.1134827,
          -3.0743663,
          -3.8406906,
          0.08762446,
          2.0967548,
          1.00295,
          -3.7658048,
          -3.8559225,
          -3.787071,
          -3.5728662,
          -3.9036667,
          -3.6194463,
          -2.5382085,
          -2.7938888,
          -1.9332933,
          -2.8182662,
          -1.7354196,
          -2.6507757,
          -1.527854,
          -2.4928527,
          -2.85435,
          -7.4072447,
          11.625528,
          -3.7870436,
          -3.2776093,
          -3.073203,
          -3.1010127,
          -3.8041527,
          -3.2412543,
          0.88297474,
          -3.9414256,
          -3.945115,
          -0.39101416,
          -4.1049023,
          -1.2548751,
          -2.4292512,
          -1.963594,
          -0.9108761,
          -2.5773675,
          -1.0095774,
          -0.818068,
          -1.9885613,
          -3.194185,
          -1.0235807,
          -2.3708868,
          -3.9081492,
          -3.740241,
          -3.8450453,
          -3.8848855,
          -3.8124118,
          -3.8545878,
          -3.9311597,
          -3.6525304,
          -3.4064016,
          -3.1042247,
          -3.7628574,
          -3.7320976,
          -4.0216737,
          -5.9292784,
          -5.2115593,
          0.7026672,
          6.8299603,
          -0.95591426,
          -0.6910203,
          6.599773,
          -1.289974,
          -1.1763082,
          -6.2384377,
          -6.2537007,
          -6.0636435,
          -6.054265,
          -2.9831011,
          -3.6855159,
          -3.01856,
          -1.8152705,
          -1.6919233,
          -1.6751064,
          -0.91007614,
          -0.2836922,
          -1.3299338,
          -1.611224,
          -1.8187367,
          -2.855032,
          0.7292983,
          0.42729855,
          -1.1202672,
          -0.50719315,
          -1.719993,
          -1.4958944,
          -1.1520399,
          0.73370385,
          2.462562,
          -0.3849564,
          -1.4849128,
          -0.7986244,
          -1.393077,
          -0.22805975,
          -0.09038437,
          2.2529447,
          0.7494357,
          -1.059854,
          1.4139651,
          -0.07396047,
          -0.20877369,
          -1.7916186,
          -2.3014603,
          -2.2377098,
          -2.4074442,
          -3.032508,
          -1.6411124,
          -1.544329,
          -0.91026944,
          -3.4468293,
          -3.8080797,
          -3.5489225,
          -3.49933,
          -3.907017,
          -3.0390553,
          -4.05226,
          -2.8947625,
          -2.6073604,
          -2.347224,
          -2.5579474,
          -2.5109417,
          -2.3209128,
          -5.0732684,
          -2.529675,
          -2.4753323,
          -2.5353155,
          -2.4000328,
          -2.2854822,
          -0.9337696,
          -4.2916727,
          -3.8208766,
          -4.068923,
          -4.092375,
          -3.6470122,
          -2.467317,
          -3.4940085,
          -2.835204,
          -2.9757574,
          -1.4675531,
          -1.713946,
          -1.9796389,
          -3.477241,
          -2.5578618,
          -1.8520881,
          -0.664773,
          -2.5429912,
          -1.4647291,
          -0.7424287,
          -0.72227144,
          -0.5036494,
          -1.4305536,
          -1.7378227,
          -1.1982667,
          -1.1641899,
          -0.5268168,
          -0.5442599,
          -0.19910793,
          7.889445,
          7.948155,
          8.109815,
          8.0877495,
          -1.2303858,
          -1.5896852,
          -1.6258152,
          -1.3765486,
          -2.0075054,
          -1.0536149,
          -1.1922607,
          -0.18382058,
          -1.7033224,
          -2.0544412,
          -2.0466213,
          -1.4146004,
          -0.9473192,
          -0.7483961,
          -0.8593799,
          -0.22983031,
          -0.9449627,
          -0.92284673,
          -0.9146682,
          -0.4214464,
          -1.301552,
          -0.2018819,
          -0.44233856,
          -0.57049906,
          -0.86577857,
          -0.65369755,
          -0.22268334,
          -1.0337298,
          -0.46436706,
          -0.051760055,
          -0.47672236,
          -1.9013184,
          -1.8711112,
          -0.40476006,
          -1.8728503,
          -2.6586342,
          -5.926424,
          -2.692949,
          -4.3954854,
          -6.9984603,
          -7.20176,
          -7.0406036,
          -6.9934993,
          -6.91494,
          -6.747273,
          -6.7232494,
          -6.9401217,
          -6.888346,
          -6.6597896,
          -6.6985097,
          -7.1328287,
          -4.36614,
          -4.1491375,
          -3.5281088,
          -2.8776197,
          -2.7444196,
          -2.7662964,
          -1.6227658,
          -1.8598881,
          -1.8922005,
          -2.2298183,
          -1.7052538,
          -1.765707,
          -1.801984,
          -1.2571666,
          -1.4814883,
          -3.8149467,
          -3.7210574,
          -3.4600873,
          -3.4215097,
          -3.7893279,
          -3.8051567,
          -3.2755702,
          -2.4698582,
          -1.3080419,
          -1.6116856,
          -2.0114572,
          -3.4855583,
          -3.7433515,
          -3.6136322,
          2.3046536,
          2.5129392,
          2.2881598,
          2.3104403,
          2.3470268,
          1.6137908,
          1.2992805,
          -3.9960165,
          -3.9557643,
          3.254338,
          3.7962396,
          1.8281447,
          2.5980303,
          2.5880961,
          -5.284825,
          -2.3965454,
          -2.316938,
          -0.54468507,
          -0.56624854,
          -2.525366,
          -3.1979125,
          -1.7167138,
          -1.5130509,
          -1.9826587,
          -2.2347014,
          -2.4132564,
          -2.0149841,
          -2.1822019,
          -1.0362254,
          -1.2812943,
          -1.803864,
          -1.952812,
          -5.0391955,
          -5.5041122,
          -4.361131,
          -5.957194,
          -6.220644,
          -6.2681866,
          -6.4865456,
          -6.811021,
          4.8380065,
          -5.138599,
          -5.44468,
          -5.5252595,
          -5.752188,
          -6.191991,
          9.187542,
          -5.155445,
          -5.014151,
          2.4791336,
          2.255545,
          2.272365,
          2.023306,
          2.2519534,
          0.515695,
          -0.7909461,
          0.528538,
          1.4941434,
          1.7748532,
          2.1906972,
          -6.664265,
          -5.0955033,
          -2.3634453,
          -1.9786797,
          -2.361928,
          -0.53406924,
          8.337418,
          9.052338,
          -3.5093694,
          -3.0016556,
          -3.5284007,
          -3.2246075,
          -5.392557,
          -5.0171595,
          -4.479782,
          -4.33876,
          -6.46606,
          2.8937356,
          3.0353901,
          2.8200607,
          2.6752923,
          2.2610486,
          2.7285066,
          2.8496277,
          -2.5777507,
          -1.8049574,
          -1.1596074,
          -0.8718226,
          -1.8239259,
          -1.7848879,
          -3.4301174,
          -3.822852,
          -2.3398159,
          -3.3745892,
          -2.5581162,
          -2.8973854,
          -1.6847157,
          -1.253875,
          -4.360184,
          -3.5075085,
          -4.115346,
          -4.2176743,
          -4.310567,
          -4.4159684,
          -3.988597,
          -3.6386147,
          -3.6005402,
          -3.966132,
          -3.8171678,
          -2.8891373,
          -3.6034045,
          -3.8170593,
          11.718311,
          -3.7780554,
          -3.6802232,
          2.3363569,
          0.4693324,
          -1.6468997,
          -2.295613,
          -3.5737658,
          -3.5461402,
          -2.9844759,
          -3.3213212,
          -3.425057,
          0.0476802,
          -3.3987265,
          -3.3521361,
          3.6700263,
          -3.5358284,
          -3.3934798,
          -2.7330353,
          -4.0227604,
          -4.3309746,
          -2.8554895,
          -2.8240826,
          -2.7814686,
          -2.6113458,
          -1.9030055,
          -0.74093044,
          -1.4340222,
          -2.1796987,
          -1.6869444,
          -0.8683669,
          -0.6669795,
          -0.44739848,
          -1.5882065,
          -0.6875462,
          -2.335837,
          -2.0882266,
          -1.818501,
          5.293495,
          4.5036097,
          -1.8117195,
          7.268591,
          7.0132647,
          0.6194958,
          5.5471826,
          4.45568,
          6.041411,
          5.5156546,
          5.673217,
          -1.4410003,
          -1.2074379,
          -1.0588955,
          -1.5680541,
          -0.64292985,
          -1.2798934,
          -0.7465933,
          -0.941715,
          -0.44420812,
          -6.439637,
          -3.7353697,
          -5.079763,
          -5.4388576,
          0.0785441,
          0.34156767,
          0.2444621,
          -2.3079762,
          -3.3127718,
          -3.071827,
          -2.6642911,
          -2.0971394,
          -1.9707564,
          -0.59059066,
          -1.0422242,
          -1.1691245,
          -0.89920187,
          -1.0206747,
          0.813692,
          -0.65818185,
          -1.1235677,
          1.5293736,
          -0.49104103,
          -0.20909746,
          -0.2945606,
          -0.72981554,
          -0.5881497,
          0.38585398,
          -0.46236387,
          1.4596368,
          1.4206767,
          -1.009788,
          0.9180112,
          0.45476684,
          0.30929467,
          -0.61559933,
          -1.2753258,
          -0.59177744,
          -1.8340794,
          -1.627961,
          -2.027653,
          -1.4715854,
          -1.3267376,
          -1.1504444,
          -1.3328499,
          1.5333259,
          1.9990294,
          2.4299123,
          6.5307407,
          0.39836574,
          0.9999121,
          0.7418176,
          -0.63456434,
          0.17806335,
          -0.8446255,
          0.792702,
          -2.4436297,
          -2.3503885,
          -0.5230819,
          -1.9341681,
          -1.8553584,
          -0.5227343,
          0.2336518,
          -0.18781577,
          0.5593738,
          6.5205293,
          0.76706994,
          0.8511833,
          0.8918098,
          1.177133,
          0.8081946,
          3.0055969,
          2.09762,
          2.0890136,
          -4.5856247,
          -0.8638315,
          3.212204,
          1.4611359,
          1.4603891,
          2.4154487,
          2.645243,
          -4.686984,
          -4.678151,
          -3.228343,
          -2.8291721,
          -2.7377656,
          -3.1692853,
          -6.1324096,
          -0.3856252,
          -0.36403665,
          -0.716163,
          4.580596,
          -1.9355168,
          -2.165139,
          -2.549825,
          -4.1473985,
          -5.185734,
          -3.6926308,
          4.5410805,
          11.395752,
          10.579623,
          11.178859,
          11.108907,
          -1.4665673,
          -1.1544362,
          -0.12259899,
          -2.2642946,
          -0.3188507,
          -0.17935291,
          -0.30667293,
          -1.098699,
          -3.6932473,
          -3.5856574,
          -3.902157,
          -3.91827,
          -3.9168506,
          -3.7496047,
          -2.5359352,
          -3.4216347,
          -3.0884266,
          -3.2676835,
          3.255106,
          2.6067092,
          3.1407275,
          -6.867006,
          -3.8458564,
          -3.8015707,
          -3.6809516,
          -3.7572424,
          -3.8120472,
          -3.4182155,
          -2.6375458,
          -1.4123628,
          -1.7528174,
          -1.8395424,
          -2.1329672,
          -3.4404194,
          -3.181516,
          -7.7768345,
          -5.6142964,
          -0.65735114,
          -0.79473156,
          1.4793324,
          -0.27430922,
          -1.2312405,
          1.4840803,
          2.4632168,
          1.8065716,
          2.0976515,
          4.1741223,
          -0.29563287,
          -5.797198,
          -3.787914,
          -5.7347245,
          1.5095377,
          0.86510634,
          0.9963119,
          2.6849873,
          4.759123,
          4.395495,
          1.783864,
          0.5180728,
          0.17024384,
          -4.6418138,
          -2.0588481,
          -1.9383669,
          -1.4457668,
          -0.7395052,
          0.056191947,
          2.991771,
          2.7972658,
          2.893537,
          -0.89517885,
          3.9665139,
          1.6968447,
          1.7225691,
          1.3517329,
          1.8156502,
          1.2660819,
          1.5659289,
          -3.9398115,
          -3.454172,
          -4.2009463,
          -3.964781,
          -4.3069453,
          -4.252543,
          -4.126426,
          -3.6984358,
          -4.0615706,
          11.323928,
          0.5781287,
          1.1589788,
          2.0420914,
          -2.2157657,
          -2.0169032,
          1.2247729,
          -0.40592402,
          -0.97275513,
          0.94351655,
          -2.514317,
          -1.8322408,
          -1.1225108,
          -0.848826,
          -1.730938,
          -0.47234768,
          0.06046614,
          0.13732627,
          -1.759583,
          -0.23244894,
          -0.5461447,
          -0.6908065,
          -1.7024949,
          -1.3496063,
          -0.43720958,
          0.12236685,
          0.30057016,
          1.8156577,
          -0.4450444,
          0.09175109,
          -1.9888444,
          5.8240175,
          4.774564,
          -4.6370645,
          -4.4598875,
          -4.9597473
         ],
         "xaxis": "x",
         "y": [
          -4.354857,
          -4.395778,
          -4.2741985,
          -4.302263,
          -3.9533648,
          2.5500414,
          -4.7337413,
          -4.5666647,
          -4.3896203,
          -4.6720886,
          -4.6964393,
          -4.487653,
          -4.47679,
          -3.4979289,
          -4.1660833,
          -4.58299,
          -4.6789966,
          -4.625409,
          -3.3639646,
          -5.547526,
          -5.584853,
          -5.6696424,
          -5.4967337,
          -5.414921,
          -5.2933865,
          -5.3959107,
          -5.351347,
          -5.4177766,
          -5.2638893,
          -5.482386,
          -5.3883305,
          -4.3260417,
          -2.7463775,
          -2.37977,
          -4.6007905,
          -5.3016706,
          -4.7217298,
          0.4072304,
          -3.8187504,
          0.25948203,
          0.92218584,
          1.3534133,
          0.31785524,
          -0.107994154,
          -0.30798358,
          -0.24670433,
          -0.18182266,
          -0.24610183,
          -0.27549103,
          -1.2807918,
          -2.0944715,
          -1.9279379,
          -1.2728561,
          -1.4687476,
          -1.4102265,
          -0.74626106,
          -0.9193016,
          -1.9737502,
          -0.90640306,
          -2.4184449,
          -1.0187329,
          -0.94034314,
          -0.5742646,
          -1.2870672,
          -1.7012831,
          -3.2756896,
          -2.4023032,
          -1.7316974,
          -3.4042163,
          -2.5117922,
          -1.8588489,
          -3.4891076,
          -2.605322,
          -0.42408693,
          -1.3790275,
          -0.8819356,
          -1.4860424,
          -1.4461617,
          -0.6233055,
          1.3242639,
          -3.7794728,
          -4.328117,
          -4.0401855,
          -4.3160896,
          -4.416146,
          2.3102727,
          0.84250075,
          1.4009899,
          1.09581,
          1.6708262,
          2.411188,
          -4.352924,
          -4.523104,
          -5.0789623,
          -4.4322753,
          -4.4962626,
          -2.216616,
          -2.5713902,
          -0.50059533,
          -3.1726522,
          -0.2077765,
          0.031847775,
          0.28473887,
          0.15304454,
          -0.24917209,
          -0.41085055,
          -0.5089561,
          -0.78117305,
          -0.72678256,
          -1.1508663,
          -0.88412905,
          -0.55716264,
          -0.6799554,
          -0.54197794,
          -0.5149489,
          -0.9299758,
          -0.82641494,
          -0.84400123,
          -1.0425153,
          -1.382254,
          -5.2555075,
          -4.8216295,
          -1.0441695,
          -1.2147241,
          -1.5032054,
          -1.1228818,
          -1.1591746,
          -1.8237265,
          -3.0409472,
          -2.236787,
          -3.1290443,
          -2.1629376,
          -4.9534235,
          -4.455226,
          -4.355752,
          -3.115615,
          -4.6785865,
          -4.406634,
          -4.364458,
          -5.168506,
          -4.5025163,
          -4.606314,
          -3.5302434,
          -4.0735083,
          -3.385557,
          -3.9502735,
          -3.9845576,
          -4.1615047,
          -3.5211976,
          -2.3210957,
          -0.05116165,
          0.16355306,
          -0.282221,
          0.7484236,
          -4.0415072,
          -4.9368205,
          -1.5102766,
          -2.9990332,
          -3.6065333,
          0.43261895,
          0.673894,
          -1.6800586,
          -2.7512429,
          -0.8693959,
          -5.486797,
          -0.42120698,
          -1.4009383,
          -4.246597,
          -5.016359,
          -5.1092353,
          -4.574135,
          -4.925933,
          -4.098332,
          -4.923392,
          -4.6752295,
          -5.5508194,
          -5.530429,
          -5.842863,
          -6.007161,
          -6.0639386,
          -5.8051443,
          -5.904059,
          2.4416518,
          -2.1881878,
          -2.0858457,
          -2.3264546,
          -1.7810571,
          -2.3699696,
          -2.106206,
          -1.4419601,
          -1.516317,
          -2.4928741,
          -1.691036,
          -0.1866881,
          0.85662097,
          -2.8006365,
          -1.6473411,
          -0.93536705,
          -0.49077168,
          -2.0182226,
          -3.4035237,
          -4.4105086,
          -4.689692,
          -4.3459415,
          -4.571879,
          -1.5701498,
          -3.6574433,
          -4.504502,
          0.059733044,
          -3.9170241,
          -4.557488,
          -4.22111,
          -3.9425886,
          -3.6120713,
          -0.03543398,
          -1.2557913,
          -3.7540517,
          -2.596496,
          -1.122099,
          -0.5995642,
          1.8212028,
          0.72699803,
          -5.540149,
          -4.2041206,
          -4.3758616,
          -4.1751065,
          -4.5262184,
          -4.5000896,
          -4.4612346,
          -3.450635,
          -3.365182,
          -3.3545437,
          -3.3800251,
          -3.191312,
          -3.3681383,
          -3.397746,
          -3.2993698,
          -3.443754,
          -3.4523702,
          -0.6679911,
          -3.5620542,
          -3.685768,
          -4.471402,
          -6.407517,
          -5.7678447,
          -6.006291,
          -6.516084,
          -5.8275924,
          -5.7260013,
          -5.8251963,
          -5.8465285,
          -5.9799714,
          -5.960057,
          -5.7638526,
          -5.4217925,
          -5.0229216,
          -5.500857,
          -4.9652476,
          -4.172122,
          -5.0227146,
          -5.187644,
          -0.9647734,
          -0.7568653,
          -1.4230872,
          -1.8465991,
          -1.7974311,
          -1.2165469,
          -0.7285909,
          -0.3479044,
          -1.5660464,
          -1.8201702,
          -0.54787356,
          -0.6594895,
          -0.09202274,
          -0.5236932,
          -3.5353317,
          -2.7273715,
          0.15406719,
          -0.12272498,
          1.7615039,
          2.5785666,
          2.5173934,
          2.1026254,
          1.4122256,
          1.894359,
          2.8265636,
          -1.4854089,
          -1.84364,
          -1.7216632,
          -1.5448859,
          0.42613754,
          -0.000083138,
          -0.17437483,
          -0.17085004,
          -2.1507564,
          -3.2028885,
          -2.8878193,
          -1.053886,
          -3.0058498,
          -2.777153,
          -2.4669833,
          -3.172991,
          -2.4687083,
          -5.6002145,
          -4.828289,
          -4.1715355,
          -6.8723826,
          -3.036936,
          -3.0643167,
          -3.105542,
          -3.231914,
          -3.1196456,
          -2.4650223,
          -2.5245368,
          -3.0311284,
          -4.2947435,
          -2.9878047,
          -1.5631003,
          -0.48226354,
          -0.7940596,
          -4.364281,
          -3.16144,
          -3.2837071,
          -2.6100035,
          -4.2269206,
          -4.060651,
          -0.6145374,
          -2.812944,
          -2.6591318,
          -3.7318635,
          -4.1247354,
          -1.6950458,
          -7.240517,
          -7.0376067,
          -7.595236,
          -6.3388133,
          -3.831574,
          -4.0864635,
          -4.929829,
          -0.61608326,
          0.17735471,
          -0.54710025,
          0.40493545,
          -0.6687044,
          0.096868746,
          -0.14842582,
          1.1442463,
          0.05960438,
          -0.021677295,
          -0.19986717,
          -4.4376917,
          -2.1313,
          -1.6186671,
          -4.7313685,
          -3.991948,
          -2.1012022,
          -0.9486156,
          -1.8848448,
          -4.8383727,
          -4.3083477,
          -4.845875,
          -5.1709747,
          -3.4367156,
          -4.119559,
          -4.242223,
          -4.4351125,
          -4.5305905,
          -4.1706486,
          -4.518552,
          -4.6652675,
          -4.115115,
          -4.419691,
          -4.715096,
          -4.014682,
          -3.5775416,
          -4.6877556,
          -4.4522386,
          -1.2947115,
          -4.117103,
          -4.1419916,
          -3.0309665,
          -2.6176481,
          -4.691901,
          -4.871385,
          -7.4304967,
          -6.2759953,
          -6.560773,
          -0.9350138,
          -0.13875088,
          -0.6707423,
          -0.024290044,
          0.1460687,
          -1.8835979,
          -1.2245692,
          -0.24002977,
          0.33824807,
          0.14100417,
          0.19962668,
          0.81685126,
          0.33722925,
          -0.31055728,
          -3.8109245,
          -1.021758,
          -1.8932952,
          -1.5340159,
          -1.9796369,
          -2.3449187,
          -5.078089,
          -2.0707955,
          -0.5912421,
          -0.43447217,
          -5.6712313,
          -5.17949,
          -5.544541,
          -5.7655425,
          -5.734133,
          -5.755614,
          -5.789145,
          -5.792115,
          -5.93227,
          -5.885945,
          -5.667921,
          -5.7850013,
          -5.7363973,
          -3.5091236,
          -3.078085,
          -2.9683518,
          -2.9486504,
          -2.4347253,
          -3.0622604,
          -2.128889,
          -2.7666147,
          -4.1823745,
          -3.0882788,
          -3.1313074,
          -3.4519818,
          -3.3641284,
          -3.1260443,
          -2.9365785,
          -2.0215032,
          -3.9836824,
          -3.569525,
          -2.9503832,
          -4.4892287,
          -3.1103957,
          -3.2373323,
          -3.3198566,
          -4.0479293,
          -3.5546281,
          -3.8944566,
          -4.0527577,
          -3.4974947,
          -3.6904848,
          -3.783666,
          -2.0613117,
          -2.6932821,
          -2.9869986,
          -2.4396572,
          -4.418832,
          -4.4067183,
          -4.054884,
          -4.5652413,
          -4.7515545,
          -4.052251,
          -4.241159,
          -4.458173,
          -4.201544,
          -4.2290215,
          -3.5884798,
          -3.9877505,
          -3.7098842,
          -2.9761708,
          -0.6325017,
          -3.1933134,
          -0.9267391,
          -1.3383272,
          -1.0711452,
          -0.9730592,
          -0.45775232,
          -3.2571018,
          -0.5428491,
          -0.6236592,
          -0.721697,
          -0.6151891,
          -1.5610105,
          -0.69791365,
          -4.2326446,
          -1.1636086,
          0.9988461,
          -3.7784638,
          -3.6958108,
          -2.9104352,
          -2.8091435,
          -3.3017888,
          -3.8800955,
          -3.7203417,
          -3.3285036,
          -3.0422976,
          -4.2721753,
          -4.158986,
          -5.355921,
          -4.281533,
          -4.406674,
          -4.4150743,
          -5.2376037,
          -5.454308,
          -5.4813986,
          -5.3882823,
          -5.1469173,
          -5.125573,
          -4.9634733,
          -4.5231533,
          -5.2776427,
          -5.2803545,
          -0.20281509,
          -3.5687463,
          0.19710727,
          -4.889704,
          -5.713311,
          -5.896341,
          0.4408667,
          -4.638566,
          -5.175271,
          -5.36785,
          -4.4415426,
          -5.157327,
          -4.754866,
          -4.2372375,
          -3.3098505,
          -3.401187,
          -4.220512,
          -3.8029363,
          -4.2622457,
          -2.7005424,
          -4.2279396,
          -2.7477589,
          -2.019632,
          -4.890052,
          -1.9253412,
          -0.5327415,
          -2.5751095,
          -3.6734965,
          -3.30186,
          -3.1181576,
          -3.3419082,
          -3.6065273,
          -4.5155263,
          -1.1245905,
          -4.582607,
          -4.51038,
          -4.393997,
          -3.2380426,
          -3.4562519,
          -4.5538397,
          -4.6611047,
          -4.6488366,
          -5.1990156,
          -5.490313,
          -5.291911,
          -5.375899,
          -4.722435,
          -4.7167735,
          -4.6913447,
          -1.7501283,
          -4.139448,
          -4.295355,
          -3.0164416,
          -2.9062035,
          -4.323207,
          -4.3865576,
          -4.394388,
          -3.1401837,
          -3.657937,
          -2.8866513,
          0.3774372,
          0.8564035,
          -5.4473295,
          2.049846,
          1.1563574,
          1.500873,
          0.850233,
          2.1759412,
          -0.6179005,
          -0.5172599,
          -5.5132303,
          -3.0175898,
          -3.1085138,
          -3.6085904,
          -1.7975676,
          -3.1223156,
          -2.952002,
          -4.7573714,
          -2.7983747,
          -2.0084128,
          -2.381327,
          -1.790811,
          -2.6047616,
          -2.803628,
          -0.74861014,
          -0.5732732,
          -0.67484874,
          -0.65649456,
          -1.6021701,
          -0.7775812,
          -0.660361,
          -1.7404274,
          -1.3181806,
          -0.8278655,
          -0.8157493,
          -0.9339364,
          -0.84342736,
          -1.4494748,
          -1.3605049,
          -1.6299335,
          -2.7570786,
          -3.044419,
          -5.893529,
          -3.9681032,
          -3.5465605,
          -2.8896215,
          -3.2888787,
          -2.9110723,
          -1.7922261,
          -2.5776017,
          -2.2607148,
          -2.0784004,
          -2.5107625,
          -3.4307895,
          -4.4611597,
          -1.3734455,
          -5.3252983,
          -5.178576,
          -5.370141,
          -5.399294,
          -3.8885384,
          -5.512482,
          -5.195844,
          -3.3527849,
          -5.3746266,
          -5.4439306,
          -5.4001927,
          -4.857518,
          -4.8501368,
          -5.2147036,
          -5.484864,
          -5.6205387,
          -5.0030856,
          -5.0674534,
          -5.0614457,
          -4.5065827,
          -4.873866,
          -5.0442915,
          -4.819899,
          -5.1871424,
          -4.8599024,
          -5.3033013,
          -4.798113,
          -3.3833027,
          -4.2770452,
          -3.7333224,
          -3.9494953,
          -3.3424199,
          -3.7891963,
          -4.0071607,
          -0.5562727,
          -0.7553437,
          -2.351604,
          -2.71083,
          -0.8457365,
          -1.6250005,
          -2.7039723,
          -2.5443895,
          -2.7866547,
          -3.206593,
          -2.9782383,
          -2.1855817,
          -2.486599,
          -2.031982,
          -1.116822,
          -2.0083501,
          -2.7510748,
          -2.397963,
          -2.0611656,
          -2.8530822,
          -3.4269276,
          -3.3848157,
          -3.508451,
          -3.5263677,
          -4.306914,
          -4.5443373,
          -4.2757745,
          -4.4127097,
          -4.404841,
          -4.289453,
          -4.580463,
          -4.463318,
          -2.826223,
          -4.324935,
          -4.041308,
          -4.146327,
          -4.2825522,
          -3.9093459,
          -4.2310476,
          -3.0862923,
          0.22095047,
          0.009202007,
          0.17545983,
          0.13340554,
          -0.3712456,
          -0.0006257756,
          0.32079515,
          0.18772504,
          -0.30163354,
          -3.1510506,
          -2.4262705,
          -3.8940117,
          -4.5923715,
          -3.7288418,
          -0.01358373,
          -0.5630387,
          0.21621493,
          -5.425313,
          -5.6412745,
          -5.5782967,
          -5.6281724,
          -5.0044236,
          -3.4210525,
          -2.8225284,
          -4.014326,
          -3.009977,
          -4.0601883,
          -2.6837604,
          -0.39333323,
          -0.825703,
          -0.56722367,
          -0.42839995,
          -1.2075897,
          -4.1701083,
          -3.060548,
          -5.182333,
          -5.139977,
          -5.0014853,
          -5.118788,
          -5.1669846,
          -2.5133421,
          -3.9684956,
          -3.1795683,
          -5.4803205,
          -3.2766223,
          -5.577091,
          -2.4812992,
          -2.823181,
          -3.500048,
          -4.509274,
          -3.8467994,
          -5.2618165,
          -4.9225736,
          -5.170033,
          -5.1549597,
          -4.92292,
          -5.675416,
          -5.5298657,
          -5.7001405,
          -5.5497384,
          -5.6121626,
          -5.6104174,
          -5.6537066,
          -5.7857275,
          -5.8930593,
          -6.249349,
          -5.6651087,
          -6.079583,
          -2.902634,
          -1.9578854,
          -2.606029,
          -4.649132,
          -3.8375678,
          -4.8199434,
          -4.8857837,
          -4.3729534,
          -4.9538045,
          -5.0021806,
          -2.5030382,
          -2.0558743,
          -1.6603777,
          -2.1792047,
          -3.4618046,
          -3.581418,
          -4.5184784,
          -4.6158094,
          -6.272588,
          -7.099085,
          -7.2232747,
          -7.079137,
          -6.990926,
          -6.9597816,
          -5.0563774,
          -1.7876921,
          -1.1319655,
          0.09150307,
          -1.8286756,
          -6.5684123,
          -3.7634969,
          -3.3719857,
          -1.4567358,
          -0.44154447,
          -0.4978454,
          0.042272463,
          -1.4834194,
          -4.409792,
          -2.9487453,
          1.182756,
          0.44544744,
          0.26044244,
          0.065661505,
          -4.4338694,
          0.15226394,
          -5.0077305,
          -0.9480805,
          -5.0973754,
          -4.531485,
          -4.7677975,
          -4.7911377,
          -4.935071,
          -3.479475,
          -3.8638213,
          -1.3374515,
          -5.0949183,
          -5.216028,
          -5.343921,
          -5.1583347,
          -4.614954,
          -4.7817936,
          -2.5937536,
          -3.4560144,
          -4.505107,
          -4.4886427,
          -4.6810246,
          -4.728786,
          -4.6384845,
          -1.9128504,
          -3.7539144,
          -4.3002877,
          -4.4403653,
          -4.479174,
          -4.7037616,
          -2.7099977,
          -1.7813259,
          -5.084198,
          -3.197445,
          -4.0967417,
          -3.554975,
          -4.144519,
          -4.32091,
          -2.5092118,
          -2.8197472,
          -3.9187524,
          -3.8935525,
          -4.4035654,
          -5.1984105,
          -4.664121,
          -4.1546593,
          -1.7818352,
          -0.63687944,
          -1.4823658,
          -1.8803303,
          -2.561051,
          -2.38874,
          -1.6287112,
          -1.0342453,
          -1.7175691,
          -1.6463816,
          -2.635426,
          -3.5102,
          -4.8246784,
          -2.0299313,
          -1.8289083,
          -1.5232798,
          -1.7228905,
          -2.411782,
          -0.69076556,
          0.023819894,
          0.034055457,
          0.8677115,
          1.8811222,
          -2.1438248,
          -4.370207,
          -1.5979236,
          -0.55407447,
          -0.92193377,
          -0.87213635,
          -1.6700635,
          -2.080534,
          -2.3989096,
          -6.127045,
          -2.6847847,
          -2.0610216,
          -2.5909011,
          -2.4118598,
          -4.320591,
          -6.081115,
          -1.950131,
          -1.3055341,
          -2.6185308,
          -3.8993182,
          0.43582994,
          0.2625216,
          -2.2661266,
          -6.2167683,
          -2.3893993,
          -1.0587162,
          -1.1138299,
          -5.157516,
          -1.1559575,
          -0.45238444,
          -0.7766179,
          -4.15726,
          -3.5703008,
          -3.42958,
          -3.601557,
          -3.6457744,
          -3.5532851,
          -3.5178425,
          -3.409142,
          -3.5136027,
          -3.5633109,
          -3.527763,
          -3.2998083,
          -3.388115,
          -3.516778,
          -3.0200143,
          -3.15885,
          -2.9541857,
          -4.8415694,
          -4.8462625,
          -4.9030886,
          -7.1424437,
          -7.078908,
          -7.1439104,
          -6.950078,
          -7.1615725,
          -7.15922,
          -6.8922114,
          -6.9749475,
          -7.158756,
          -5.612994,
          -6.1511292,
          -6.128851,
          -6.095383,
          -6.072067,
          -5.7588925,
          -4.7106204,
          -5.5364623,
          -5.378681,
          -5.526996,
          -5.5245047,
          -5.5590997,
          -5.3037314,
          -5.7247148,
          -4.5915875,
          -4.540708,
          -4.6441574,
          -4.518819,
          -4.79976,
          -4.6724086,
          -4.477962,
          -0.5786463,
          -3.6608467,
          0.24055955,
          -0.8496062,
          0.44217488,
          0.46573922,
          0.14400837,
          -2.1989295,
          -4.507619,
          -4.7000165,
          -4.5953717,
          -4.8246284,
          -4.1303525,
          -4.1862574,
          -2.8844774,
          -7.3733125,
          -7.11943,
          -6.878433,
          -6.718526,
          -7.1174397,
          -7.0348573,
          -7.021378,
          -7.279921,
          -7.324139,
          -7.172472,
          1.1030092,
          -0.5898378,
          -0.2191296,
          -0.5580785,
          -0.96205145,
          2.3740273,
          2.6377594,
          3.9549508,
          -1.8559226,
          -1.9576505,
          -1.5821962,
          -1.3400576,
          -2.1068184,
          -1.9553884,
          -4.803203,
          -2.843383,
          -2.7250643,
          -4.4089723,
          -4.550402,
          -4.658127,
          -4.630371,
          -4.816621,
          -4.468968,
          -4.4248586,
          -4.8682775,
          -4.841135,
          -4.675798,
          -4.594302,
          -5.491433,
          -2.824686,
          -4.5880156,
          -4.7911024,
          -4.6770697,
          -2.7686636,
          -2.703642,
          -3.0890176,
          -3.0208085,
          -3.3645194,
          -3.026388,
          -1.584934,
          -2.121927,
          -2.4342017,
          -2.8843472,
          -2.860828,
          -3.135346,
          -4.184623,
          -4.0863953,
          -4.411138,
          -4.57456,
          -4.6590877,
          -4.5790873,
          -4.319102,
          -0.40708145,
          0.2799173,
          0.15338312,
          0.5246894,
          -0.16669028,
          -0.1635184,
          0.72986335,
          -5.2336955,
          -3.7647665,
          -4.2736416,
          -2.4712048,
          -2.688976,
          -4.044979,
          -3.7938883,
          -2.6821656,
          -3.4059362,
          -3.6998918,
          -3.791469,
          -3.6871252,
          -1.0342261,
          -5.111069,
          -4.8174434,
          -4.3664374,
          -5.3520994,
          -5.5487375,
          -4.8639035,
          -4.8189063,
          -5.4445662,
          -4.0590434,
          -4.925068,
          -4.629772,
          -4.4179,
          -4.2995777,
          -4.459684,
          -4.291749,
          -5.0382695,
          -5.2403464,
          -4.511694,
          -4.642807,
          -5.28674,
          -2.7358816,
          -5.111093,
          -4.9854784,
          -3.5306873,
          -4.384718,
          -4.1205993,
          -4.1910267,
          -3.716807,
          -2.7524748,
          -0.74761784,
          -4.831047,
          -4.795962,
          -4.880411,
          -0.9949633,
          -3.875324,
          -1.3117088,
          -1.246831,
          -0.32487255,
          0.21368103,
          -0.13675739,
          -6.0082073,
          -3.6662204,
          -3.0630019,
          -4.4858756,
          -4.347512,
          -4.310248,
          -1.3373318,
          -2.0122554,
          -2.1641262,
          -1.2037767,
          -1.9244934,
          -3.4622114,
          -1.8488388,
          -2.4537406,
          -2.0842242,
          -2.3614573,
          -1.2758752,
          -1.0014077,
          -1.6661446,
          -2.3735278,
          -3.1342456,
          2.1126451,
          2.509255,
          1.0330745,
          -2.4226863,
          0.23207176,
          -2.7480342,
          -3.0870395,
          -2.2485855,
          -0.7058115,
          -4.245077,
          -4.0953436,
          -4.471673,
          -4.711881,
          -4.724465,
          -3.408558,
          -4.917451,
          -5.0000277,
          -4.9138985,
          -4.066244,
          -0.9743507,
          -3.924778,
          -3.678821,
          -1.8672905,
          -1.2471616,
          -0.020832725,
          -1.1441976,
          0.20759201,
          -6.302135,
          -7.3334823,
          -7.0602055,
          -4.886683,
          -4.9353704,
          -0.5300377,
          0.19951077,
          0.36981574,
          0.15081201,
          -0.11228148,
          0.537418,
          0.26039198,
          0.4412337,
          -4.36232,
          -0.10194172,
          -5.226181,
          -1.338231,
          -2.1989179,
          -2.9143927,
          -2.2635252,
          -2.5328586,
          -0.6812254,
          0.054249838,
          -4.1433344,
          -4.008172,
          -4.5880966,
          -4.2577834,
          -4.6735325,
          -4.574965,
          -4.051519,
          -4.8824778,
          -4.819457,
          -5.0081615,
          -4.46932,
          -5.1971755,
          -4.8476777,
          -4.4541717,
          -4.7770667,
          -4.238834,
          -4.4351854,
          -4.58473,
          -4.5713854,
          -4.1429205,
          -3.6999536,
          -4.454303,
          -4.3695507,
          -4.5732837,
          -3.7544029,
          -4.686407,
          -3.3950012,
          -4.567255,
          -4.517776,
          -1.0245128,
          -0.30207726,
          -3.7779675,
          -4.728636,
          -4.4702177,
          -4.048732,
          -4.1080656,
          -1.7469182,
          -2.2326434,
          -5.130046,
          -5.0988364,
          -5.016169,
          -5.107351,
          0.5224832,
          -4.011556,
          -3.6089208,
          -2.7688506,
          -3.2493649,
          -4.2609267,
          -4.340174,
          -4.294257,
          -3.6732144,
          -2.6314256,
          -4.354189,
          -0.7815533,
          -4.599019,
          -3.7554774,
          -4.0701513,
          -3.8376837,
          -0.54367477,
          -4.168411,
          0.11957653,
          -1.2504606,
          -5.7788634,
          -6.5313177,
          -6.4647536,
          -0.21837938,
          -3.9783347,
          -4.9749794,
          -5.519329,
          -5.60274,
          -5.694424,
          -4.2218947,
          -2.7513518,
          -4.0920286,
          -4.549774,
          -4.908126,
          -2.0308144,
          -2.042153,
          -2.234749,
          -0.0033129866,
          -5.4157634,
          -5.8711853,
          -6.1493855,
          -6.034449,
          -5.820581,
          -4.994228,
          -5.4322524,
          -5.6766844,
          -5.46635,
          -5.741244,
          -5.4265122,
          -5.176329,
          -4.794937,
          5.3611655,
          -0.009492612,
          -2.352369,
          -2.3507307,
          -1.9159044,
          -2.6403835,
          -2.8010533,
          -1.8666327,
          -2.0127676,
          -1.8092723,
          -2.125784,
          -2.4842815,
          -1.6424588,
          -0.91553324,
          -3.0801187,
          -2.123736,
          -4.5306993,
          -4.735353,
          -4.571715,
          -3.6252117,
          -3.0710914,
          -3.228158,
          -4.5278306,
          -4.7067437,
          -4.4794593,
          -2.556146,
          -2.4929292,
          -3.149852,
          -3.734287,
          -4.1064205,
          -4.8191776,
          -4.0333166,
          -4.3550787,
          -4.159659,
          -4.23145,
          -0.14115486,
          0.14612602,
          0.076229475,
          0.092177495,
          -0.37551454,
          -0.17439558,
          -0.056614187,
          -3.72891,
          -4.5847907,
          -4.1208153,
          -4.0518994,
          -4.087408,
          -3.911037,
          -4.241032,
          -3.072873,
          -1.4476149,
          -4.48746,
          -4.518481,
          -4.5148244,
          -4.514374,
          -1.5876312,
          -4.2359295,
          -2.7899537,
          -4.3951716,
          -4.646545,
          -4.542205,
          -4.146675,
          -2.426828,
          -0.67999727,
          -2.2229621,
          -3.4365587,
          -2.5650167,
          0.51555467,
          -0.07246471,
          1.0457424,
          -4.184174,
          -4.616165,
          -0.3346966,
          -4.187381,
          -3.6370203,
          0.04805591,
          -0.204046,
          0.5776327,
          -0.09204582,
          -0.09412688,
          0.43095163,
          -1.1010112,
          -1.5286486,
          -2.0132515,
          -0.08207788,
          -0.45052212,
          -0.54594
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Vers..."
          ],
          [
           "Its architecture includes 3 main components:\n1. [FLAN-UL2](https://huggingface.co/google/flan-ul2), ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\n\nLoRA is very versatile and supported for [DreamBooth](https://github.com/huggingface/diffuser..."
          ],
          [
           "```\n\nNavigate to the example folder with the training script and install the required dependencies f..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri..."
          ],
          [
           "```py\nlora_attn_procs = {}\nfor name in unet.attn_processors.keys():\n    cross_attention_dim = None i..."
          ],
          [
           "```\n\nThe [optimizer](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab00..."
          ],
          [
           "```\n\nAside from setting up the LoRA layers, the training script is more or less the same as train_te..."
          ],
          [
           "accelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\n  --pretrained_model_name_..."
          ],
          [
           "```\n\nOnce training has been completed, you can use your model for inference:\n\n```py\nfrom diffusers i..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Text-to-audio (TTA) system has recently gained attention for its a..."
          ],
          [
           "<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "For a more technical overview of LCMs, refer to [the paper](https://huggingface.co/papers/2310.04378..."
          ],
          [
           "Before going through this guide, we'll take a look at the general workflow for performing inference ..."
          ],
          [
           "```\n\n## Text-to-image\n\nYou'll use the [`StableDiffusionXLPipeline`] with the scheduler: [`LCMSchedul..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npi..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "# Combine LoRAs\npipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])\n\nprompt = \"paperc..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npi..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "# load adapter\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_d..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "pipe.set_adapters([\"lcm\", \"motion-lora\"], adapter_weights=[0.55, 1.2])\n\nprompt = \"best quality, mast..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "Latent Consistency Distillation Example:\n\n[Latent Consistency Models (LCMs)](https://arxiv.org/abs/2..."
          ],
          [
           "```\n\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramati..."
          ],
          [
           "```bash\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path/to/save..."
          ],
          [
           "```\n\n## LCM-LoRA\n\nInstead of fine-tuning the full model, we can also just train a LoRA that can be i..."
          ],
          [
           "```bash\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path/to/save..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*There is large consent that successful training of deep networks r..."
          ],
          [
           "Stable Diffusion\n\n## Overview\n\nStable Diffusion was proposed in [Stable Diffusion Announcement](http..."
          ],
          [
           "## Available Pipelines:\n\n| Pipeline | Tasks | Colab\n|---|---|:---:|\n| [pipeline_stable_diffusion.py]..."
          ],
          [
           "## Examples:\n\n### Using Stable Diffusion without being logged into the Hub.\n\nIf you want to download..."
          ],
          [
           "```\n\nThis however can make it difficult to build applications on top of `diffusers` as you will alwa..."
          ],
          [
           "```\n\n### Text-to-Image with K-LMS scheduler\n\n```python\n# make sure you're logged in with `huggingfac..."
          ],
          [
           "```\n\n### CycleDiffusion using Stable Diffusion and DDIM scheduler\n\n```python\nimport requests\nimport ..."
          ],
          [
           "image.save(\"horse_to_elephant.png\")\n\n# let's try another example\n# See more samples at the original ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Vers..."
          ],
          [
           "The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandins..."
          ],
          [
           "## KandinskyV22Img2ImgPipeline\n\n[[autodoc]] KandinskyV22Img2ImgPipeline\n\t- all\n\t- __call__\n\n## Kandi..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "You can find additional information about MultiDiffusion on the [project page](https://multidiffusio..."
          ],
          [
           "<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Vers..."
          ],
          [
           "The original codebase can be found at [openai/shap-e](https://github.com/openai/shap-e).\n\n<Tip>\n\nSee..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Ve..."
          ],
          [
           "- **Self-contained**: An example script shall only depend on \"pip-install-able\" Python packages that..."
          ],
          [
           "We provide **official** examples that cover the most popular tasks of diffusion models.\n*Official* e..."
          ],
          [
           "Training examples show how to pretrain or fine-tune diffusion models for a variety of tasks. Current..."
          ],
          [
           "## Community\n\nIn addition, we provide **community** examples, which are examples added and maintaine..."
          ],
          [
           "```\nThen cd in the example folder of your choice and run\n```bash\npip install -r requirements.txt\n```..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Creating noise from data is easy; creating data from noise is gene..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThe `apply_patch` function exposes a number of [arguments](https://github.com/dbolya/tomesd#usa..."
          ],
          [
           "```bash\n- `diffusers` version: 0.15.1\n- Python version: 3.8.16\n- PyTorch version (GPU?): 1.13.1+cu11..."
          ],
          [
           "```\n\nTo reproduce this benchmark, feel free to use this [script](https://gist.github.com/sayakpaul/2..."
          ],
          [
           "| **GPU**  | **Resolution** | **Batch size** | **Vanilla** | **ToMe**       | **ToMe + xFormers** |\n..."
          ],
          [
           "| **V100** |            512 |             10 |         OOM |          10.03 |                9.29 |\n..."
          ],
          [
           "As seen in the tables above, the speed-up from `tomesd` becomes more pronounced for larger image res..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "You can find additional information about InstructPix2Pix on the [project page](https://www.timothyb..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline cla..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "response = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\nimage.thum..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n## Use multiple pipelines\n\nFor some workflows or if you're loading many pipelines, it is more m..."
          ],
          [
           "```\n\nIf you passed an optional argument - like disabling the safety checker - to the original pipeli..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Free-form inpainting is the task of adding new content to an image..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelin..."
          ],
          [
           "```\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/optimum/document..."
          ],
          [
           "--\n{{ card_data }}\n---\n\n<!-- This model card has been generated automatically according to the infor..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "You can find additional information about Attend-and-Excite on the [project page](https://attendande..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Vers..."
          ],
          [
           "<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper is the following:\n\n*The incredible generative ability of large-scale text-..."
          ],
          [
           "## Usage example with the base model of StableDiffusion-1.4/1.5\n\nIn the following we give a simple e..."
          ],
          [
           "```\n\n![img](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_ref..."
          ],
          [
           "```\n\n![img](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_out..."
          ],
          [
           "```\n\n![img](https://huggingface.co/Adapter/t2iadapter/resolve/main/sketch.png)\n\nThen, create the ada..."
          ],
          [
           "```\n\n![img](https://huggingface.co/Adapter/t2iadapter/resolve/main/sketch_output.png)\n\n## Available ..."
          ],
          [
           "| Model Name | Control Image Overview| Control Image Example | Generated Image Example |\n|---|---|--..."
          ],
          [
           "|[TencentARC/t2iadapter_canny_sd14v1](https://huggingface.co/TencentARC/t2iadapter_canny_sd14v1)<br/..."
          ],
          [
           "|[TencentARC/t2iadapter_sketch_sd14v1](https://huggingface.co/TencentARC/t2iadapter_sketch_sd14v1)<b..."
          ],
          [
           "|[TencentARC/t2iadapter_depth_sd14v1](https://huggingface.co/TencentARC/t2iadapter_depth_sd14v1)<br/..."
          ],
          [
           "|[TencentARC/t2iadapter_openpose_sd14v1](https://huggingface.co/TencentARC/t2iadapter_openpose_sd14v..."
          ],
          [
           "|[TencentARC/t2iadapter_keypose_sd14v1](https://huggingface.co/TencentARC/t2iadapter_keypose_sd14v1)..."
          ],
          [
           "|[TencentARC/t2iadapter_seg_sd14v1](https://huggingface.co/TencentARC/t2iadapter_seg_sd14v1)<br/>*Tr..."
          ],
          [
           "|[TencentARC/t2iadapter_zoedepth_sd15v1](https://huggingface.co/TencentARC/t2iadapter_zoedepth_sd15v..."
          ],
          [
           "## Combining multiple adapters\n\n[`MultiAdapter`] can be used for applying multiple conditionings at ..."
          ],
          [
           "```\n\nThe two control images look as such:\n\n![img](https://huggingface.co/datasets/diffusers/docs-ima..."
          ],
          [
           "```\n\n![img](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_d..."
          ],
          [
           "Adapt a model to a new task\n\nMany diffusion systems share the same components, allowing you to adapt..."
          ],
          [
           "```\n\nTo adapt your text-to-image model for inpainting, you'll need to change the number of `in_chann..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\n## StableDiffusionLatentUpscalePipeline\n\n[[autodoc]] StableDiffusionLatentUpscalePipeline\n\t-..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Text-to-3D\n\nTo generate a gif of a 3D object, pass a text prompt to the [`ShapEPipeline`]. T..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "prompt = \"A cheeseburger, white background\"\n\nimage_embeds, negative_image_embeds = prior_pipeline(pr..."
          ],
          [
           "```\n\nPass the cheeseburger to the [`ShapEImg2ImgPipeline`] to generate a 3D representation of it.\n\n`..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```\n\nUse the [`~utils.export_to_ply`] function to save the mesh output as a `ply` file:\n\n<Tip>\n\nYou ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper is the following:\n\n*With the advance of text-to-image models (e.g., Stable..."
          ],
          [
           "## Usage example\n\nAnimateDiff works with a MotionAdapter checkpoint and a Stable Diffusion model che..."
          ],
          [
           "```\n\nHere are some sample outputs:\n\n<table>\n    <tr>\n        <td><center>\n        masterpiece, bestq..."
          ],
          [
           "scheduler = DDIMScheduler.from_pretrained(\n    model_id, subfolder=\"scheduler\", clip_sample=False, t..."
          ],
          [
           "```\n\n<table>\n    <tr>\n        <td><center>\n        masterpiece, bestquality, sunset.\n        <br>\n  ..."
          ],
          [
           "```\n\nThen you can use the following code to combine Motion LoRAs.\n\n```python\nimport torch\nfrom diffu..."
          ],
          [
           "```\n\n<table>\n    <tr>\n        <td><center>\n        masterpiece, bestquality, sunset.\n        <br>\n  ..."
          ],
          [
           "!--Copyright 2023 The GLIGEN Authors and The HuggingFace Team. All rights reserved.\n\nLicensed under ..."
          ],
          [
           "The abstract from the [paper](https://huggingface.co/papers/2301.07093) is:\n\n*Large-scale text-to-im..."
          ],
          [
           "## StableDiffusionGLIGENPipeline\n\n[[autodoc]] StableDiffusionGLIGENPipeline\n\t- all\n\t- __call__\n\t- en..."
          ],
          [
           "WÃ¼rstchen text-to-image fine-tuning\n\n## Running locally with PyTorch\n\nBefore running the scripts, ma..."
          ],
          [
           "```\n\n## Prior training\n\nYou can fine-tune the WÃ¼rstchen prior model with the `train_text_to_image_pr..."
          ],
          [
           "```\n<!-- accelerate_snippet_end -->\n\n## Training with LoRA\n\nLow-Rank Adaption of Large Language Mode..."
          ],
          [
           "```bash\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch train_text_to_imag..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "You can find additional information about Self-Attention Guidance on the [project page](https://ku-c..."
          ],
          [
           "# [Deprecated] Multi Token Textual Inversion\n\n**IMPORTART: This research project is deprecated. Mult..."
          ],
          [
           "Colab for inference\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](htt..."
          ],
          [
           "```\n\nThen cd in the example folder  and run\n```bash\npip install -r requirements.txt\n```\n\nAnd initial..."
          ],
          [
           "```\n\nIf you have already cloned the repo, then you won't need to go through these steps.\n\n<br>\n\nNow ..."
          ],
          [
           "```\n\nA full training run takes ~1 hour on one V100 GPU.\n\n### Inference\n\nOnce you have trained a mode..."
          ],
          [
           "```\nIt should be at least 70% faster than the PyTorch script with the same configuration.\n\n### Train..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original codebase can be found at [CompVis/latent-diffusion](https://github.com/CompVis/latent-d..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n\nNext, load a LoRA checkpoint with the [`~diffusers.loaders.StableDiffusionXLLoraLoaderMixin.lo..."
          ],
          [
           "```\n\nLet's now generate an image with the second adapter and check the result:\n\n```python\nprompt = \"..."
          ],
          [
           "```\n\nNow that we have set these two adapters, let's generate an image from the combined adapters!\n\n<..."
          ],
          [
           "```\n\n![toy-face-pixel-art](https://huggingface.co/datasets/huggingface/documentation-images/resolve/..."
          ],
          [
           "```\n\n![no-lora](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffus..."
          ],
          [
           "```\n\n## Fusing adapters into the model\n\nYou can use PEFT to easily fuse/unfuse multiple adapters dir..."
          ],
          [
           "Consistency Decoder\n\nConsistency decoder can be used to decode the latents from the denoising UNet i..."
          ],
          [
           "# Amused training\n\nAmused can be finetuned on simple datasets relatively cheaply and quickly. Using ..."
          ],
          [
           "#### Full finetuning\n\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 750-1000 steps\n\n| ..."
          ],
          [
           "```\n\n#### Full finetuning + 8 bit adam\n\nNote that this training config keeps the batch size low and ..."
          ],
          [
           "```sh\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --train_batch_size <b..."
          ],
          [
           "```\n\n#### Full finetuning + lora\n\nBatch size: 16, Learning rate: 8e-4, Gives decent results in 1000-..."
          ],
          [
           "```\n\n### Finetuning the 512 checkpoint\n\nThese examples finetune on this [minecraft](https://huggingf..."
          ],
          [
           "```sh\naccelerate launch train_amused.py \\\n    --output_dir <output path> \\\n    --train_batch_size <b..."
          ],
          [
           "```\n\n#### Full finetuning + 8 bit adam\n\nBatch size: 8, Learning rate: 5e-6, Gives decent results in ..."
          ],
          [
           "```\n\n#### Full finetuning + lora \n\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 500-1..."
          ],
          [
           "```\n\n### Styledrop\n\n[Styledrop](https://arxiv.org/abs/2306.00983) is an efficient finetuning method ..."
          ],
          [
           "```\n\n#### 256\n\nExample results:\n\n![glowing_256_1](https://huggingface.co/datasets/diffusers/docs-ima..."
          ],
          [
           "```\n\n#### 512\n\nExample results:\n\n![glowing_512_1](https://huggingface.co/datasets/diffusers/docs-ima..."
          ],
          [
           "# Diffusers examples with Intel optimizations\n\n**This research project is not actively maintained by..."
          ],
          [
           "ðŸ§¨ Diffusers Experimental\n\nWe are adding experimental code to support novel applications and usages o..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\n```bash\ncd examples/dreambooth\npip install -r requirements_fl..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nSome basic and important parameters to know and specify are:\n\n- `--pretrained_model_name_or_pat..."
          ],
          [
           "```\n\n### Prior preservation loss\n\nPrior preservation loss is a method that uses a model's own genera..."
          ],
          [
           "```\n\n## Training script\n\nDreamBooth comes with its own dataset classes:\n\n- [`DreamBoothDataset`](htt..."
          ],
          [
           "```\n\nNext is the [`main()`](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec..."
          ],
          [
           "if model_has_vae(args):\n    vae = AutoencoderKL.from_pretrained(\n        args.pretrained_model_name_..."
          ],
          [
           "```\n\nThen, it's time to [create the training dataset](https://github.com/huggingface/diffusers/blob/..."
          ],
          [
           "```\n\nLastly, the [training loop](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347..."
          ],
          [
           "```\n\nOne more thing before you launch the script! Depending on the GPU you have, you may need to ena..."
          ],
          [
           "```\n\nDuring configuration, confirm that you want to use DeepSpeed. Now it should be possible to trai..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\n```bash\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax..."
          ],
          [
           "```\n\n</Tip>\n\n<hfoptions id=\"training-inference\">\n<hfoption id=\"PyTorch\">\n\n```py\nfrom diffusers impor..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n## LoRA\n\nLoRA is a training technique for significantly reducing the ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n2. Set the number of timesteps to run the denoising process for:\n\n```py\n>>> scheduler.set_times..."
          ],
          [
           "```\n\n5. Now write a loop to iterate over the timesteps. At each timestep, the model does a [`UNet2DM..."
          ],
          [
           "```\n\nIn the next section, you'll put your skills to the test and breakdown the more complex Stable D..."
          ],
          [
           "</Tip>\n\nNow that you know what you need for the Stable Diffusion pipeline, load all these components..."
          ],
          [
           "```\n\nInstead of the default [`PNDMScheduler`], exchange it for the [`UniPCMultistepScheduler`] to se..."
          ],
          [
           "```\n\nTokenize the text and generate the embeddings from the prompt:\n\n```py\n>>> text_input = tokenize..."
          ],
          [
           "```\n\n### Create random noise\n\nNext, generate some initial random noise as a starting point for the d..."
          ],
          [
           "```\n\nThe last step is to create the denoising loop that'll progressively transform the pure noise in..."
          ],
          [
           "```\n\nLastly, convert the image to a `PIL.Image` to see your generated image!\n\n```py\n>>> image = (ima..."
          ],
          [
           "Overview\n\nThese examples show how to run [Diffuser](https://arxiv.org/abs/2205.09991) in Diffusers. ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "There are two options for converting a `.ckpt` file: use a Space to convert the checkpoint or conver..."
          ],
          [
           "```\n\nTo use the script:\n\n1. Git clone the repository containing the `.ckpt` file you want to convert..."
          ],
          [
           "```\n\n## Keras .pb or .h5\n\n<Tip warning={true}>\n\nðŸ§ª This is an experimental feature. Only Stable Diffu..."
          ],
          [
           "The Convert KerasCV Space allows you to input the following:\n\n* Your Hugging Face token.\n* Paths to ..."
          ],
          [
           "```\n\nThen, you can generate an image like:\n\n```py\nfrom diffusers import DiffusionPipeline\n\npipeline ..."
          ],
          [
           "```\n\nLoad the LoRA checkpoint into the pipeline with the [`~loaders.LoraLoaderMixin.load_lora_weight..."
          ],
          [
           "InstructPix2Pix SDXL training example\n\n***This is based on the original InstructPix2Pix training exa..."
          ],
          [
           "### Toy example\n\nAs mentioned before, we'll use a [small toy dataset](https://huggingface.co/dataset..."
          ],
          [
           "```\n\nNow, we can launch training:\n\n```bash\naccelerate launch train_instruct_pix2pix_sdxl.py \\\n    --..."
          ],
          [
           "```\n\nAdditionally, we support performing validation inference to monitor training progress\nwith Weig..."
          ],
          [
           "```\n\n We recommend this type of validation as it can be useful for model debugging. Note that you ne..."
          ],
          [
           "```bash \naccelerate launch --mixed_precision=\"fp16\" --multi_gpu train_instruct_pix2pix_sdxl.py \\\n   ..."
          ],
          [
           "```\n\n ## Inference\n\n Once training is complete, we can perform inference:\n\n ```python\nimport PIL\nimp..."
          ],
          [
           "```\n\nWe encourage you to play with the following three parameters to control\nspeed and quality durin..."
          ],
          [
           "accelerate launch train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    ..."
          ],
          [
           "```\n\nWe discovered that compared to training with SD-1.5 as the pretrained model, SDXL-0.9 results i..."
          ],
          [
           "<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/re..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Here's the overview from the [project page](https://vislearn.github.io/ControlNet-XS/):\n\n*With incre..."
          ],
          [
           "## StableDiffusionXLControlNetXSPipeline\n[[autodoc]] StableDiffusionXLControlNetXSPipeline\n\t- all\n\t-..."
          ],
          [
           "ControlNet training example\n\n[Adding Conditional Control to Text-to-Image Diffusion Models](https://..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell e.g. a notebook\n\n```python\nfrom acc..."
          ],
          [
           "```\n\n\n```bash\nexport MODEL_DIR=\"runwayml/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path to save mode..."
          ],
          [
           "```\n\n## Training with multiple GPUs\n\n`accelerate` allows for seamless multi-GPU training. Follow the..."
          ],
          [
           "```\n\n## Example results\n\n#### After 300 steps with batch size 8\n\n| |  | \n|-------------------|:-----..."
          ],
          [
           "#### After 6000 steps with batch size 8:\n\n| |  | \n|-------------------|:-------------------------:|\n..."
          ],
          [
           "```bash\nexport MODEL_DIR=\"runwayml/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path to save model\"\n\nac..."
          ],
          [
           "```\n\n## Training on a 12 GB GPU\n\nOptimizations:\n- Gradient checkpointing\n- bitsandbyte's 8-bit optim..."
          ],
          [
           "```\n\nWhen using `enable_xformers_memory_efficient_attention`, please make sure to install `xformers`..."
          ],
          [
           "```\n\nSee [documentation](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) for more Dee..."
          ],
          [
           "```\n\n## Performing inference with the trained ControlNet\n\nThe trained model can be run the same as t..."
          ],
          [
           "```\n\n## Training with Flax/JAX\n\nFor faster training on TPUs and GPUs you can leverage the flax train..."
          ],
          [
           "```\n\nIf you want to use Weights and Biases logging, you should also install `wandb` now\n\n```bash\npip..."
          ],
          [
           "```\n\nAnd finally start the training\n\n```bash\npython3 train_controlnet_flax.py \\\n --pretrained_model_..."
          ],
          [
           "```\n\nSince we passed the `--push_to_hub` flag, it will automatically create a model repo under your ..."
          ],
          [
           "```\n\nNote, however, that the performance of the TPUs might get bottlenecked as streaming with `datas..."
          ],
          [
           "```\n\nWe support training with the Min-SNR weighting strategy proposed in [Efficient Diffusion Traini..."
          ],
          [
           "```\n\nThe profile can then be inspected at http://localhost:6006/#profile\n\nSometimes you'll get versi..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n2. Pass a prompt to the pipeline to generate an image:\n\n```py\nimage = pipeline(\n\t\"stained glass..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n\t<img src=\"https://huggingface.co/datasets/huggingface/docume..."
          ],
          [
           "```\n\n### Stable Diffusion XL\n\nSDXL is a much larger version of the previous Stable Diffusion models,..."
          ],
          [
           "```\n\n### ControlNet\n\nControlNet models are auxiliary models or adapters that are finetuned on top of..."
          ],
          [
           "```\n\nPass the `controlnet` to the [`AutoPipelineForText2Image`], and provide the prompt and pose est..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "## Configure pipeline parameters\n\nThere are a number of parameters that can be configured in the pip..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n\t<img class=\"rounded-xl\" src=\"https://huggingface.co/datasets..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2I..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2I..."
          ],
          [
           "```\n\n## Control image generation\n\nThere are several ways to exert more control over how an image is ..."
          ],
          [
           "```\n\n### ControlNet\n\nAs you saw in the [ControlNet](#controlnet) section, these models offer a more ..."
          ],
          [
           "```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2I..."
          ],
          [
           "```\n\nFor more tips on how to optimize your code to save memory and speed up inference, read the [Mem..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "For more details about how Stable Diffusion 2 works and how it differs from the original Stable Diff..."
          ],
          [
           "Stable Diffusion 2 is available for tasks like text-to-image, inpainting, super-resolution, and dept..."
          ],
          [
           "<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to exp..."
          ],
          [
           "```\n\n## Inpainting\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepSc..."
          ],
          [
           "```\n\n## Super-resolution\n\n```py\nfrom diffusers import StableDiffusionUpscalePipeline\nfrom diffusers...."
          ],
          [
           "```\n\n## Depth-to-image\n\n```py\nimport torch\nfrom diffusers import StableDiffusionDepth2ImgPipeline\nfr..."
          ],
          [
           "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a ..."
          ],
          [
           "```\n\nNote: Bfloat16 is available on Intel Xeon Scalable Processors Cooper Lake or Sapphire Rapids. Y..."
          ],
          [
           "```\nThe above is a simple distributed training usage on 2 nodes with 2 processes on each node. Add t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\nBefore you begin, make sure you have ðŸ¤— Datasets installed to load and preprocess image datas..."
          ],
          [
           "```\n\nWe encourage you to share your model with the community, and in order to do that, you'll need t..."
          ],
          [
           "```\n\n## Training configuration\n\nFor convenience, create a `TrainingConfig` class containing the trai..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ You can find additional datasets from the [HugGan Community Event](https://huggingface..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nFeel free to visualize the images again to confirm that they've been resized. Now you're ready ..."
          ],
          [
           "```\n\n## Create a UNet2DModel\n\nPretrained models in ðŸ§¨ Diffusers are easily created from their model c..."
          ],
          [
           "```\n\nIt is often a good idea to quickly check the sample image shape matches the model output shape:..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nThen, you'll need a way to evaluate the model. For evaluation, you can use the [`DDPMPipeline`]..."
          ],
          [
           "```\n\nNow you can wrap all these components together in a training loop with ðŸ¤— Accelerate for easy Te..."
          ],
          [
           "...     # Prepare everything\n...     # There is no specific order to remember, you just need to unpa..."
          ],
          [
           "...                 accelerator.clip_grad_norm_(model.parameters(), 1.0)\n...                 optimiz..."
          ],
          [
           "```\n\nPhew, that was quite a bit of code! But you're finally ready to launch the training with ðŸ¤— Acce..."
          ],
          [
           "ConsistencyDecoderScheduler\n\nThis scheduler is a part of the [`ConsistencyDecoderPipeline`] and was ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The callback function should have the following arguments:\n\n* `pipe` (or the pipeline instance) prov..."
          ],
          [
           "```\n\nNow, you can pass the callback function to the `callback_on_step_end` parameter and the `prompt..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to..."
          ],
          [
           "- **Encouraging safety in deployment**\n\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nNow pass your prompt to the pipeline. You can also pass a `negative_prompt` to prevent certain ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Stable Diffusion text-to-image fine-tuning\n\nThe `train_text_to_image.py` script shows how to fine-tu..."
          ],
          [
           "```\n\nAnd initialize an [ðŸ¤—Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n..."
          ],
          [
           "```\n\nIf you have already cloned the repo, then you won't need to go through these steps.\n\n<br>\n\n####..."
          ],
          [
           "```\n<!-- accelerate_snippet_end -->\n\n\nTo run on your own training files prepare the dataset accordin..."
          ],
          [
           "```\n\nCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\n\n```p..."
          ],
          [
           "```\n\n\n#### Training with Min-SNR weighting\n\nWe support training with the Min-SNR weighting strategy ..."
          ],
          [
           "In a nutshell, LoRA allows adapting pretrained models by adding pairs of rank-decomposition matrices..."
          ],
          [
           "**___Note: It is quite useful to monitor the training progress by regularly generating sample images..."
          ],
          [
           "```\n\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so\nwe need t..."
          ],
          [
           "```\n\nThe above command will also run inference as fine-tuning progresses and log the results to Weig..."
          ],
          [
           "```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_path = \"sayakpaul/sd-mod..."
          ],
          [
           "```\n\nIf you are loading the LoRA parameters from the Hub and if the Hub repository has\na `base_model..."
          ],
          [
           "```\n\n```bash\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport DATASET_NAME=\"lambdalabs/..."
          ],
          [
           "```\n\n### Training with xFormers:\n\nYou can enable memory efficient attention by [installing xFormers]..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "# Diffusers examples with ONNXRuntime optimizations\n\n**This research project is not actively maintai..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The Intel Labs Team Authors and HuggingFace Team. All rights reserved.\n\nLicensed u..."
          ],
          [
           "The abstract from the paper is:\n\n*This research paper proposes a Latent Diffusion Model for 3D (LDM3..."
          ],
          [
           "# Upscaler\n\n[LDM3D-VR](https://arxiv.org/pdf/2311.03226.pdf) is an extended version of LDM3D. \n\nThe ..."
          ],
          [
           "# Training examples\n\nCreating a training image set is [described in a different document](https://hu..."
          ],
          [
           "Models\n\nFor more detail on the models, please refer to the [docs](https://huggingface.co/docs/diffus..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Diffusion models have significantly advanced the fields of image, ..."
          ],
          [
           "!--Copyright 2023 Custom Diffusion authors The HuggingFace Team. All rights reserved.\n\nLicensed unde..."
          ],
          [
           "```\n\nNavigate to the example folder with the training script and install the required dependencies:\n..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nMany of the basic parameters are described in the [DreamBooth](dreambooth#script-parameters) tr..."
          ],
          [
           "```\n\nTo enable regularization, add the following parameters:\n\n- `--with_prior_preservation`: whether..."
          ],
          [
           "```\n\n## Training script\n\n<Tip>\n\nA lot of the code in the Custom Diffusion training script is similar..."
          ],
          [
           "```py\nparams_to_freeze = itertools.chain(\n    text_encoder.text_model.encoder.parameters(),\n    text..."
          ],
          [
           "```\n\nNow you'll need to add the [Custom Diffusion weights](https://github.com/huggingface/diffusers/..."
          ],
          [
           "```py\nst = unet.state_dict()\nfor name, _ in unet.attn_processors.items():\n    cross_attention_dim = ..."
          ],
          [
           "weights[\"to_out_custom_diffusion.0.bias\"] = st[layer_name + \".to_out.0.bias\"]\n    if cross_attention..."
          ],
          [
           "```\n\nThe [optimizer](https://github.com/huggingface/diffusers/blob/84cd9e8d01adb47f046b1ee449fc76a0c..."
          ],
          [
           "```\n\nIn the [training loop](https://github.com/huggingface/diffusers/blob/84cd9e8d01adb47f046b1ee449..."
          ],
          [
           "```\n\n## Launch the script\n\nOnce youâ€™ve made all your changes or youâ€™re okay with the default configu..."
          ],
          [
           "```bash\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport OUTPUT_DIR=\"path-to-save-model\"\nexp..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"multiple concepts\">\n\nCustom Diffusion can also learn multiple concept..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\nOnce training is finished, you can use your new Custom Diffusion mode..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n## Next steps\n\nCongratulations on training a model with Custom Diffus..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Vers..."
          ],
          [
           "The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandins..."
          ],
          [
           "[[autodoc]] KandinskyInpaintPipeline\n\t- all\n\t- __call__\n\n## KandinskyInpaintCombinedPipeline\n\n[[auto..."
          ],
          [
           "Stable Diffusion text-to-image fine-tuning\n\nThe `train_text_to_image.py` script shows how to fine-tu..."
          ],
          [
           "```\n\nIf you have already cloned the repo, then you won't need to go through these steps.\n\n<br>\n\n## U..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Prompt weighting works by increasing or decreasing the scale of the text embedding vector that corre..."
          ],
          [
           "```\n\nFor this guide, let's generate an image with the prompt `\"a red cat playing with a ball\"` using..."
          ],
          [
           "```\n\ncompel uses `+` or `-` to increase or decrease the weight of a word in the prompt. To increase ..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n  <img class=\"rounded-xl\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n  <img class=\"rounded-xl\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n  <img class=\"rounded-xl\" src=\"https://huggingface.co/dataset..."
          ],
          [
           "```\n\nIncorporate the concept to condition a prompt with using the `<concept>` syntax:\n\n```py\nprompt_..."
          ],
          [
           "```\n\nCreate a `Compel` class with a tokenizer and text encoder, and pass your prompt to it. Dependin..."
          ],
          [
           "```\n\nThis time, let's upweight \"ball\" by a factor of 1.5 for the first prompt, and downweight \"ball\"..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nNow you can load a pipeline, and pass the pre-learned concept to it:\n\n```py\npipeline = StableDi..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nThere are two tensors, `\"clip_g\"` and `\"clip_l\"`.\n`\"clip_g\"` corresponds to the bigger text enc..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "You can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://g..."
          ],
          [
           "<div class=\"flex justify-center\">\n    <div class=\"rounded-xl border border-gray-200\">\n    <table cla..."
          ],
          [
           "</td>\n        </tr>\n        <tr>\n            <td class=\"px-4 py-2 text-gray-700\">\n            <a hre..."
          ],
          [
           "<a href=\"./depth2img\">StableDiffusionDepth2Img</a>\n            </td>\n            <td class=\"px-4 py-..."
          ],
          [
           "</td>\n            <td class=\"px-4 py-2 text-gray-700\">filtered text-to-image</td>\n            <td cl..."
          ],
          [
           "</td>\n            <td class=\"px-4 py-2 text-gray-700\">text-to-image, image-to-image</td>\n           ..."
          ],
          [
           "</tr>\n        <tr>\n            <td class=\"px-4 py-2 text-gray-700\">\n            <a href=\"./ldm3d_dif..."
          ],
          [
           "## Tips\n\nTo help you get the most out of the Stable Diffusion pipelines, here are a few tips for imp..."
          ],
          [
           "```\n\n### Reuse pipeline components to save memory\n\nTo save memory and use the same components across..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nIf a community pipeline doesn't work as expected, please open a GitHub issue and mention the au..."
          ],
          [
           "diffuser_pipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    cust..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://user-images.githubusercontent.com/43138..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://user-images.githubuse..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThe example prompt you'll use is a portrait of an old warrior chief, but feel free to use your ..."
          ],
          [
           "```\n\nNow you can generate an image:\n\n```python\nimage = pipeline(prompt, generator=generator).images[..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/diffusers/docs-..."
          ],
          [
           "```python\npipeline.scheduler.compatibles\n[\n    diffusers.schedulers.scheduling_lms_discrete.LMSDiscr..."
          ],
          [
           "```\n\nThe Stable Diffusion model uses the [`PNDMScheduler`] by default which usually requires ~50 inf..."
          ],
          [
           "```\n\nStart with `batch_size=4` and see how much memory you've consumed:\n\n```python\nfrom diffusers.ut..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/diffusers/docs-..."
          ],
          [
           "```python\nfrom diffusers import AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-v..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/diffusers/docs-..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/diffusers/docs-..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/diffusers/docs-..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## VQModel\n\n[[autodoc]] VQModel\n\n## VQEncoderOutput\n\n[[autodoc]] models.vq_model.VQEncoderOutput..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "You can find additional smaller Stable Diffusion XL (SDXL) ControlNet checkpoints from the ðŸ¤— [Diffus..."
          ],
          [
           "## StableDiffusionXLControlNetInpaintPipeline\n[[autodoc]] StableDiffusionXLControlNetInpaintPipeline..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/..."
          ],
          [
           "The abstract of the paper is the following:\n\n*Although audio generation shares commonalities across ..."
          ],
          [
           "All checkpoints share the same model size for the text encoders and VAE. They differ in the size and..."
          ],
          [
           "### Controlling inference\n\n* The _quality_ of the predicted audio sample can be controlled by the `n..."
          ],
          [
           "!---\nCopyright 2022 - The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License,..."
          ],
          [
           "<p align=\"center\">\n    <br>\n    <img src=\"https://raw.githubusercontent.com/huggingface/diffusers/ma..."
          ],
          [
           "ðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating ima..."
          ],
          [
           "```\n\nWith `conda` (maintained by the community):\n\n```sh\nconda install -c conda-forge diffusers\n```\n\n..."
          ],
          [
           "```\n\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\n\n``..."
          ],
          [
           "```\n\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diff..."
          ],
          [
           "| **Documentation**                                                   | **What can I learn?**       ..."
          ],
          [
           "| [Optimization](https://huggingface.co/docs/diffusers/optimization/opt_overview)                   ..."
          ],
          [
           "We â¤ï¸  contributions from the open-source community!\nIf you want to contribute to this library, plea..."
          ],
          [
           "<table>\n  <tr>\n    <th>Task</th>\n    <th>Pipeline</th>\n    <th>ðŸ¤— Hub</th>\n  </tr>\n  <tr style=\"borde..."
          ],
          [
           "</tr>\n  <tr>\n    <td>Text-to-Image</td>\n    <td><a href=\"https://huggingface.co/docs/diffusers/api/p..."
          ],
          [
           "</tr>\n  <tr>\n    <td>Text-guided Image-to-Image</td>\n    <td><a href=\"https://huggingface.co/docs/di..."
          ],
          [
           "</tr>\n  <tr style=\"border-top: 2px solid black\">\n    <td>Image Variation</td>\n    <td><a href=\"https..."
          ],
          [
           "## Popular libraries using ðŸ§¨ Diffusers\n\n- https://github.com/microsoft/TaskMatrix\n- https://github.c..."
          ],
          [
           "We also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffus..."
          ],
          [
           "Stable Diffusion text-to-image fine-tuning\nThis extended LoRA training script was authored by [haofa..."
          ],
          [
           "With LoRA, it's possible to fine-tune Stable Diffusion on a custom image-caption pair dataset\non con..."
          ],
          [
           "```\n\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so \nwe need ..."
          ],
          [
           "```\n\nThe above command will also run inference as fine-tuning progresses and log the results to Weig..."
          ],
          [
           "```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_path = \"sayakpaul/sd-mod..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Depending on the use case, one should choose a technique accordingly. In many cases, these technique..."
          ],
          [
           "|                     **Method**                      | **Inference only** | **Requires training /<b..."
          ],
          [
           "|              [DreamBooth](#dreambooth)              |         âŒ         |                   âœ…     ..."
          ],
          [
           "[Paper](https://arxiv.org/abs/2211.09800)\n\n[InstructPix2Pix](../api/pipelines/pix2pix) is fine-tuned..."
          ],
          [
           "Pix2Pix Zero can be used both to edit synthetic images as well as real images.\n\n- To edit synthetic ..."
          ],
          [
           "</Tip>\n\n## Attend and Excite\n\n[Paper](https://arxiv.org/abs/2301.13826)\n\n[Attend and Excite](../api/..."
          ],
          [
           "## Self-attention Guidance (SAG)\n\n[Paper](https://arxiv.org/abs/2210.00939)\n\n[Self-attention Guidanc..."
          ],
          [
           "## Textual Inversion\n\n[Paper](https://arxiv.org/abs/2208.01618)\n\n[Textual Inversion](../training/tex..."
          ],
          [
           "## DiffEdit\n\n[Paper](https://arxiv.org/abs/2210.11427)\n\n[DiffEdit](../api/pipelines/diffedit) allows..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Sliced VAE\n\nSliced VAE enables decoding large batches of images with limited VRAM or batches with..."
          ],
          [
           "```\n\nYou may see a small performance boost in VAE decoding on multi-image batches, and there should ..."
          ],
          [
           "```\n\nThe output image has some tile-to-tile tone variation because the tiles are decoded separately,..."
          ],
          [
           "```\n\nCPU offloading works on submodules rather than whole models. This is the best way to minimize m..."
          ],
          [
           "During model offloading, only one of the main components of the pipeline (typically the text encoder..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nIn order to properly offload models after they're called, it is required ..."
          ],
          [
           "```\n\n## Tracing\n\nTracing runs an example input tensor through the model and captures the operations ..."
          ],
          [
           "# trace\nprint(\"tracing..\")\nunet_traced = torch.jit.trace(unet, inputs)\nunet_traced.eval()\nprint(\"don..."
          ],
          [
           "```\n\nReplace the `unet` attribute of the pipeline with the traced model:\n\n```python\nfrom diffusers i..."
          ],
          [
           "```\n\n## Memory-efficient attention\n\nRecent work on optimizing bandwidth in the attention block has g..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Tips\n\n- SDXL Turbo uses the exact same architecture as [SDXL](./stable_diffusion_xl), which means..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Loading from the original format\n\nBy default the [`AutoencoderKL`] should be loaded with [`~Model..."
          ],
          [
           "```\n\n## AutoencoderKL\n\n[[autodoc]] AutoencoderKL\n\n## AutoencoderKLOutput\n\n[[autodoc]] models.autoenc..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "We enormously value feedback from the community, so please do not be afraid to speak up if you belie..."
          ],
          [
           "* 1. Asking and answering questions on [the Diffusers discussion forum](https://discuss.huggingface...."
          ],
          [
           "* 9. Add a new pipeline, model, or scheduler, see [\"New Pipeline/Model\"](https://github.com/huggingf..."
          ],
          [
           "As said before, **all contributions are valuable to the community**.\nIn the following, we will expla..."
          ],
          [
           "**NOTE about channels**:\n[*The forum*](https://discuss.huggingface.co/c/discussion-related-to-httpsg..."
          ],
          [
           "**Please consider the following guidelines when opening a new issue**:\n- Make sure you have searched..."
          ],
          [
           "New issues usually include the following.\n\n#### 2.1. Reproducible, minimal bug reports\n\nA bug report..."
          ],
          [
           "#### 2.2. Feature requests\n\nA world-class feature request addresses the following points:\n\n1. Motiva..."
          ],
          [
           "#### 2.4 Technical questions\n\nTechnical questions are mainly about why certain code of the library w..."
          ],
          [
           "### 3. Answering issues on the GitHub issues tab\n\nAnswering issues on GitHub might require some tech..."
          ],
          [
           "### 4. Fixing a \"Good first issue\"\n\n*Good first issues* are marked by the [Good first issue](https:/..."
          ],
          [
           "Contributing to the library can have many forms:\n\n- Correcting spelling or grammatical errors.\n- Cor..."
          ],
          [
           "- Official Pipelines\n- Community Pipelines\n\nBoth official and community pipelines follow the same de..."
          ],
          [
           "An example can be seen [here](https://github.com/huggingface/diffusers/pull/2400).\n\nCommunity pipeli..."
          ],
          [
           "```\n\nas well as to install all additional dependencies required for training:\n\n```bash\npip install -..."
          ],
          [
           "```\n\nTherefore when adding an example, the `requirements.txt` file shall define all pip dependencies..."
          ],
          [
           "To contribute an example, it is highly recommended to look at already existing examples such as [dre..."
          ],
          [
           "### 8. Fixing a \"Good second issue\"\n\n*Good second issues* are marked by the [Good second issue](http..."
          ],
          [
           "By adding a new model, pipeline, or scheduler you might enable a new powerful use case for any of th..."
          ],
          [
           "If you are unsure or stuck in the PR, don't hesitate to leave a message to ask for a first review or..."
          ],
          [
           "```\n\nTo learn more, read this section of the [~Don't~ Repeat Yourself*](https://huggingface.co/blog/..."
          ],
          [
           "1. Make sure that you've used the correct template for your issue. You can pick between *Bug Report*..."
          ],
          [
           "4. **Minimalistic**: Try to help the reader as much as you can to understand the issue as quickly as..."
          ],
          [
           "## How to write a good PR..."
          ],
          [
           "1. Be a chameleon. Understand existing design patterns and syntax and make sure your code additions ..."
          ],
          [
           "11. Due to the rapidly growing repository, it is important to make sure that no files that would sig..."
          ],
          [
           "## How to open a PR\n\nBefore writing code, we strongly advise you to search through the existing PRs ..."
          ],
          [
           "```\n\n3. Create a new branch to hold your development changes:\n\n ```bash\n $ git checkout -b a-descrip..."
          ],
          [
           "```\n\nIt is a good idea to sync your copy of the code with the original\nrepository regularly. This wa..."
          ],
          [
           "```\n\n### Syncing forked main with upstream (HuggingFace) main\n\nTo avoid pinging the upstream reposit..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "| Training | SDXL-support | LoRA-support | Flax-support |\n|---|---|---|---|\n| [unconditional image g..."
          ],
          [
           "| [ControlNet](https://github.com/huggingface/diffusers/tree/main/examples/controlnet) | ðŸ‘ |  | ðŸ‘ |\n..."
          ],
          [
           "These examples are **actively** maintained, so please feel free to open an issue if they aren't work..."
          ],
          [
           "```\n\nThen navigate to the folder of the training script (for example, [DreamBooth](https://github.co..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Recent text-to-video generation approaches rely on computationally..."
          ],
          [
           "prompt = \"A panda is playing guitar on times square\"\nresult = pipe(prompt=prompt).images\nresult = [(..."
          ],
          [
           "```\nYou can change these parameters in the pipeline call:\n* Motion field strength (see the [paper](h..."
          ],
          [
           "# Generate the video chunk-by-chunk\nresult = []\nchunk_ids = np.arange(0, video_length, chunk_size - ..."
          ],
          [
           "```\n\n\n- #### SDXL Support\nIn order to use the SDXL model when generating a video from prompt, use th..."
          ],
          [
           "```\n    To extract pose from actual video, read [ControlNet documentation](controlnet).\n\n3. Run `Sta..."
          ],
          [
           "```\n- #### SDXL Support\n\t\n\tSince our attention processor also works with SDXL, it can be utilized to..."
          ],
          [
           "```\n\n### Text-To-Video with Edge Control\n\nTo generate a video from prompt with additional Canny edge..."
          ],
          [
           "```\n\n3. Run `StableDiffusionInstructPix2PixPipeline` with our custom attention processor\n    ```pyth..."
          ],
          [
           "```\n\n2. Read video from path\n    ```python\n    from PIL import Image\n    import imageio\n\n    reader ..."
          ],
          [
           "```\n\nYou can filter out some available DreamBooth-trained models with [this link](https://huggingfac..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\nA ControlNet model has two sets of weights (or blocks) connected by a zero-convolution layer..."
          ],
          [
           "```\n\n## Text-to-image\n\nFor text-to-image, you normally pass a text prompt to the model. But with Con..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```\n\nNow pass your prompt and canny image to the pipeline:\n\n```py\noutput = pipe(\n    \"the mona lisa\"..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/docum..."
          ],
          [
           "```\n\nNext, load a ControlNet model conditioned on depth maps and pass it to the [`StableDiffusionCon..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```\n\nCreate a function to prepare the control image from the initial and mask images. This'll create..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```\n\nNow pass your prompt, initial image, mask image, and control image to the pipeline:\n\n```py\noutp..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/docum..."
          ],
          [
           "original_image = load_image(\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x5..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/takuma..."
          ],
          [
           "original_image = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-imag..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```\n\nNow pass your prompt (and optionally a negative prompt if you're using one) and canny image to ..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/diffu..."
          ],
          [
           "image = np.array(original_image)\nimage = cv2.Canny(image, 100, 200)\nimage = image[:, :, None]\nimage ..."
          ],
          [
           "```\n\n### MultiControlNet\n\n<Tip>\n\nReplace the SDXL model with a model like [runwayml/stable-diffusion..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_..."
          ],
          [
           "```\n\nNow you can pass your prompt (an optional negative prompt if you're using one), canny image, an..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "As said before, **all contributions are valuable to the community**.\nIn the following, we will expla..."
          ],
          [
           "#### 2.4 Technical questions\n\nTechnical questions are mainly about why certain code of the library w..."
          ],
          [
           "Contributing to the library can have many forms:\n\n- Correcting spelling or grammatical errors.\n- Cor..."
          ],
          [
           "To contribute an example, it is highly recommended to look at already existing examples such as [dre..."
          ],
          [
           "By adding a new model, pipeline, or scheduler you might enable a new powerful use case for any of th..."
          ],
          [
           "## How to open a PR\n\nBefore writing code, we strongly advise you to search through the existing PRs ..."
          ],
          [
           "```\n\n3. Create a new branch to hold your development changes:\n\n ```bash\n $ git checkout -b a-descrip..."
          ],
          [
           "```\n\nIt is a good idea to sync your copy of the code with the original\nrepository regularly. This wa..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Tips\n\nIt is recommended to set `solver_order` to 2 for guide sampling, and `solver_order=3` for u..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "To load any community pipeline on the Hub, pass the repository id of the community pipeline to the `..."
          ],
          [
           "```\n\nLoading an official community pipeline is similar, but you can mix loading weights from an offi..."
          ],
          [
           "```\n\nFor more information about community pipelines, take a look at the [Community pipelines](custom..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nIn steps 4 and 5, the custom [UNet](https://github.com/showlab/Show-1/blo..."
          ],
          [
           "```\n\n5. Finally, you'll load the custom pipeline code. For this example, it has already been created..."
          ],
          [
           "```\n\nAfter the pipeline is successfully pushed, you need a couple of changes:\n\n1. Change the `_class..."
          ],
          [
           "```\n\nAs an additional reference example, you can refer to the repository structure of [stabilityai/j..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*We present the vector quantized diffusion (VQ-Diffusion) model for..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nTo generate images with Stable Diffusion 1 and 2 on Gaudi, you need to instantiate two instance..."
          ],
          [
           "```\n\nFor more information, check out ðŸ¤— Optimum Habana's [documentation](https://huggingface.co/docs/..."
          ],
          [
           "For [Stable Diffusion v2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1) on 768x768 imag..."
          ],
          [
           "DreamBooth training example for Stable Diffusion XL (SDXL)\n\n[DreamBooth](https://arxiv.org/abs/2208...."
          ],
          [
           "```\n\nOr for a default accelerate configuration without answering questions about your environment\n\n`..."
          ],
          [
           "```\n\nThis will also allow us to push the trained LoRA parameters to the Hugging Face Hub platform. \n..."
          ],
          [
           "```\n\nTo better track our training experiments, we're using the following flags in the command above:..."
          ],
          [
           "```\n\nand making sure that you have the following libraries installed:\n\n```\nbitsandbytes>=0.40.0\nxfor..."
          ],
          [
           "```\n\nWe can further refine the outputs with the [Refiner](https://huggingface.co/stabilityai/stable-..."
          ],
          [
           "```\n\nHere's a side-by-side comparison of the with and without Refiner pipeline outputs:\n\n| Without R..."
          ],
          [
           "## Results\n\nYou can explore the results from a couple of our internal experiments by checking out th..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nIf you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-di..."
          ],
          [
           "```\n\n## Convert to safetensors\n\nNot all weights on the Hub are available in the `.safetensors` forma..."
          ],
          [
           "```\n\n## Why use safetensors?\n\nThere are several reasons for using safetensors:\n\n- Safety is the numb..."
          ],
          [
           "Deprecated Pipelines\n\nThis folder contains pipelines that have very low usage as measured by model d..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper is as follows:\n\n*Latent Diffusion models (LDMs) have achieved remarkable r..."
          ],
          [
           "## LatentConsistencyModelPipeline\n\n[[autodoc]] LatentConsistencyModelPipeline\n    - all\n    - __call..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This guide will explore the [train_text_to_image_sdxl.py](https://github.com/huggingface/diffusers/b..."
          ],
          [
           "```\n\nThen navigate to the example folder containing the training script and install the required dep..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script..."
          ],
          [
           "```\n\n## Training script\n\nThe training script is also similar to the [Text-to-image](text2image#train..."
          ],
          [
           "Within the [`main()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807d6109..."
          ],
          [
           "```\n\nThe [prompt and image embeddings](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01..."
          ],
          [
           "```\n\nFinally, the [training loop](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc..."
          ],
          [
           "```\n\nIf you want to learn more about how the training loop works, check out the [Understanding pipel..."
          ],
          [
           "accelerate launch train_text_to_image_sdxl.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\n  --..."
          ],
          [
           "```\n\nAfter you've finished training, you can use your newly trained SDXL model for inference!\n\n<hfop..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n## Next steps\n\nCongratulations on training a SDXL model! To learn mor..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## FromOriginalVAEMixin\n\n[[autodoc]] loaders.single_file.FromOriginalVAEMixin\n\n## FromOriginalContro..."
          ],
          [
           "Schedulers\n\nFor more information on the schedulers, please refer to the [docs](https://huggingface.c..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usage\n\nBefore you can use IF, you need to accept its usage conditions. To do so:\n1. Make sure to ..."
          ],
          [
           "```\n\nrun the login function in a Python shell:\n\n```py\nfrom huggingface_hub import login\n\nlogin()\n```..."
          ],
          [
           "```\n\nThe following sections give more in-detail examples of how to use IF. Specifically:\n\n- [Text-to..."
          ],
          [
           "- *Stage-3*\n  - [stabilityai/stable-diffusion-x4-upscaler](https://huggingface.co/stabilityai/stable..."
          ],
          [
           "prompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of th..."
          ],
          [
           "```\n\n### Text Guided Image-to-Image Generation\n\nThe same IF model weights can be used for text-guide..."
          ],
          [
           "# stage 3\nsafety_modules = {\n    \"feature_extractor\": stage_1.feature_extractor,\n    \"safety_checker..."
          ],
          [
           "```\n\n### Text Guided Inpainting Generation\n\nThe same IF model weights can be used for text-guided im..."
          ],
          [
           "# stage 3\nsafety_modules = {\n    \"feature_extractor\": stage_1.feature_extractor,\n    \"safety_checker..."
          ],
          [
           "# stage 3\nstage_3_output = stage_3(prompt=prompt, image=stage_2_output, generator=generator, noise_l..."
          ],
          [
           "```\n\n### Converting between different pipelines\n\nIn addition to being loaded with `from_pretrained`,..."
          ],
          [
           "```\n\nWhen doing image variation or inpainting, you can also decrease the number of timesteps\nwith th..."
          ],
          [
           "```\n\nor the more aggressive layer based CPU offloading.\n\n```py\npipe = DiffusionPipeline.from_pretrai..."
          ],
          [
           "```\n\nFor CPU RAM constrained machines like Google Colab free tier where we can't load all model comp..."
          ],
          [
           "#pt_to_pil(stage_1_output)[0].save(\"./if_stage_I.png\")\n\n# Remove the pipeline so we can load the sup..."
          ],
          [
           "```\n\n## Available Pipelines:..."
          ],
          [
           "## Available Pipelines:\n\n| Pipeline | Tasks | Colab\n|---|---|:---:|\n| [pipeline_if.py](https://githu..."
          ],
          [
           "## IFPipeline\n[[autodoc]] IFPipeline\n\t- all\n\t- __call__\n\n## IFSuperResolutionPipeline\n[[autodoc]] IF..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThen enable the FreeU mechanism with the FreeU-specific hyperparameters. These values are scali..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/fre..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/fre..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThen navigate to the example folder containing the training script and install the required dep..."
          ],
          [
           "```\n\nTo setup a default ðŸ¤— Accelerate environment without choosing any configurations:\n\n```bash\naccel..."
          ],
          [
           "```\n\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri..."
          ],
          [
           "```\n\n## Training script\n\nAs with the script parameters, a general walkthrough of the training script..."
          ],
          [
           "```\n\nWithin the [`main()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883..."
          ],
          [
           "```\n\nFinally, in the [training loop](https://github.com/huggingface/diffusers/blob/64603389da0108205..."
          ],
          [
           "```\n\nOne more thing before you launch the script! Depending on the GPU you have, you may need to ena..."
          ],
          [
           "```\n\nDuring configuration, confirm that you want to use DeepSpeed stage 2. Now it should be possible..."
          ],
          [
           "```\n\nYou should also change the default Adam optimizer to DeepSpeedâ€™s optimized version of Adam [`de..."
          ],
          [
           "```\n\nThen you can inspect the profile at [http://localhost:6006/#profile](http://localhost:6006/#pro..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\nOnce training is complete, you can use your newly trained model for i..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Usability over Performance\n\n- While Diffusers has many built-in performance-enhancing features (s..."
          ],
          [
           "## Simple over easy\n\nAs PyTorch states, **explicit is better than implicit** and **simple is better ..."
          ],
          [
           "## Tweakable, contributor-friendly over abstraction\n\nFor large parts of the library, Diffusers adopt..."
          ],
          [
           "In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffu..."
          ],
          [
           "### Pipelines\n\nPipelines are designed to be easy to use (therefore do not follow [*Simple over easy*..."
          ],
          [
           "The following design principles are followed:\n- Pipelines follow the single-file policy. All pipelin..."
          ],
          [
           "- Pipelines should be used **only** for inference.\n- Pipelines should be very readable, self-explana..."
          ],
          [
           "### Models\n\nModels are designed as configurable toolboxes that are natural extensions of [PyTorch's ..."
          ],
          [
           "The following design principles are followed:\n- Models correspond to **a type of model architecture*..."
          ],
          [
           "- Models intend to expose complexity, just like PyTorch's `Module` class, and give clear error messa..."
          ],
          [
           "### Schedulers\n\nSchedulers are responsible to guide the denoising process for inference as well as t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Stable Diffusion weights (or checkpoints) are stored in the PyTorch format, so you need to convert t..."
          ],
          [
           "## Selecting the Core ML Variant to Use\n\nStable Diffusion models can be converted to different Core ..."
          ],
          [
           "The official Core ML Stable Diffusion [models](https://huggingface.co/apple/coreml-stable-diffusion-..."
          ],
          [
           "```\ncoreml-stable-diffusion-v1-4\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ original\nâ”‚   â”œâ”€â”€ compiled\nâ”‚   â””â”€â”€ packages\nâ””â”€â”€ sp..."
          ],
          [
           "```\n\nPass the path of the downloaded checkpoint with `-i` flag to the script. `--compute-unit` indic..."
          ],
          [
           "```\n\n## Core ML inference in Swift\n\nRunning inference in Swift is slightly faster than in Python bec..."
          ],
          [
           "```\n\nYou have to specify in `--resource-path` one of the checkpoints downloaded in the previous step..."
          ],
          [
           "If you feel strongly about any missing features, please feel free to open a feature request or, bett..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ¤— Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-prec..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script..."
          ],
          [
           "```\n\nYou'll also load the [`WuerstchenPrior`] model for optimization.\n\n```py\nprior = WuerstchenPrior..."
          ],
          [
           "```\n\nFinally, the [training loop](https://github.com/huggingface/diffusers/blob/65ef7a0c5c594b4f8409..."
          ],
          [
           "```\n\nIf you want to learn more about how the training loop works, check out the [Understanding pipel..."
          ],
          [
           "```\n\nOnce training is complete, you can use your newly trained model for inference!\n\n```py\nimport to..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*There is large consent that successful training of deep networks r..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*There is large consent that successful training of deep networks r..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This guide explores the [train_text_to_image_prior.py](https://github.com/huggingface/diffusers/blob..."
          ],
          [
           "```\n\nThen navigate to the example folder containing the training script and install the required dep..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script..."
          ],
          [
           "```\n\n## Training script\n\nThe training script is also similar to the [Text-to-image](text2image#train..."
          ],
          [
           "with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n    image_encoder = CLIPVision..."
          ],
          [
           "```\n\nKandinsky uses a [`PriorTransformer`] to generate the image embeddings, so you'll want to setup..."
          ],
          [
           "```\n\nFinally, the [training loop](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb..."
          ],
          [
           "```\n\nIf you want to learn more about how the training loop works, check out the [Understanding pipel..."
          ],
          [
           "```\n\nNext, the script includes several image transforms and a [preprocessing](https://github.com/hug..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n## Launch the script\n\nOnce youâ€™ve made all your changes or youâ€™re oka..."
          ],
          [
           "```bash\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate launch --mixed_precision=..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"decoder model\">\n\n```bash\nexport DATASET_NAME=\"lambdalabs/pokemon-blip..."
          ],
          [
           "```\n\n<Tip>\n\nFeel free to replace `kandinsky-community/kandinsky-2-2-decoder` with your own trained d..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n## Next steps\n\nCongratulations on training a Kandinsky 2.2 model! To ..."
          ],
          [
           "InstructPix2Pix training example\n\n[InstructPix2Pix](https://arxiv.org/abs/2211.09800) is a method to..."
          ],
          [
           "## Running locally with PyTorch\n\n### Installing the dependencies\n\nBefore running the scripts, make s..."
          ],
          [
           "```\n\nThen cd in the example folder and run\n```bash\npip install -r requirements.txt\n```\n\nAnd initiali..."
          ],
          [
           "```\n\nNow, we can launch training:\n\n```bash\naccelerate launch --mixed_precision=\"fp16\" train_instruct..."
          ],
          [
           "```\n\nAdditionally, we support performing validation inference to monitor training progress\nwith Weig..."
          ],
          [
           "```\n\n We recommend this type of validation as it can be useful for model debugging. Note that you ne..."
          ],
          [
           "```\n\n ## Inference\n\n Once training is complete, we can perform inference:\n\n ```python\nimport PIL\nimp..."
          ],
          [
           "```\n\nAn example model repo obtained using this training script can be found\nhere - [sayakpaul/instru..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nFor example, let's use the [`ptx0/pseudo-journey-v2`](https://huggingface.co/ptx0/pseudo-journe..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The SDE variant of DPMSolver and DPM-Solver++ is also supported, but only for the first and second-o..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Tips\n\n- Using SDXL with a DPM++ scheduler for less than 50 steps is known to produce [visual arti..."
          ],
          [
           "Check out the [Stability AI](https://huggingface.co/stabilityai) Hub organization for the official b..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n\nAll methods of the logging module are documented below. The main methods are\n[`logging.get_ver..."
          ],
          [
           "[[autodoc]] utils.logging.set_verbosity_debug\n\n## Other functions\n\n[[autodoc]] utils.logging.get_ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "# Training an unconditional diffusion model\n\nCreating a training image set is [described in a differ..."
          ],
          [
           "```\nAn example trained model: https://huggingface.co/anton-l/ddpm-ema-flowers-64\n\nA full training ru..."
          ],
          [
           "```\nAn example trained model: https://huggingface.co/anton-l/ddpm-ema-pokemon-64\n\nA full training ru..."
          ],
          [
           "```\n\nTo be able to use Weights and Biases (`wandb`) as a logger you need to install the library: `pi..."
          ],
          [
           "```\n\nInternally, the script will use the [`ImageFolder`](https://huggingface.co/docs/datasets/v2.0.0..."
          ],
          [
           "```\n\n`ImageFolder` will create an `image` column containing the PIL-encoded images.\n\nNext, push it t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*The most advanced text-to-image (T2I) models require significant t..."
          ],
          [
           "You can find the original codebase at [PixArt-alpha/PixArt-alpha](https://github.com/PixArt-alpha/Pi..."
          ],
          [
           "Run the [`PixArtAlphaPipeline`] with under 8GB GPU VRAM by loading the text encoder in 8-bit precisi..."
          ],
          [
           "```\n\nThen load the text encoder in 8-bit:\n\n```python\nfrom transformers import T5EncoderModel\nfrom di..."
          ],
          [
           "```\n\nThen compute the latents with the prompt embeddings as inputs:\n\n```python\npipe = PixArtAlphaPip..."
          ],
          [
           "```\n\nBy deleting components you aren't using and flushing the GPU VRAM, you should be able to run [`..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Scenarios\n\nWe cover Diffusion models with the following pipelines:\n\n- Text-guided image generatio..."
          ],
          [
           "![parti-prompts](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffu..."
          ],
          [
           "# Fixing these sample prompts in the interest of reproducibility.\nsample_prompts = [\n    \"a corgi\",\n..."
          ],
          [
           "```\n\nNow we can use these prompts to generate some images using Stable Diffusion ([v1-4 checkpoint](..."
          ],
          [
           "```\n\n![parti-prompts-14](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluati..."
          ],
          [
           "Let's first load a [`StableDiffusionPipeline`]:\n\n```python\nfrom diffusers import StableDiffusionPipe..."
          ],
          [
           "```\n\nGenerate some images with multiple prompts:\n\n```python\nprompts = [\n    \"a photo of an astronaut..."
          ],
          [
           "```\n\nIn the above example, we generated one image per prompt. If we generated multiple images per pr..."
          ],
          [
           "```\n\nIt seems like the [v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint perf..."
          ],
          [
           "Following is a pictorial overview:\n\n![edit-consistency](https://huggingface.co/datasets/diffusers/do..."
          ],
          [
           "```\n\n```bash\n{'input': Value(dtype='string', id=None),\n 'edit': Value(dtype='string', id=None),\n 'ou..."
          ],
          [
           "```\n\nAnd here is the image:\n\n```python\ndataset[idx][\"image\"]\n```\n\n![edit-dataset](https://huggingfac..."
          ],
          [
           "```\n\nTo measure the directional similarity, we first load CLIP's image and text encoders:\n\n```python..."
          ],
          [
           "```\n\nNotice that we are using a particular CLIP checkpoint, i.e.,Â `openai/clip-vit-large-patch14`. T..."
          ],
          [
           "def encode_text(self, text):\n        tokenized_text = self.tokenize_text(text)\n        text_features..."
          ],
          [
           "```\n\nLet's putÂ `DirectionalSimilarity`Â to use now.\n\n```python\ndir_similarity = DirectionalSimilarity..."
          ],
          [
           "```\n\nLike the CLIP Score, the higher the CLIP directional similarity, the better it is.\n\nIt should b..."
          ],
          [
           "***Using the above metrics helps evaluate models that are class-conditioned. For example, [DiT](http..."
          ],
          [
           "Let's first download a few images from the ImageNet-1k training set:\n\n```python\nfrom zipfile import ..."
          ],
          [
           "```\n\n```python\nfrom PIL import Image\nimport os\n\ndataset_path = \"sample-imagenet-images\"\nimage_paths ..."
          ],
          [
           "```\n\nWe now load theÂ [`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit) to gen..."
          ],
          [
           "```\n\nThe lower the FID, the better it is. Several things can influence FID here:\n\n- Number of images..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*This paper proposes a unified diffusion framework (dubbed UniDiffu..."
          ],
          [
           "</Tip>\n\nThis pipeline was contributed by [dg845](https://github.com/dg845). â¤ï¸\n\n## Usage Examples\n\nB..."
          ],
          [
           "```\n\nThis is also called \"joint\" generation in the UniDiffuser paper, since we are sampling from the..."
          ],
          [
           "```\n\n### Text-to-Image Generation\n\nUniDiffuser is also capable of sampling from conditional distribu..."
          ],
          [
           "```\n\nThe `text2img` mode requires that either an input `prompt` or `prompt_embeds` be supplied. You ..."
          ],
          [
           "```\n\nThe `img2text` mode requires that an input `image` be supplied. You can set the `img2text` mode..."
          ],
          [
           "```\n\n### Text Variation\n\nSimilarly, text variation can be performed on an input prompt with a text-t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri..."
          ],
          [
           "```py\nin_channels = 8\nout_channels = unet.conv_in.out_channels\nunet.register_to_config(in_channels=i..."
          ],
          [
           "```\n\nThese UNet parameters are [updated](https://github.com/huggingface/diffusers/blob/64603389da010..."
          ],
          [
           "```\n\nNext, the edited images and and edit instructions are [preprocessed](https://github.com/hugging..."
          ],
          [
           "```\n\nFinally, in the [training loop](https://github.com/huggingface/diffusers/blob/64603389da0108205..."
          ],
          [
           "```\n\nThen, the script applies dropout to the original image and edit instruction embeddings to suppo..."
          ],
          [
           "```\n\nThat's pretty much it! Aside from the differences described here, the rest of the script is ver..."
          ],
          [
           "</Tip>\n\nIf youâ€™re training on more than one GPU, add the `--multi_gpu` parameter to the `accelerate ..."
          ],
          [
           "```\n\nAfter training is finished, you can use your new InstructPix2Pix for inference:\n\n```py\nimport P..."
          ],
          [
           "```\n\nYou should experiment with different `num_inference_steps`, `image_guidance_scale`, and `guidan..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nYou can also try experimenting with the `num_inference_steps` parameter, which controls the num..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nYou can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model che..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original codebase can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-..."
          ],
          [
           "* The pipeline can generate masks that can be fed into other inpainting pipelines.\n* In order to gen..."
          ],
          [
           "* Swap the `source_prompt` and `target_prompt` in the arguments to `generate_mask`.\n    * Change the..."
          ],
          [
           "## StableDiffusionDiffEditPipeline\n[[autodoc]] StableDiffusionDiffEditPipeline\n    - all\n    - gener..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```py\nimport torch\nfrom accelerate import PartialState\nfrom diffusers import DiffusionPipeline\n\npipe..."
          ],
          [
           "```\n\nUse the `--num_processes` argument to specify the number of GPUs to use, and call `accelerate l..."
          ],
          [
           "```\n\nYou'll want to create a function to run inference; [`init_process_group`](https://pytorch.org/d..."
          ],
          [
           "T2I-Adapter training example for Stable Diffusion XL (SDXL)\n\nThe `train_t2i_adapter_sdxl.py` script ..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\n\n```python\nfrom ..."
          ],
          [
           "```\n\nThen run `huggingface-cli login` to log into your Hugging Face account. This is needed to be ab..."
          ],
          [
           "```\n\nTo better track our training experiments, we're using the following flags in the command above:..."
          ],
          [
           "```\n\n## Notes\n\n### Specifying a better VAE\n\nSDXL's VAE is known to suffer from numerical instability..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Scaled dot product attention\n\n[`torch.nn.functional.scaled_dot_product_attention`](https://p..."
          ],
          [
           "```\n\nSDPA should be as fast and memory efficient as `xFormers`; check the [benchmark](#benchmark) fo..."
          ],
          [
           "```\n\nDepending on GPU type, `torch.compile` can provide an *additional speed-up* of **5-300x** on to..."
          ],
          [
           "if run_compile:\n    print(\"Run torch compile\")\n    pipe.unet = torch.compile(pipe.unet, mode=\"reduce..."
          ],
          [
           "```\n\n### Stable Diffusion image-to-image\n\n```python\nfrom diffusers import StableDiffusionImg2ImgPipe..."
          ],
          [
           "```\n\n### Stable Diffusion inpainting\n\n```python\nfrom diffusers import StableDiffusionInpaintPipeline..."
          ],
          [
           "```\n\n### ControlNet\n\n```python\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetMo..."
          ],
          [
           "```\n\n### DeepFloyd IF text-to-image + upscaling\n\n```python\nfrom diffusers import DiffusionPipeline\ni..."
          ],
          [
           "prompt = \"the blue hulk\"\n\nprompt_embeds = torch.randn((1, 2, 4096), dtype=torch.float16)\nneg_prompt_..."
          ],
          [
           "```\n</details>\n\nThe graph below highlights the relative speed-ups for the [`StableDiffusionPipeline`..."
          ],
          [
           "In the following tables, we report our findings in terms of the *number of iterations/second*.\n\n### ..."
          ],
          [
           "### A100 (batch size: 4)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no..."
          ],
          [
           "### V100 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no..."
          ],
          [
           "### V100 (batch size: 16)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>n..."
          ],
          [
           "### T4 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no c..."
          ],
          [
           "### T4 (batch size: 4)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no c..."
          ],
          [
           "### T4 (batch size: 16)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <br>no ..."
          ],
          [
           "### RTX 3090 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <b..."
          ],
          [
           "### RTX 3090 (batch size: 16)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <..."
          ],
          [
           "### RTX 4090 (batch size: 1)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <b..."
          ],
          [
           "### RTX 4090 (batch size: 4)\n\n| **Pipeline** | **torch 2.0 - <br>no compile** | **torch nightly - <b..."
          ],
          [
           "## Notes\n\n* Follow this [PR](https://github.com/huggingface/diffusers/pull/3313) for more details on..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n**ðŸ“‹ Copy-paste the English version with a new language code**\n\nThe documentation files are in o..."
          ],
          [
           "```\n\nHere, `<LANG-ID>` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](http..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\n## StableDiffusionInpaintPipeline\n\n[[autodoc]] StableDiffusionInpaintPipeline\n\t- all\n\t- __ca..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "ðŸ’¡ Skip to the [DiffusionPipeline explained](#diffusionpipeline-explained) section if you are interes..."
          ],
          [
           "```\n\nYou can also load a checkpoint with its specific pipeline class. The example above loaded a Sta..."
          ],
          [
           "```\n\nThen pass the local path to [`~DiffusionPipeline.from_pretrained`]:\n\n```python\nfrom diffusers i..."
          ],
          [
           "```\n\nLet's use the [`SchedulerMixin.from_pretrained`] method to replace the default [`PNDMScheduler`..."
          ],
          [
           "```\n\n### Safety checker\n\nDiffusion models like Stable Diffusion can generate harmful content, which ..."
          ],
          [
           "```\n\nThen you can pass the `components` to another pipeline without reloading the weights into RAM:\n..."
          ],
          [
           "```\n\n## Checkpoint variants\n\nA checkpoint variant is usually a checkpoint whose weights are:\n\n- Stor..."
          ],
          [
           "There are two important arguments to know for loading variants:\n\n- `torch_dtype` defines the floatin..."
          ],
          [
           "```\n\nTo save a checkpoint stored in a different floating-point type or as a non-EMA variant, use the..."
          ],
          [
           "```\n\nHowever, this behavior is now deprecated since the \"revision\" argument should (just as it's don..."
          ],
          [
           "```\n\nYou can also load and save model variants by specifying the `variant` argument in [`ModelMixin...."
          ],
          [
           "```\n\n## Schedulers\n\nSchedulers are loaded from the [`SchedulerMixin.from_pretrained`] method, and un..."
          ],
          [
           "# replace `dpm` with any of `ddpm`, `ddim`, `pndm`, `lms`, `euler_anc`, `euler`\npipeline = StableDif..."
          ],
          [
           "```\n\n## DiffusionPipeline explained\n\nAs a class method, [`DiffusionPipeline.from_pretrained`] is res..."
          ],
          [
           "```\n\nYou'll see pipeline is an instance of [`StableDiffusionPipeline`], which consists of seven comp..."
          ],
          [
           "```\n\nCompare the components of the pipeline instance to the [`runwayml/stable-diffusion-v1-5`](https..."
          ],
          [
           "```\n\nYou can access each of the components of the pipeline as an attribute to view its configuration..."
          ],
          [
           "```\n\nEvery pipeline expects a [`model_index.json`](https://huggingface.co/runwayml/stable-diffusion-..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Creating noise from data is easy; creating data from noise is gene..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original codebase can be found at [facebookresearch/dit](https://github.com/facebookresearch/dit..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nWe recommend installing the [invisible-watermark](https://pypi.org/projec..."
          ],
          [
           "```\n\nYou can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model che..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "refiner = DiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n    ..."
          ],
          [
           "```\n\nTo use this approach, you need to define the number of timesteps for each model to run through ..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/..."
          ],
          [
           "```\n\nThis ensemble of expert denoisers method works well for all available schedulers!\n\n### Base to ..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "</Tip>\n\n### Size conditioning\n\nThere are two types of size conditioning:\n\n- [`original_size`](https:..."
          ],
          [
           "ðŸ¤— Diffusers also lets you specify negative conditions about an image's size to steer generation away..."
          ],
          [
           "```\n\n<div class=\"flex flex-col justify-center\">\n  <img src=\"https://huggingface.co/datasets/diffuser..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n## Use a different prompt for each text-encoder\n\nSDXL uses two text-encoders, so it is possible..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n## Other resources\n\nIf you're interested in experimenting with a minimal version of the [`UNet2..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Tips\n\nUse the `safety_concept` property of [`StableDiffusionPipelineSafe`] to check and edit the ..."
          ],
          [
           "```\nFor each image generation the active concept is also contained in [`StableDiffusionSafePipelineO..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nGreat, now you can import the rest of the dependencies you'll need:\n\n```python\nimport jax.numpy..."
          ],
          [
           "```\n\nModel parameters and inputs have to be replicated across the 8 parallel devices. The parameters..."
          ],
          [
           "```\n\nTo take advantage of JAX's optimized speed on a TPU, pass `jit=True` to the pipeline to compile..."
          ],
          [
           "```\n\n![img](https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/stable_diffusion_jax..."
          ],
          [
           "```\n\n![img](https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/stable_diffusion_jax..."
          ],
          [
           "```\n\nAfter calling `pmap`, the prepared function `p_generate` will:\n\n1. Make a copy of the underlyin..."
          ],
          [
           "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pled..."
          ],
          [
           "**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attri..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*There is large consent that successful training of deep networks r..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThen navigate to the example folder containing the training script and install the required dep..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri..."
          ],
          [
           "```py\nconditioning_image_transforms = transforms.Compose(\n    [\n        transforms.Resize(args.resol..."
          ],
          [
           "```\n\nWithin the [`main()`](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7bc81c0807..."
          ],
          [
           "```\n\nLastly, in the [training loop](https://github.com/huggingface/diffusers/blob/aab6de22c33cc01fb7..."
          ],
          [
           "```\n\nIf you want to learn more about how the training loop works, check out the [Understanding pipel..."
          ],
          [
           "```\n\n<Tip>\n\nTo monitor training progress with Weights & Biases, add the `--report_to=wandb` paramete..."
          ],
          [
           "```\n\nOnce training is complete, you can use your T2I-Adapter for inference:\n\n```py\nfrom diffusers im..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Stable Diffusion XL for JAX + TPUv5e\n\n[TPU v5e](https://cloud.google.com/blog/products/compute/how-c..."
          ],
          [
           "ðŸ‘‰ Try it out for yourself:\n\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugg..."
          ],
          [
           "```\npip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n```\n\nNe..."
          ],
          [
           "```\nHere, a pre-trained model `stable-diffusion-xl-base-1.0` from the namespace `stabilityai` is loa..."
          ],
          [
           "```\nTo utilize JAX's parallel capabilities, the parameters and input tensors are duplicated across d..."
          ],
          [
           "```\nNow that the function is compiled, this section shows how to use it for fast inference. It measu..."
          ],
          [
           "Once the function is compiled, these parameters are omitted from future calls and\ncannot be changed ..."
          ],
          [
           "```\n\nNext we can compile the generate function by executing `aot_compile`.\n\n```python\nstart = time.t..."
          ],
          [
           "```\n\nFrom this point forward, any calls to generate should result in a faster inference\ntime and it ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "For a more technical overview of LCMs, refer to [the paper](https://huggingface.co/papers/2310.04378..."
          ],
          [
           "```python\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\nimport..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "pipe = AutoPipelineForImage2Image.from_pretrained(\n    \"Lykon/dreamshaper-7\",\n    unet=unet,\n    tor..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\ngenerator = torch...."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "# load adapter\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_d..."
          ],
          [
           "```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Diffusion probabilistic models (DPMs) have demonstrated a very pro..."
          ],
          [
           "## UniPCMultistepScheduler\n[[autodoc]] UniPCMultistepScheduler\n\n## SchedulerOutput\n[[autodoc]] sched..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Take a look at the tensor values in the [`DDIMPipeline`] after two inference steps:\n\n```python\nfrom ..."
          ],
          [
           "```\n\nRunning the code above prints one value, but if you run it again you get a different value. Wha..."
          ],
          [
           "```\n\nNow when you run the code above, it always prints a value of `1491.1711` no matter what because..."
          ],
          [
           "```\n\nThe result is not the same even though you're using an identical seed because the GPU uses a di..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ If reproducibility is important, we recommend always passing a CPU generator.\nThe perf..."
          ],
          [
           "```\n\nNow when you run the same pipeline twice, you'll get identical results.\n\n```py\nimport torch\nfro..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Guided image synthesis enables everyday users to create and edit p..."
          ],
          [
           "## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutp..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nNext, we move it to GPU:\n\n```python\npipeline.to(\"cuda\")\n```\n\n## Access the scheduler\n\nThe sched..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/imag..."
          ],
          [
           "```\n\n**Output**:\n```\n[diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler,\n diffu..."
          ],
          [
           "```\n\nCool, lots of schedulers to look at. Feel free to have a look at their respective class definit..."
          ],
          [
           "```\n\nreturns a dictionary of the configuration of the scheduler:\n\n**Output**:\n```py\nFrozenDict([('nu..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/imag..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/imag..."
          ],
          [
           "```\n\n<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/imag..."
          ],
          [
           "prng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 25\n\n# shard inputs and rng\nparams = replicat..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nThe following Flax schedulers are _not yet compatible_ with the Flax Stab..."
          ],
          [
           "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a ..."
          ],
          [
           "```\n\n### Cat toy example\n\nFirst, let's login so that we can upload the checkpoint to the Hub during ..."
          ],
          [
           "```\n\nA full training run takes ~1 hour on one V100 GPU.\n\n**Note**: As described in [the official pap..."
          ],
          [
           "```\n\n```bash\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\nexport DATA_DIR=\"path-to-dir-con..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<Tip>\n\nThe quicktour is a simplified version of the introductory ðŸ§¨ Diffusers [notebook](https://cola..."
          ],
          [
           "```\n\n- [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) speeds up model loading for infe..."
          ],
          [
           "| **Task**                     | **Description**                                                    ..."
          ],
          [
           "Start by creating an instance of a [`DiffusionPipeline`] and specify which pipeline checkpoint you w..."
          ],
          [
           "```\n\nThe [`DiffusionPipeline`] downloads and caches all modeling, tokenization, and scheduling compo..."
          ],
          [
           "```\n\n### Local pipeline\n\nYou can also use the pipeline locally. The only difference is you need to d..."
          ],
          [
           "```\n\nTry generating an image with the new scheduler and see if you notice a difference!\n\nIn the next..."
          ],
          [
           "```\n\nTo access the model parameters, call `model.config`:\n\n```py\n>>> model.config\n```\n\nThe model con..."
          ],
          [
           "```\n\nFor inference, pass the noisy image and a `timestep` to the model. The `timestep` indicates how..."
          ],
          [
           "```\n\nTo generate actual examples though, you'll need a scheduler to guide the denoising process. In ..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ’¡ Unlike a model, a scheduler does not have trainable weights and is parameter-free!\n\n</..."
          ],
          [
           "```\n\nTo speed up the denoising process, move the input and model to a GPU:\n\n```py\n>>> model.to(\"cuda..."
          ],
          [
           "```\n\nSit back and watch as a cat is generated from nothing but noise! ðŸ˜»\n\n<div class=\"flex justify-ce..."
          ],
          [
           "ðŸ§¨ Diffusers Pipelines\n\nPipelines provide a simple way to run state-of-the-art diffusion models in in..."
          ],
          [
           "To that end, we strive to offer all open-sourced, state-of-the-art diffusion system under a unified ..."
          ],
          [
           "| Pipeline                                                                                          ..."
          ],
          [
           "| [ddim](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/ddim)           ..."
          ],
          [
           "| [score_sde_ve](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/score_sd..."
          ],
          [
           "| [stable_diffusion](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stab..."
          ],
          [
           "**Note**: Pipelines are simple examples of how to play around with the diffusion systems as describe..."
          ],
          [
           "- [`from_pretrained` method](https://github.com/huggingface/diffusers/blob/5cbed8e0d157f65d3ddc2420d..."
          ],
          [
           "In addition, a `model_index.json` file is created at the root of the local path, *e.g.* `./stable_di..."
          ],
          [
           "**Note**: All pipelines have PyTorch's autograd disabled by decorating the `__call__` method with a ..."
          ],
          [
           "- **Self-contained**: A pipeline shall be as self-contained as possible. More specifically, this mea..."
          ],
          [
           "- **One-purpose-only**: Pipelines should be used for one task and one task only. Even if two tasks a..."
          ],
          [
           "## Examples\n\n### Text-to-Image generation with Stable Diffusion\n\n```python\n# make sure you're logged..."
          ],
          [
           "```\n\n### Image-to-Image text-guided generation with Stable Diffusion\n\nThe `StableDiffusionImg2ImgPip..."
          ],
          [
           "```\nYou can also run this example on colab [![Open In Colab](https://colab.research.google.com/asset..."
          ],
          [
           "def download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response..."
          ],
          [
           "```\n\nYou can also run this example on colab [![Open In Colab](https://colab.research.google.com/asse..."
          ],
          [
           "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual invers..."
          ],
          [
           "```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/di..."
          ],
          [
           "```\n\n## Do distillation for quantization\n\nDistillation for quantization is a method that combines [i..."
          ],
          [
           "```\n\nAfter the distillation for quantization process, the quantized UNet would be 4 times smaller (3..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<br>\n\nPipelines do not offer any training functionality. You'll notice PyTorch's autograd is disable..."
          ],
          [
           "| Pipeline | Tasks |\n|---|---|\n| [AltDiffusion](alt_diffusion) | image2image |\n| [AnimateDiff](anima..."
          ],
          [
           "| [DiT](dit) | text2image |\n| [GLIGEN](stable_diffusion/gligen) | text2image |\n| [InstructPix2Pix](p..."
          ],
          [
           "| [Self-Attention Guidance](self_attention_guidance) | text2image |\n| [Semantic Guidance](semantic_s..."
          ],
          [
           "| [Value-guided planning](value_guided_sampling) | value guided sampling |\n| [Versatile Diffusion](v..."
          ],
          [
           "## DiffusionPipeline\n\n[[autodoc]] DiffusionPipeline\n\t- all\n\t- __call__\n\t- device\n\t- to\n\t- components..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The library has three main components:\n\n- State-of-the-art diffusion pipelines for inference with ju..."
          ],
          [
           "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2..."
          ],
          [
           "</a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" ..."
          ],
          [
           "Latent Consistency Distillation Example:\n\n[Latent Consistency Models (LCMs)](https://arxiv.org/abs/2..."
          ],
          [
           "```\n\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramati..."
          ],
          [
           "```bash\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path/to/saved/model\"\n\n..."
          ],
          [
           "```\n\n## LCM-LoRA\n\nInstead of fine-tuning the full model, we can also just train a LoRA that can be i..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Loading from the original format\n\nBy default the [`ControlNetModel`] should be loaded with [`~Mod..."
          ],
          [
           "```\n\n## ControlNetModel\n\n[[autodoc]] ControlNetModel\n\n## ControlNetOutput\n\n[[autodoc]] models.contro..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[Kandinsky 3](../api/pipelines/kandinsky3) simplifies the architecture and shifts away from the two-..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nKandinsky 2.1 and 2.2 usage is very similar! The only difference is Kandi..."
          ],
          [
           "```\n\nNow pass all the prompts and embeddings to the [`KandinskyPipeline`] to generate an image:\n\n```..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datas..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\nðŸ¤— Diffusers also provides an end-to-end API with the [`KandinskyCombi..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Kandinsky 2.2\">\n\n```py\nfrom diffusers import AutoPipelineForText2Imag..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Kandinsky 2.2\">\n\n```py\nimport torch\nfrom diffusers import KandinskyV2..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://raw.githubuserconten..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datas..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\nðŸ¤— Diffusers also provides an end-to-end API with the [`KandinskyImg2I..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Kandinsky 2.2\">\n\n```py\nfrom diffusers import AutoPipelineForImage2Ima..."
          ],
          [
           "```\n\n</Tip>\n\nFor inpainting, you'll need the original image, a mask of the area to replace in the or..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\nLoad an initial image and create a mask:\n\n```py\ninit_image = load_ima..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datas..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datas..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Kandinsky 2.2\">\n\n```py\nimport torch\nimport numpy as np\nfrom PIL impor..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n## Interpolation\n\nInterpolation allows you to explore the latent spac..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Kandinsky 2.2\">\n\n```py\nfrom diffusers import KandinskyV22PriorPipelin..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"htt..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datas..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datas..."
          ],
          [
           "```\n\nGenerate the image embeddings from a prompt and negative prompt:\n\n```py\nprompt = \"A robot, 4k p..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datas..."
          ],
          [
           "```\n\nLoad the prior pipeline and the [`KandinskyV22ControlnetImg2ImgPipeline`]:\n\n```py\nprior_pipelin..."
          ],
          [
           "```\n\nNow you can run the [`KandinskyV22ControlnetImg2ImgPipeline`] to generate an image from the ini..."
          ],
          [
           "```\n\n2. Enable `torch.compile` if you're using PyTorch >= 2.0 to automatically use scaled dot-produc..."
          ],
          [
           "# Diffusers examples with ONNXRuntime optimizations\n\n**This research project is not actively maintai..."
          ],
          [
           "Multi Subject DreamBooth training\n\n[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to per..."
          ],
          [
           "```\n\n### Multi Subject Training Example\nIn order to have your model learn multiple concepts at once,..."
          ],
          [
           "accelerate launch train_multi_subject_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME ..."
          ],
          [
           "```\n\nThis example shows training for 2 subjects, but please note that the model can be trained on an..."
          ],
          [
           "An example of how to generate the file:\n```python\nimport json\n\n# here we are using parameters for pr..."
          ],
          [
           "```\nAnd then just point to the file when executing the script:\n\n```bash\n# exports...\naccelerate laun..."
          ],
          [
           "```\n\n### Inference from a training checkpoint\n\nYou can also perform inference from one of the checkp..."
          ],
          [
           "accelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instanc..."
          ],
          [
           "```\n\n### Training with prior-preservation loss\n\nPrior-preservation is used to avoid overfitting and ..."
          ],
          [
           "```\n\n\n### Training on a 16GB GPU:\n\nWith the help of gradient checkpointing and the 8-bit optimizer f..."
          ],
          [
           "```\n\n### Training on a 8 GB GPU:\n\nBy using [DeepSpeed](https://www.deepspeed.ai/) it's possible to o..."
          ],
          [
           "accelerate launch --mixed_precision=\"fp16\" train_dreambooth.py \\\n  --pretrained_model_name_or_path=$..."
          ],
          [
           "```\n\n### Fine-tune text encoder with the UNet.\n\nThe script also allows to fine-tune the `text_encode..."
          ],
          [
           "```\n\n### Using DreamBooth for other pipelines than Stable Diffusion\n\nAltdiffusion also support dream..."
          ],
          [
           "Inference Examples\n\n**The inference examples folder is deprecated and will be removed in a future ve..."
          ],
          [
           "ControlNet training example for Stable Diffusion XL (SDXL)\n\nThe `train_controlnet_sdxl.py` script sh..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\n\n```python\nfrom ..."
          ],
          [
           "```\n\nThen run `huggingface-cli login` to log into your Hugging Face account. This is needed to be ab..."
          ],
          [
           "```\n\nTo better track our training experiments, we're using the following flags in the command above:..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<Tip>\n\nYou'll notice throughout the guide, we use [`~DiffusionPipeline.enable_model_cpu_offload..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```py\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "# prepare image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```py\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "```py\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "```py\nimport torch\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import make..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\nfrom ..."
          ],
          [
           "```\n\nNow you can pass this generated image to the image-to-image pipeline:\n\n```py\npipeline = AutoPip..."
          ],
          [
           "```\n\n### Image-to-image-to-image\n\nYou can also chain multiple image-to-image pipelines together to c..."
          ],
          [
           "```\n\n<Tip>\n\nIt is important to specify `output_type=\"latent\"` in the pipeline to keep all the output..."
          ],
          [
           "```\n\n### Image-to-upscaler-to-super-resolution\n\nAnother way you can chain your image-to-image pipeli..."
          ],
          [
           "```\n\n<Tip>\n\nIt is important to specify `output_type=\"latent\"` in the pipeline to keep all the output..."
          ],
          [
           "```\n\n## Control image generation\n\nTrying to generate an image that looks exactly the way you want ca..."
          ],
          [
           "```\n\n### ControlNet\n\nControlNets provide a more flexible and accurate way to control image generatio..."
          ],
          [
           "```\n\nLoad a ControlNet model conditioned on depth maps and the [`AutoPipelineForImage2Image`]:\n\n```p..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "prompt = \"elden ring style astronaut in a jungle\" # include the token \"elden ring style\" in the prom..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/docum..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nGenerating multiple prompts in a batch seems to take too much memory. Whi..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/optimum/documen..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## StableDiffusionPipelineOutput\n\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutp..."
          ],
          [
           "Stable Diffusion XL text-to-image fine-tuning\n\nThe `train_text_to_image_sdxl.py` script shows how to..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell (e.g., a notebook)\n\n```python\nfrom ..."
          ],
          [
           "```\n\n**Notes**:\n\n*  The `train_text_to_image_sdxl.py` script pre-computes text embeddings and the VA..."
          ],
          [
           "```\n\n### Inference in Pytorch XLA\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\nimp..."
          ],
          [
           "```\n\nNote: There is a warmup step in PyTorch XLA. This takes longer because of\ncompilation and optim..."
          ],
          [
           "With LoRA, it's possible to fine-tune Stable Diffusion on a custom image-caption pair dataset\non con..."
          ],
          [
           "```\n\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so\nwe need t..."
          ],
          [
           "```\n\nThe above command will also run inference as fine-tuning progresses and log the results to Weig..."
          ],
          [
           "```\n\n### Inference\n\nOnce you have trained a model using above command, the inference can be done sim..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nGenerating multiple prompts in a batch can [crash](https://github.com/hug..."
          ],
          [
           "```\n\n## Troubleshoot\n\nM1/M2 performance is very sensitive to memory pressure. When this occurs, the ..."
          ],
          [
           "<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ve..."
          ],
          [
           "```\n\nNavigate to the example folder with the training script and install the required dependencies f..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nSome other basic and important parameters to specify include:\n\n- `--pretrained_model_name_or_pa..."
          ],
          [
           "Next, you'll find the dataset preprocessing code and training loop in the [`main()`](https://github...."
          ],
          [
           "# Load scheduler and models\nnoise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_na..."
          ],
          [
           "```\n\nThe special [placeholder token](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0b..."
          ],
          [
           "```\n\nFinally, the [training loop](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73..."
          ],
          [
           "```\n\nSet the environment variable `MODEL_NAME` to a model id on the Hub or a path to a local model, ..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\n```bash\nexport MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\nFlax doesn't support the [`~loaders.TextualInversionLoaderMix..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n## Next steps\n\nCongratulations on training your own Textual Inversion..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original codebase can be found at [openai/consistency_models](https://github.com/openai/consiste..."
          ],
          [
           "```\n\n\n## ConsistencyModelPipeline\n[[autodoc]] ConsistencyModelPipeline\n    - all\n    - __call__\n\n## ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nTo use with Stable Diffusion XL 1.0\n\n```python\nimport torch\nfrom diffusers import DiffusionPipe..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This guide will explore the [train_lcm_distill_sd_wds.py](https://github.com/huggingface/diffusers/b..."
          ],
          [
           "```\n\nThen navigate to the example folder containing the training script and install the required dep..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nMost of the parameters are identical to the parameters in the [Text-to-image](text2image#script..."
          ],
          [
           "```py\ndef transform(example):\n    image = example[\"image\"]\n    image = TF.resize(image, resolution, ..."
          ],
          [
           "```\n\nFor improved performance on reading and writing large datasets stored in the cloud, this script..."
          ],
          [
           "```\n\nNow you can create the [optimizer](https://github.com/huggingface/diffusers/blob/3b37488fa3280a..."
          ],
          [
           "```\n\nNext, you're ready to setup the [training loop](https://github.com/huggingface/diffusers/blob/3..."
          ],
          [
           "```\n\nIt gets the [teacher model predictions](https://github.com/huggingface/diffusers/blob/3b37488fa..."
          ],
          [
           "```\n\nIf you want to learn more about how the training loop works, check out the [Understanding pipel..."
          ],
          [
           "```bash\nexport MODEL_DIR=\"runwayml/stable-diffusion-v1-5\"\nexport OUTPUT_DIR=\"path/to/saved/model\"\n\na..."
          ],
          [
           "```\n\nOnce training is complete, you can use your new LCM for inference.\n\n```py\nfrom diffusers import..."
          ],
          [
           "```\n\n## LoRA\n\nLoRA is a training technique for significantly reducing the number of trainable parame..."
          ],
          [
           "## Next steps\n\nCongratulations on distilling a LCM model! To learn more about LCM, the following may..."
          ],
          [
           "Custom Diffusion training example \n\n[Custom Diffusion](https://arxiv.org/abs/2212.04488) is a method..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell e.g. a notebook\n\n```python\nfrom acc..."
          ],
          [
           "```\n\n**___Note: Change the `resolution` to 768 if you are using the [stable-diffusion-2](https://hug..."
          ],
          [
           "```\n\n**Use `--enable_xformers_memory_efficient_attention` for faster training with lower VRAM requir..."
          ],
          [
           "```\n\nHere is an example [Weights and Biases page](https://wandb.ai/sayakpaul/custom-diffusion/runs/2..."
          ],
          [
           "```\n\nHere is an example [Weights and Biases page](https://wandb.ai/sayakpaul/custom-diffusion/runs/3..."
          ],
          [
           "```\n\n## Inference\n\nOnce you have trained a model using the above command, you can run inference usin..."
          ],
          [
           "```\n\nHere is an example of performing inference with multiple concepts:\n\n```python\nimport torch\nfrom..."
          ],
          [
           "```\n\nHere, `cat` and `wooden pot` refer to the multiple concepts.\n\n### Inference from a training che..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*There is large consent that successful training of deep networks r..."
          ],
          [
           "!---\nCopyright 2023- The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, ..."
          ],
          [
           "```\n\nFor example:\n\n```bash\ndoc-builder preview diffusers docs/source/en\n```\n\nThe docs will be viewab..."
          ],
          [
           "```\nand of course, if you moved it to another file, then:\n\n```md\nSections that were moved:\n\n[ <a hre..."
          ],
          [
           "```\n\nUse the relative style to link to the new file so that the versioned docs continue to work.\n\nFo..."
          ],
          [
           "```\n[[autodoc]] XXXPipeline\n    - all\n\t- __call__\n```\n\nThis will include every public method of the ..."
          ],
          [
           "```\n\nYou can follow the same process to create a new scheduler under the `docs/source/<languageCode>..."
          ],
          [
           "```\n\nIf the description is too long to fit in one line, another indentation is necessary before writ..."
          ],
          [
           "```\n\n#### Adding an image\n\nDue to the rapidly growing repository, it is important to make sure that ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Tips\n\nThe paper [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingfac..."
          ],
          [
           "```\n\n2. train a model with `v_prediction` (add the following argument to the [train_text_to_image.py..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThen navigate to the example folder containing the training script and install the required dep..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```..."
          ],
          [
           "```\n\nSome basic and important parameters include:\n\n- `--pretrained_model_name_or_path`: the name of ..."
          ],
          [
           "```\n\nYou can compare the loss surfaces for different `snr_gamma` values in this [Weights and Biases]..."
          ],
          [
           "```\n\nThen the script [loads the UNet](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d..."
          ],
          [
           "```\n\nLastly, the [training loop](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba48..."
          ],
          [
           "</Tip>\n\n```bash\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\nexport dataset_name=\"lambdalabs/p..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\nTraining with Flax can be faster on TPUs and GPUs thanks to [..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"Flax\">\n\n```py\nimport jax\nimport numpy as np\nfrom flax.jax_utils impor..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*StableDiffusion is a revolutionary text-to-image generator that is..."
          ],
          [
           "Evaluation results can be found in section 4.1 of the original paper.\n\n## Available checkpoints\n\n* [..."
          ],
          [
           "image = pipe(prompt=prompt, image=original_image, mask_image=mask_image).images[0]\nmake_image_grid([..."
          ],
          [
           "```\n\n## AsymmetricAutoencoderKL\n\n[[autodoc]] models.autoencoders.autoencoder_asym_kl.AsymmetricAutoe..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*We introduce WÃ¼rstchen, a novel architecture for text-to-image syn..."
          ],
          [
           "## WÃ¼rstchen Overview\nWÃ¼rstchen is a diffusion model, whose text-conditional model works in a highly..."
          ],
          [
           "<img src=\"https://github.com/dome272/Wuerstchen/assets/61938694/2914830f-cbd3-461c-be64-d50734f4b49d..."
          ],
          [
           "```\n\nFor explanation purposes, we can also initialize the two main pipelines of WÃ¼rstchen individual..."
          ],
          [
           "caption = \"Anthropomorphic cat dressed as a fire fighter\"\nnegative_prompt = \"\"\n\nprior_output = prior..."
          ],
          [
           "```\n\n## Speed-Up Inference\nYou can make use of `torch.compile` function and gain a speed-up of about..."
          ],
          [
           "```\n\n## Limitations\n\n- Due to the high compression employed by WÃ¼rstchen, generations can lack a goo..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*By decomposing the image formation process into a sequential appli..."
          ],
          [
           "</Tip>\n\n## StableDiffusionPipeline\n\n[[autodoc]] StableDiffusionPipeline\n\t- all\n\t- __call__\n\t- enable..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Simple over easy\n\nAs PyTorch states, **explicit is better than implicit** and **simple is better ..."
          ],
          [
           "In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffu..."
          ],
          [
           "- Models intend to expose complexity, just like PyTorch's `Module` class, and give clear error messa..."
          ],
          [
           "### Schedulers\n\nSchedulers are responsible to guide the denoising process for inference as well as t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n</pt>\n<jax>\n```bash\npip install diffusers[\"flax\"] transformers\n```\n</jax>\n</frameworkcontent>\n\n#..."
          ],
          [
           "```\n</pt>\n<jax>\n```bash\npip install -e \".[flax]\"\n```\n</jax>\n</frameworkcontent>\n\nThese commands will..."
          ],
          [
           "```\n\nFor more details about managing and cleaning the cache, take a look at the [caching](https://hu..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "| A1111/k-diffusion    | ðŸ¤— Diffusers                         | Usage                                ..."
          ],
          [
           "| DPM++ SDE           | [`DPMSolverSinglestepScheduler`]    |                                       ..."
          ],
          [
           "All schedulers are built from the base [`SchedulerMixin`] class which implements low level utilities..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*The past few years have witnessed the great success of Diffusion m..."
          ],
          [
           "## DEISMultistepScheduler\n[[autodoc]] DEISMultistepScheduler\n\n## SchedulerOutput\n[[autodoc]] schedul..."
          ],
          [
           "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a ..."
          ],
          [
           "```\n\nAnd initialize an [ðŸ¤—Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n..."
          ],
          [
           "```\n\nThis will be our training data.\nNow we can launch the training using\n\n## Use ONNXRuntime to acc..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original codebase can be found at [hohonathanho/diffusion](https://github.com/hojonathanho/diffu..."
          ],
          [
           "Kandinsky2.2 text-to-image fine-tuning\n\nKandinsky 2.2 includes a prior pipeline that generates image..."
          ],
          [
           "```\n\nAnd initialize an [ðŸ¤—Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n..."
          ],
          [
           "```\n\nTo disable wandb logging, remove the `--report_to==\"wandb\"` and `--validation_prompts=\"A robot ..."
          ],
          [
           "```\n<!-- accelerate_snippet_end -->\n\n\nTo train on your own training files, prepare the dataset accor..."
          ],
          [
           "```\n\nCheckpoints only save the unet, so to run inference from a checkpoint, just load the unet\n```py..."
          ],
          [
           "```\n<!-- accelerate_snippet_end -->\n\n\nTo perform inference with the fine-tuned prior model, you will..."
          ],
          [
           "```\n\nIf you want to use a fine-tuned decoder checkpoint along with your fine-tuned prior checkpoint,..."
          ],
          [
           "```\n\n\n#### Training with Min-SNR weighting\n\nWe support training with the Min-SNR weighting strategy ..."
          ],
          [
           "With LoRA, it's possible to fine-tune Kandinsky 2.2 on a custom image-caption pair dataset\non consum..."
          ],
          [
           "```\n\n#### Train prior\n\n```bash\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\n\naccelerate la..."
          ],
          [
           "```\n\n**___Note: When using LoRA we can use a much higher learning rate compared to non-LoRA fine-tun..."
          ],
          [
           "```\n\n### Training with xFormers:\n\nYou can enable memory efficient attention by [installing xFormers]..."
          ],
          [
           "e don't yet support training T2I-Adapters on Stable Diffusion yet. For training T2I-Adapters on Stab..."
          ],
          [
           "Research projects\n\nThis folder contains various research projects using ðŸ§¨ Diffusers.\nThey are not re..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Here's the overview from the [project page](https://vislearn.github.io/ControlNet-XS/):\n\n*With incre..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Use TensorFloat-32\n\nOn Ampere and later CUDA devices, matrix multiplications and convolutions can..."
          ],
          [
           "```\n\nYou can learn more about TF32 in the [Mixed precision training](https://huggingface.co/docs/tra..."
          ],
          [
           "[DreamBooth](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) by [colossalai]..."
          ],
          [
           "```\n\n## Dataset for Teyvat BLIP captions\nDataset used to train [Teyvat characters text to image mode..."
          ],
          [
           "torchrun --nproc_per_node 2 train_dreambooth_colossalai.py \\\n  --pretrained_model_name_or_path=$MODE..."
          ],
          [
           "```\n\n\n### Training with prior-preservation loss\n\nPrior-preservation is used to avoid overfitting and..."
          ],
          [
           "```\n\n## Inference\n\nOnce you have trained a model using above command, the inference can be done simp..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThe partially inverted latents are generated from the [`~StableDiffusionDiffEditPipeline.invert..."
          ],
          [
           "```\n\nUse the [`~StableDiffusionDiffEditPipeline.generate_mask`] function to generate the image mask...."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://github.com/Xiang-cd/D..."
          ],
          [
           "```\n\nNext, create a utility function to generate the prompts:\n\n```py\n@torch.no_grad()\ndef generate_p..."
          ],
          [
           "```\n\n<Tip>\n\nCheck out the [generation strategy](https://huggingface.co/docs/transformers/main/en/gen..."
          ],
          [
           "```\n\nFinally, pass the embeddings to the [`~StableDiffusionDiffEditPipeline.generate_mask`] and [`~S..."
          ],
          [
           "```\n\n## Generate a caption for inversion\n\nWhile you can use the `source_prompt` as a caption to help..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <figure>\n        <img class=\"rounded-xl\" src=\"https://git..."
          ],
          [
           "Community Examples\n\n> **For more information about community pipelines, please have a look at [this ..."
          ],
          [
           "| Example                                                                                           ..."
          ],
          [
           "--------------------------------------------|:------------------------------------------------------..."
          ],
          [
           "---------------------------------------------------------------------------|:-----------------------..."
          ],
          [
           "------------|--------------------------------------------------------------:|..."
          ],
          [
           "| LLM-grounded Diffusion (LMD+)                                                                     ..."
          ],
          [
           "| Stable Diffusion Interpolation                                                                    ..."
          ],
          [
           "| Wild Card Stable Diffusion                                                                        ..."
          ],
          [
           "| Image to Image Inpainting Stable Diffusion                                                        ..."
          ],
          [
           "MagicMix                                                                                            ..."
          ],
          [
           "| CLIP Guided Img2Img Stable Diffusion Pipeline                                                     ..."
          ],
          [
           "| CLIP Guided Images Mixing Stable Diffusion Pipeline | Ð¡ombine images using usual diffusion models...."
          ],
          [
           "FABRIC - Stable Diffusion with feedback Pipeline | pipeline supports feedback from liked and dislike..."
          ],
          [
           "|   Latent Consistency Interpolation Pipeline                                                       ..."
          ],
          [
           "| AnimateDiff ControlNet Pipeline                                                                   ..."
          ],
          [
           "To load a custom pipeline you just need to pass the `custom_pipeline` argument to `DiffusionPipeline..."
          ],
          [
           "```\n\n## Example usages\n\n### LLM-grounded Diffusion\n\nLMD and LMD+ greatly improves the prompt underst..."
          ],
          [
           "#### Use this pipeline with an LLM\n```python\nimport torch\nfrom diffusers import DiffusionPipeline\n\np..."
          ],
          [
           "```\n\n#### Use this pipeline on its own for layout generation\n```python\nimport torch\nfrom diffusers i..."
          ],
          [
           "```\n\n### CLIP Guided Stable Diffusion\n\nCLIP guided stable diffusion can help to generate more realis..."
          ],
          [
           "```\n\nThe `images` list contains a list of PIL images that can be saved locally or displayed directly..."
          ],
          [
           "```\n\nThe output of the `walk(...)` function returns a list of images saved under the folder as defin..."
          ],
          [
           "images = pipe.img2img(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n\n##..."
          ],
          [
           "```\n\nAs shown above this one pipeline can run all both \"text-to-image\", \"image-to-image\", and \"inpai..."
          ],
          [
           "torch_dtype=torch.float16\n)\npipe=pipe.to(\"cuda\")\n\nprompt = \"best_quality (1girl:1.3) bow bride brown..."
          ],
          [
           "```\n\n#### onnxruntime\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = Diffus..."
          ],
          [
           "```\n\nif you see `Token indices sequence length is longer than the specified maximum sequence length ..."
          ],
          [
           "```\nThis example produces the following image:\n\n![image](https://user-images.githubusercontent.com/4..."
          ],
          [
           "```\ndog\ncat\nmouse\n```\n\ncreate `object.txt`, with contents like:\n\n```\nchair\nsofa\nbench\n```\n\n```python..."
          ],
          [
           "```\n\n### Composable Stable diffusion\n\n[Composable Stable Diffusion](https://energy-based-model.githu..."
          ],
          [
           "pipe.safety_checker = None\n\nimages = []\ngenerator = th.Generator(\"cuda\").manual_seed(args.seed)\nfor ..."
          ],
          [
           "```\n\n### Imagic Stable Diffusion\nAllows you to edit an image using stable diffusion...."
          ],
          [
           "```python\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport torch\nimport os\nfrom d..."
          ],
          [
           "os.makedirs(\"imagic\", exist_ok=True)\nimage = res.images[0]\nimage.save('./imagic/imagic_image_alpha_1..."
          ],
          [
           "```\n\n### Seed Resizing\nTest seed resizing. Originally generate an image in 512 by 512, then generate..."
          ],
          [
           "width = 512\nheight = 592\n\nres = pipe(\n    prompt,\n    guidance_scale=7.5,\n    num_inference_steps=50..."
          ],
          [
           "```\n\n### Multilingual Stable Diffusion Pipeline\n\nThe following code can generate an images from text..."
          ],
          [
           "diffuser_pipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    cust..."
          ],
          [
           "```\n\nThis example produces the following images:\n![image](https://user-images.githubusercontent.com/..."
          ],
          [
           "```\n\n![2 by 2 grid demonstrating image to image inpainting.](https://user-images.githubusercontent.c..."
          ],
          [
           "```\n\n### Bit Diffusion\nBased https://arxiv.org/abs/2208.04202, this is used for diffusion on discret..."
          ],
          [
           "```\n\nTo make sure that K Diffusion and `diffusers` yield the same results:\n\n**Diffusers**:\n```python..."
          ],
          [
           "```\n\n![diffusers_euler](https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/k_diffu..."
          ],
          [
           "#Three checkpoint merging. Only \"add_difference\" method actually works on all three checkpoints. Usi..."
          ],
          [
           "```\nSome examples along with the merge details:\n\n1. \"CompVis/stable-diffusion-v1-4\" + \"hakurei/waifu..."
          ],
          [
           "### Stable Diffusion Comparisons\n\nThis Community Pipeline enables the comparison between the 4 check..."
          ],
          [
           "```\n\nAs a result, you can look at a grid of all 4 generated images being shown together, that captur..."
          ],
          [
           "```\nThe `mix_img` is a PIL image that can be saved locally or displayed directly in a google colab. ..."
          ],
          [
           "```python\nimport torch\nfrom diffusers import DiffusionPipeline\n\ndevice = torch.device(\"cpu\" if not t..."
          ],
          [
           "# this pipeline only use prior module in \"kakaobrain/karlo-v1-alpha\"\n# It is used to convert clip te..."
          ],
          [
           "```\n\n\n`shiba-inu.jpg`\n\n\n![shiba-inu](https://user-images.githubusercontent.com/16448529/209185639-6e..."
          ],
          [
           "```\n\nThe resulting images in order:-\n\n![result_0](https://huggingface.co/datasets/NagaSaiAbhinay/UnC..."
          ],
          [
           "```python\nimport torch\nfrom diffusers import DiffusionPipeline\nfrom PIL import Image\n\ndevice = torch..."
          ],
          [
           "```\nThe original images:-\n\n![starry](https://huggingface.co/datasets/NagaSaiAbhinay/UnCLIPImageInter..."
          ],
          [
           "### DDIM Noise Comparative Analysis Pipeline\n#### **ResearchÂ question: What visual concepts do the d..."
          ],
          [
           "for strength in np.linspace(0.1, 1, 25):\n    denoised_image, latent_timestep = pipe(\n        image_p..."
          ],
          [
           "```\n\nHere is the result of this pipeline (which is DDIM) on CelebA-HQ dataset.\n\n![noise-comparative-..."
          ],
          [
           "The following code requires roughly 12GB of GPU RAM.\n\n```python\nfrom io import BytesIO\nimport reques..."
          ],
          [
           "```\n\nInit Image\n\n![img2img_init_clip_guidance](https://huggingface.co/datasets/njindal/images/resolv..."
          ],
          [
           "```\n\n### EDICT Image Editing Pipeline\n\nThis pipeline implements the text-guided image editing approa..."
          ],
          [
           "# initialize pipeline\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model_name_or_pat..."
          ],
          [
           "```\n\nInit Image\n\n![img2img_init_edict_text_editing](https://huggingface.co/datasets/Joqsan/images/re..."
          ],
          [
           "Disclaimer: The mask gets transferred into latent space, this may lead to unexpected changes on the ..."
          ],
          [
           "```\n\n### TensorRT Image2Image Stable Diffusion Pipeline\n\nThe TensorRT Pipeline can be used to accele..."
          ],
          [
           "```\n\n### Stable Diffusion Reference\n\nThis pipeline uses the Reference Control. Refer to the [sd-webu..."
          ],
          [
           "```\n\nReference Image\n\n![reference_image](https://hf.co/datasets/huggingface/documentation-images/res..."
          ],
          [
           "```py\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom diffusers import UniPCMu..."
          ],
          [
           "```\n\nReference Image\n\n![reference_image](https://hf.co/datasets/huggingface/documentation-images/res..."
          ],
          [
           "```\n**Note:** To install a specific version, run with the following command:\n```\npython -m pip insta..."
          ],
          [
           "```\n\nThe following code compares the performance of the original stable diffusion pipeline with the ..."
          ],
          [
           "##############     fp32 inference performance    ###############\n\n# 1. IPEX Pipeline initialization\n..."
          ],
          [
           "```\n\n### CLIP Guided Images Mixing With Stable Diffusion\n\n![clip_guided_images_mixing_examples](http..."
          ],
          [
           "# text2img\nt2i_images = pipe(\n    prompt=prompt,\n    negative_prompt=neg_prompt,\n).images # alternat..."
          ],
          [
           "```\n\nIn the above code, the `prompt2` is appended to the `prompt`, which is more than 77 tokens. \"bi..."
          ],
          [
           "# Pipeline creating\nmixing_pipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusi..."
          ],
          [
           "```\n\n![image_mixing_result](https://huggingface.co/datasets/TheDenk/images_mixing/resolve/main/borom..."
          ],
          [
           "```\n![mixture_tiling_results](https://huggingface.co/datasets/kadirnar/diffusers_readme_images/resol..."
          ],
          [
           "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples..."
          ],
          [
           "```\n\n### Stable Diffusion Mixture Canvas\n\nThis pipeline uses the Mixture. Refer to the [Mixture](htt..."
          ],
          [
           "```\n![Input_Image](https://huggingface.co/datasets/kadirnar/diffusers_readme_images/resolve/main/inp..."
          ],
          [
           "```\n\nThe training loop is also straightforward:\n\n```python\n\n# Training loop\nwhile True:\n    x0 = sam..."
          ],
          [
           "```\n\n### Zero1to3 pipeline\n\nThis pipeline is the implementation of the [Zero-1-to-3: Zero-shot One I..."
          ],
          [
           "# load image\n# H, W = (256, 256) # H, W = (512, 512)   # zero123 training is 256,256\n\n# for batch in..."
          ],
          [
           "# better do preprocessing\nfrom gradio_new import preprocess_image, create_carvekit_interface\nimport ..."
          ],
          [
           "```\n\n### Stable Diffusion XL Reference\n\nThis pipeline uses the Reference . Refer to the [stable_diff..."
          ],
          [
           "```\n\nReference Image\n\n![reference_image](https://hf.co/datasets/huggingface/documentation-images/res..."
          ],
          [
           "### Stable diffusion fabric pipeline\n\nFABRIC approach applicable to a wide range of popular diffusio..."
          ],
          [
           "```\n\n*With enough feedbacks you can create very similar high quality images.*\n\nThe original codebase..."
          ],
          [
           "```python\nimg = PIL.Image.open(\"./mech.png\")\n# read image with mask painted over\nimg_paint = PIL.Ima..."
          ],
          [
           "```\n\noriginal image mech.png\n\n<img src=https://github.com/noskill/diffusers/assets/733626/10ad972d-d..."
          ],
          [
           "```\n\nAnd abbreviated examples for the other edits:\n\n`ReplaceEdit with local blend`\n```python\nprompts..."
          ],
          [
           "```\n\nSide note: See [this GitHub gist](https://gist.github.com/UmerHA/b65bb5fb9626c9c73f3ade2869e361..."
          ],
          [
           "The model can be used with `diffusers` as follows:\n\n - *1. Load the model from the community pipelin..."
          ],
          [
           "```\n\n- 2. Run inference with as little as 4 steps:\n\n```py\nprompt = \"Self-portrait oil painting, a be..."
          ],
          [
           "```\n\n- 2. Run inference with as little as 4 steps:\n\n```py\nprompt = \"Self-portrait oil painting, a be..."
          ],
          [
           "```\n\n\n\n### Latent Consistency Interpolation Pipeline\n\nThis pipeline extends the Latent Consistency P..."
          ],
          [
           "torch.manual_seed(seed)\nnp.random.seed(seed)\n\nimages = pipe(\n    prompt=prompts,\n    height=512,\n   ..."
          ],
          [
           "```\n\n###  StableDiffusionUpscaleLDM3D Pipeline\n[LDM3D-VR](https://arxiv.org/pdf/2311.03226.pdf) is a..."
          ],
          [
           "prompt =f\"A picture of some lemons on a table\"\noutput = pipe_ldm3d(prompt)\nrgb_image, depth_image = ..."
          ],
          [
           "### ControlNet + T2I Adapter Pipeline\nThis pipelines combines both ControlNet and T2IAdapter into a ..."
          ],
          [
           "pipe = StableDiffusionXLControlNetAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion..."
          ],
          [
           "```\n\n### ControlNet + T2I Adapter + Inpainting Pipeline\n```py\nimport cv2\nimport numpy as np\nimport t..."
          ],
          [
           "pipe = StableDiffusionXLControlNetAdapterInpaintPipeline.from_pretrained(\n    \"diffusers/stable-diff..."
          ],
          [
           "depth_image = midas_depth(\n  image, detect_resolution=512, image_resolution=1024\n)\n\nstrength = 0.4\n\n..."
          ],
          [
           "```\n\n### Regional Prompting Pipeline\nThis pipeline is a port of the [Regional Prompter extension](ht..."
          ],
          [
           "```\n### Cols, Rows mode\nIn the Cols, Rows mode, you can split the screen vertically and horizontally..."
          ],
          [
           "```\n![sample](https://github.com/hako-mikan/sd-webui-regional-prompter/blob/imgs/rp_pipeline4.png)\n\n..."
          ],
          [
           "```\n![sample](https://github.com/hako-mikan/sd-webui-regional-prompter/blob/imgs/rp_pipeline3.png)\n#..."
          ],
          [
           "```\nbest quality, 3persons in garden, ADDCOMM\na girl white dress BREAK\na boy blue shirt BREAK\nan old..."
          ],
          [
           "```\n    @article{chung2022diffusion,\n    title={Diffusion posterior sampling for general noisy inver..."
          ],
          [
           "```\n* This pipeline allows zero-shot conditional sampling from the posterior distribution $p(x|y)$, ..."
          ],
          [
           "def forward(self, x):\n                    return self.seq(x)\n\n                def weights_init(self)..."
          ],
          [
           "```\n* Next, you should obtain the corrupted image $y$ by the operator. In this example, we generate ..."
          ],
          [
           "```\n* We provide an example pair of saved source and corrupted images, using the Gaussian blur opera..."
          ],
          [
           "```\n* The zeta is a hyperparameter that is in range of $[0,1]$. It need to be tuned for best effect...."
          ],
          [
           "motion_id = \"guoyww/animatediff-motion-adapter-v1-5-2\"\nadapter = MotionAdapter.from_pretrained(motio..."
          ],
          [
           "<table>\n  <tr><td colspan=\"2\" align=center><b>Conditioning Frames</b></td></tr>\n  <tr align=center>\n..."
          ],
          [
           "<tr><td colspan=\"2\" align=center><b>AnimateDiff model: CardosAnime</b></td></tr>\n  <tr>\n    <td alig..."
          ],
          [
           "- `stride` (`int`, defaults to 64):\n  The stride of moving local patches. A smaller stride is better..."
          ],
          [
           "images = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    height=3072,\n    width=3072,\n   ..."
          ],
          [
           "```\nYou can display and save the generated images as:\n```py\ndef image_grid(imgs, save_path=None):\n\n ..."
          ],
          [
           "```\n ![output_example](https://github.com/PRIS-CV/DemoFusion/blob/main/output_example.png)\n\n### SDE ..."
          ],
          [
           "# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n# If not train..."
          ],
          [
           "DreamBooth training example\n\n[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personali..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell e.g. a notebook\n\n```python\nfrom acc..."
          ],
          [
           "```\n\n### Training with prior-preservation loss\n\nPrior-preservation is used to avoid overfitting and ..."
          ],
          [
           "```\n\n\n### Training on a 16GB GPU:\n\nWith the help of gradient checkpointing and the 8-bit optimizer f..."
          ],
          [
           "```\n\n\n### Training on a 12GB GPU:\n\nIt is possible to run dreambooth on a 12GB GPU by using the follo..."
          ],
          [
           "```\n\n\n### Training on a 8 GB GPU:\n\nBy using [DeepSpeed](https://www.deepspeed.ai/) it's possible to ..."
          ],
          [
           "accelerate launch --mixed_precision=\"fp16\" train_dreambooth.py \\\n  --pretrained_model_name_or_path=$..."
          ],
          [
           "```\n\n### Fine-tune text encoder with the UNet.\n\nThe script also allows to fine-tune the `text_encode..."
          ],
          [
           "```\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\" --> export MODEL_NAME=\"BAAI/AltDiffusion-m9\"\no..."
          ],
          [
           "```\n\n### Inference from a training checkpoint\n\nYou can also perform inference from one of the checkp..."
          ],
          [
           "### Training\n\nLet's get started with a simple example. We will re-use the dog example of the [previo..."
          ],
          [
           "```\n\nFor this example we want to directly store the trained LoRA embeddings on the Hub, so \nwe need ..."
          ],
          [
           "```\n\n**___Note: When using LoRA we can use a much higher learning rate compared to vanilla dreamboot..."
          ],
          [
           "### Inference\n\nAfter training, LoRA weights can be loaded very easily into the original pipeline. Fi..."
          ],
          [
           "```\n\nNext, we can load the adapter layers into the UNet with the [`load_attn_procs` function](https:..."
          ],
          [
           "```\n\nIf you used `--train_text_encoder` during training, then use `pipe.load_lora_weights()` to load..."
          ],
          [
           "```\n\n* LoRA parameters that have separate identifiers for the UNet and the text encoder such as: [`\"..."
          ],
          [
           "```\n\n\n### Training with prior preservation loss\n\n```bash\nexport MODEL_NAME=\"duongna/stable-diffusion..."
          ],
          [
           "```\n\n### Training with xformers:\nYou can enable memory efficient attention by [installing xFormers](..."
          ],
          [
           "Note that IF has a predicted variance, and our finetuning scripts only train the models predicted er..."
          ],
          [
           "```\n\nAdditionally, a few alternative cli flags are needed for IF.\n\n`--resolution=64`: IF is a pixel ..."
          ],
          [
           "### Stage II additional validation images\n\nThe stage II validation requires images to upscale, we ca..."
          ],
          [
           "```\n\n### IF stage I LoRA Dreambooth\nThis training configuration requires ~28 GB VRAM.\n\n```sh\nexport ..."
          ],
          [
           "```\n\n### IF stage II LoRA Dreambooth\n\n`--validation_images`: These images are upscaled during valida..."
          ],
          [
           "```\n\n### IF Stage I Full Dreambooth\n`--skip_save_text_encoder`: When training the full model, this w..."
          ],
          [
           "```\n\n### IF Stage II Full Dreambooth\n\n`--learning_rate=5e-6`: With a smaller effective batch size of..."
          ],
          [
           "```\n\n## Stable Diffusion XL\n\nWe support fine-tuning of the UNet shipped in [Stable Diffusion XL](htt..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\n## DreamBooth\n\n[DreamBooth](https://dreambooth.github.io/) finetunes an *entire diffusion mo..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n### Load multiple LoRAs\n\nIt can be fun to use multiple LoRAs together to create something entir..."
          ],
          [
           "```\n\nThen fuse this pipeline with the next set of LoRA weights:\n\n```py\npipeline.load_lora_weights(\"o..."
          ],
          [
           "```\n\nNow use the [`~loaders.UNet2DConditionLoadersMixin.set_adapters`] to activate both LoRAs, and y..."
          ],
          [
           "```\n\nLoad the LoRA checkpoint with the [`~loaders.LoraLoaderMixin.load_lora_weights`] method, and sp..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nSome limitations of using Kohya LoRAs with ðŸ¤— Diffusers include:\n\n- Images..."
          ],
          [
           "```\n\n## IP-Adapter \n\n[IP-Adapter](https://ip-adapter.github.io/) is an effective and lightweight ada..."
          ],
          [
           "```\n\n<Tip>\nIP-Adapter relies on an image encoder to generate the image features, if your IP-Adapter ..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\nÂ  Â  <img src=\"https://huggingface.co/datasets/YiYiXu/testing-..."
          ],
          [
           "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\nge..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"inpaint\">\n\n```py\nfrom diffusers import AutoPipelineForInpaint\nimport ..."
          ],
          [
           "```\n</hfoption>\n</hfoptions>\n\n\nIP-Adapters can also be used with [SDXL](../api/pipelines/stable_diff..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "```\n\n<Tip>\n\nIt is recommended to use `DDIMScheduler` and `EulerDiscreteScheduler` for face model. \n\n..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "prompt = \"best quality, high quality\"\nimage = load_image(\"https://user-images.githubusercontent.com/..."
          ],
          [
           "```\n\n### Other pipelines\n\nIP-Adapter is compatible with any pipeline that (1) uses a text prompt and..."
          ],
          [
           "```\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nimport torch\nfrom diffu..."
          ],
          [
           "```\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https:..."
          ],
          [
           "# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\n# load ip_adapter..."
          ],
          [
           "export_to_gif(output_frames, \"test_out_animation.gif\")..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nTo ensure your pipeline and its components (`unet` and `scheduler`) can be saved with [`~Diffus..."
          ],
          [
           "```\n\nThat's it! ðŸš€ You can now run this pipeline by passing a `unet` and `scheduler` to it:\n\n```pytho..."
          ],
          [
           "```\n\nAnother way to share your community pipeline is to upload the `one_step_unet.py` file directly ..."
          ],
          [
           "```\n\nTake a look at the following table to compare the two sharing workflows to help you decide the ..."
          ],
          [
           "</Tip>\n\n## How do community pipelines work?\n\nA community pipeline is a class that inherits from [`Di..."
          ],
          [
           "```\n\nThe magic behind community pipelines is contained in the following code. It allows the communit..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Transformer2DModel\n\n[[autodoc]] Transformer2DModel\n\n## Transformer2DModelOutput\n\n[[autodoc]] mode..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Image to Video Generation\n\nThe are two variants of SVD. [SVD](https://huggingface.co/stabili..."
          ],
          [
           "```\n\n<video controls width=\"1024\" height=\"576\">\n  <source src=\"https://huggingface.co/datasets/huggi..."
          ],
          [
           "```\n\n### Low-memory\n\nVideo generation is very memory intensive as we have to essentially generate `n..."
          ],
          [
           "```\n\n\nIncluding all these tricks should lower the memory requirement to less than 8GB VRAM.\n\n### Mic..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "RealFill\n\n[RealFill](https://arxiv.org/abs/2309.16668) is a method to personalize text2image inpaint..."
          ],
          [
           "```\n\nOr if your environment doesn't support an interactive shell e.g. a notebook\n\n```python\nfrom acc..."
          ],
          [
           "```\n\n### Training on a low-memory GPU:\n\nIt is possible to run realfill on a low-memory GPU by using ..."
          ],
          [
           "```\n\n### Training with gradient checkpointing and 8-bit optimizers:\n\nWith the help of gradient check..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nTo setup a default ðŸ¤— Accelerate environment without choosing any configurations:\n\n```bash\naccel..."
          ],
          [
           "```\n\nSome basic and important parameters to specify include:\n\n- `--dataset_name`: the name of the da..."
          ],
          [
           "```py\nmodel = UNet2DModel(\n    sample_size=args.resolution,\n    in_channels=3,\n    out_channels=3,\n ..."
          ],
          [
           "```\n\nNext, the script initializes a [scheduler](https://github.com/huggingface/diffusers/blob/096f84..."
          ],
          [
           "```\n\nThen it [loads a dataset](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f185c..."
          ],
          [
           "```\n\nFinally, the [training loop](https://github.com/huggingface/diffusers/blob/096f84b05f9514fae9f1..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\nThe training script creates and saves a checkpoint file in your repos..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Tips\n\nStable unCLIP takes  `noise_level` as input during inference which determines how much nois..."
          ],
          [
           "stable_unclip_model_id = \"stabilityai/stable-diffusion-2-1-unclip-small\"\n\npipe = StableUnCLIPPipelin..."
          ],
          [
           "```\n<Tip warning={true}>\n\nFor text-to-image we use `stabilityai/stable-diffusion-2-1-unclip-small` a..."
          ],
          [
           "```\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract of the paper is the following:\n\n*Diffusion models have shown promising results in cross..."
          ],
          [
           "During inference:\n\n* The _quality_ of the generated audio sample can be controlled by the `num_infer..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<Tip>\n\nTo check a specific pipeline or model output, refer to its corresponding API documentati..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/ma..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "This model was contributed by [takuma104](https://huggingface.co/takuma104). â¤ï¸\n\nThe original codeba..."
          ],
          [
           "## StableDiffusionControlNetInpaintPipeline\n[[autodoc]] StableDiffusionControlNetInpaintPipeline\n\t- ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n<Tip>\n\nYou'll notice throughout the guide, we use [`~DiffusionPipeline.enable_model_cpu_offload..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "## Popular models\n\n[Stable Diffusion Inpainting](https://huggingface.co/runwayml/stable-diffusion-in..."
          ],
          [
           "generator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elv..."
          ],
          [
           "```\n\n### Stable Diffusion XL (SDXL) Inpainting\n\nSDXL is a larger and more powerful version of Stable..."
          ],
          [
           "```\n\n### Kandinsky 2.2 Inpainting\n\nThe Kandinsky model family is similar to SDXL because it uses two..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "## Non-inpaint specific checkpoints\n\nSo far, this guide has used inpaint specific checkpoints such a..."
          ],
          [
           "generator = torch.Generator(\"cuda\").manual_seed(92)\nprompt = \"concept art digital painting of an elv..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"runwayml/stable-diffusion-inpainting\">\n\n```py\nimport torch\nfrom diffu..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"htt..."
          ],
          [
           "# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/docu..."
          ],
          [
           "```\n\n</hfoption>\n<hfoption id=\"runwayml/stable-diffusion-inpaint\">\n\n```py\nimport torch\nfrom diffuser..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"htt..."
          ],
          [
           "```py\nimport PIL\nimport numpy as np\nimport torch\n\nfrom diffusers import AutoPipelineForInpainting\nfr..."
          ],
          [
           "# Convert mask to grayscale NumPy array\nmask_image_arr = np.array(mask_image.convert(\"L\"))\n# Add a c..."
          ],
          [
           "```\n\n## Configure pipeline parameters\n\nImage features - like quality and \"creativity\" - are dependen..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "You can use `strength` and `guidance_scale` together for more control over how expressive the model ..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "```py\nimport torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n  <figure>\n    <img class=\"rounded-xl\" src=\"https://huggingfa..."
          ],
          [
           "```\n\nLoad the mask image of the output from above:\n\n```py\nmask_image = load_image(\"https://huggingfa..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "# load base and mask image\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/docu..."
          ],
          [
           "```\n\nNow let's pass the image to another inpainting pipeline with SDXL's refiner model to enhance th..."
          ],
          [
           "```\n\n<Tip>\n\nIt is important to specify `output_type=\"latent\"` in the pipeline to keep all the output..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "### Prompt weighting\n\nPrompt weighting provides a quantifiable way to scale the representation of co..."
          ],
          [
           "```\n\n### ControlNet\n\nControlNet models are used with other diffusion models like Stable Diffusion, a..."
          ],
          [
           "# prepare control image\ndef make_inpaint_condition(init_image, mask_image):\n    init_image = np.arra..."
          ],
          [
           "```\n\nNow generate an image from the base, mask and control images. You'll notice features of the bas..."
          ],
          [
           "```\n\n<div class=\"flex flex-row gap-4\">\n  <div class=\"flex-1\">\n    <img class=\"rounded-xl\" src=\"https..."
          ],
          [
           "You can also offload the model to the CPU to save even more memory:\n\n```diff\n+ pipeline.enable_xform..."
          ],
          [
           "```\n\nTo speed-up your inference code even more, use [`torch_compile`](../optimization/torch2.0#torch..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "You can find additional information about Text-to-Video on the [project page](https://modelscope.cn/..."
          ],
          [
           "```\n\nDiffusers supports different optimization techniques to improve the latency\nand memory footprin..."
          ],
          [
           "```\n\nHere are some sample outputs:\n\n<table>\n    <tr>\n        <td><center>\n        An astronaut ridin..."
          ],
          [
           "# memory optimization\npipe.unet.enable_forward_chunking(chunk_size=1, dim=1)\npipe.enable_vae_slicing..."
          ],
          [
           "```\n\nNow the video can be upscaled:\n\n```py\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeros..."
          ],
          [
           "```\n\nHere are some sample outputs:\n\n<table>\n    <tr>\n        <td ><center>\n        Darth vader surfi..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## AttnAddedKVProcessor2_0\n[[autodoc]] models.attention_processor.AttnAddedKVProcessor2_0\n\n## LoRAAt..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## DPMSolverMultistepInverseScheduler\n[[autodoc]] DPMSolverMultistepInverseScheduler\n\n## SchedulerOu..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Models\n\nTo push a model to the Hub, call [`~diffusers.utils.PushToHubMixin.push_to_hub`] and..."
          ],
          [
           "```\n\nThe [`~diffusers.utils.PushToHubMixin.push_to_hub`] function saves the scheduler's `scheduler_c..."
          ],
          [
           "```\n\n## Pipeline\n\nYou can also push an entire pipeline with all it's components to the Hub. For exam..."
          ],
          [
           "```\n\nPass all of the components to the [`StableDiffusionPipeline`] and call [`~diffusers.utils.PushT..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The original codebase can be found at [Fantasy-Studio/Paint-by-Example](https://github.com/Fantasy-S..."
          ],
          [
           "Dreambooth for the inpainting model\n\nThis script was added by @thedarkzeno .\n\nPlease note that this ..."
          ],
          [
           "```\n\n### Training with prior-preservation loss\n\nPrior-preservation is used to avoid overfitting and ..."
          ],
          [
           "```\n\n\n### Training with gradient checkpointing and 8-bit optimizer:\n\nWith the help of gradient check..."
          ],
          [
           "```\n\n### Fine-tune text encoder with the UNet.\n\nThe script also allows to fine-tune the `text_encode..."
          ],
          [
           "Create a dataset for training\n\nThere are many datasets on the [Hub](https://huggingface.co/datasets?..."
          ],
          [
           "```\n\n## Upload your data to the Hub\n\n<Tip>\n\nðŸ’¡ For more details and context about creating and upload..."
          ],
          [
           "```\n\nThen use the [`~datasets.Dataset.push_to_hub`] method to upload the dataset to the Hub:\n\n```pyt..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nNow, define four different `Generator`s and assign each `Generator` a seed (`0` to `3`) so you ..."
          ],
          [
           "```\n\nCreate four generators with seed `0`, and generate another batch of images, all of which should..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nGiven a prompt, get the inference time for the original model:\n\n```py\nimport time\n\nseed = 2023\n..."
          ],
          [
           "```\n\n<div class=\"flex gap-4\">\n  <div>\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/datase..."
          ],
          [
           "```\n\nTime the distilled model and distilled VAE inference:\n\n```py\nstart = time.time_ns()\nfor _ in ra..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "You can find additional information about the model on the [project page](https://diffusion-planning..."
          ]
         ],
         "hovertemplate": "source=diffusers<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "diffusers, circle",
         "marker": {
          "color": "#FECB52",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "diffusers, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -0.7318202,
          -0.18688634,
          -0.29768854,
          -0.6892882,
          0.06676113,
          0.74323463,
          10.163272,
          -0.04222114,
          -0.20889692,
          0.26692834,
          0.45898688,
          0.45699465,
          1.0157629,
          -7.8958015,
          -7.9594393,
          1.302423,
          -0.6755686,
          -0.44968593,
          0.1607913,
          0.50429106,
          0.41728735,
          2.2788112,
          0.77291524,
          0.51395786,
          2.2564976,
          1.0483788,
          1.9680492,
          0.7947766,
          1.946378,
          1.6566569,
          0.5986395,
          2.2960534,
          0.5270253,
          -0.36844507,
          -0.077558585,
          0.55197954,
          -0.39253342,
          0.58002114,
          -0.45406893,
          -10.150271,
          -0.654976,
          0.8045265,
          0.9930166,
          1.4343187,
          1.8587593,
          1.3902202,
          2.2516456,
          -0.8056126,
          1.121613,
          4.3187103,
          -0.73976034,
          -0.35169944,
          1.4087583,
          0.26472583,
          1.3888327,
          0.16115248,
          0.3459042,
          -0.05376691,
          -0.26926708,
          0.5166864,
          2.657714,
          -0.46838257,
          -0.98223823,
          0.81851184,
          -0.34147617,
          0.5588775,
          -2.3172257,
          -2.6114812,
          14.190661,
          -1.2500495,
          -0.5391388,
          1.3570611,
          0.6983362,
          1.6599342,
          1.3118162,
          2.423071,
          1.680592,
          1.2801192,
          1.5033814,
          1.570063,
          -0.22116117,
          -0.674534,
          -3.0571735,
          0.847121,
          0.582269,
          -0.12447619,
          -1.0345579,
          1.246627,
          -0.717795,
          -0.8177924,
          1.3101944,
          -0.7267158,
          -0.8548418,
          0.9277932,
          1.6347014,
          1.1063483,
          1.7413269,
          0.66476476,
          0.64291054,
          0.68866754,
          0.4238246,
          0.5675347,
          0.55408835,
          0.6108598,
          -9.396087,
          6.324736,
          1.4381883,
          1.7718227,
          0.3050014,
          0.8528661,
          0.9356126,
          -0.041456863,
          -0.38886243,
          -0.29259032,
          1.2690321,
          1.8368399,
          2.2909193,
          2.0150683,
          2.5196722,
          2.3828204,
          2.1383913,
          2.6056473,
          -0.728996,
          -0.6073124,
          1.5620052,
          1.1745384,
          1.871743,
          0.8002358,
          2.0939004,
          0.88427246,
          -0.4124296,
          -0.7625261,
          1.3179592,
          0.31199172,
          0.23144704,
          -0.6422595,
          0.73583287,
          0.7588117,
          -0.3668478,
          -0.9462115,
          1.2827936,
          -0.118472524,
          0.2798819,
          2.7722552,
          0.43398336,
          0.1508469,
          -0.61699975,
          -0.80549973,
          1.2860647,
          0.4927748,
          0.9903323,
          1.1766906,
          0.62800324,
          1.0242198,
          0.773456,
          1.0745772,
          -7.411592,
          -1.5562085,
          -1.685693,
          -1.8324426,
          0.3531079,
          -1.3501964,
          -2.0245843,
          0.49593154,
          -1.6965852,
          -1.6399976,
          0.13116358,
          -0.41051126,
          -0.6199205,
          -0.66920465,
          0.030627206,
          -0.17920889,
          0.82955575,
          10.164178,
          0.26497623,
          -0.14883962,
          1.2124583,
          1.3126591,
          -0.4292618,
          0.6501436,
          0.50232285,
          -0.75782114,
          -0.83806276,
          0.74137473,
          1.0714482,
          -0.37370098,
          0.3470572,
          0.37320343,
          0.47019377,
          0.5500779,
          0.612876,
          -0.27329016,
          1.051295,
          0.9486164,
          -2.2856972,
          0.5062597,
          0.6553592,
          0.85894334,
          0.7334182,
          -0.11181463,
          0.7506524,
          3.7138762,
          -0.21153134,
          0.0379223,
          1.1433572,
          1.0004342,
          -0.27496302,
          0.14777783,
          0.29371065,
          0.32320425,
          -0.47398448,
          0.39037186,
          1.1095458,
          -0.47600535,
          0.40451285,
          -0.45350802,
          0.42158478,
          -0.06577168,
          -0.22118777,
          0.08878127,
          1.2542621,
          0.3999612,
          0.2966077,
          0.53216714,
          -0.31351182,
          -0.12540284,
          -0.20501626,
          0.5486955,
          -0.23931661,
          -1.0262481,
          -0.79237425,
          1.1093026,
          -0.21549116,
          1.8129156,
          0.7188339,
          2.6993442,
          -1.1664182,
          -1.1098456,
          1.1936427,
          -0.264007,
          0.21241058,
          2.4288552,
          0.25176522,
          0.68763036,
          1.2065014,
          1.9942153,
          0.9997383,
          1.9262586,
          1.5267446,
          1.8298957,
          2.279671,
          1.785543,
          2.3270519,
          1.9568524,
          1.1190649,
          2.2394273,
          -1.1341517,
          -0.49373513,
          -0.43215904,
          -0.5169503,
          1.1376364,
          2.1265342,
          1.9842702,
          2.3454456,
          -0.22284476,
          -0.78317434,
          -0.849143,
          -0.027033705,
          -0.1171579,
          3.658412,
          0.31039304,
          2.0266583,
          1.2629281,
          0.78165066,
          0.8500839,
          0.66332895,
          0.18357019,
          1.2129747,
          1.3228987,
          0.71054965,
          0.08721209,
          0.02236899,
          4.486625,
          -0.42766076,
          -0.8629938,
          0.115800895,
          1.4704263,
          0.18417537,
          0.1395766,
          0.42520434,
          -0.3537756,
          0.38007322,
          2.5202832,
          -0.93654686,
          -0.024162285,
          2.149849,
          -0.07413833,
          0.5384565,
          0.43079114,
          -0.92153555,
          -0.50345606,
          0.24281363,
          2.9831629,
          0.6915659,
          2.111644,
          0.44059303,
          0.63405967,
          -0.049240977,
          -0.88651884,
          0.260567,
          -0.39801538,
          -0.5906677,
          -0.37869585,
          -0.32399786,
          -0.08004383,
          0.18680438,
          -0.24679874,
          -0.98890275,
          -0.23301175,
          0.7705897,
          10.163071,
          -0.13833608,
          -0.08264798,
          0.119129114,
          0.17010915,
          0.09201309,
          -0.2887783,
          -0.36057812,
          0.18092018,
          0.42262846,
          0.38651964,
          0.6257401,
          0.07673361,
          0.9232201,
          -0.3836446,
          -0.69810647,
          1.082586,
          3.720138,
          -0.016541053,
          0.23790395,
          0.3734238,
          1.014004,
          1.8997446,
          1.9401344,
          1.5094342,
          1.7153989,
          0.27466306,
          1.7572662,
          1.7846406,
          1.2708812,
          0.77174294,
          0.06786806,
          1.8653417,
          0.16369496,
          1.8145348,
          -0.500431,
          0.15194304,
          0.3941901,
          0.046727594,
          -0.046434015,
          -0.19461125,
          -0.18224533,
          -0.16294818,
          1.2270057,
          1.4702286,
          0.8307208,
          0.917925,
          1.8955181,
          1.3731085,
          7.9616194,
          0.7054513,
          2.0980797,
          1.0965568,
          -0.026649535,
          1.2587322,
          0.35969576,
          1.0410392,
          -0.12035621,
          2.2770092,
          1.7068998,
          0.9277291,
          -0.44729817,
          -0.84871143,
          1.428804,
          -8.810224,
          -11.81651,
          -0.7104464,
          0.4777524,
          1.2715816,
          -7.9274497,
          -7.9085035,
          -7.8443933,
          -7.302864,
          -7.323975,
          6.0305276,
          0.15674897,
          0.10079928,
          0.7579461,
          0.6662767,
          0.02573071,
          0.24310495,
          -0.16063501,
          0.16713439,
          0.8887207,
          0.49569753,
          0.22552435,
          -0.05431542,
          -0.30239215,
          -0.2767503,
          -0.60160357,
          -0.40640303,
          3.114205,
          0.6640747,
          2.063989,
          -0.49654162,
          -0.43493512,
          -7.362345,
          -0.55330193,
          -0.6272028,
          -0.06316881,
          -0.3145449,
          -0.6429735,
          -0.4998012,
          -0.29846895,
          -1.0201247,
          0.78502893,
          -0.14755325,
          1.5938995,
          -1.2904811,
          -0.6871096,
          -0.7656497,
          -0.35770455,
          -0.6509049,
          1.7741743,
          -0.61771524,
          -0.88522995,
          -0.08928321,
          -8.9988165,
          -0.22868203,
          -11.847991,
          0.2158952,
          5.8972373,
          0.28601363,
          0.40301993,
          0.17741299,
          0.14568387,
          0.33375603,
          0.5026634,
          0.27238116,
          0.29539824,
          0.36026132,
          0.37887558,
          0.5145614,
          1.0136193,
          0.711387,
          2.6092794,
          0.48012823,
          0.21148036,
          0.3113276,
          0.42747214,
          1.3059404,
          5.3849936,
          0.4068724,
          3.814178,
          6.6138062,
          4.8764424,
          4.777068,
          0.42322975,
          4.132927,
          3.935613,
          4.7879558,
          0.002418521,
          0.43836933,
          0.3433803,
          0.28488863,
          0.030302946,
          -0.6512578,
          -0.4757704,
          2.6676211,
          1.7210921,
          2.244289,
          1.9791545,
          1.9583057,
          2.162352,
          1.7632327,
          2.064304,
          2.0933015,
          0.6588874,
          -0.21920592,
          0.18547589,
          1.7308127,
          1.442205,
          2.5676541,
          1.7299099,
          1.0316095,
          1.7526324,
          2.5447972,
          1.268965,
          2.3575108,
          1.1200944,
          2.2122793,
          1.1610131,
          2.0076826,
          1.2909446,
          1.8918258,
          1.5399512,
          2.5066433,
          0.746365,
          1.4702911,
          1.4561595,
          1.7681478,
          2.3390615,
          0.23005286,
          0.21176922,
          0.27511677,
          0.526227,
          0.42339024,
          0.25984243,
          0.46089932,
          4.2260876,
          4.1039515,
          -0.39808154,
          -0.38072154,
          -0.8463744,
          0.36911342,
          0.7003085,
          1.3821001,
          1.278249,
          1.4337379,
          1.1328841,
          2.055402,
          1.6493633,
          1.5041792,
          -0.76026374,
          -0.9750712,
          0.43765554,
          0.9351645,
          -0.82641876,
          -0.76194125,
          -0.061243217,
          0.0010187934,
          0.73947084,
          -0.050662674,
          1.2162187,
          1.174046,
          0.20521031,
          0.3059093,
          0.28423816,
          1.3135579,
          1.1528914,
          0.15538144,
          1.332752,
          -8.893272,
          -0.95117044,
          1.2735044,
          -0.14072345,
          -0.64214385,
          0.24970539,
          0.7551358,
          10.164247,
          0.29056832,
          0.38607684,
          -1.5950143,
          0.64417464,
          0.6014161,
          0.41557372,
          0.34428775,
          1.0105946,
          -0.13496208,
          0.90457106,
          5.042739,
          0.25923118,
          -0.8332267,
          3.420152,
          4.034724,
          0.3996267,
          0.7595482,
          2.4353082,
          1.2848283,
          1.8836893,
          1.2533522,
          1.7163941,
          2.426638,
          1.2927977,
          0.662717,
          0.18801445,
          0.24618132,
          2.0216732,
          1.3656026,
          1.0792404,
          3.1621766,
          -0.03196093,
          0.9044791,
          0.9071719,
          1.5189227,
          -0.047707077,
          0.49627215,
          10.163143,
          -0.0045538372,
          0.4563175,
          1.100868,
          0.52176213,
          -0.6442708,
          -0.62067723,
          -0.5271052,
          0.82646906,
          0.9788381,
          0.017287843,
          0.07670085,
          -0.022877287,
          0.12650894,
          0.45772317,
          1.3500651,
          0.74071956,
          1.2412518,
          -1.1629249,
          -2.5338988,
          -1.5981088,
          0.072652474,
          -0.7735088,
          -0.9229216,
          -1.0175655,
          -0.6111861,
          -0.35880542,
          0.25659224,
          -1.593475,
          -1.0030906,
          -0.7537488,
          -0.019031474,
          -0.13194236,
          0.35639378,
          10.1641,
          0.5308292,
          0.39227542,
          0.41857225,
          0.4950519,
          1.3528497,
          -0.6135802,
          -10.403509,
          -0.53231204,
          -10.163658,
          -0.573425,
          0.44044977,
          0.81163377,
          10.163481,
          0.3330963,
          0.35205224,
          -0.113256544,
          0.53337806,
          0.480548,
          0.5604062,
          0.65724534,
          0.6353126,
          0.6555713,
          0.7019637,
          0.9061769,
          -0.11338198,
          -0.26619476,
          0.7053509,
          0.6418409,
          0.33613187,
          0.1902883,
          -0.1289524,
          1.2367517,
          -0.42154345,
          0.118912615,
          0.9214309,
          -0.56585675,
          -0.13764349,
          -0.7480598,
          -0.13127787,
          -1.4943314,
          1.0530272,
          1.7841779,
          1.8892257,
          -3.11736,
          0.11179954,
          -0.26971388,
          -1.071306,
          0.87027043,
          2.8949974,
          2.9874327,
          -0.5395092,
          -0.878268,
          0.024135703,
          -0.8806707,
          0.760495,
          1.9574553,
          -1.3041044,
          -0.27752778,
          -0.20292054,
          -0.18058766,
          6.533404,
          1.2879001,
          -0.20480141,
          1.8807672,
          2.2784703,
          0.7697438,
          -0.30580497,
          0.53687817,
          1.3893527,
          1.388503,
          -8.723259,
          -0.33458543,
          -6.136114,
          -0.062370334,
          -0.3478992,
          -9.341747,
          1.8853207,
          0.65806013,
          2.033682,
          -0.22018886,
          -0.6093775,
          -0.7853206,
          1.881505,
          1.6247911,
          1.7733272,
          1.8213519,
          1.7428124,
          1.7831043,
          0.018175213,
          10.16426,
          0.48007163,
          -0.08544474,
          0.0693609,
          0.80543447,
          1.0432353,
          2.116675,
          0.33627203,
          -0.539562,
          1.2023156,
          -0.51836896,
          0.42593443,
          1.2639074,
          0.40027487,
          0.79706925,
          1.816189,
          1.4376706,
          -0.2585012,
          -0.77143204,
          0.40322354,
          1.5244799,
          1.9229772,
          1.2248061,
          -0.2991446,
          -0.24863787,
          2.1493301,
          -0.25812438,
          -0.4096997,
          0.17113511,
          -0.17374304,
          2.5019464,
          0.26894176,
          -1.226609,
          -0.4511705,
          -0.046743218,
          0.82730263,
          -0.9343474,
          2.3278031,
          2.057567,
          2.0297086,
          1.9427266,
          2.0201626,
          2.5104704,
          -1.0366004,
          -1.6394714,
          -1.7164489,
          -1.6721722,
          -1.6229649,
          -1.7063549,
          -1.6598451,
          -1.7359375,
          -1.6686875,
          -1.6479483,
          -1.6509134,
          -1.6730174,
          -1.361862,
          0.27817303,
          5.16274,
          5.207396,
          0.008444422,
          1.2465043,
          0.3379962,
          0.97545886,
          1.1208552,
          1.2754939,
          1.150461,
          1.1796103,
          1.4336681,
          -0.6717057,
          1.1600555,
          1.2296896,
          1.3177488,
          -0.56208044,
          1.1299187,
          1.1756473,
          1.3798909,
          1.3421596,
          1.2279314,
          3.297962,
          1.3744898,
          -0.4334646,
          -0.93922156,
          -0.76360965,
          1.3684372,
          -0.070860974,
          1.3252947,
          1.0162576,
          1.722946,
          1.8379892,
          0.6105758,
          1.6237961,
          0.52123064,
          1.2765582,
          1.4197217,
          0.48966324,
          0.56699276,
          0.48574975,
          1.727493,
          1.266959,
          1.7339032,
          1.9365827,
          -0.01478449,
          -2.4690306,
          -0.8304349,
          1.5802597,
          1.2281835,
          0.52570945,
          0.24632949,
          0.61963403,
          0.78265446,
          0.49502435,
          1.9280578,
          0.48634994,
          0.9306266,
          6.8415504,
          6.850476,
          0.4058332,
          -1.6044874,
          -0.2976451,
          -0.4752479,
          -10.146133,
          0.27356893,
          0.7388955,
          10.16418,
          0.21028967,
          2.4024286,
          0.7908374,
          0.47893992,
          0.5081312,
          0.3858634,
          0.9817077,
          -0.35563976,
          -0.30516884,
          0.17050335,
          0.41233915,
          0.39520827,
          0.79672104,
          0.3646265,
          0.73600686,
          1.2759762,
          1.4266479,
          -0.8109594,
          -0.30491158,
          2.0664594,
          0.43162605,
          2.2409284,
          0.52728677,
          0.8449571,
          2.0477483,
          0.70111454,
          1.8490366,
          0.48566777,
          -0.6180298,
          -0.88967335,
          4.53222,
          -0.14924933,
          -0.12156352,
          0.712955,
          0.16298935,
          0.650309,
          -0.74349654,
          0.9010615,
          -0.62651646,
          -0.9313354,
          1.2612935,
          0.49350506,
          1.5649537,
          0.8220904,
          -0.27616546,
          1.3407062,
          0.048548914,
          0.51387775,
          1.1124308,
          0.5488947,
          1.0508941,
          0.88930064,
          -0.1410062,
          0.8408322,
          -0.7870824,
          0.40240845,
          -0.17477341,
          0.16967642,
          0.27253163,
          0.90031564,
          0.9316504,
          1.0085903,
          1.1114618,
          0.68247813,
          -0.8850582,
          0.7245724,
          0.3897957,
          0.4712139,
          0.54005784,
          -0.009588456,
          0.4727505,
          0.3328069,
          -0.08388724,
          -0.020942757,
          -0.1361839,
          -0.059867367,
          0.5867021,
          1.3360062,
          1.2387472,
          0.943483,
          1.0536044,
          1.2754645,
          1.0721387,
          1.4783455,
          1.124291,
          2.2087154,
          0.33797136,
          -0.43891233,
          0.550254,
          -3.730262,
          -3.480408,
          0.5260859,
          1.503294,
          -0.51510006,
          -0.44854623,
          -0.5948104,
          -0.4111005,
          1.2829643,
          -0.00019543612,
          0.78256524,
          -0.066474184,
          7.4709544,
          -0.47356823,
          -0.205763,
          0.53246766,
          -0.12611076,
          -0.81238043,
          1.4518919,
          -11.859849,
          -0.9697161,
          -0.2722686,
          1.5739704,
          2.2895074,
          1.9167485,
          1.9193788,
          2.0261521,
          2.011783,
          2.1404083,
          1.8200554,
          1.7405622,
          1.8968112,
          2.0126674,
          2.1598475,
          1.8631588,
          1.7812312,
          2.0585108,
          1.6800623,
          1.9890869,
          1.8543204,
          1.5499802,
          1.824672,
          2.4952588,
          1.8307878,
          2.1931255,
          1.7494881,
          -0.19782203,
          -0.18306425,
          0.054222938,
          1.0898168,
          0.34602764,
          1.0638138,
          2.5033934,
          1.0955242,
          0.30340847,
          0.39019886,
          -0.18592553,
          -0.6690758,
          -1.0018642,
          0.31167963,
          0.33551,
          0.42921048,
          0.50339204,
          0.08765831,
          -0.0723558,
          2.7615054,
          0.29447135,
          0.34294188,
          0.57240456,
          0.092994235,
          2.2788444,
          1.0496006,
          2.220293,
          1.192989,
          2.1840162,
          1.3582817,
          2.304299,
          1.6723503,
          2.3195937,
          1.8553141,
          2.4056342,
          1.7880032,
          2.4433975,
          2.1589053,
          1.8623596,
          1.7637072,
          1.7327087,
          1.5863888,
          2.0763812,
          1.600624,
          1.8367147,
          1.4881622,
          2.5796394,
          -0.33517843,
          -0.35958838,
          0.6524529,
          -1.3315705,
          0.69786805,
          -0.22365315,
          1.167169,
          -0.12831788,
          -0.1440883,
          0.3153733,
          1.5077001,
          -0.58676076,
          -0.42025816,
          3.1304262,
          0.38502163,
          1.1262156,
          0.23401348,
          0.654166,
          0.17113216,
          -0.010246787,
          0.88211685,
          10.163726,
          0.5599193,
          0.66389734,
          1.2368814,
          -1.6630028,
          0.47017682,
          0.60426223,
          0.77388984,
          0.6566777,
          -0.32784656,
          -0.9164379,
          0.7853213,
          2.9475405,
          1.1970644,
          0.5221905,
          -0.19019848,
          0.17528321,
          1.6239965,
          -0.75711614,
          0.42495182,
          0.7269947,
          10.164172,
          -0.17053245,
          0.966954,
          -0.20191373,
          0.1869859,
          0.27744168,
          -0.7612846,
          0.44673458,
          0.5870757,
          1.0241859,
          -0.39316532,
          -0.35879445,
          -0.004071245,
          0.3324346,
          0.50663537,
          0.07364323,
          0.3466066,
          -0.18233782,
          1.0050666,
          1.2471495,
          0.54846144,
          -0.3726653,
          -10.278415,
          5.115158,
          5.776653,
          5.8671436,
          5.4320717,
          2.8419619,
          1.5882894,
          0.4049985,
          4.8921685,
          0.7508921,
          -0.9509103,
          -0.29011905,
          0.6412116,
          -0.13128178,
          0.79925567,
          10.164218,
          0.47376543,
          0.08494293,
          0.78662336,
          0.33386618,
          0.60568553,
          -0.25287378,
          0.627407,
          -9.257164,
          -0.89662164,
          0.13512063,
          2.4100459,
          -11.600165,
          -0.7116231,
          -0.9086704,
          -0.6357255,
          1.892008,
          1.7742698,
          2.548993,
          -0.86378586,
          -0.75418705,
          -0.70921934,
          -0.7437071,
          1.2701463,
          -0.10165662,
          -0.02410875,
          0.42921734,
          -1.8991311,
          0.08405185,
          0.31947625,
          0.48619467,
          1.1467203,
          5.091419,
          -0.12864038,
          -0.25353956,
          -0.05705409,
          0.12256572,
          -0.57802343,
          -0.9849926,
          4.590353,
          -0.1675445,
          2.1339538,
          0.42937067,
          -0.77291757,
          1.3086349,
          0.06878077,
          2.847827,
          1.602971,
          0.4985505,
          0.51101184,
          0.8893052,
          -1.2150996,
          -0.7201513,
          0.045184318,
          0.81549764,
          0.66471076,
          -0.8546929,
          -0.28288063,
          0.7847977,
          -0.25270873,
          0.034247242,
          -1.1996387,
          -1.2386374,
          -0.5962591,
          -0.3402702,
          -0.11748648,
          0.12685585,
          -0.32029784,
          1.0911063,
          0.34569559,
          0.9284969,
          2.1834006,
          1.653508,
          -1.9059417,
          -4.1785746,
          1.7709478,
          2.0341237,
          0.89165026,
          0.60627747,
          5.9411287,
          10.103671,
          10.482767,
          9.887641,
          -0.59760535,
          -0.032071844,
          -0.32783282,
          -0.23448725,
          -0.29289696,
          -0.19953953,
          -0.1362345,
          -0.2008335,
          -0.1744678,
          0.9938036,
          1.3687223,
          0.12650347,
          1.5165275,
          1.82808,
          0.76144123,
          1.0177557,
          1.0573323,
          2.269171,
          1.4019457,
          2.593773,
          2.0524602,
          -6.550093,
          1.4559023,
          1.8929259,
          1.0989312,
          2.1856537,
          -0.5806082,
          2.1338391,
          2.4074867,
          1.659273,
          1.9787804,
          0.1538635,
          1.9081576,
          1.7919376,
          1.4514314,
          1.0276357,
          1.0800191,
          0.945413,
          0.9513534,
          -0.14380258,
          -0.00091218244,
          0.060289472,
          2.0204089,
          1.9920169,
          1.5195276,
          1.6461587,
          1.4022259,
          2.0422366,
          6.944159,
          -0.3751163,
          1.0873741,
          -0.1905746,
          0.9981856,
          0.27372172,
          1.5666198,
          2.0583022,
          0.27779216,
          1.857826,
          0.36577037,
          1.392968,
          0.78895545,
          1.8076066,
          0.1097054,
          1.3638635,
          0.25839356,
          -0.30866823,
          0.8325939,
          2.1787217,
          2.284095,
          1.4620512,
          1.2218168,
          0.13045456,
          2.1850412,
          1.3347666,
          0.93300813,
          -0.19507252,
          1.5694871,
          1.9226905,
          11.158064,
          1.4623919,
          0.8968586,
          0.6274135,
          0.13097873,
          2.1764975,
          1.8387436,
          -0.16620988,
          -0.8788844,
          0.860036,
          0.8963847,
          1.3631052,
          1.1226443,
          1.9710411,
          -0.24368627,
          2.333258,
          1.7310013,
          1.5254661,
          1.9656284,
          1.4615331,
          2.3215199,
          1.3078567,
          8.430857,
          1.6435353,
          1.2389334,
          1.538572,
          -0.64837396,
          1.3201531,
          -0.28088742,
          1.7385824,
          0.5475051,
          1.3707364,
          2.0905662,
          0.51306695,
          0.13036399,
          0.96987057,
          2.573911,
          2.4936306,
          1.4991226,
          2.0474784,
          -0.010712478,
          0.03085363,
          -0.2535008,
          -0.77155733,
          -0.76232886,
          -0.9473785,
          0.37544453,
          0.071822695,
          1.0723803,
          -0.5509003,
          0.17535895,
          2.9267218,
          -0.28268763,
          0.9204449,
          0.92408526,
          0.8065369,
          0.13387708,
          0.31065932,
          0.3295265,
          1.0373247,
          -0.27025732,
          1.3317372,
          -0.799954,
          -0.11134729,
          -0.67585504,
          -1.0554754,
          -0.53826404,
          0.3912382,
          -0.13500874,
          1.0670741,
          1.0941113,
          1.2126418,
          -0.06665052,
          0.9517144,
          0.8893179,
          0.881132,
          1.0805459,
          1.1554654,
          1.0538853,
          1.1119307,
          0.80033934,
          1.5793345,
          1.5720594,
          2.313461,
          2.212407,
          1.364998,
          1.5293992,
          1.3738602,
          1.6000978,
          2.5516121,
          1.0724938,
          2.283444,
          1.8512366,
          1.9309767,
          2.327357,
          5.8369627,
          1.1269872,
          1.2571985,
          1.2117432,
          1.1432983,
          1.2096739,
          1.4566276,
          1.3272789,
          -0.91160965,
          -9.627776,
          -6.6708446,
          0.5739551,
          1.2455015,
          -0.06263306,
          0.2758109,
          0.087858096,
          -0.18791573,
          0.43495122,
          -0.040451296,
          -0.7183934,
          -1.1309757,
          0.24488696,
          10.163918,
          0.68160486,
          -0.29622033,
          0.74198955,
          1.5018291,
          0.29030803,
          0.7253998,
          -8.0125675,
          -0.76781785,
          0.161893,
          1.8424555,
          1.373377,
          1.333516,
          -7.842306,
          -7.895089,
          -7.171425,
          -0.2146805,
          -0.34380797,
          -0.37247124,
          1.443375,
          2.432595,
          -0.80705047,
          1.2851574,
          -0.61307305,
          1.2603922,
          1.2690222,
          0.31480113,
          1.5583701,
          0.57897294,
          1.7057325,
          0.39141622,
          2.3269258,
          1.102917,
          1.4859784,
          0.7555052,
          0.8695967,
          2.3539264,
          2.2210574,
          1.2687017,
          2.1709201,
          2.3147461,
          1.6345211,
          2.285224,
          2.2493055,
          1.9009548,
          1.38238,
          1.2800624,
          1.8198657,
          2.270604,
          1.615528,
          2.1024578,
          1.5826768,
          2.0706365,
          1.5616473,
          1.385373,
          1.7831088,
          1.8721313,
          0.93383044,
          2.436816,
          2.2069407,
          0.26270658,
          -1.1443154,
          -0.7747771,
          -0.7719167,
          1.8663726,
          0.4515653,
          1.6481737,
          1.7523365,
          2.0524635,
          0.7551749,
          -8.721433,
          -11.292087,
          -0.61852616,
          4.4620175,
          0.71214837,
          2.3083935,
          3.6316316,
          1.4630015,
          1.1746638,
          -0.5403648,
          0.83323854,
          0.53616947,
          -0.20899129,
          -0.7425755,
          0.36424148,
          2.534263,
          3.0451653,
          3.3428283,
          1.0478303,
          1.9342349,
          1.9404919,
          -0.32382447,
          -0.7748658,
          0.27676657,
          0.4176201,
          -8.783062,
          0.58650374
         ],
         "xaxis": "x",
         "y": [
          7.311314,
          7.557447,
          7.409045,
          4.4928675,
          6.2647047,
          3.177115,
          -19.297937,
          4.7743077,
          -1.7531174,
          5.1343613,
          3.82203,
          3.184701,
          5.363867,
          -6.4451447,
          -6.4106836,
          8.792378,
          6.706308,
          6.0498548,
          5.690705,
          6.5231752,
          6.5814214,
          6.8562512,
          6.4483433,
          6.424147,
          6.1225295,
          6.5880947,
          6.908866,
          6.6056056,
          6.657843,
          7.081267,
          6.8311696,
          6.2473297,
          7.5529075,
          6.372177,
          3.409399,
          4.181441,
          5.7318225,
          4.103759,
          8.122247,
          -0.27078366,
          7.4426327,
          7.674995,
          7.7283792,
          7.228398,
          6.86916,
          6.9501243,
          6.4127975,
          7.421979,
          8.279487,
          -2.9494724,
          7.5866838,
          7.3156695,
          8.652364,
          6.340453,
          8.519593,
          8.201594,
          7.58268,
          8.121672,
          6.5163283,
          8.564023,
          -0.057828795,
          8.545941,
          7.7558894,
          6.9767537,
          6.3986316,
          7.050964,
          3.2126665,
          3.156372,
          8.094485,
          4.349351,
          6.7660975,
          8.631605,
          7.6833496,
          6.6531544,
          7.2368197,
          6.199663,
          7.2506986,
          6.556496,
          7.228162,
          7.2680306,
          8.067396,
          7.695727,
          2.3836472,
          6.3531437,
          7.258166,
          7.9351006,
          7.647737,
          8.471333,
          8.044655,
          7.573607,
          8.626102,
          7.512701,
          7.4960337,
          7.029247,
          6.26719,
          6.853122,
          6.8354177,
          7.3407483,
          7.439631,
          7.594508,
          7.5502334,
          7.472294,
          7.3225055,
          7.40008,
          0.08917925,
          -1.7714909,
          6.533772,
          6.8418245,
          7.407168,
          6.497988,
          6.5152535,
          4.919064,
          8.419293,
          7.69275,
          8.689346,
          5.243002,
          6.3134294,
          6.791309,
          6.2846956,
          6.1035953,
          6.411927,
          5.3735223,
          7.2439075,
          7.155982,
          6.407044,
          6.799464,
          7.027502,
          6.832066,
          6.3332953,
          7.8248854,
          7.0642776,
          7.4870076,
          8.661729,
          6.377327,
          4.385088,
          4.7203455,
          3.2027972,
          8.072845,
          8.374152,
          7.6403913,
          8.37003,
          6.7954044,
          7.164754,
          0.99011654,
          4.0777354,
          4.581892,
          3.5739806,
          7.640693,
          8.615794,
          6.574466,
          5.345616,
          5.4969654,
          5.8008285,
          5.4172993,
          5.2258277,
          4.937261,
          -4.2822413,
          4.133135,
          3.896571,
          3.2461433,
          3.3650575,
          4.061991,
          3.3866022,
          3.0159426,
          3.4950564,
          3.994125,
          6.4650016,
          5.1593657,
          5.384774,
          6.172521,
          8.062464,
          6.226299,
          3.1876607,
          -19.295612,
          3.4384937,
          4.845855,
          5.6481752,
          6.3911033,
          -0.760627,
          4.2014756,
          5.259317,
          3.422478,
          3.400386,
          4.4301805,
          5.1796737,
          5.667237,
          7.985336,
          8.331597,
          7.1638584,
          6.7094507,
          6.9571676,
          7.584557,
          7.33979,
          6.851756,
          -2.8941643,
          6.850769,
          6.561572,
          6.9814773,
          6.6346292,
          7.5960135,
          7.2498136,
          -0.47114533,
          7.1932473,
          6.2088876,
          6.923707,
          5.6524663,
          6.349281,
          5.3597174,
          2.9140465,
          3.872581,
          2.9501274,
          3.167248,
          5.5381794,
          6.2319865,
          3.2453268,
          6.1594076,
          7.201459,
          8.348207,
          7.528527,
          7.547445,
          8.699472,
          6.3370028,
          4.386642,
          4.138157,
          3.0964732,
          6.7717295,
          6.9176264,
          4.1837573,
          3.7334728,
          3.137945,
          3.1611965,
          5.9301925,
          3.9208848,
          1.7135136,
          2.0439563,
          0.33488956,
          0.61139953,
          0.6869438,
          -0.37462452,
          7.716501,
          7.349692,
          6.329599,
          7.0943837,
          7.389102,
          6.65803,
          6.568256,
          7.7012844,
          6.7093296,
          7.1566744,
          7.1849356,
          6.988034,
          7.127234,
          7.0390005,
          6.8125877,
          6.7717834,
          6.9239526,
          3.6618,
          7.595245,
          7.5883794,
          7.5516505,
          7.3295636,
          7.0399137,
          6.910014,
          6.84136,
          6.838483,
          2.7560456,
          5.6153603,
          7.5361714,
          3.0721786,
          -0.28119743,
          1.953778,
          -3.5812118,
          -2.5263972,
          -3.0310323,
          6.3101497,
          6.8296623,
          6.098294,
          6.7075543,
          0.22790934,
          6.128541,
          0.48395535,
          5.661152,
          -2.764544,
          8.672666,
          -0.5699442,
          -0.6127377,
          6.6597724,
          8.459912,
          8.620338,
          8.283972,
          8.652732,
          7.3005624,
          6.4012775,
          7.190312,
          6.8593125,
          1.7710531,
          3.9188137,
          3.9998097,
          4.457311,
          4.643897,
          5.6376395,
          4.6264772,
          0.34119454,
          5.344061,
          6.769587,
          3.1828496,
          3.8445725,
          5.497062,
          5.430348,
          6.902001,
          8.693303,
          7.4539247,
          7.069947,
          7.274276,
          6.1244144,
          8.314545,
          8.46937,
          7.7398944,
          7.085284,
          3.3248966,
          -19.29529,
          5.7998185,
          4.9369817,
          6.3585854,
          -1.5496974,
          5.977617,
          -1.6961844,
          -1.486014,
          5.8408213,
          6.0319963,
          4.6905427,
          4.1689625,
          6.067625,
          5.559676,
          6.8311663,
          7.3948393,
          8.289527,
          -2.5781894,
          6.7916284,
          3.4890213,
          6.993887,
          6.8021693,
          6.8041415,
          6.765022,
          6.779447,
          6.8311396,
          6.832138,
          6.2612777,
          6.756866,
          6.8861203,
          7.0775733,
          7.1185217,
          6.4284906,
          7.0992045,
          5.6807775,
          7.626841,
          7.6133647,
          8.072673,
          7.855676,
          7.735221,
          7.807701,
          7.842798,
          7.9148526,
          8.084445,
          7.4445386,
          8.46875,
          7.164676,
          6.9999847,
          7.0401144,
          -3.959256,
          7.0676165,
          6.1382184,
          6.9434752,
          7.4522824,
          7.8035,
          6.8055735,
          5.661126,
          7.337367,
          6.9541655,
          6.783717,
          6.9072285,
          5.8705535,
          7.5959663,
          8.6852665,
          -1.7597882,
          -3.5048301,
          7.6362357,
          7.7149634,
          8.716427,
          -6.337714,
          -6.4943337,
          -6.4011536,
          -5.8116384,
          -7.0574207,
          1.4079092,
          8.339375,
          8.173712,
          6.8372793,
          7.2188416,
          8.102303,
          8.490941,
          7.655595,
          8.552169,
          7.834586,
          7.8249645,
          7.8094187,
          7.7626386,
          7.7390575,
          8.095692,
          5.667549,
          6.026618,
          0.32707077,
          5.197325,
          6.8622384,
          7.652442,
          6.3827467,
          -0.64738905,
          6.862781,
          6.9088864,
          6.8566775,
          6.890717,
          7.413827,
          7.0153036,
          7.3783193,
          4.960328,
          5.35004,
          5.160398,
          6.1162915,
          2.9223144,
          2.7177734,
          1.1844761,
          -0.93496907,
          1.2870626,
          6.871045,
          4.9605412,
          7.6673365,
          7.3004932,
          -0.96216434,
          -0.6778758,
          -3.5508347,
          8.328353,
          -2.2409227,
          9.023313,
          8.7329235,
          8.9254465,
          8.799814,
          9.065692,
          9.185244,
          9.027799,
          8.698671,
          9.028324,
          9.063776,
          8.873305,
          8.462,
          8.500352,
          -0.0552835,
          7.8870063,
          7.8244557,
          9.03974,
          8.538701,
          7.8692274,
          -2.1645887,
          9.043086,
          -2.2191098,
          -3.4178667,
          -1.0328106,
          -0.6726865,
          9.000635,
          -0.25807768,
          -0.33036235,
          -0.45372567,
          8.047678,
          7.228491,
          7.6672254,
          8.489295,
          5.789547,
          7.275546,
          7.1878595,
          6.0039554,
          6.2222347,
          6.1168,
          6.210979,
          6.1914644,
          6.13585,
          6.2592773,
          6.2681975,
          6.2084427,
          7.5575404,
          7.267817,
          6.9328394,
          6.587729,
          7.1207957,
          6.3854666,
          6.954437,
          6.393137,
          7.337334,
          6.446157,
          7.267494,
          6.52625,
          6.7277246,
          6.3654585,
          7.034676,
          6.687976,
          7.2607255,
          6.787754,
          6.9384575,
          6.28607,
          6.8747673,
          7.122598,
          7.224557,
          6.8812504,
          6.5095415,
          8.404953,
          9.025767,
          8.7640505,
          9.058151,
          7.8310494,
          8.590315,
          9.070946,
          -0.33965978,
          -0.05432015,
          8.394839,
          7.9947467,
          7.555429,
          8.096359,
          8.157557,
          8.302254,
          8.19293,
          8.302506,
          6.197266,
          6.2647386,
          6.932195,
          7.2090187,
          7.8344584,
          7.6774178,
          8.346591,
          7.0199003,
          7.01065,
          7.19002,
          6.3232665,
          3.1066666,
          3.4414485,
          3.2026105,
          5.5865865,
          6.0302615,
          6.321728,
          6.9047728,
          -0.2348809,
          7.9690704,
          7.914092,
          0.055853643,
          8.286173,
          -1.1306461,
          7.506824,
          8.7113695,
          7.4896445,
          7.1154156,
          6.29849,
          3.2489228,
          -19.298315,
          5.173535,
          5.75207,
          -1.568225,
          5.4424024,
          6.3477287,
          4.770978,
          3.3063874,
          5.7799582,
          6.15578,
          7.8978624,
          -3.027224,
          8.459215,
          7.5846443,
          0.27102584,
          -0.15131266,
          6.628152,
          6.957226,
          6.1540785,
          6.804611,
          6.5312324,
          6.7609015,
          6.6807456,
          6.3228674,
          7.184445,
          6.2286687,
          3.275221,
          3.4791496,
          6.384825,
          8.399568,
          7.9155207,
          -2.2575157,
          7.2926636,
          6.307805,
          6.996152,
          6.22277,
          6.1438704,
          2.866314,
          -19.297216,
          3.7940168,
          5.9177046,
          7.0418744,
          5.7185087,
          3.1534653,
          3.18529,
          3.3980384,
          -0.61678797,
          6.1394463,
          8.2094555,
          7.874372,
          7.845129,
          8.276515,
          8.454517,
          8.181652,
          8.462811,
          8.715339,
          -0.15061161,
          -0.09789287,
          -0.04121325,
          7.9741616,
          7.001787,
          6.8933325,
          6.928652,
          7.243217,
          6.4110985,
          5.138201,
          5.5509777,
          6.838606,
          7.261116,
          8.27817,
          7.0028806,
          3.062681,
          -19.298649,
          5.571304,
          6.035312,
          6.0439844,
          4.6508584,
          5.8348336,
          8.094079,
          -0.2058418,
          8.127035,
          -0.2832729,
          7.051009,
          6.1396985,
          3.2959635,
          -19.29734,
          5.198413,
          5.791039,
          -1.4016666,
          6.2396812,
          5.9252024,
          5.5366693,
          5.996019,
          3.3293016,
          3.191431,
          4.2347627,
          5.1858487,
          6.104474,
          6.287584,
          7.4677663,
          3.685587,
          2.6695843,
          3.5784073,
          3.2211668,
          5.371689,
          6.2328696,
          7.295538,
          7.6405635,
          8.175043,
          8.135076,
          7.628939,
          6.916138,
          1.1530261,
          7.7109723,
          -0.9615006,
          -1.1467888,
          0.5665862,
          6.402554,
          3.1424098,
          2.959721,
          -0.08831188,
          -3.619598,
          -2.9383883,
          7.558365,
          7.521469,
          6.8337526,
          3.0738187,
          3.6086814,
          5.7585735,
          4.6131177,
          7.675302,
          7.499205,
          7.164709,
          -3.9448485,
          6.8715267,
          7.272593,
          6.9576645,
          6.260605,
          6.033722,
          7.231777,
          6.8219113,
          -3.6164358,
          6.616823,
          -0.8238013,
          -2.6433933,
          -2.245336,
          -5.0715466,
          7.263565,
          -0.71002775,
          -3.4453282,
          -2.8249433,
          6.4522185,
          7.2201524,
          8.016819,
          7.618243,
          6.582739,
          6.5201244,
          6.690168,
          6.715966,
          6.756513,
          6.777474,
          6.822882,
          -19.29861,
          6.036315,
          -1.4773794,
          5.49071,
          6.121366,
          5.8739176,
          6.149628,
          5.5220647,
          2.7059352,
          5.4892898,
          6.437676,
          7.21704,
          6.1358957,
          7.0839953,
          5.972939,
          6.945414,
          6.5690684,
          5.212194,
          7.5312133,
          7.8527293,
          6.7953424,
          6.830749,
          8.711909,
          6.9694085,
          2.0674162,
          7.0422397,
          2.213032,
          2.1214726,
          6.4539056,
          3.3317266,
          0.75126946,
          4.117068,
          -0.04038499,
          5.88761,
          5.5411882,
          6.0616336,
          4.785957,
          5.791437,
          6.9390006,
          7.060168,
          7.014825,
          6.93711,
          6.1835694,
          4.4135303,
          4.3387275,
          4.756728,
          4.7060304,
          4.8021636,
          4.692223,
          4.719747,
          4.70882,
          4.711825,
          4.8445053,
          4.724204,
          4.7529593,
          4.7700744,
          8.393528,
          -1.5072421,
          -2.781377,
          7.7421627,
          8.652179,
          8.248359,
          8.074189,
          7.178197,
          7.8979483,
          7.848233,
          7.7170362,
          7.15344,
          -0.10646096,
          5.845126,
          7.1529703,
          7.6536164,
          -0.36491862,
          7.3543253,
          7.671741,
          8.130233,
          8.092836,
          8.262856,
          -2.1572444,
          8.120814,
          8.543216,
          7.785923,
          7.8760066,
          8.637844,
          7.532963,
          6.6834435,
          6.473223,
          7.158644,
          7.2785196,
          7.0579786,
          6.8962684,
          7.087595,
          7.1812806,
          6.745025,
          7.126659,
          6.676792,
          7.1200404,
          6.6985393,
          7.169128,
          7.1547422,
          6.7409844,
          6.3342805,
          -0.15884992,
          7.4116664,
          7.102398,
          7.4461527,
          8.242129,
          5.319439,
          5.000202,
          4.9328756,
          4.70965,
          6.3312683,
          5.334714,
          4.7970285,
          -1.9462147,
          -2.0737646,
          7.7609415,
          4.4726706,
          8.532842,
          8.06658,
          -0.20874211,
          7.161479,
          3.102959,
          -19.296642,
          5.912086,
          6.0672507,
          5.9452524,
          5.8786626,
          5.340575,
          4.0072064,
          6.100401,
          8.354714,
          5.6495705,
          5.9753113,
          5.301001,
          4.828198,
          4.987596,
          4.981116,
          -1.3217067,
          5.244041,
          5.298179,
          7.29663,
          6.3247237,
          6.8973846,
          6.362218,
          6.906693,
          6.57053,
          6.6376324,
          6.890871,
          6.702787,
          6.7039647,
          7.503488,
          8.200083,
          7.9362154,
          -2.7993343,
          7.1502843,
          -1.0775338,
          5.0892615,
          3.5147471,
          5.143026,
          3.1715229,
          5.9633985,
          7.5466967,
          7.6709123,
          8.692709,
          8.109536,
          6.6653895,
          7.904593,
          -4.1170278,
          7.593377,
          2.785408,
          7.241597,
          6.912005,
          6.303479,
          4.9178843,
          7.2037864,
          6.744421,
          3.7354043,
          3.2000792,
          4.227187,
          8.009021,
          7.7730584,
          7.925307,
          7.0616035,
          7.2925267,
          7.1614494,
          7.7982683,
          7.0051813,
          -0.8811339,
          6.828917,
          7.5432158,
          6.9758253,
          6.7796116,
          7.674701,
          8.402246,
          8.024069,
          7.826078,
          7.6635447,
          7.600778,
          7.7568836,
          8.483406,
          8.204406,
          6.639965,
          6.3326874,
          8.2872715,
          8.395103,
          7.1584005,
          7.0490513,
          7.1937804,
          6.424296,
          7.511543,
          6.739549,
          4.2071733,
          2.397928,
          2.6002111,
          8.42004,
          7.613021,
          7.468637,
          7.494699,
          7.349024,
          7.0099077,
          8.4902315,
          8.222552,
          8.582513,
          8.078671,
          -3.8654718,
          6.480439,
          3.1865184,
          4.2075167,
          5.1723046,
          7.5712156,
          7.97052,
          -3.4771636,
          7.3429446,
          6.8359194,
          6.689987,
          6.644018,
          6.89038,
          6.7787676,
          6.732319,
          6.898132,
          6.629855,
          7.1916666,
          6.8437743,
          6.90083,
          6.8210316,
          6.834501,
          7.021337,
          7.225724,
          7.0502753,
          6.6509094,
          6.8909016,
          6.7671533,
          7.1243906,
          6.960329,
          6.2540092,
          6.895137,
          6.452972,
          6.912705,
          5.3695188,
          7.805379,
          6.1684065,
          3.785582,
          3.288405,
          5.388682,
          -3.7334082,
          4.8077807,
          4.8545613,
          3.2861269,
          4.843452,
          3.6179407,
          3.119313,
          3.148533,
          5.2470393,
          5.853779,
          7.457817,
          5.924128,
          3.2812805,
          0.64647794,
          3.861087,
          7.2291336,
          5.668313,
          7.614619,
          7.030514,
          7.2741656,
          6.9399285,
          7.245384,
          6.9836287,
          7.2185464,
          7.0324388,
          7.2625394,
          7.014844,
          7.308958,
          7.079797,
          7.105263,
          7.034368,
          6.546547,
          6.6572294,
          6.313399,
          6.525368,
          6.5081778,
          6.5553966,
          6.76669,
          6.8192487,
          7.24939,
          6.31663,
          6.521338,
          8.580711,
          6.949554,
          1.0408202,
          7.0121336,
          7.4637876,
          8.6122,
          6.369594,
          3.0115128,
          5.397385,
          5.9665265,
          5.459576,
          6.426901,
          0.37614864,
          4.7781687,
          5.5434494,
          6.6860685,
          5.3251424,
          5.3114357,
          6.4339123,
          3.1961145,
          -19.296223,
          3.3708873,
          5.9115443,
          7.395422,
          -3.4589977,
          5.832493,
          3.6504104,
          4.4061093,
          4.753074,
          6.638932,
          7.662142,
          6.351956,
          -1.9349421,
          7.1481214,
          7.2975254,
          8.458801,
          7.188054,
          6.862945,
          6.6227655,
          6.7608027,
          3.1667697,
          -19.296507,
          5.270118,
          -2.9182007,
          -3.3002763,
          5.393234,
          6.236673,
          -3.1239915,
          5.0925555,
          4.0602884,
          5.7496243,
          5.6799226,
          5.666968,
          6.5595484,
          3.908786,
          4.3861127,
          4.039526,
          4.949867,
          5.089822,
          5.504005,
          5.312975,
          4.902419,
          8.061026,
          -0.25875956,
          0.18885823,
          -1.5440354,
          -1.770817,
          -1.5246049,
          -1.8628311,
          7.3666186,
          -2.7335277,
          -0.6580753,
          -1.2439852,
          7.834319,
          7.0147367,
          6.072806,
          6.186488,
          3.1504908,
          -19.298553,
          3.7305903,
          5.926659,
          5.960049,
          5.260277,
          3.883687,
          3.841624,
          4.951005,
          -1.2087556,
          7.5109625,
          6.964216,
          6.3828654,
          -3.50336,
          7.4508924,
          7.550617,
          7.032269,
          6.5314927,
          6.507297,
          6.2993264,
          3.6669657,
          7.2352242,
          7.644309,
          7.476117,
          8.652412,
          6.4652996,
          7.9331036,
          8.431221,
          -0.17444137,
          7.9406567,
          7.4529324,
          6.0158825,
          8.131474,
          -1.8191993,
          8.305751,
          8.118843,
          8.1099205,
          8.14105,
          8.099161,
          7.8815503,
          -2.8446224,
          6.8335977,
          1.5889546,
          3.760424,
          7.9278784,
          8.671275,
          6.180824,
          0.9227234,
          -0.90344006,
          3.456746,
          4.3270116,
          5.1312947,
          0.78951126,
          5.115376,
          5.131379,
          3.1292727,
          5.143894,
          4.932594,
          7.113206,
          8.479431,
          7.338502,
          7.4487042,
          4.6583347,
          2.2819114,
          3.4618478,
          6.154671,
          6.3696256,
          1.8463876,
          5.0702176,
          5.4026814,
          7.193053,
          7.006299,
          6.5600104,
          6.751962,
          -2.7956076,
          -2.4237995,
          6.920142,
          6.528136,
          7.202928,
          8.646731,
          -3.1389997,
          0.63184893,
          1.1070914,
          0.36503926,
          7.422803,
          7.4151525,
          7.3779254,
          7.531431,
          7.354315,
          7.4705434,
          7.3529224,
          7.481583,
          7.3431516,
          5.917179,
          8.219796,
          6.698292,
          6.527064,
          6.7411222,
          6.925398,
          7.1240993,
          7.1263957,
          6.455847,
          7.04872,
          6.181034,
          6.950393,
          -6.3550243,
          6.9939165,
          6.715546,
          6.8392386,
          6.328955,
          7.397512,
          6.9295216,
          6.1789017,
          6.700016,
          6.811167,
          6.895244,
          6.9762416,
          6.8674316,
          7.1307745,
          7.153817,
          6.8552084,
          7.3466454,
          6.3207016,
          7.429971,
          7.427726,
          6.8612366,
          6.685203,
          6.9893737,
          7.5404215,
          6.9006968,
          6.724477,
          6.871605,
          -3.4203236,
          7.1555204,
          6.0701957,
          7.162563,
          5.699106,
          7.155258,
          6.8482857,
          6.6784177,
          7.4793463,
          6.701433,
          6.672542,
          7.2155385,
          7.214108,
          6.887886,
          6.8204165,
          6.679602,
          5.875279,
          5.1218305,
          7.154523,
          6.3389597,
          6.2109337,
          7.000219,
          7.2865767,
          7.234256,
          6.3391733,
          6.956785,
          7.0756965,
          -0.62678295,
          6.727508,
          5.6213713,
          -4.8010716,
          6.9450693,
          7.1550837,
          7.19462,
          7.4934273,
          6.517913,
          6.862258,
          -4.458816,
          7.493481,
          6.6094265,
          5.956858,
          5.1683373,
          6.7760506,
          5.7576075,
          7.2034354,
          6.291853,
          6.9192195,
          6.9471726,
          6.8708673,
          6.970867,
          6.227832,
          7.195194,
          -3.877853,
          6.568282,
          6.8318644,
          6.656713,
          7.888028,
          6.4488697,
          -1.6925347,
          5.9363403,
          6.823543,
          6.1247554,
          6.301805,
          7.1211267,
          6.9578247,
          6.76605,
          6.2332735,
          6.0861573,
          6.7445474,
          6.023602,
          6.3707285,
          3.2355483,
          5.0281067,
          3.5763118,
          3.6469295,
          3.213702,
          3.1635368,
          5.676384,
          5.622907,
          4.8429174,
          5.2713504,
          0.32104516,
          4.9439406,
          5.5290103,
          5.4718804,
          5.26002,
          4.1332245,
          4.289557,
          4.997777,
          6.5003114,
          5.893497,
          5.8695126,
          3.8419533,
          4.450422,
          4.176947,
          3.9369102,
          6.726736,
          8.324399,
          7.09829,
          5.8772106,
          6.818263,
          6.8193197,
          5.11301,
          5.490046,
          4.9358854,
          5.183295,
          5.3334866,
          5.510904,
          5.502054,
          5.658362,
          6.850089,
          6.949828,
          7.1655827,
          6.191677,
          7.0655155,
          6.902265,
          6.82876,
          6.95951,
          7.037981,
          6.24383,
          6.9209957,
          6.8916407,
          6.7277093,
          6.0671983,
          5.712978,
          -3.754427,
          8.153569,
          7.467624,
          7.9673357,
          8.270851,
          8.34484,
          8.29874,
          8.372165,
          7.7090654,
          -0.79828924,
          -0.30691004,
          7.0727477,
          6.287568,
          5.3480687,
          5.2318964,
          5.677514,
          8.280833,
          6.415992,
          2.9897943,
          3.6302695,
          3.0831952,
          4.4526997,
          -19.298676,
          3.614611,
          -1.7349579,
          6.3299427,
          6.3064337,
          4.5959363,
          4.959655,
          -0.9766578,
          7.6123075,
          7.1691146,
          6.8671155,
          7.0302725,
          8.621638,
          -6.450621,
          -6.485475,
          -6.978688,
          8.227365,
          8.510278,
          8.634707,
          -2.3517451,
          -1.9516807,
          7.5186977,
          8.476668,
          7.5427675,
          8.509219,
          8.818846,
          8.291315,
          7.230402,
          5.7686896,
          7.285773,
          7.4049807,
          6.225845,
          7.2577753,
          6.9756584,
          7.8617773,
          7.3614163,
          6.197637,
          7.122204,
          7.4171324,
          6.886825,
          7.064418,
          7.4181867,
          7.0672626,
          6.3753705,
          6.6704226,
          7.0987782,
          6.691534,
          7.23779,
          7.09806,
          7.404626,
          6.9743176,
          7.264125,
          6.965919,
          6.7770114,
          6.943281,
          7.3547244,
          6.7113943,
          6.885262,
          6.2922916,
          6.4728513,
          6.4221487,
          2.836955,
          3.1015427,
          7.1896925,
          6.3381863,
          6.0031714,
          6.350647,
          5.952153,
          6.086989,
          7.8376174,
          -2.142316,
          -2.7197104,
          8.171308,
          -3.0089235,
          8.183034,
          0.03301143,
          -0.918343,
          7.779356,
          8.233623,
          7.413933,
          7.615881,
          4.396167,
          4.8692155,
          3.7158332,
          5.044424,
          -3.468853,
          -3.1159854,
          -2.635742,
          6.9383917,
          6.0729256,
          6.645844,
          7.185936,
          2.3348117,
          7.4394817,
          6.7931867,
          5.91249,
          8.36081
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Datasets server - worker\n\n> Workers that pre-compute and cache the response to /splits, /first-rows,..."
          ],
          [
           "- `WORKER_CONTENT_MAX_BYTES`: the maximum size in bytes of the response content computed by a worker..."
          ],
          [
           "- `WORKER_MAX_LOAD_PCT`: maximum load of the machine (in percentage: the max between the 1m load and..."
          ],
          [
           "Also, it's possible to force the parent directory in which the temporary files (as the current job s..."
          ],
          [
           "- `NUMBA_CACHE_DIR`: directory where the `numba` decorators (used by `librosa`) can write cache.\n\nNo..."
          ],
          [
           "### First rows worker\n\nSet environment variables to configure the `first-rows` worker (`FIRST_ROWS_`..."
          ],
          [
           "- `PARQUET_AND_INFO_COMMIT_MESSAGE`: the git commit message when the worker uploads the parquet file..."
          ],
          [
           "- `PARQUET_AND_INFO_SOURCE_REVISION`: the git revision of the dataset to use to prepare the parquet ..."
          ],
          [
           "### Duckdb Index worker\n\nSet environment variables to configure the `duckdb-index` worker (`DUCKDB_I..."
          ],
          [
           "### Descriptive statistics worker\n\nSet environment variables to configure the `descriptive-statistic..."
          ],
          [
           "`column_statistics` content depends on the feature type, see examples below.\n##### class_label\n\n<det..."
          ],
          [
           "```\n</p>\n</details> \n\n##### float\n\nBin size for histogram is counted as `(max_value - min_value) / D..."
          ],
          [
           "```\n</p>\n</details> \n\n##### int\n\nAs bin edges for integer values also must be integers, bin size is ..."
          ],
          [
           "```python\n{\n    \"column_name\": \"direction\",\n    \"column_type\": \"int\",\n    \"column_statistics\": {\n   ..."
          ],
          [
           "],\n            \"bin_edges\": [\n                0,\n                3,\n                6,\n             ..."
          ],
          [
           "\"nan_count\": 0,\n        \"nan_proportion\": 0.0,\n        \"min\": 0,\n        \"max\": 6,\n        \"mean\": 3..."
          ],
          [
           "```\n\n</p>\n</details>\n\n##### string_label\n\nIf the number of unique values in a column (within request..."
          ],
          [
           "```\n</p>\n</details>\n\n##### bool\n\n<details><summary>example: </summary>\n<p>\n\n```python\n{\n    'column_..."
          ],
          [
           "--\ntitle: Datasets Server Admin UI\nemoji: ðŸ“Š\ncolorFrom: gray\ncolorTo: purple\nsdk: gradio\nsdk_version:..."
          ],
          [
           "Filter rows in a dataset\n\nDatasets Server provides a `/filter` endpoint for filtering rows in a data..."
          ],
          [
           "```\nwhere=age>30 AND (name='Simone' OR children=0)\n```\nwill filter the data to select only those row..."
          ],
          [
           "List Parquet files\n\nDatasets can be published in any format (CSV, JSONL, directories of images, etc...."
          ],
          [
           "The `/parquet` endpoint accepts the dataset name as its query parameter:\n\n<inferencesnippet>\n<python..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing a list of the dataset's ..."
          ],
          [
           "```json\n{\n  \"parquet_files\": [\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"ParaphraseRC\",\n     ..."
          ],
          [
           "},\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"SelfRC\",\n      \"split\": \"test\",\n      \"url\": \"ht..."
          ],
          [
           "```\n\n## Sharded Parquet files\n\nBig datasets are partitioned into Parquet files (shards) of about 500..."
          ],
          [
           "```json\n{\n  \"parquet_files\": [\n    {\n      \"dataset\": \"amazon_polarity\",\n      \"config\": \"amazon_pol..."
          ],
          [
           "{\n      \"dataset\": \"amazon_polarity\",\n      \"config\": \"amazon_polarity\",\n      \"split\": \"train\",\n   ..."
          ],
          [
           "```\n\nTo read and query the Parquet files, take a look at the [Query datasets from Datasets Server](p..."
          ],
          [
           "```\n</js>\n<curl>\n```curl\ncurl https://huggingface.co/api/datasets/duorc/parquet \\\n        -X GET \\\n ..."
          ],
          [
           "```\n\nOptionally you can specify which configuration name to return, as well as which split:\n\n<infere..."
          ],
          [
           "datasets-server Helm chart\n\nThe `datasets-server` Helm [chart](https://helm.sh/docs/topics/charts/) ..."
          ],
          [
           "How to contribute to the Datasets Server?\n\n[![Contributor Covenant](https://img.shields.io/badge/Con..."
          ],
          [
           "```\n\n3. Create a new branch to hold your development changes:\n\n   ```bash\n   git checkout -b a-descr..."
          ],
          [
           "DuckDB\n\n[DuckDB](https://duckdb.org/docs/) is a database that supports reading and querying Parquet ..."
          ],
          [
           "```\n</js>\n</inferencesnippet>\n\nNow you can write and execute your SQL query on the Parquet file:\n\n<i..."
          ],
          [
           "```\n</js>\n</inferencesnippet>\n\nTo query multiple files - for example, if the dataset is sharded:\n\n<i..."
          ],
          [
           "Overview\n\nDatasets Server automatically converts and publishes public datasets less than 5GB on the ..."
          ],
          [
           "Pandas\n\n[Pandas](https://pandas.pydata.org/docs/index.html) is a popular DataFrame library for data ..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nYou can adapt the `BUILD_DIR` environment variable to set any temporary folder that you prefer...."
          ],
          [
           "Check dataset validity\n\nBefore you download a dataset from the Hub, it is helpful to know if a speci..."
          ],
          [
           "## Check if a dataset is valid\n\n`/is-valid` checks whether a specific dataset loads without any erro..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "Security Policy\n\n## Supported Versions\n\n<!--\nUse this section to tell people about which versions of..."
          ],
          [
           "Datasets server admin machine\n\n> Admin endpoints\n\n## Configuration\n\nThe worker can be configured usi..."
          ],
          [
           "### Prometheus\n\n- `PROMETHEUS_MULTIPROC_DIR`: the directory where the uvicorn workers share their pr..."
          ],
          [
           "Get dataset information\n\nDatasets Server provides an `/info` endpoint for exploring the general info..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON with the `dataset_info` key. Its st..."
          ],
          [
           "```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answe..."
          ],
          [
           "\"answers\": {\n                \"feature\": {\n                    \"dtype\": \"string\",\n                   ..."
          ],
          [
           "\"num_bytes\": 24388192,\n                \"checksum\": null\n            },\n            \"https://raw.gith..."
          ],
          [
           "Datasets server\n\n> Integrate into your apps over 10,000 datasets via simple HTTP requests, with pre-..."
          ],
          [
           "You can also report bugs and propose enhancements on the code, or the documentation, in the [GitHub ..."
          ],
          [
           "Download slices of rows\n\nDatasets Server provides a `/rows` endpoint for visualizing any slice of ro..."
          ],
          [
           "The `/rows` endpoint accepts five query parameters:\n\n- `dataset`: the dataset name, for example `glu..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing two keys:\n\n- The [`featu..."
          ],
          [
           "```json\n// https://datasets-server.huggingface.co/rows?dataset=duorc&config=SelfRC&split=train&offse..."
          ],
          [
           "\"_type\": \"Sequence\"\n      }\n    },\n    {\n      \"feature_idx\": 6,\n      \"name\": \"no_answer\",\n      \"t..."
          ],
          [
           "\"plot\": \"The film is centered on Mortal Kombat, a fighting tournament between the representatives of..."
          ],
          [
           "thunder and defender of the Earth realm, to overcome their powerful adversaries in order to prevent ..."
          ],
          [
           "daughter. Aware that Kitana is a dangerous adversary because she is the rightful heir to Outworld an..."
          ],
          [
           "freezing abilities, until Liu recalls Kitana's advice and uses it to kill Sub-Zero.\\nPrince Goro ent..."
          ],
          [
           "accepts on the condition that he be allowed to challenge any opponent of his choosing, anytime and a..."
          ],
          [
           "to Outworld, intending to fight her as his opponent. Knowing that his powers are ineffective there a..."
          ],
          [
           "castle tower, Shang Tsung challenges Sonya to fight him, claiming that her refusal to accept will re..."
          ],
          [
           "tournaments. In a last-ditch attempt to take advantage, Tsung morphs into Chan. Seeing through the c..."
          ],
          [
           "at the Shaolin temple. The jubilation abruptly stops, however, when Shao Kahn's giant figure suddenl..."
          ],
          [
           "\"title\": \"Mortal Kombat\",\n        \"question_id\": \"40c1866a-b214-11ba-be57-8979d2cefa90\",\n        \"qu..."
          ],
          [
           "\"title\": \"Mortal Kombat\",\n        \"question_id\": \"f1fdefcf-1191-b5f9-4cae-4ce4d0a59da7\",\n        \"qu..."
          ],
          [
           "```\n\n## Image and audio samples\n\nImage and audio are represented by a URL that points to the file.\n\n..."
          ],
          [
           "Quickstart\n\n[[open-in-colab]]\n\nIn this quickstart, you'll learn how to use the Datasets Server's RES..."
          ],
          [
           "| Endpoint                    | Method | Description                                             | Q..."
          ],
          [
           "| [/rows](./rows)             | GET    | Get a slice of rows of a dataset split.                 | -..."
          ],
          [
           "There is no installation or setup required to use Datasets Server.\n\n<Tip>\n  Sign up for a <a href=\"h..."
          ],
          [
           "```\nhttps://datasets-server.huggingface.co\n```\n\n## Gated datasets\n\nFor gated datasets, you'll need t..."
          ],
          [
           "```\n\n## Check dataset validity\n\nTo check whether a specific dataset is valid, for example, [Rotten T..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThis returns the first 100 rows of the dataset:\n\n```json\n{\n  \"datas..."
          ],
          [
           "```\n\n## Download slices of a dataset\n\nThe `/rows` endpoint returns a JSON list of a slice of rows of..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nYou can download slices of 100 rows maximum at a time.\n\nThe respons..."
          ],
          [
           "```\n\n## Search text in a dataset\n\nThe `/search` endpoint returns a JSON list of a slice of rows of a..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nYou can get slices of 100 rows maximum at a time, and you can ask f..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/parquet?dataset=rotten_tomatoes..."
          ],
          [
           "```\n\n## Get the size of the dataset\n\nThe `/size` endpoint returns a JSON with the size (number of ro..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThis returns a URL to the Parquet file for each split:\n\n```json\n{\n ..."
          ],
          [
           "Datasets server SSE API\n\n> Server-sent events API for the Datasets server. It's used to update the H..."
          ],
          [
           "libapi\n\nA Python library for the API services\n\n## Configuration\n\nThe APIs can be configured using en..."
          ],
          [
           "- `API_HF_AUTH_PATH`: the path of the external authentication service, on the hub (see `HF_ENDPOINT`..."
          ],
          [
           "- `API_MAX_AGE_LONG`: number of seconds to set in the `max-age` header on data endpoints. Defaults t..."
          ],
          [
           "### Uvicorn\n\nThe following environment variables are used to configure the Uvicorn server (`API_UVIC..."
          ],
          [
           "ClickHouse\n\n[ClickHouse](https://clickhouse.com/docs/en/intro) is a fast and efficient column-orient..."
          ],
          [
           "```\n\n## Aggregate functions\n\nNow you can begin to analyze the dataset. Use the `-q` argument to spec..."
          ],
          [
           "```\n\nClickHouse also provides functions for visualizing your queries. For example, you can use the [..."
          ],
          [
           "```\n\nRemember to set `enable_url_encoding` to 0 and `max_https_get_redirects` to 1 to redirect to th..."
          ],
          [
           "```\n\nYou can make this even easier by creating another function that calls `hugging_paths` and outpu..."
          ],
          [
           "```\n\nNow use the `hf` function to query any dataset by passing the dataset name:\n\n```bash\nSELECT hor..."
          ],
          [
           "Analyze a dataset on the Hub\n\n[[open-in-colab]]\n\nIn the Quickstart, you were introduced to various e..."
          ],
          [
           "Use the `/parquet` endpoint to convert the dataset to a Parquet file and return the URL to it:\n\n```p..."
          ],
          [
           "```\n\n## Read dataset with Pandas\n\nWith the URL, you can read the Parquet file into a Pandas DataFram..."
          ],
          [
           "```\n\n|                                               src | complexity |                         prob..."
          ],
          [
           "Polars \n\n[Polars](https://pola-rs.github.io/polars-book/user-guide/) is a fast DataFrame library wri..."
          ],
          [
           "```\n\nTo read from a single Parquet file, use the [`read_parquet`](https://pola-rs.github.io/polars/p..."
          ],
          [
           "```\n\nTo read multiple Parquet files - for example, if the dataset is sharded - you'll need to use th..."
          ],
          [
           "```\n\n## Lazy API\n\nPolars offers a [lazy API](https://pola-rs.github.io/polars-book/user-guide/lazy/u..."
          ],
          [
           "Explore statistics over split data\n\nDatasets Server provides a `/statistics` endpoint for fetching s..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThe response JSON contains two keys:\n* `num_examples` - number of s..."
          ],
          [
           "```json\n{\n  \"num_examples\": 8551,\n  \"statistics\": [\n    {\n      \"column_name\": \"idx\",\n      \"column_..."
          ],
          [
           "```\n\n## Response structure by data type\n\nCurrently, statistics are supported for strings, float and ..."
          ],
          [
           "```\n\n</p>\n</details>\n\n### float\n\nThe following measures are returned for float data types:\n\n* minimu..."
          ],
          [
           "```\n\n</p>\n</details>\n\n### bool\n\nThe following measures are returned for bool data type:\n\n* number an..."
          ],
          [
           "```\n\n</p>\n</details>\n\n### string_text\n\nIf string column has more than 30 unique values within the re..."
          ],
          [
           "libcommon\n\nA Python library with common code (cache, queue, workers logic, processing steps, configu..."
          ],
          [
           "## Cached Assets configuration\n\nSet the cached-assets (images and audio files) environment variables..."
          ],
          [
           "## Common configuration\n\nSet the common environment variables to configure the following aspects:\n\n-..."
          ],
          [
           "## Cache configuration\n\nSet environment variables to configure the storage of precomputed API respon..."
          ],
          [
           "Splits and configurations\n\nMachine learning datasets are commonly organized in *splits* and they may..."
          ],
          [
           "Configurations are flexible, and can be used to organize a dataset along whatever objective you'd li..."
          ],
          [
           "e2e\n\nEnd to end tests, written in Python..."
          ],
          [
           "Preview a dataset\n\nDatasets Server provides a `/first-rows` endpoint for visualizing the first 100 r..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing two keys:\n\n- The [`featu..."
          ],
          [
           "```json\n{\n  \"dataset\": \"duorc\",\n  \"config\": \"SelfRC\",\n  \"split\": \"train\",\n  \"features\": [\n    {\n    ..."
          ],
          [
           "}\n    },\n    {\n      \"feature_idx\": 6,\n      \"name\": \"no_answer\",\n      \"type\": { \"dtype\": \"bool\", \"..."
          ],
          [
           "\"title\": \"Ghosts of Mars\",\n        \"question_id\": \"b440de7d-9c3f-841c-eaec-a14bdff950d1\",\n        \"q..."
          ],
          [
           "\"title\": \"Ghosts of Mars\",\n        \"question_id\": \"a9f95c0d-121f-3ca9-1595-d497dc8bc56c\",\n        \"q..."
          ],
          [
           "```\n\n## Truncated responses\n\nFor some datasets, the response size from `/first-rows` may exceed 1MB,..."
          ],
          [
           "```json\n  ...\n  \"rows\": [\n    {\n      \"row_idx\": 0,\n      \"row\": {\n        \"start\": \"2016-07-01T00:0..."
          ],
          [
           "Search text in a dataset\n\nDatasets Server provides a `/search` endpoint for searching words in a dat..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing two keys (same format as..."
          ],
          [
           "```json\n{\n  \"features\": [\n    {\n      \"feature_idx\": 0,\n      \"name\": \"plot_id\",\n      \"type\": { \"dt..."
          ],
          [
           "\"name\": \"no_answer\",\n      \"type\": { \"dtype\": \"bool\", \"_type\": \"Value\" }\n    }\n  ],\n  \"rows\": [\n    ..."
          ],
          [
           "\"plot\": \"The film begins with clips that track a telephone call between London and Geneva, where a u..."
          ],
          [
           "the Criminal Code opened at random, and concentrates on that passage. As she drives back to her apar..."
          ],
          [
           "Valentine is walking Rita the next day the dog runs away and Valentine eventually finds her back at ..."
          ],
          [
           "a contented nuclear family, causing her to change her mind about exposing their secrets. She returns..."
          ],
          [
           "bowling. Valentine covers her ears but from the very little she hears she concludes that they love e..."
          ],
          [
           "books. Auguste says yes. Karin gives him a fancy fountain pen as a gift and he wonders what the firs..."
          ],
          [
           "the news about a retired judge who spied on his neighbours and rushes to Kern's house to tell him th..."
          ],
          [
           "on selfish grounds and from fear than about obeying the law or being decent. It is his birthday and ..."
          ],
          [
           "condemned might have seen a different life had he decided otherwise. Valentine tells Kern about her ..."
          ],
          [
           "when she rushes outside, he hides from her. In a temper, he ties his dog by a quayside and abandons ..."
          ],
          [
           "Kern to a fashion show where she is modeling. After the show they speak about the dream Kern had abo..."
          ],
          [
           "and he accidentally dropped one of his books. When he picked it up, Kern studied the chapter where t..."
          ],
          [
           "the man guilty. He tells Valentine the judgment was entirely legal but also that he subsequently req..."
          ],
          [
           "the trilogy, Julie and Olivier from Blue, Karol and Dominique from White, and Valentine and Auguste,..."
          ],
          [
           "\"title\": \"Three Colors: Red\",\n        \"question_id\": \"7c583513-0b7f-ddb3-be43-64befc7e90cc\",\n       ..."
          ],
          [
           "\"title\": \"Three Colors: Red\",\n        \"question_id\": \"80becb22-908d-84bc-3a5f-00b620d551bc\",\n       ..."
          ],
          [
           "```\n\nIf the result has `partial: true` it means that the search couldn't be run on the full dataset ..."
          ],
          [
           "Developer guide\n\nThis document is intended for developers who want to install, test or contribute to..."
          ],
          [
           "```\n\nIt will create a virtual environment in a `./.venv/` subdirectory.\n\nIf you use VSCode, it might..."
          ],
          [
           "If you have access to the internal HF notion, see https://www.notion.so/huggingface2/Datasets-server..."
          ],
          [
           "Note that every worker has its own job queue:\n\n- `/splits`: the job is to refresh a dataset, namely ..."
          ],
          [
           "The following environments contain all the modules: reverse proxy, API server, admin API server, wor..."
          ],
          [
           "```\n\nTo check the quality (which includes checking the style, but also security vulnerabilities):\n\n`..."
          ],
          [
           "```\n\n## Set up development environment\n\n### Linux\n\nInstall pyenv:\n\n```bash\n$ curl https://pyenv.run ..."
          ],
          [
           "```\n\n#### Then: as a normal user\n\nAdd ICU to the path:\n\n```bash\n$ echo 'export PATH=\"/opt/homebrew/o..."
          ],
          [
           "```\n\nSet the python version to use with poetry:\n\n```bash\npoetry env use 3.9.18\n```\n\nAvoid an issue w..."
          ],
          [
           "Server infrastructure\n\nThe [Datasets Server](https://github.com/huggingface/datasets-server) has two..."
          ],
          [
           "You might've noticed the `/rows` and `/search` endpoints don't have a job in the queue. The response..."
          ],
          [
           "Datasets server maintenance job\n\n> Job to run maintenance actions on the datasets-server\n\nAvailable ..."
          ],
          [
           "Datasets server - storage admin\n\n> A Ubuntu machine to log into and manage the storage manually..."
          ],
          [
           "ðŸ¤— Datasets Server\n\nDatasets Server is a lightweight web API for visualizing and exploring all types ..."
          ],
          [
           "<div class=\"flex justify-center\">\n  <img\n    style=\"margin-bottom: 0;\"\n    class=\"block dark:hidden\"..."
          ],
          [
           "Get the number of rows and the size in bytes\n\nThis guide shows you how to use Datasets Server's `/si..."
          ],
          [
           "```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing the size of the dataset,..."
          ],
          [
           "```json\n{\n  \"size\": {\n    \"dataset\": {\n      \"dataset\": \"duorc\",\n      \"num_bytes_original_files\": 9..."
          ],
          [
           "\"num_bytes_parquet_files\": 26005668,\n        \"num_bytes_memory\": 496682909,\n        \"num_rows\": 6952..."
          ],
          [
           "\"config\": \"SelfRC\",\n        \"split\": \"validation\",\n        \"num_bytes_parquet_files\": 3114390,\n     ..."
          ],
          [
           "```\n\nIf the size has `partial: true` it means that the actual size of the dataset couldn't been dete..."
          ],
          [
           "Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configur..."
          ],
          [
           "Datasets server - reverse proxy\n\n> Reverse-proxy in front of the API\n\nSee [docker-compose-datasets-s..."
          ],
          [
           "It takes various environment variables, all of them are mandatory:\n\n- `ASSETS_DIRECTORY`: the direct..."
          ],
          [
           "Data types\n\nDatasets supported by Datasets Server have a tabular format, meaning a data point is rep..."
          ],
          [
           "```\n\nThis dataset has two columns, `text` and `label`:\n\n- The `text` column has a type of `Value`. T..."
          ],
          [
           "List splits and configurations\n\nDatasets typically have splits and may also have configurations. A _..."
          ],
          [
           "```\n</python>\n<js>\n```js\nimport fetch from \"node-fetch\";\nasync function query(data) {\n    const resp..."
          ],
          [
           "Datasets server API\n\n> API on ðŸ¤— datasets\n\n## Configuration\n\nThe service can be configured using envi..."
          ],
          [
           "Datasets server API - search service\n\n> /search endpoint\n> /filter endpoint\n\n## Configuration\n\nThe s..."
          ],
          [
           "Datasets server databases migrations\n\n> Scripts to migrate the datasets server databases\n\n## Configu..."
          ]
         ],
         "hovertemplate": "source=datasets-server<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "datasets-server, circle",
         "marker": {
          "color": "#636efa",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "datasets-server, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          3.606383,
          3.4576733,
          3.4321015,
          3.967037,
          4.878384,
          2.847591,
          4.388145,
          4.257116,
          4.1180434,
          1.2281263,
          1.173701,
          1.1036767,
          0.99024427,
          0.9370847,
          1.2027653,
          1.1858302,
          1.1106727,
          1.8263834,
          6.0434456,
          2.8330562,
          1.8536919,
          3.5811806,
          3.6468434,
          4.295165,
          3.7132561,
          4.2516155,
          7.9772844,
          3.0537384,
          4.1371837,
          4.851587,
          3.5546484,
          4.091399,
          3.8620646,
          3.6935503,
          4.0857735,
          4.5153494,
          3.4300225,
          3.3459656,
          3.4689226,
          3.3575163,
          3.1764343,
          5.317003,
          5.1520267,
          3.5638115,
          3.633628,
          4.1472836,
          5.9045787,
          4.0193396,
          3.3320372,
          3.633863,
          3.7168596,
          4.0392556,
          4.3745985,
          5.5194683,
          3.524654,
          4.089666,
          3.476345,
          3.4303987,
          4.33899,
          3.4338682,
          4.3578424,
          7.048989,
          7.053663,
          7.0507655,
          7.048041,
          7.091011,
          6.98016,
          7.0669446,
          7.0212703,
          7.0472326,
          7.007384,
          6.5776887,
          6.7714825,
          2.3211625,
          3.5696223,
          3.315744,
          2.1209433,
          3.849132,
          3.9645197,
          3.7802305,
          3.931798,
          4.1221757,
          3.3555024,
          3.6563969,
          3.4237702,
          3.5581543,
          3.4789417,
          4.1968703,
          3.870474,
          3.7199023,
          3.8585231,
          3.7838373,
          4.3925724,
          5.3645425,
          4.3471413,
          3.9748833,
          3.4091096,
          3.2770326,
          3.3298972,
          3.6627972,
          2.668763,
          2.6630042,
          3.4088945,
          3.4374418,
          3.029262,
          1.6831515,
          3.3090024,
          3.2386785,
          3.1635919,
          3.24231,
          3.583321,
          3.0586348,
          3.6989505,
          1.2644225,
          1.3264979,
          1.6549505,
          1.3299807,
          4.4881887,
          4.818565,
          4.034058,
          3.9142406,
          2.090005,
          2.677332,
          1.3795036,
          3.3103406,
          4.317934,
          3.5517712,
          4.257126,
          7.0339518,
          6.727622,
          6.552327,
          2.0762527,
          3.6300802,
          3.0962515,
          4.296527,
          3.3998017,
          4.38594,
          4.253014,
          7.052951,
          7.070692,
          7.121196,
          7.0871954,
          7.05955,
          7.052073,
          7.0703464,
          7.098689,
          7.122195,
          7.0810113,
          7.0853276,
          7.1109996,
          7.06937,
          7.0480933,
          6.675523,
          6.708937,
          1.8128817,
          3.7963965,
          3.932233,
          3.978162,
          4.0183783,
          4.1738925,
          3.8082852,
          2.265731,
          2.5616264,
          2.3450267,
          3.6859345,
          3.657954,
          3.6894329,
          4.0733056,
          3.432897,
          4.49967,
          3.5375836,
          3.5648577,
          4.0080376,
          3.0778284,
          3.44331,
          1.442582,
          3.6370149,
          4.146448,
          4.8665075,
          1.9526314,
          1.4181476,
          2.7556257,
          4.094484,
          3.6543994,
          3.5791874,
          3.6523418
         ],
         "xaxis": "x",
         "y": [
          -3.2875385,
          -3.3979156,
          -2.4272773,
          -2.0098271,
          -1.8226035,
          -4.0442247,
          -1.2794241,
          -1.5442507,
          -2.8456802,
          -4.875671,
          -4.70924,
          -5.179717,
          -5.0355744,
          -5.0563483,
          -5.131979,
          -5.1997247,
          -5.1635847,
          -4.568906,
          -2.94038,
          -4.545303,
          -4.716458,
          -4.271908,
          -4.574945,
          -4.6821647,
          -4.721418,
          -4.707775,
          -1.6909541,
          -4.492057,
          -4.709653,
          -3.7965868,
          -4.497532,
          -4.6412983,
          -4.566653,
          -3.740121,
          -2.1484468,
          -0.30139273,
          -4.4776263,
          -4.5501933,
          -4.6478972,
          -4.4422536,
          -4.5587254,
          0.28502256,
          -1.0981122,
          -3.5867987,
          -4.3633604,
          -4.7749023,
          -1.5405917,
          -3.0634153,
          -2.827157,
          -4.474132,
          -4.5803723,
          -4.8033442,
          -3.1343203,
          -3.4988446,
          -3.5179434,
          -2.9301426,
          -4.0985284,
          -4.664856,
          -4.7201595,
          -4.5316916,
          -4.57635,
          -3.644254,
          -4.6836667,
          -4.6698728,
          -4.683853,
          -4.6755075,
          -4.7387624,
          -4.753958,
          -4.742363,
          -4.62125,
          -4.562648,
          -4.404662,
          -4.460067,
          -3.7319446,
          -4.122353,
          -4.316246,
          -4.5842085,
          -3.525765,
          -4.2524614,
          -4.6933403,
          -4.8185306,
          -4.810042,
          -4.796928,
          -4.706718,
          -4.86081,
          -4.7497835,
          -4.7761784,
          -4.8472056,
          -4.712186,
          -4.805989,
          -4.8264365,
          -3.8585865,
          -2.5462072,
          -1.5569941,
          -3.6611185,
          -2.3563895,
          -4.2709284,
          -4.6688657,
          -4.386984,
          -4.6474166,
          -3.8683836,
          -4.6120186,
          -4.2846136,
          -4.4978604,
          -4.4009194,
          -4.6712737,
          -4.616464,
          -4.688011,
          -4.5894876,
          -4.7093105,
          -4.521645,
          -4.836948,
          -4.83034,
          -5.0140185,
          -5.1928105,
          -5.071651,
          -5.000228,
          -2.3003926,
          -2.2199705,
          -2.9582345,
          -3.0778975,
          -4.1007905,
          -4.075969,
          -1.9487262,
          -4.2859683,
          -4.744601,
          -4.7399583,
          -4.6638117,
          -4.8694243,
          -4.5127177,
          -4.4799976,
          -4.5305266,
          -4.8413067,
          -4.6593156,
          -4.76486,
          -4.578641,
          -4.6615114,
          -4.7222548,
          -5.1117344,
          -5.1434813,
          -5.1319056,
          -5.214873,
          -4.9851136,
          -4.958056,
          -5.0075827,
          -5.1143365,
          -5.133189,
          -5.129131,
          -4.981264,
          -5.0724893,
          -5.037287,
          -4.801527,
          -4.442208,
          -4.476706,
          -4.5538435,
          -0.69128436,
          -1.589125,
          -3.535365,
          -3.2784796,
          -1.9978335,
          -0.80638385,
          0.1705133,
          -0.06615078,
          -0.011573747,
          -3.8350725,
          -3.724822,
          -3.3103669,
          -3.0515594,
          -3.8546786,
          -2.5242243,
          -4.7414465,
          -4.728137,
          -4.7321134,
          -4.5744867,
          -4.5142293,
          -4.639417,
          -4.165088,
          -3.4754558,
          -2.2529502,
          -4.420386,
          -4.3649316,
          -4.448482,
          -4.7784615,
          -3.9721453,
          -4.0509315,
          -3.1329336
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Differences between Dataset and IterableDataset\n\nThere are two types of dataset objects, a [`Dataset..."
          ],
          [
           "```\n\nStreaming can read online data without writing any file to disk.\nFor example, you can stream da..."
          ],
          [
           "```\n\n## Loading local files entirely and progressively\n\nIt is possible to convert local or remote da..."
          ],
          [
           "```\n\nOn the other hand, due to the \"lazy\" nature of an `IterableDataset`, calling [`IterableDataset...."
          ],
          [
           "```\n\nSince we don't have random access to the rows in the case of an `IterableDataset`, we can't use..."
          ],
          [
           "```\n\nBut using a shuffle buffer is not enough to provide a satisfactory shuffling for machine learni..."
          ],
          [
           "```\n\n## Speed differences\n\nRegular [`Dataset`] objects are based on Arrow which provides fast random..."
          ],
          [
           "```\n\n\nIn this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approxi..."
          ],
          [
           "```\n\nIf you want to shuffle your dataset or [use it with a PyTorch DataLoader](./use_with_pytorch#st..."
          ],
          [
           "Metric Card for F1\n\n\n## Metric Description\n\nThe F1 score is the harmonic mean of the precision and r..."
          ],
          [
           "```\n\n\n### Inputs\n- **predictions** (`list` of `int`): Predicted labels.\n- **references** (`list` of ..."
          ],
          [
           "### Output Values\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending ..."
          ],
          [
           "```\n```python\n{'f1': array([0.8, 0.0, 0.0])}\n```\n\nThis metric outputs a dictionary, with either a si..."
          ],
          [
           "```\n\nExample 4-A multiclass example, with different values for the `average` input.\n```python\n>>> pr..."
          ],
          [
           "Cache management\n\nWhen you download a dataset, the processing scripts and data are stored locally on..."
          ],
          [
           "```\n\nRefer to [`DownloadMode`] for a full list of download modes.\n\n## Cache files\n\nClean up the cach..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nKeeping the predictions in-memory is not possible in a distributed settin..."
          ],
          [
           "Metric Card for MSE\n\n\n## Metric Description\n\nMean Squared Error(MSE) represents the average of the s..."
          ],
          [
           "```\n\n### Inputs\n\nMandatory inputs: \n- `predictions`: numeric array-like of shape (`n_samples,`) or (..."
          ],
          [
           "```\n\nIf `multioutput=\"raw_values\"`:\n```python\n{'mse': array([0.41666667, 1. ])}\n```\n\n#### Values fro..."
          ],
          [
           "```\n\n## Limitations and Bias\nMSE has the disadvantage of heavily weighting outliers -- given that it..."
          ],
          [
           "All about metrics\n\n<Tip warning={true}>\n\nMetrics is deprecated in ðŸ¤— Datasets. To learn more about ho..."
          ],
          [
           "## Distributed evaluation\n\nComputing metrics in a distributed environment can be tricky. Metric eval..."
          ],
          [
           "Metric Card for METEOR\n\n## Metric description\n\nMETEOR (Metric for Evaluation of Translation with Exp..."
          ],
          [
           "```\n\n## Output values\n\nThe metric outputs a dictionary containing the METEOR score. Its values range..."
          ],
          [
           "```\n\n## Limitations and bias\n\nWhile the correlation between METEOR and human judgments was measured ..."
          ],
          [
           "Preprocess\n\nIn addition to loading datasets, ðŸ¤— Datasets other main goal is to offer a diverse set of..."
          ],
          [
           "```\n\nGrab a dataset of your choice and follow along!\n\n## Tokenize text\n\nModels cannot process raw te..."
          ],
          [
           "```\n\n**2**. Call your tokenizer on the first row of `text` in the dataset:\n\n```py\n>>> tokenizer(data..."
          ],
          [
           "```\n\nThe tokenizer returns a dictionary with three items:\n\n- `input_ids`: the numbers representing t..."
          ],
          [
           "```\n</pt>\n<tf>\nUse the [`~Dataset.to_tf_dataset`] function to set the dataset format to be compatibl..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n**5**. The dataset is now ready for training with your machine learni..."
          ],
          [
           "```\n\n**2**. Index into the first row of the dataset. When you call the `audio` column of the dataset..."
          ],
          [
           "```\n\n**3**. Reading a dataset card is incredibly useful and can give you a lot of information about ..."
          ],
          [
           "```\n\n**4**. Use the [`~Dataset.map`] function to resample the entire dataset to 16kHz. This function..."
          ],
          [
           "```\n\n**2**. Index into the first row of the dataset. When you call the `image` column of the dataset..."
          ],
          [
           "Image classification\n\nImage classification datasets are used to train a model to classify an entire ..."
          ],
          [
           "```\n\nThe dataset has three fields:\n\n* `image`: a PIL image object.\n* `image_file_path`: the path to ..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"block dark:hidden\" src=\"https://huggingface.c..."
          ],
          [
           "Beam Datasets\n\nSome datasets are too large to be processed on a single machine. Instead, you can pro..."
          ],
          [
           "```\n\n4. Run the pipeline:\n\n```\ndatasets-cli run_beam datasets/$DATASET_NAME \\\n--name $CONFIG_NAME \\\n..."
          ],
          [
           "Load image data\n\nImage datasets have [`Image`] type columns, which contain PIL objects. \n\n<Tip>\n\nTo ..."
          ],
          [
           "```\n\nIf you only want to load the underlying path to the image dataset without decoding the image ob..."
          ],
          [
           "```\n\nLoad your dataset by specifying `imagefolder` and the directory of your dataset in `data_dir`:\n..."
          ],
          [
           "```\n\n<Tip>\n\nFor more information about creating your own `ImageFolder` dataset, take a look at the [..."
          ],
          [
           "Metric Card for Recall\n\n\n## Metric Description\n\nRecall is the fraction of the positive examples that..."
          ],
          [
           "### Inputs\n- **predictions** (`list` of `int`): The predicted labels.\n- **references** (`list` of `i..."
          ],
          [
           "- `'samples'`: Calculate metrics for each instance, and find their average (only meaningful for mult..."
          ],
          [
           "### Output Values\n- **recall**(`float`, or `array` of `float`, for multiclass targets): Either the g..."
          ],
          [
           "```\n```python\n{'recall': array([1., 0., 0.])}\n```\n\nThis metric outputs a dictionary with one entry, ..."
          ],
          [
           "```\n\nExample 4-A multiclass example, using different averages.\n```python\n>>> recall_metric = dataset..."
          ],
          [
           "Dataset features\n\n[`Features`] defines the internal structure of a dataset. It is used to specify th..."
          ],
          [
           "```\n\nThe [`Value`] feature tells ðŸ¤— Datasets:\n\n- The `idx` data type is `int32`.\n- The `sentence1` an..."
          ],
          [
           "```\n\nThe `answers` field is constructed using the [`Sequence`] feature because it contains two subfi..."
          ],
          [
           "```\n\n## Audio feature\n\nAudio datasets have a column with type [`Audio`], which contains three import..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nIndex into an audio dataset using the row index first and then the `audio..."
          ],
          [
           "```\n\n## Image feature\n\nImage datasets have a column with type [`Image`], which loads `PIL.Image` obj..."
          ],
          [
           "```\n\nDepending on the dataset, you may get the path to the local downloaded image, or the content of..."
          ],
          [
           "Metric Card for GLUE\n\n## Metric description\nThis metric is used to compute the GLUE evaluation metri..."
          ],
          [
           "```\n## Output values\n\nThe output of the metric depends on the GLUE subset chosen, consisting of a di..."
          ],
          [
           "### Values from popular papers\nThe [original GLUE paper](https://huggingface.co/datasets/glue) repor..."
          ],
          [
           "```\n\nMinimal values for the STSB subset (which outputs `pearson` and `spearmanr`):\n\n```python\nfrom d..."
          ],
          [
           "```\n    \n## Further References \n\n- [GLUE benchmark homepage](https://gluebenchmark.com/)\n- [Fine-tun..."
          ],
          [
           "Metric Card for Matthews Correlation Coefficient\n\n## Metric Description\nThe Matthews correlation coe..."
          ],
          [
           "```\n\nThe same example as above, but also including sample weights:\n```python\n>>> matthews_metric = d..."
          ],
          [
           "```\n\n## Further References\n\n- This Hugging Face implementation uses [this scikit-learn implementatio..."
          ],
          [
           "Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge..."
          ],
          [
           "**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attri..."
          ],
          [
           "Table Classes\n\nEach `Dataset` object is backed by a PyArrow Table.\nA Table can be loaded from either..."
          ],
          [
           "## ConcatenationTable\n\n[[autodoc]] datasets.table.ConcatenationTable\n    - validate\n    - equals\n   ..."
          ],
          [
           "Metric Card for Precision\n\n\n## Metric Description\n\nPrecision is the fraction of correctly labeled po..."
          ],
          [
           "### Inputs\n- **predictions** (`list` of `int`): Predicted class labels.\n- **references** (`list` of ..."
          ],
          [
           "- 'samples': Calculate metrics for each instance, and find their average (only meaningful for multil..."
          ],
          [
           "### Output Values\n- **precision**(`float` or `array` of `float`): Precision score or list of precisi..."
          ],
          [
           "```\n```python\n{'precision': array([0.66666667, 0.0, 0.0])}\n```\n\n\n\n\n#### Values from Popular Papers\n\n..."
          ],
          [
           "```\n\nExample 4-A multiclass example, with different values for the `average` input.\n```python\n>>> pr..."
          ],
          [
           "```\n\n\n## Limitations and Bias\n\n[Precision](https://huggingface.co/metrics/precision) and [recall](ht..."
          ],
          [
           "Load tabular data\n\nA tabular dataset is a generic dataset used to describe any data stored in rows a..."
          ],
          [
           "```\n\nTo load zipped CSV files:\n\n```py\n>>> url = \"https://domain.org/train_data.zip\"\n>>> data_files =..."
          ],
          [
           "```\n\nIf the dataset doesn't look as expected, you should explicitly [specify your dataset features](..."
          ],
          [
           "```\n\nThis creates a `states` table in the `us_covid_data.db` database which you can now load into a ..."
          ],
          [
           "```\n\nYou can also load a dataset from a SQL query instead of an entire table, which is useful for qu..."
          ],
          [
           "--\nTODO: Add YAML tags here. Copy-paste the tags obtained with the online tagging app: https://huggi..."
          ],
          [
           "[More Information Needed]\n\n### Languages\n\n[More Information Needed]\n\n## Dataset Structure\n\n### Data ..."
          ],
          [
           "Use with Spark\n\nThis document is a quick introduction to using ðŸ¤— Datasets with Spark, with a particu..."
          ],
          [
           "```\n\n### Caching\n\nWhen using [`Dataset.from_spark`], the resulting [`Dataset`] is cached; if you cal..."
          ],
          [
           "```py\n>>> from datasets import Dataset, Features, Image, Value\n>>> data = [(0, open(\"image.png\", \"rb..."
          ],
          [
           "```\n\nYou can check the [`Features`] documentation to know about all the feature types available...."
          ],
          [
           "p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggi..."
          ],
          [
           "<p align=\"center\">\n    <a href=\"https://github.com/huggingface/datasets/actions/workflows/ci.yml?que..."
          ],
          [
           "</a>\n    <a href=\"CODE_OF_CONDUCT.md\">\n        <img alt=\"Contributor Covenant\" src=\"https://img.shie..."
          ],
          [
           "ðŸ¤— Datasets is a lightweight library providing **two** main features:\n\n- **one-line dataloaders for m..."
          ],
          [
           "ðŸ¤— Datasets is designed to let the community easily add and share new datasets.\n\nðŸ¤— Datasets has many ..."
          ],
          [
           "```\n\n## With conda\n\nðŸ¤— Datasets can be installed using conda as follows:\n\n```bash\nconda install -c hu..."
          ],
          [
           "```\n\nFollow the installation pages of TensorFlow and PyTorch to see how to install them with conda.\n..."
          ],
          [
           "```\n\nIf your dataset is bigger than your disk or if you don't want to wait to download the data, you..."
          ],
          [
           "```\n\nFor more details on using the library, check the quick start page in the documentation: https:/..."
          ],
          [
           "# Main differences between ðŸ¤— Datasets and `tfds`\n\nIf you are familiar with the great TensorFlow Data..."
          ],
          [
           "```bibtex\n@inproceedings{lhoest-etal-2021-datasets,\n    title = \"Datasets: A Community Library for N..."
          ],
          [
           "month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publi..."
          ],
          [
           "```\n\nIf you need to cite a specific version of our ðŸ¤— Datasets library for reproducibility, you can u..."
          ],
          [
           "The cache\n\nThe cache is one of the reasons why ðŸ¤— Datasets is so efficient. It stores previously down..."
          ],
          [
           "```\n\nIn order for a transform to be hashable, it needs to be picklable by [dill](https://dill.readth..."
          ],
          [
           "```\n\nThe hash is computed by dumping the object using a `dill` pickler and hashing the dumped bytes...."
          ],
          [
           "Metric Card for chrF(++)\n\n\n## Metric Description\nChrF and ChrF++ are two MT evaluation metrics that ..."
          ],
          [
           "```\n\n### Inputs\n- **`predictions`** (`list` of `str`): The predicted sentences.\n- **`references`** (..."
          ],
          [
           "The output is formatted as below:\n```python\n{'score': 61.576379378113785, 'char_order': 6, 'word_ord..."
          ],
          [
           "```\n\nThe chrF(++) score can be any value between `0.0` and `100.0`, inclusive.\n\n#### Values from Pop..."
          ],
          [
           "```\n\nThe same chrF++ example as above, but with `lowercase=True` to normalize all case:\n```python\n>>..."
          ],
          [
           "```\n\n\n## Limitations and Bias\n- According to [PopoviÄ‡ 2017](https://www.statmt.org/wmt17/pdf/WMT70.p..."
          ],
          [
           "## Citation\n```bibtex\n@inproceedings{popovic-2015-chrf,\n    title = \"chr{F}: character n-gram {F}-sc..."
          ],
          [
           "```\n\n## Further References\n- See the [sacreBLEU README.md](https://github.com/mjpost/sacreBLEU#chrf-..."
          ],
          [
           "Metric Card for BERT Score\n\n## Metric description\n\nBERTScore is an automatic evaluation metric for t..."
          ],
          [
           "```\n\nBERTScore also accepts multiple optional arguments: \n\n\n`num_layers` (int): The layer of represe..."
          ],
          [
           "`recall`: The [recall](https://huggingface.co/metrics/recall) for each sentence from the `prediction..."
          ],
          [
           "```\n\nPartial match with the `bert-base-uncased` model:\n\n```python\nfrom datasets import load_metric\nb..."
          ],
          [
           "```\n\n## Limitations and bias\n\nThe [original BERTScore paper](https://openreview.net/pdf?id=SkeHuCVFD..."
          ],
          [
           "Metric Card for ROUGE\n\n## Metric Description\nROUGE, or Recall-Oriented Understudy for Gisting Evalua..."
          ],
          [
           "```\n\n### Inputs\n- **predictions** (`list`): list of predictions to score. Each prediction\n        sh..."
          ],
          [
           "```python\n{'rouge1': [Score(precision=1.0, recall=0.5, fmeasure=0.6666666666666666), Score(precision..."
          ],
          [
           "```\n\nIf `rouge_types=['rouge1', 'rouge2']` and `use_aggregator=True`, the output is of the following..."
          ],
          [
           "```\n\nThe same example, but with aggregation:\n```python\n>>> rouge = datasets.load_metric('rouge')\n>>>..."
          ],
          [
           "Metric Card for Exact Match\n\n\n## Metric Description\nA given predicted string's exact match score is ..."
          ],
          [
           "```\n\n### Inputs\n- **`predictions`** (`list` of `str`): List of predicted texts.\n- **`references`** (..."
          ],
          [
           "```\n\nThis metric's range is 0-100, inclusive. Here, 0.0 means no prediction/reference pairs were mat..."
          ],
          [
           "```\nNote that in the example above, because the regexes are ignored before the case is normalized, \"..."
          ],
          [
           "```\n\nAn example that includes sentences:\n```python\n>>> exact_match = datasets.load_metric(\"exact_mat..."
          ],
          [
           "Metric Card for COMET\n\n## Metric description\n\nCrosslingual Optimized Metric for Evaluation of Transl..."
          ],
          [
           "```\n\nIt has several configurations, named after the COMET model to be used. It will default to `wmt2..."
          ],
          [
           "## Examples\n\nFull match:\n\n```python\nfrom datasets import load_metric\ncomet_metric = load_metric('com..."
          ],
          [
           "```\n\nPartial match:\n\n```python\nfrom datasets import load_metric\ncomet_metric = load_metric('comet') ..."
          ],
          [
           "```\n\n## Limitations and bias\n\nThe models provided for calculating the COMET metric are built on top ..."
          ],
          [
           "## Citation\n\n```bibtex\n@inproceedings{rei-EtAl:2020:WMT,\n   author    = {Rei, Ricardo  and  Stewart,..."
          ],
          [
           "```\n\n```bibtex\n@inproceedings{rei-etal-2020-comet,\n   title = \"{COMET}: A Neural Framework for {MT} ..."
          ],
          [
           "Metric Card for seqeval\n\n## Metric description\n\nseqeval is a Python framework for sequence labeling ..."
          ],
          [
           "```python\n>>> from datasets import load_metric\n>>> seqeval = load_metric('seqeval')\n>>> predictions ..."
          ],
          [
           "```\n\n## Output values\n\nThis metric returns a dictionary with a summary of scores for overall and per..."
          ],
          [
           "More recently, seqeval continues being used for reporting performance on tasks such as [named entity..."
          ],
          [
           "```\n\nMinimal values (no match):\n\n```python\n>>> from datasets import load_metric\n>>> seqeval = load_m..."
          ],
          [
           "```\n\nPartial match:\n\n```python\n>>> from datasets import load_metric\n>>> seqeval = load_metric('seqev..."
          ],
          [
           "```\n\n## Limitations and bias\n\nseqeval supports following IOB formats (short for inside, outside, beg..."
          ],
          [
           "Utilities\n\n## Configure logging\n\nðŸ¤— Datasets strives to be transparent and explicit about how it work..."
          ],
          [
           "```\n\nAll the methods of this logging module are documented below. The main ones are:\n\n- [`logging.ge..."
          ],
          [
           "[[autodoc]] datasets.logging.disable_propagation\n\n[[autodoc]] datasets.logging.enable_propagation\n\n#..."
          ],
          [
           "Use with PyTorch\n\nThis document is a quick introduction to using `datasets` with PyTorch, with a par..."
          ],
          [
           "```\n\n## N-dimensional arrays\n\nIf your dataset consists of N-dimensional arrays, you will see that by..."
          ],
          [
           "```\n\n\n## Other feature types\n\n[`ClassLabel`] data are properly converted to tensors:\n\n```py\n>>> from..."
          ],
          [
           "```\n\nString and binary objects are unchanged, since PyTorch only supports numbers.\n\nThe [`Image`] an..."
          ],
          [
           "```\n\n<Tip>\n\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\n`pip inst..."
          ],
          [
           "```\n\n## Data loading\n\nLike `torch.utils.data.Dataset` objects, a [`Dataset`] can be passed directly ..."
          ],
          [
           "```\n\n### Optimize data loading\n\nThere are several ways you can increase the speed your data is loade..."
          ],
          [
           "```\n\n### Stream data\n\nStream a dataset by loading it as an [`IterableDataset`]. This allows you to p..."
          ],
          [
           "```\n\nIn this case each worker is given a subset of the list of shards to stream from.\n\n### Distribut..."
          ],
          [
           "Semantic segmentation\n\nSemantic segmentation datasets are used to train a model to classify every pi..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           ">>> visualize_seg_mask(\n...     np.array(dataset[index][\"image\"]),\n...     np.array(dataset[index][\"..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nYou can verify the transformation worked by indexing into the `pixel_values` and `label` of an ..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "Metric Card for *Current Metric*\n\n***Metric Card Instructions:*** *Copy this file into the relevant ..."
          ],
          [
           "Metric Card for WikiSplit\n\n## Metric description\n\nWikiSplit is the combination of three metrics: [SA..."
          ],
          [
           "```\n## Output values\n\nThis metric outputs a dictionary containing three scores:\n\n`sari`: the [SARI](..."
          ],
          [
           "```\n\n### Values from popular papers\n\nThis metric was initially used by [Rothe et al.(2020)](https://..."
          ],
          [
           "```\n\nNo match between prediction and reference:\n\n```python\n>>> from datasets import load_metric\n>>> ..."
          ],
          [
           "Metric Card for FrugalScore\n\n\n## Metric Description\nFrugalScore is a reference-based metric for Natu..."
          ],
          [
           "```\n\n### Values from popular papers\nThe [original FrugalScore paper](https://arxiv.org/abs/2110.0855..."
          ],
          [
           "How to add one new datasets\n\nAdd datasets directly to the ðŸ¤— Hugging Face Hub!\n\nYou can share your da..."
          ],
          [
           "Metric Card for Google BLEU (GLEU)\n\n\n## Metric Description\nThe BLEU score has some undesirable prope..."
          ],
          [
           "```\n\n### Inputs\n- **predictions** (list of list of str): list of translations to score. Each transla..."
          ],
          [
           "```\n\n#### Values from Popular Papers\n\n\n### Examples\nExample with one reference per sample:\n```python..."
          ],
          [
           "```\n\nExample with multiple references for the first sample:\n```python\n>>> hyp1 = ['It', 'is', 'a', '..."
          ],
          [
           ">>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\n>>> hypotheses = [hyp1, hyp2]\n>>> google_b..."
          ],
          [
           "```\n\nExample with multiple references for the first sample, and with `min_len` adjusted to `2`, inst..."
          ],
          [
           ">>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n...         'interested', 'in', 'wo..."
          ],
          [
           "```\n\nExample with multiple references for the first sample, with `min_len` adjusted to `2`, instead ..."
          ],
          [
           ">>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n...         'interested', 'in', 'wo..."
          ],
          [
           "```\n\n## Limitations and Bias\n\n\n## Citation\n```bibtex\n@misc{wu2016googles,\ntitle={Google's Neural Mac..."
          ],
          [
           "# Add Dummy data test\n\n**Important** In order to pass the `load_dataset_<dataset_name>` test, dummy ..."
          ],
          [
           "Here we have to pay close attention to the ``_split_generators(self, dl_manager)`` function of the d..."
          ],
          [
           "**Note** if ``<value_of_dict>`` is a zipped file then the dummy data folder structure should contain..."
          ],
          [
           "Main classes\n\n\n## DatasetInfo\n\n[[autodoc]] datasets.DatasetInfo\n\n## Dataset\n\nThe base class [`Datase..."
          ],
          [
           "[[autodoc]] datasets.concatenate_datasets\n\n[[autodoc]] datasets.interleave_datasets\n\n[[autodoc]] dat..."
          ],
          [
           "## IterableDataset\n\nThe base class [`IterableDataset`] implements an iterable Dataset backed by pyth..."
          ],
          [
           "[[autodoc]] datasets.Image\n\n## MetricInfo\n\n[[autodoc]] datasets.MetricInfo\n\n## Metric\n\nThe base clas..."
          ],
          [
           "Metric Card for COVAL\n\n## Metric description\n\nCoVal is a coreference evaluation tool for the [CoNLL]..."
          ],
          [
           "```python\nfrom datasets import load_metric\ncoval = load_metric('coval')\nwords = ['bc/cctv/00/cctv_00..."
          ],
          [
           "```\nIt also has several optional arguments:\n\n`keep_singletons`: After extracting all mentions of key..."
          ],
          [
           "### Values from popular papers\n\nGiven that many of the metrics returned by COVAL come from different..."
          ],
          [
           "## Examples \n\nMaximal values\n\n```python\nfrom datasets import load_metric\ncoval = load_metric('coval'..."
          ],
          [
           "```\n\n## Limitations and bias\n\nThis wrapper of CoVal currently only works with [CoNLL line format](ht..."
          ],
          [
           "| Column | Type                  | Description                                                      ..."
          ],
          [
           "| 5      | Part-of-Speech        |                                                                  ..."
          ],
          [
           "## Citations\n\n```bibtex\n@InProceedings{moosavi2019minimum,\n  author = { Nafise Sadat Moosavi, Leo Bo..."
          ],
          [
           "```\n```bibtex\n@inproceedings{10.3115/1072399.1072405,\n  author = {Vilain, Marc and Burger, John and ..."
          ],
          [
           "```\n\n```bibtex\n@inproceedings{moosavi-strube-2016-coreference,\n    title = \"Which Coreference Evalua..."
          ],
          [
           "Overview\n\nThe how-to guides offer a more comprehensive overview of all the tools ðŸ¤— Datasets offers a..."
          ],
          [
           "If you have any questions about ðŸ¤— Datasets, feel free to join and ask the community on our [forum](h..."
          ],
          [
           "Metric Card for SacreBLEU\n\n\n## Metric Description\nSacreBLEU provides hassle-free computation of shar..."
          ],
          [
           "### Inputs\n- **`predictions`** (`list` of `str`): list of translations to score. Each translation sh..."
          ],
          [
           "- `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n    - `'char'`: La..."
          ],
          [
           "### Output Values\n- `score`: BLEU score\n- `counts`: Counts\n- `totals`: Totals\n- `precisions`: Precis..."
          ],
          [
           "```\nThe score can take any value between `0.0` and `100.0`, inclusive.\n\n#### Values from Popular Pap..."
          ],
          [
           "Cloud storage\n\nðŸ¤— Datasets supports access to cloud storage providers through a `fsspec` FileSystem i..."
          ],
          [
           "```\n>>> pip install s3fs\n```\n\n2. Define your credentials\n\nTo use an anonymous connection, use `anon=..."
          ],
          [
           "```\n\n3. Create your FileSystem instance\n\n```py\n>>> import gcsfs\n>>> fs = gcsfs.GCSFileSystem(**stora..."
          ],
          [
           "```\n\n3. Create your FileSystem instance\n\n```py\n>>> import ocifs\n>>> fs = ocifs.OCIFileSystem(**stora..."
          ],
          [
           "```\n\nUse your own data files (see [how to load local and remote files](./loading#local-and-remote-fi..."
          ],
          [
           "```\n\n#### Dask\n\nDask is a parallel computing library and it has a pandas-like API for working with l..."
          ],
          [
           "```\n\nYou can find more about dask dataframes in their [documentation](https://docs.dask.org/en/stabl..."
          ],
          [
           "```\n\n### Load serialized datasets\n\nWhen you are ready to use your dataset again, reload it with [`Da..."
          ],
          [
           "Metric Card for BLEU\n\n\n## Metric Description\nBLEU (Bilingual Evaluation Understudy) is an algorithm ..."
          ],
          [
           "```\n\n### Inputs\n- **predictions** (`list`): Translations to score. Each translation should be tokeni..."
          ],
          [
           "```\n\nBLEU's output is always a number between 0 and 1. This value indicates how similar the candidat..."
          ],
          [
           "### Examples\n\nExample where each sample has 1 reference:\n```python\n>>> predictions = [\n...     [\"hel..."
          ],
          [
           "```\n\nExample where the first sample has 2 references:\n```python\n>>> predictions = [\n...     [\"hello\"..."
          ],
          [
           "```\n\n## Limitations and Bias\nThis metric hase multiple known limitations and biases:\n- BLEU compares..."
          ],
          [
           "Create an image dataset\n\nThere are two methods for creating and sharing an image dataset. This guide..."
          ],
          [
           "```\n\nYou can also use `imagefolder` to load datasets involving multiple splits. To do so, your datas..."
          ],
          [
           "```\n\nor using `metadata.jsonl`:\n\n```jsonl\n{\"file_name\": \"0001.png\", \"additional_feature\": \"This is a..."
          ],
          [
           "```\n\n### Object detection\n\nObject detection datasets have bounding boxes and categories identifying ..."
          ],
          [
           "```\n\n### Upload dataset to the Hub\n\nOnce you've created a dataset, you can share it to the Hub with ..."
          ],
          [
           "```\n\nYou can put your images labels/captions/bounding boxes using JSON or text files for example.\n\nF..."
          ],
          [
           "```\n\nThis guide will show you how to create a dataset loading script for image datasets, which is a ..."
          ],
          [
           "```\n\n#### Multiple configurations\n\nIn some cases, a dataset may have more than one configuration. Fo..."
          ],
          [
           "```\n\nNow you can define your subsets at the top of [`GeneratorBasedBuilder`]. Imagine you want to cr..."
          ],
          [
           "```\n\n### Add dataset metadata\n\nAdding information about your dataset is useful for users to learn mo..."
          ],
          [
           "```\n\n### Download and define the dataset splits\n\nNow that you've added some information about your d..."
          ],
          [
           "</Tip>\n\n```py\ndef _split_generators(self, dl_manager):\n    archive_path = dl_manager.download(_BASE_..."
          ],
          [
           "```\n\n### Generate the dataset\n\nThe last method in the [`GeneratorBasedBuilder`] class actually gener..."
          ],
          [
           "```\n\nIf your loading script passed the test, you should now have the `dataset_info` YAML fields in t..."
          ],
          [
           "Create a dataset card\n\nEach dataset should have a dataset card to promote responsible usage and info..."
          ],
          [
           "</Tip>\n\n3. Click on the **Import dataset card template** link to automatically create a template wit..."
          ],
          [
           "Process\n\nðŸ¤— Datasets provides many tools for modifying the structure and content of a dataset. These ..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nAll processing methods in this guide return a new [`Dataset`] object. Mod..."
          ],
          [
           "```\n\nShuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to create an indices ..."
          ],
          [
           "```\n\n- [`~Dataset.filter`] returns rows that match a specified condition:\n\n```py\n>>> start_with_ar =..."
          ],
          [
           "```\n\nUnless the list of indices to keep is contiguous, those methods also create an indices mapping ..."
          ],
          [
           "```\n\nAfter sharding the dataset into four chunks, the first shard will only have 6250 examples:\n\n```..."
          ],
          [
           "```\n\n### Remove\n\nWhen you need to remove one or more columns, provide the column name to remove to t..."
          ],
          [
           "```\n\n### Cast\n\nThe [`~Dataset.cast`] function transforms the feature type of one or more columns. Th..."
          ],
          [
           "```\n\n<Tip>\n\nCasting only works if the original feature type and new feature type are compatible. For..."
          ],
          [
           "```\n\nThe `answers` field contains two subfields: `text` and `answer_start`. Use the [`~Dataset.flatt..."
          ],
          [
           "```\n\nNow use [`~Dataset.map`] to apply the `add_prefix` function to the entire dataset:\n\n```py\n>>> u..."
          ],
          [
           "```\n\n<Tip>\n\nðŸ¤— Datasets also has a [`~Dataset.remove_columns`] function which is faster because it do..."
          ],
          [
           "```\n\nThe [`~Dataset.map`] also works with the rank of the process if you set `with_rank=True`. This ..."
          ],
          [
           "```\n\n### Batch processing\n\nThe [`~Dataset.map`] function supports working with batches of examples. ..."
          ],
          [
           "```\n\nNotice how the sentences are split into shorter chunks now, and there are more rows in the data..."
          ],
          [
           "```\n\nUse [`~Dataset.map`] to apply the function over the whole dataset:\n\n```py\n>>> augmented_dataset..."
          ],
          [
           "```\n\nFor each original sentence, RoBERTA augmented a random word with three alternatives. The origin..."
          ],
          [
           "```\n\n### Distributed usage\n\nWhen you use [`~Dataset.map`] in a distributed setting, you should also ..."
          ],
          [
           "```\n\nYou can also concatenate two datasets horizontally by setting `axis=1` as long as the datasets ..."
          ],
          [
           "```\n\nYou can also specify the `stopping_strategy`. The default strategy, `first_exhausted`, is a sub..."
          ],
          [
           "```\n\nThe [`~Dataset.with_format`] function also changes the format of a column, except it returns a ..."
          ],
          [
           "```\n\n### Format transform\n\nThe [`~Dataset.set_transform`] function applies a custom formatting trans..."
          ],
          [
           "```\n\nYou can also use the [`~Dataset.set_transform`] function to decode formats not supported by [`F..."
          ],
          [
           ">>> audio_dataset_amr.set_transform(decode_audio_with_pydub)..."
          ],
          [
           "```\n\n## Save\n\nOnce you are done processing your dataset, you can save and reuse it later with [`~Dat..."
          ],
          [
           "Metric Card for SQuAD v2\n\n## Metric description\nThis metric wraps the official scoring script for ve..."
          ],
          [
           "The range of `total` depends on the length of predictions/references: its minimal value is 0, and ma..."
          ],
          [
           "```\n\nMinimal values for both exact match and F1 (no match):\n\n```python\nfrom datasets import load_met..."
          ],
          [
           "```\n\nPartial match (2 out of 3 answers correct) : \n\n```python\nfrom datasets import load_metric\nsquad..."
          ],
          [
           "Metric Card for Perplexity\n\n## Metric Description\nGiven a model and an input text sequence, perplexi..."
          ],
          [
           "```\n\n### Inputs\n- **model_id** (str): model used for calculating Perplexity. NOTE: Perplexity can on..."
          ],
          [
           "```\n\nThis metric's range is 0 and up. A lower score is better.\n\n#### Values from Popular Papers\n\n\n##..."
          ],
          [
           "```\n\n## Limitations and Bias\nNote that the output value is based heavily on what text the model was ..."
          ],
          [
           "Overview\n\nWelcome to the ðŸ¤— Datasets tutorials! These beginner-friendly tutorials will guide you thro..."
          ],
          [
           "Metric Card for Pearson Correlation Coefficient (pearsonr)\n\n\n## Metric Description\n\nPearson correlat..."
          ],
          [
           "```\n\n\n### Inputs\n- **predictions** (`list` of `int`): Predicted class labels, as returned by a model..."
          ],
          [
           "```\n\nExample 2-The same as Example 1, but that also returns the `p-value`.\n```python\n>>> pearsonr_me..."
          ],
          [
           "Load audio data\n\nYou can load an audio dataset using the [`Audio`] feature that automatically decode..."
          ],
          [
           "```\nfolder/train/metadata.csv\nfolder/train/first_audio_file.mp3\nfolder/train/second_audio_file.mp3\nf..."
          ],
          [
           "```\n\nYou can load remote datasets from their URLs with the data_files parameter:\n\n```py\n>>> dataset ..."
          ],
          [
           "```\n\n<Tip>\n\nFor more information about creating your own `AudioFolder` dataset, take a look at the [..."
          ],
          [
           "Search index\n\n[FAISS](https://github.com/facebookresearch/faiss) and [Elasticsearch](https://www.ela..."
          ],
          [
           "```\n\n3. Create the index with [`Dataset.add_faiss_index`]:\n\n```py\n>>> ds_with_embeddings.add_faiss_i..."
          ],
          [
           "```\n\n6. When you are done querying, save the index on disk with [`Dataset.save_faiss_index`]:\n\n```py..."
          ],
          [
           "```\n\n4. If you want to reuse the index, define the `es_index_name` parameter when you build the inde..."
          ],
          [
           "```\n\nFor more advanced Elasticsearch usage, you can specify your own configuration with custom setti..."
          ],
          [
           "How to contribute to Datasets?\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20C..."
          ],
          [
           "```\n\n3. Create a new branch to hold your development changes:\n\n    ```bash\n    git checkout -b a-des..."
          ],
          [
           "```\n\n   Go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project m..."
          ],
          [
           "If you are a **dataset author**... you know what to do, it is your dataset after all ;) ! We would e..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<div class=\"mt-4\">\n   <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"#nl..."
          ],
          [
           "<Tip>\n\nCheck out [Chapter 5](https://huggingface.co/course/chapter5/1?fw=pt) of the Hugging Face cou..."
          ],
          [
           "```\n\nðŸ¤— Datasets also support audio and image data formats:\n\n* To work with audio datasets, install t..."
          ],
          [
           "```\n\n**2**. Next, load a pretrained [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) model ..."
          ],
          [
           "```\n\n**3**. The [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset card indicates th..."
          ],
          [
           "```\n\n**4**. Create a function to preprocess the audio `array` with the feature extractor, and trunca..."
          ],
          [
           "```\n\n**6**. Set the dataset format according to the machine learning framework you're using.\n\n<frame..."
          ],
          [
           "```\n</tf>\n</frameworkcontent>\n\n**7**. Start training with your machine learning framework! Check out..."
          ],
          [
           "```\n\n**2**. Now you can add some data augmentations with any library ([Albumentations](https://album..."
          ],
          [
           "```\n\n**5**. Set the dataset format according to the machine learning framework you're using.\n\n<frame..."
          ],
          [
           "```\n\n```py\n>>> import albumentations\n>>> import numpy as np\n\n>>> transform = albumentations.Compose(..."
          ],
          [
           "```\n\n**2**. Next, load a pretrained [BERT](https://huggingface.co/bert-base-uncased) model and its c..."
          ],
          [
           "```\n\n**3**. Create a function to tokenize the dataset, and you should also truncate and pad the text..."
          ],
          [
           ">>> dataset = dataset.map(encode, batched=True)\n>>> dataset[0]\n{'sentence1': 'Amrozi accused his bro..."
          ],
          [
           "```\n\n**4**. Rename the `label` column to `labels`, which is the expected input name in [BertForSeque..."
          ],
          [
           "```\n</pt>\n<tf>\n\nUse the [`~transformers.TFPreTrainedModel.prepare_tf_dataset`] method from ðŸ¤— Transfo..."
          ],
          [
           "Process text data\n\nThis guide shows specific methods for processing text datasets. Learn how to:\n\n- ..."
          ],
          [
           "```\n\nSet the `batched` parameter to `True` in the [`~Dataset.map`] function to apply the tokenizer t..."
          ],
          [
           "```\n\nThe [`~Dataset.map`] function converts the returned values to a PyArrow-supported format. But e..."
          ],
          [
           "Security Policy\n\n## Supported Versions\n<!--\nUse this section to tell people about which versions of ..."
          ],
          [
           "Process audio data\n\nThis guide shows specific methods for processing audio datasets. Learn how to:\n\n..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img class=\"block dark:hidden\" src=\"https://huggingface.c..."
          ],
          [
           "```\n\n- For fine-tuned speech recognition models, you only need to load a `processor`:\n\n    ```py\n   ..."
          ],
          [
           "Task templates\n\n<Tip warning={true}>\n\nThe Task API is deprecated in favor of [`train-eval-index`](ht..."
          ],
          [
           "Object detection\n\nObject detection models identify something in an image, and object detection datas..."
          ],
          [
           "```\n\nThe dataset has the following fields:\n\n- `image`: PIL.Image.Image object containing the image.\n..."
          ],
          [
           ">>> categories = ds['train'].features['objects'].feature['category']\n\n>>> boxes_xywh = torch.tensor(..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nNow when you visualize the result, the image should be flipped, but the `bboxes` should still b..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nYou can verify the transform works by visualizing the 10th example:\n\n```py\n>>> example = ds['tr..."
          ],
          [
           "Load\n\nYour data can be stored in various places; they can be on your local machine's disk, in a Gith..."
          ],
          [
           "```\n\nSome datasets may have more than one version based on Git tags, branches, or commits. Use the `..."
          ],
          [
           "```\n\nThe `split` parameter can also map a data file to a specific split:\n\n```py\n>>> data_files = {\"v..."
          ],
          [
           "```\n\n## Local and remote files\n\nDatasets can be loaded from local files stored on your computer and ..."
          ],
          [
           "```\n\nAnother JSON format you may encounter is a nested field, in which case you'll need to specify t..."
          ],
          [
           "```\n\nTo load remote Parquet files via HTTP, pass the URLs instead:\n\n```py\n>>> base_url = \"https://st..."
          ],
          [
           "```\n\nUnlike [`load_dataset`], [`Dataset.from_file`] memory maps the Arrow file without preparing the..."
          ],
          [
           "```\n\nTo load remote WebDatasets via HTTP, pass the URLs instead:\n\n```python\n>>> from datasets import..."
          ],
          [
           "```\n\n### Python list of dictionaries\n\nLoad a list of Python dictionaries with [`~Dataset.from_list`]..."
          ],
          [
           "```\n\n### Pandas DataFrame\n\nLoad Pandas DataFrames with [`~Dataset.from_pandas`]:\n\n```py\n>>> from dat..."
          ],
          [
           "```\n\nSelect specific rows of the `train` split:\n\n```py\n>>> train_10_20_ds = datasets.load_dataset(\"b..."
          ],
          [
           "```\n\nFinally, you can even create cross-validated splits. The example below creates 10-fold cross-va..."
          ],
          [
           "```\n\n### Percent slicing and rounding\n\nThe default behavior is to round the boundaries to the neares..."
          ],
          [
           "```\n\nIf you want equal sized splits, use `pct1_dropremainder` rounding instead. This treats the spec..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\n`pct1_dropremainder` rounding may truncate the last examples in a dataset..."
          ],
          [
           "For example, if you try to download a configuration from the [MATINF](https://huggingface.co/dataset..."
          ],
          [
           "```\n\nIf you've already downloaded a dataset from the *Hub with a loading script* to your computer, t..."
          ],
          [
           "```\n\n## Metrics\n\n<Tip warning={true}>\n\nMetrics is deprecated in ðŸ¤— Datasets. To learn more about how ..."
          ],
          [
           "```\n\n### Distributed setup\n\nWhen working in a distributed or parallel processing environment, loadin..."
          ],
          [
           "Metric Card for SuperGLUE\n\n## Metric description\nThis metric is used to compute the SuperGLUE evalua..."
          ],
          [
           "Format of `references`:\n- for `record`: list of question-answers dictionaries with the following key..."
          ],
          [
           "```\n## Output values\n\nThe output of the metric depends on the SuperGLUE subset chosen, consisting of..."
          ],
          [
           "```\n\nMinimal values for the MultiRC subset (which outputs `pearson` and `spearmanr`):\n\n```python\nfro..."
          ],
          [
           "Stream\n\nDataset streaming lets you work with a dataset without downloading it.\nThe data is streamed ..."
          ],
          [
           "```\n\nDataset streaming also lets you work with a dataset made of local files without doing any conve..."
          ],
          [
           "```\n\nLoading a dataset in streaming mode creates a new dataset type instance (instead of the classic..."
          ],
          [
           "```\n\nThe [`~Dataset.to_iterable_dataset`] function supports sharding when the [`IterableDataset`] is..."
          ],
          [
           "```\n\n<Tip>\n\n[`IterableDataset.shuffle`] will also shuffle the order of the shards if the dataset is ..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\n`take` and `skip` prevent future calls to `shuffle` because they lock in ..."
          ],
          [
           "```\n\nAround 80% of the final dataset is made of the `en_dataset`, and 20% of the `fr_dataset`.\n\nYou ..."
          ],
          [
           "```\n\n### Remove\n\nWhen you need to remove one or more columns, give [`IterableDataset.remove_columns`..."
          ],
          [
           "```\n\n### Cast\n\n[`IterableDataset.cast`] changes the feature type of one or more columns. This method..."
          ],
          [
           "```\n\n<Tip>\n\nCasting only works if the original feature type and new feature type are compatible. For..."
          ],
          [
           "```\n\nNext, apply this function to the dataset with [`IterableDataset.map`]:\n\n```py\n>>> from datasets..."
          ],
          [
           "```\n\n### Batch processing\n\n[`IterableDataset.map`] also supports working with batches of examples. O..."
          ],
          [
           "```\n\n<Tip>\n\nSee other examples of batch processing in the [batched map processing](./process#batch-p..."
          ],
          [
           "```\n\nLastly, create a simple training loop and start training:\n\n```py\n>>> import torch\n>>> from torc..."
          ],
          [
           "Depth estimation\n\nDepth estimation datasets are used to train a model to approximate the relative di..."
          ],
          [
           "```\n\nThe dataset has two fields:\n\n* `image`: a PIL PNG image object with `uint8` data type.\n* `depth..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "...    d_min = np.min(depth_map)\n...    d_max = np.max(depth_map)\n...    depth_map = colored_depthma..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nWith `additional_targets` defined, you can pass the target depth maps to the `depth` argument o..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "Using Datasets with TensorFlow\n\nThis document is a quick introduction to using `datasets` with Tenso..."
          ],
          [
           "```\n\n## N-dimensional arrays\n\nIf your dataset consists of N-dimensional arrays, you will see that by..."
          ],
          [
           "```\n\n\n## Other feature types\n\n[`ClassLabel`] data are properly converted to tensors:\n\n```py\n>>> from..."
          ],
          [
           "```\n\nString and binary objects are unchanged, since PyTorch only supports numbers.\n\nThe [`Image`] an..."
          ],
          [
           "```\n\n<Tip>\n\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\n`pip inst..."
          ],
          [
           "```\n\n## Data loading\n\nAlthough you can load individual samples and batches just by indexing into you..."
          ],
          [
           "Since the entire data preprocessing pipeline can be compiled in a `tf.data.Dataset`, this approach a..."
          ],
          [
           "```\n\nThe returned `tf_ds` object here is now fully ready to train on, and can be passed directly to ..."
          ],
          [
           "```\n\nFor a full description of the arguments, please see the [`~Dataset.to_tf_dataset`] documentatio..."
          ],
          [
           "The key thing to recognize is that when you convert the whole dataset to `Tensor`s, it is static and..."
          ],
          [
           "Metric Card for Mahalanobis Distance\n\n## Metric Description\nMahalonobis distance is the distance bet..."
          ],
          [
           "```\n\n## Limitations and Bias\n\nThe Mahalanobis distance is only able to capture linear relationships ..."
          ],
          [
           "Troubleshooting\n\nThis guide aims to provide you the tools and knowledge required to navigate some co..."
          ],
          [
           "```\n\nIf you encounter this issue, you need to upgrade the `datasets` library to the latest version (..."
          ],
          [
           "```\n\nThis error can also occur when using a generator function that uses a global object that is not..."
          ],
          [
           "If you are facing issues creating a custom dataset with a script on Hub, you can ask the Hugging Fac..."
          ],
          [
           "```\n\n### GitHub Issues\n\nFinally, if you suspect to have found a bug related to the library itself, c..."
          ],
          [
           "Process image data\n\nThis guide shows specific methods for processing image datasets. Learn how to:\n\n..."
          ],
          [
           "```\n\nThe cache file saves time because you don't have to execute the same transform twice. The [`~Da..."
          ],
          [
           "For example, if you'd like to change the color properties of an image randomly:\n\n```py\n>>> from torc..."
          ],
          [
           "```\n\nCreate a function to apply the `ColorJitter` transform:\n\n```py\n>>> def transforms(examples):\n....."
          ],
          [
           "Builder classes\n\n## Builders\n\nðŸ¤— Datasets relies on two main classes during the dataset building proc..."
          ],
          [
           "Metrics\n\n<Tip warning={true}>\n\nMetrics is deprecated in ðŸ¤— Datasets. To learn more about how to use m..."
          ],
          [
           "```\n\n<Tip>\n\nMetrics accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc...."
          ],
          [
           "```\n\n<a id='metric_script'></a>\n\n## Custom metric loading script\n\nWrite a metric loading script to u..."
          ],
          [
           "After you've filled out all these fields in the template, it should look like the following example ..."
          ],
          [
           "```\n\n### Download metric files\n\nIf your metric needs to download, or retrieve local files, you will ..."
          ],
          [
           "```\n\n<Tip>\n\nIf the files are stored locally, provide a dictionary of path(s) instead of URLs.\n\n</Tip..."
          ],
          [
           "```\n\n### Compute score\n\n[`DatasetBuilder._compute`] provides the actual instructions for how to comp..."
          ],
          [
           "```\n\n2. Create [`DatasetBuilder._compute`] with instructions for what metric to calculate for each c..."
          ],
          [
           "Metric Card for MAUVE\n\n## Metric description\n\nMAUVE is a library built on PyTorch and HuggingFace Tr..."
          ],
          [
           "```\n\nIt also has several optional arguments:\n\n`num_buckets`: the size of the histogram to quantize P..."
          ],
          [
           "`seed`: random seed to initialize k-means cluster assignments, randomly assigned by default.\n    \n\n\n..."
          ],
          [
           "```\n\nPartial match between prediction and reference:\n\n```python\nfrom datasets import load_metric\nmau..."
          ],
          [
           "```\n\n## Further References \n- [Official MAUVE implementation](https://github.com/krishnap25/mauve)\n-..."
          ],
          [
           "Create a dataset loading script\n\n\n<Tip>\n\nThe dataset loading script is likely not needed if your dat..."
          ],
          [
           "```\n\n```py\n>>> from datasets import load_dataset\n>>> load_dataset(\"path/to/my_dataset\")\n```\n\nThe fol..."
          ],
          [
           "```\n\n3. `DatasetInfo.homepage` contains the URL to the dataset homepage so users can find more detai..."
          ],
          [
           "```\n\n### Multiple configurations\n\nIn some cases, your dataset may have multiple configurations. For ..."
          ],
          [
           "Args:\n        features: *list[string]*, list of the features that will appear in the\n            fea..."
          ],
          [
           "```\n\n2. Create instances of your config to specify the values of the attributes of each configuratio..."
          ],
          [
           "```\n\nAdditionally, users can instantiate a custom builder configuration by passing the builder confi..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nOnly use a default configuration when it makes sense. Don't set one becau..."
          ],
          [
           "```\n\n<Tip>\n\nIf the data files live in the same folder or repository of the dataset script, you can j..."
          ],
          [
           "```\n\n## Generate samples\n\nAt this point, you have:\n\n- Added the dataset attributes.\n- Provided instr..."
          ],
          [
           "```\n\n## (Optional) Generate dataset metadata\n\nAdding dataset metadata is a great way to include info..."
          ],
          [
           "```\n\n## Advanced features\n\n### Sharding\n\nIf your dataset is made of many big files, ðŸ¤— Datasets autom..."
          ],
          [
           "```\n\nTo yield Arrow tables instead of single examples, make your dataset builder inherit from [`Arro..."
          ],
          [
           "Metric Card for Competition MATH\n\n## Metric description\n\nThis metric is used to assess performance o..."
          ],
          [
           "```\n\nN.B. To be able to use Competition MATH, you need to install the `math_equivalence` dependency ..."
          ],
          [
           "```\n\nPartial match:\n\n```python\n>>> from datasets import load_metric\n>>> math = load_metric(\"competit..."
          ],
          [
           "Metric Card for SARI\n\n\n## Metric description\nSARI (***s**ystem output **a**gainst **r**eferences and..."
          ],
          [
           "```\n## Output values\n\nThis metric outputs a dictionary with the SARI score:\n\n```\nprint(sari_score)\n{..."
          ],
          [
           "```\n\nPartial match between prediction and reference:\n\n```python\nfrom datasets import load_metric\nsar..."
          ],
          [
           "Metric Card for Mean IoU \n\n\n## Metric Description\n\nIoU (Intersection over Union) is the area of over..."
          ],
          [
           "```python\n>>> from datasets import load_metric\n>>> import numpy as np\n>>> mean_iou = load_metric(\"me..."
          ],
          [
           "Metric Card for ROC AUC\n\n\n## Metric Description\nThis metric computes the area under the curve (AUC) ..."
          ],
          [
           "```\n\nThe default implementation of this metric is the **binary** implementation. If employing the **..."
          ],
          [
           "```\n\nSee the [Examples Section Below](#examples_section) for more extensive examples...."
          ],
          [
           "### Inputs\n- **`references`** (array-like of shape (n_samples,) or (n_samples, n_classes)): Ground t..."
          ],
          [
           "- `None`:  No average is calculated, and scores for each class are returned. Only works with the mul..."
          ],
          [
           "### Output Values\nThis metric returns a dict containing the `roc_auc` score. The score is a `float`,..."
          ],
          [
           "```\n\nIn contrast, though, the output takes the following format in the multilabel case when `average..."
          ],
          [
           "```\n\nExample 3, the **multilabel** use case:\n```python\n>>> roc_auc_score = datasets.load_metric(\"roc..."
          ],
          [
           "```\n\n```bibtex\n@article{10.1023/A:1010920819831,\nauthor = {Hand, David J. and Till, Robert J.},\ntitl..."
          ],
          [
           "```\n\n## Further References\nThis implementation is a wrapper around the [Scikit-learn implementation]..."
          ],
          [
           "Use with JAX\n\nThis document is a quick introduction to using `datasets` with JAX, with a particular ..."
          ],
          [
           "```\n\n<Tip>\n\nA [`Dataset`] object is a wrapper of an Arrow table, which allows fast reads from arrays..."
          ],
          [
           "```\n\nAnother thing you'll need to take into consideration is that the formatting is not applied\nunti..."
          ],
          [
           "```\n\nNote that if the `device` argument is not provided to `with_format` then it will use the defaul..."
          ],
          [
           "```\n\nString and binary objects are unchanged, since JAX only supports numbers.\n\nThe [`Image`] and [`..."
          ],
          [
           "```\n\n<Tip>\n\nTo use the [`Audio`] feature type, you'll need to install the `audio` extra as\n`pip inst..."
          ],
          [
           "```\n\n## Data loading\n\nJAX doesn't have any built-in data loading capabilities, so you'll need to use..."
          ],
          [
           "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"mnist\")\n>>> ds = ds.with_format(\"..."
          ],
          [
           "```\n\nOnce the format is set we can feed the dataset to the JAX model in batches using the `Dataset.i..."
          ],
          [
           "Build and load\n\nNearly every deep learning workflow begins with loading a dataset, which makes it on..."
          ],
          [
           "* [`datasets.packaged_modules.text.Text`] for text\n* [`datasets.packaged_modules.csv.Csv`] for CSV a..."
          ],
          [
           "<div class=\"flex justify-center\">\n   <img src=\"https://huggingface.co/datasets/huggingface/documenta..."
          ],
          [
           "You can also set the [`DatasetBuilder.BUILDER_CONFIG_CLASS`] to any custom subclass of [`BuilderConf..."
          ],
          [
           "Once the files are downloaded, [`SplitGenerator`] organizes them into splits. The [`SplitGenerator`]..."
          ],
          [
           "If the dataset doesn't pass the verifications, it is likely that the original host of the dataset ma..."
          ],
          [
           "Metric Card for Accuracy\n\n\n## Metric Description\n\nAccuracy is the proportion of correct predictions ..."
          ],
          [
           "```\n\nThis metric outputs a dictionary, containing the accuracy score.\n\n\n#### Values from Popular Pap..."
          ],
          [
           "```\n\n\n## Limitations and Bias\nThis metric can be easily misleading, especially in the case of unbala..."
          ],
          [
           "Metric Card for CER\n\n## Metric description\n\nCharacter error rate (CER) is a common metric of the per..."
          ],
          [
           "```\n## Output values\n\nThis metric outputs a float representing the character error rate.\n\n```\nprint(..."
          ],
          [
           "```\n\nNo match between prediction and reference:\n\n```python\nfrom datasets import load_metric\ncer = lo..."
          ],
          [
           "Metric Card for XTREME-S\n\n\n## Metric Description\n\nThe XTREME-S metric aims to evaluate model perform..."
          ],
          [
           "- `cer`:  Character error rate (CER) is similar to WER, but operates on character instead of word. T..."
          ],
          [
           "```\n\nFor the `covost2` subset (which outputs `bleu`):\n\n```python\n>>> from datasets import load_metri..."
          ],
          [
           "```\n \nFor the `minds14` subset (which outputs `f1` and `accuracy`):\n\n```python\n>>> from datasets imp..."
          ],
          [
           "```\n    \n## Further References \n\n- [XTREME-S dataset](https://huggingface.co/datasets/google/xtreme_..."
          ],
          [
           "Metric Card for WER\n\n## Metric description\nWord error rate (WER) is a common metric of the performan..."
          ],
          [
           "```\n## Output values\n\nThis metric outputs a float representing the word error rate.\n\n```\nprint(wer_s..."
          ],
          [
           "```\n\nNo match between prediction and reference:\n\n```python\nfrom datasets import load_metric\nwer = lo..."
          ],
          [
           "Share a dataset to the Hub\n\nThe [Hub](https://huggingface.co/datasets) is home to an extensive colle..."
          ],
          [
           "Text file extensions are not tracked by Git LFS by default, and if they're greater than 10MB, they w..."
          ],
          [
           "1. Click on **Create Dataset Card** to create a Dataset card. This button creates a `README.md` file..."
          ],
          [
           "Once your dataset is stored on the Hub, anyone can load it with the [`load_dataset`] function:\n\n```p..."
          ],
          [
           "```\n\n## Upload with Python\n\nUsers who prefer to upload a dataset programmatically can use the [huggi..."
          ],
          [
           "```\n\nTo add a new configuration (or subset) to a dataset or to add a new split (train/validation/tes..."
          ],
          [
           "Metric Card for SQuAD\n\n## Metric description\nThis metric wraps the official scoring script for versi..."
          ],
          [
           "```\n{'exact_match': 100.0, 'f1': 100.0}\n```\n\nThe range of `exact_match` is 0-100, where 0.0 means no..."
          ],
          [
           "```\n\nMinimal values for both exact match and F1 (no match):\n\n```python\nfrom datasets import load_met..."
          ],
          [
           "```\n\nPartial match (2 out of 3 answers correct) : \n\n```python\nfrom datasets import load_metric\nsquad..."
          ],
          [
           "Loading methods\n\nMethods for listing and loading datasets and metrics:\n\n## Datasets\n\n[[autodoc]] dat..."
          ],
          [
           "```\n\n### Text\n\n[[autodoc]] datasets.packaged_modules.text.TextConfig\n\n[[autodoc]] datasets.packaged_..."
          ],
          [
           "Metric Card for CUAD\n\n## Metric description\n\nThis metric wraps the official scoring script for versi..."
          ],
          [
           "Note that `answer_start` values are not taken into account to compute the metric.\n\n```python\nfrom da..."
          ],
          [
           "```\n## Output values\n\nThe output of the CUAD metric consists of a dictionary that contains one or se..."
          ],
          [
           "`prec_at_90_recall`: The fraction of true examples among the predicted examples at a recall rate of ..."
          ],
          [
           "```\n\nMinimal values:\n\n```python\nfrom datasets import load_metric\ncuad_metric = load_metric(\"cuad\")\np..."
          ],
          [
           "```\n\nPartial match: \n\n```python\nfrom datasets import load_metric\ncuad_metric = load_metric(\"cuad\")\np..."
          ],
          [
           "```\n\n## Limitations and bias\nThis metric works only with datasets that have the same format as the [..."
          ],
          [
           "Datasets ðŸ¤ Arrow\n\n## What is Arrow?\n\n[Arrow](https://arrow.apache.org/) enables large amounts of dat..."
          ],
          [
           "```\n\nThis is possible because the Arrow data is actually memory-mapped from disk, and not loaded in ..."
          ],
          [
           "Metric Card for IndicGLUE\n\n## Metric description\nThis metric is used to compute the evaluation metri..."
          ],
          [
           "```\n    \n## Output values\n\nThe output of the metric depends on the IndicGLUE subset chosen, consisti..."
          ],
          [
           "```\n\nMinimal values for the Wiki-NER subset (which outputs `accuracy` and `f1`):\n\n```python\n>>> from..."
          ],
          [
           "```\n    \n## Further References \n- [IndicNLP website](https://indicnlp.ai4bharat.org/home/)\n-..."
          ],
          [
           "Structure your repository\n\nTo host and share your dataset, create a dataset repository on the Huggin..."
          ],
          [
           "```\n\n\nYou can select multiple files per split using a list of paths:\n\n```\nmy_dataset_repository/\nâ”œâ”€â”€..."
          ],
          [
           "```\n\n## Builder parameters\n\nNot only `data_files`, but other builder-specific parameters can be pass..."
          ],
          [
           "```\n\n### Filename splits\n\nIf you don't have any non-traditional splits, then you can place the split..."
          ],
          [
           "```\n\n### Single split\n\nWhen ðŸ¤— Datasets can't find any of the above patterns, then it'll treat all th..."
          ],
          [
           "```\n\nFor more flexibility over how to load and generate a dataset, you can also write a [dataset loa..."
          ],
          [
           "Load a dataset from the Hub\n\nFinding high-quality datasets that are reproducible and accessible can ..."
          ],
          [
           "```\n\nIf you're happy with the dataset, then load it with [`load_dataset`]:\n\n```py\n>>> from datasets ..."
          ],
          [
           "```\n\n## Configurations\n\nSome datasets contain several sub-datasets. For example, the [MInDS-14](http..."
          ],
          [
           "```\n\n## Remote code\n\nCertain datasets repositories contain a loading script with the Python code use..."
          ],
          [
           "Metric Card for Code Eval\n\n## Metric description\n\nThe CodeEval metric estimates the pass@k metric fo..."
          ],
          [
           "```\n\n## Output values\n\nThe Code Eval metric outputs two things:\n\n`pass_at_k`: a dictionary with the ..."
          ],
          [
           "```\n\nPartial match at k=1, full match at k=2:\n\n```python\nfrom datasets import load_metric\ncode_eval ..."
          ],
          [
           "Create a dataset\n\nSometimes, you may need to create a dataset if you're working with your own data. ..."
          ],
          [
           "* [`ImageFolder`] uses the [`~datasets.Image`] feature to decode an image file. Many image extension..."
          ],
          [
           "```\npokemon/train/grass/bulbasaur.png\npokemon/train/fire/charmander.png\npokemon/train/water/squirtle..."
          ],
          [
           "```\n\nTo learn more about each of these folder-based builders, check out the and <a href=\"https://hug..."
          ],
          [
           "```\n\n    A generator-based [`IterableDataset`] needs to be iterated over with a `for` loop for examp..."
          ],
          [
           "```\n\n## Next steps\n\nWe didn't mention this in the tutorial, but you can also create a dataset with a..."
          ],
          [
           "Create an audio dataset\n\nYou can share a dataset with your team or with anyone in the community by c..."
          ],
          [
           "```\n\nThen upload the dataset to the Hugging Face Hub using [`Dataset.push_to_hub`]:\n\n```py\naudio_dat..."
          ],
          [
           "```\nmy_dataset/\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ metadata.csv\nâ””â”€â”€ data/\n```\n\nThe `data` folder can be any name you ..."
          ],
          [
           "```\nmetadata.csv\ndata/first_audio_file.mp3\ndata/second_audio_file.mp3\ndata/third_audio_file.mp3\n\n```..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\n    Note that if audio files are located not right next to a metadata fil..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nIf all audio files are contained in a single directory or if they are not..."
          ],
          [
           "```\n\nThis guide will show you how to create a dataset loading script for audio datasets, which is a ..."
          ],
          [
           "```\n\nIn addition to learning how to create a streamable dataset, you'll also learn how to:\n\n* Create..."
          ],
          [
           "```py\nclass VivosDataset(datasets.GeneratorBasedBuilder):\n    \"\"\"VIVOS is a free Vietnamese speech c..."
          ],
          [
           "```\n\n#### Multiple configurations\n\nIn some cases, a dataset may have more than one configuration. Fo..."
          ],
          [
           "```\n\nDefine your configurations in the `BUILDER_CONFIGS` class variable inside [`GeneratorBasedBuild..."
          ],
          [
           "```\n\n### Add dataset metadata\n\nAdding information about your dataset helps users to learn more about..."
          ],
          [
           "```\n\n### Download and define the dataset splits\n\nNow that you've added some information about your d..."
          ],
          [
           "return [\n        datasets.SplitGenerator(\n            name=datasets.Split.TRAIN,\n            gen_kwa..."
          ],
          [
           "```\n\n\n<Tip warning={true}>\n\nThis implementation does not extract downloaded archives. If you want to..."
          ],
          [
           "```\n\nFinally, iterate over files in `audio_files` and yield them along with their corresponding meta..."
          ],
          [
           "```\n\n### Upload the dataset to the Hub\n\nOnce your script is ready, [create a dataset card](./dataset..."
          ],
          [
           "```\n\n3. Use the [`~DownloadManager.iter_archive`] method to iterate over the archive at `audio_path`..."
          ],
          [
           "In the `gen_kwargs` parameter, specify the file paths to `local_extracted_archive`, `audio_files`, `..."
          ],
          [
           "```\n\n#### Generate the dataset\n\nHere `_generate_examples` accepts `local_extracted_archive`, `audio_..."
          ],
          [
           "```\n\nPut both of these steps together, and the whole `_generate_examples` method should look like:\n\n..."
          ],
          [
           "Share a dataset using the CLI\n\nAt Hugging Face, we are on a mission to democratize good Machine Lear..."
          ],
          [
           "For more information on how to load a dataset from the Hub, take a look at the [load a dataset from ..."
          ],
          [
           "```\nhuggingface-cli login\n```\n\n2. Login using your Hugging Face Hub credentials, and create a new da..."
          ],
          [
           "```\ncp /somewhere/data/*.json .\ngit lfs track *.json\ngit add .gitattributes\ngit add *.json\ngit commi..."
          ],
          [
           "Metric Card for Spearman Correlation Coefficient Metric (spearmanr)\n\n\n## Metric Description\nThe Spea..."
          ],
          [
           "```\n\n### Inputs\n- **`predictions`** (`list` of `float`): Predicted labels, as returned by a model.\n-..."
          ],
          [
           "```\n\nThe same example, but that also returns the pvalue:\n```python\n>>> spearmanr_metric = datasets.l..."
          ],
          [
           "Datasets\n\n<img class=\"float-left !m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[150px]\" src..."
          ],
          [
           "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./a..."
          ],
          [
           "Load text data\n\nThis guide shows you how to load text datasets. To learn how to load any type of dat..."
          ],
          [
           "```\n\nTo load remote text files via HTTP, pass the URLs instead:\n\n```py\n>>> dataset = load_dataset(\"t..."
          ],
          [
           "Metric Card for TER\n\n## Metric Description\nTER (Translation Edit Rate, also called Translation Error..."
          ],
          [
           "```\n\nThe metric can take on any value `0` and above. `0` is a perfect score, meaning the predictions..."
          ],
          [
           "```\n\nExample ignoring punctuation and capitalization, and everything matches:\n```python\n>>> predicti..."
          ],
          [
           "Metric Card for MAE\n\n\n## Metric Description\n\nMean Absolute Error (MAE) is the mean of the magnitude ..."
          ],
          [
           "```\n\n### Inputs\n\nMandatory inputs: \n- `predictions`: numeric array-like of shape (`n_samples,`) or (..."
          ],
          [
           "```\n\nIf `multioutput=\"raw_values\"`:\n```python\n{'mae': array([0.5, 1. ])}\n```\n\n#### Values from Popul..."
          ],
          [
           "--\nYAML tags (full spec here: https://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain..."
          ],
          [
           "## Dataset Description\n\n- **Homepage:** [Add homepage URL here if available (unless it's a GitHub re..."
          ],
          [
           "- `task-category-tag`: The dataset can be used to train a model for [TASK NAME], which consists in [..."
          ],
          [
           "```\n{\n  'example_field': ...,\n  ...\n}..."
          ],
          [
           "```\n\nProvide any additional information that is not covered in the other sections about the data her..."
          ],
          [
           "### Curation Rationale\n\nWhat need motivated the creation of this dataset? What are some of the reaso..."
          ],
          [
           "### Annotations\n\nIf the dataset contains annotations which are not part of the initial data collecti..."
          ],
          [
           "State whether the dataset contains other data that might be considered sensitive (e.g., data that re..."
          ],
          [
           "If studies of the datasets have outlined other limitations of the dataset, such as annotation artifa..."
          ],
          [
           "```\n@article{article_id,\n  author    = {Author List},\n  title     = {Dataset Paper Title},\n  journal..."
          ],
          [
           "Know your dataset\n\nThere are two types of dataset objects, a regular [`Dataset`] and then an âœ¨ [`Ite..."
          ],
          [
           "```\n\nUse the `-` operator to start from the end of the dataset:\n\n```py\n# Get the last row in the dat..."
          ],
          [
           "```\n\nBut it is important to remember that indexing order matters, especially when working with large..."
          ],
          [
           "```\n\n### Slicing\n\nSlicing returns a slice - or subset - of the dataset, which is useful for viewing ..."
          ],
          [
           "```\n\nYou can also create an [`IterableDataset`] from an *existing* [`Dataset`], but it is faster tha..."
          ],
          [
           "```\n\nYou can return a subset of the dataset with a specific number of examples in it with [`Iterable..."
          ],
          [
           "Metric Card for XNLI\n\n## Metric description\n\nThe XNLI metric allows to evaluate a model's score on t..."
          ],
          [
           "```\n\n## Output values\n\nThe output of the XNLI metric is simply the `accuracy`, i.e. the proportion o..."
          ],
          [
           "Evaluate predictions\n\n<Tip warning={true}>\n\nMetrics is deprecated in ðŸ¤— Datasets. To learn more about..."
          ],
          [
           "```\n\nThis will load the metric associated with the MRPC dataset from the GLUE benchmark.\n\n## Select ..."
          ],
          [
           "```\n\n## Metrics object\n\nBefore you begin using a [`Metric`] object, you should get to know it a litt..."
          ],
          [
           "```\n\nNotice for the MRPC configuration, the metric expects the input format to be zero or one. For a..."
          ],
          [
           "Batch mapping\n\nCombining the utility of [`Dataset.map`] with batch mode is very powerful. It allows ..."
          ],
          [
           "For example, from a dataset of 1 column and 3 rows, if you use `map` to return a new column with twi..."
          ],
          [
           "```\n\nTo make it valid, you have to drop one of the columns:\n\n```py\n>>> from datasets import Dataset\n..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "### Documentation notebooks\n\nYou can open any page of the documentation as a notebook in Colab (ther..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nThen run the following command:\n\n```bash\ndoc-builder preview datasets docs/source/\n```\n\nThe doc..."
          ],
          [
           "```\nand of course if you moved it to another file, then:\n\n```\nSections that were moved:\n\n[ <a href=\"..."
          ],
          [
           "```\n\nUse the relative style to link to the new file so that the versioned docs continue to work.\n\nFo..."
          ],
          [
           "If you want to create a link to some internal class or function, you need to\nprovide its path. For i..."
          ],
          [
           "```\n    Args:\n        n_layers (`int`): The number of layers of the model.\n```\n\nIf the description i..."
          ],
          [
           "```\n```\n# first line of code\n# second line\n# etc\n```\n````\n\n#### Writing a return block\n\nThe return b..."
          ],
          [
           "```\n\n#### Adding an image\n\nDue to the rapidly growing repository, it is important to make sure that ..."
          ],
          [
           "```\n```\n\nThe docstring should give a minimal, clear example of how the respective class or function ..."
          ],
          [
           "Installation\n\nBefore you start, you'll need to setup your environment and install the appropriate pa..."
          ],
          [
           "```\n\nThis command downloads version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://r..."
          ],
          [
           "```\n\n## Audio\n\nTo work with audio datasets, you need to install the [`Audio`] feature as an extra de..."
          ]
         ],
         "hovertemplate": "source=datasets<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "datasets, circle",
         "marker": {
          "color": "#EF553B",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "datasets, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2.075352,
          2.1891448,
          2.2078252,
          1.3472862,
          1.6177065,
          1.4281456,
          1.8429136,
          1.4662937,
          1.4911803,
          -0.34695974,
          0.09923555,
          -0.1270141,
          -0.1815143,
          -0.053376883,
          2.7810051,
          2.4527261,
          1.8417617,
          -0.41706416,
          0.2827513,
          0.12814325,
          -1.1674749,
          -0.39818045,
          0.007066161,
          -1.0757246,
          -0.35815227,
          -1.8219782,
          -0.10388952,
          -2.9305356,
          -1.5856731,
          -2.1217554,
          -0.35730964,
          -7.312202,
          -7.112248,
          -7.299389,
          -7.2971983,
          1.1030339,
          -9.281845,
          1.2242105,
          -8.786734,
          2.284532,
          3.2188065,
          1.9633727,
          2.1772566,
          2.284443,
          2.4313161,
          -0.51219445,
          0.041143686,
          0.1426777,
          -0.212747,
          -0.166801,
          -0.14362292,
          1.30536,
          0.9857141,
          -0.87919694,
          -7.07582,
          2.614167,
          1.9692267,
          1.9820577,
          -0.8875312,
          -0.417437,
          -0.61661345,
          -1.0920004,
          -2.9434671,
          -0.38042852,
          -0.13234632,
          -7.255076,
          6.7850237,
          6.8689823,
          2.4282212,
          2.0813084,
          -0.526364,
          0.0019186884,
          0.07111697,
          -0.22827291,
          -0.04538201,
          -0.01731276,
          -1.0245414,
          2.6167498,
          2.6678524,
          1.6356251,
          2.963962,
          2.606883,
          2.9025362,
          3.0968013,
          2.149281,
          2.298179,
          1.7899083,
          4.287665,
          6.5121136,
          6.8233786,
          6.8822446,
          2.918859,
          2.762133,
          3.3268933,
          0.88617665,
          2.3299685,
          2.7961605,
          -2.9428568,
          -2.5218985,
          -5.399102,
          4.3366895,
          1.9044968,
          1.0494908,
          1.2235786,
          -1.0004131,
          -0.64561194,
          -0.26707017,
          -0.6201813,
          -0.55554205,
          -1.8436056,
          -2.3188884,
          5.837418,
          -1.0520928,
          -3.3478363,
          -0.8688361,
          -0.6292649,
          -5.0653844,
          -1.2736843,
          -0.5135917,
          -0.1321958,
          -0.19992179,
          -0.18671308,
          -0.5137726,
          -0.62584805,
          -0.509756,
          -0.60914135,
          -0.5866701,
          -1.3187047,
          -1.1949108,
          -0.14502245,
          -0.1821627,
          -1.5444198,
          -2.3154545,
          -2.305274,
          -0.6932534,
          -0.060897026,
          -0.45033288,
          -0.5138979,
          -0.1252556,
          -0.14480752,
          -2.556152,
          1.7871797,
          1.6491476,
          1.8295646,
          0.40677792,
          0.46330622,
          0.4369196,
          1.0282577,
          -7.0945473,
          0.33049095,
          1.2978932,
          1.9241825,
          1.6170843,
          -9.630831,
          1.6185344,
          1.4341912,
          0.98118347,
          1.2470816,
          1.1926897,
          -9.145618,
          -0.56866163,
          -0.9768771,
          -0.38098902,
          -0.85470045,
          -0.8943098,
          -1.0618296,
          -0.83032376,
          3.637023,
          -1.1466762,
          -0.58867276,
          -0.27874368,
          -0.40075168,
          -0.19873044,
          -0.9038722,
          -0.22375631,
          -0.53947335,
          -0.23072435,
          -6.1387663,
          2.6716015,
          2.5503943,
          2.546195,
          2.4698892,
          2.0572798,
          1.76714,
          2.3013234,
          -1.0287664,
          -0.077912755,
          -1.0868,
          -0.8614214,
          -0.17904733,
          -2.2057095,
          5.633239,
          -3.4571004,
          -2.3737726,
          -2.2344086,
          -1.9068205,
          3.4813654,
          3.3235633,
          -1.1917107,
          -0.77873105,
          -3.5867472,
          -0.22075902,
          -0.75674796,
          2.9984226,
          4.443681,
          4.8976207,
          2.7907999,
          2.6887848,
          2.6894152,
          2.7154107,
          2.6751986,
          -1.3629596,
          -0.698194,
          -1.8955439,
          -0.25578076,
          -0.16396911,
          -1.6427909,
          2.601522,
          2.424656,
          2.2008135,
          2.0495028,
          3.4985223,
          2.435606,
          2.5419815,
          2.6629646,
          2.354118,
          2.17485,
          2.6216054,
          2.3682084,
          2.2502255,
          2.8814483,
          3.6465006,
          3.7627158,
          0.4447218,
          1.3031847,
          1.5292225,
          0.8281219,
          1.2912889,
          0.80713755,
          0.8407523,
          1.0338765,
          1.1218773,
          -0.021475643,
          0.66598755,
          0.7799183,
          0.5804399,
          -0.06678653,
          -1.5781788,
          0.89550936,
          -2.3342838,
          1.2455053,
          1.2945176,
          0.7207895,
          0.20865208,
          -0.9146741,
          -7.027353,
          -6.90831,
          2.8348145,
          -1.1157966,
          -0.3653736,
          -0.14782509,
          -0.10180288,
          -1.0857062,
          -2.4148946,
          -0.4534823,
          -2.1308434,
          3.098769,
          -0.43036988,
          -0.07588259,
          -0.06499595,
          2.7587245,
          2.6837842,
          2.731738,
          2.7656782,
          -1.915474,
          -0.9270108,
          0.30959147,
          2.099545,
          3.2560034,
          3.8730266,
          4.4664207,
          3.8286169,
          3.1222758,
          -3.8350112,
          -7.7179227,
          -2.8595138,
          3.3246703,
          -7.346479,
          -1.502467,
          -7.283862,
          -7.2724786,
          0.18081373,
          -7.550935,
          1.0415424,
          0.16725923,
          0.3346433,
          -2.1470642,
          -1.6430283,
          0.78527933,
          -0.37912408,
          -0.570875,
          -1.2616584,
          -0.7750443,
          -0.5348643,
          5.913972,
          -7.173901,
          1.5882435,
          -7.2283325,
          -4.292649,
          -9.719336,
          1.1827557,
          0.6993545,
          1.3297987,
          0.9072595,
          1.1467624,
          1.0704703,
          2.8550787,
          2.9764047,
          2.662458,
          2.7785063,
          3.2371836,
          2.8937168,
          2.4619813,
          2.6002383,
          1.4821867,
          2.501436,
          1.1388906,
          1.0103394,
          1.0184244,
          0.963158,
          1.9922309,
          2.935358,
          2.574333,
          -0.107646234,
          0.6492466,
          -0.96342415,
          -0.17448148,
          -0.42827174,
          -0.094970256,
          2.4708996,
          2.1066074,
          1.907429,
          1.513137,
          1.4170302,
          1.4555779,
          0.7676793,
          0.9432301,
          1.2939066,
          1.3271964,
          1.0651966,
          -1.1565572,
          0.761147,
          -0.09710469,
          -9.636914,
          1.7159685,
          1.5523573,
          1.4139549,
          1.3492758,
          1.47968,
          1.2253287,
          1.5067089,
          -0.2951273,
          -0.04846572,
          0.23318782,
          1.4045079,
          -7.130495,
          -0.46051535,
          -0.35441494,
          -0.6564144,
          -0.46255574,
          -0.5655655,
          -0.57878643,
          -1.0959779,
          5.003748,
          2.4338155,
          2.185075,
          3.539024,
          4.9162793,
          1.006287,
          1.3914337,
          1.2383318,
          0.9846517,
          2.5870588,
          -0.18817586,
          -0.2797425,
          0.23057339,
          1.1043937,
          2.5314307,
          2.2358012,
          -0.13313475,
          -0.065372884,
          -1.0963694,
          0.43695933,
          -1.7949142,
          -0.8767099,
          6.4957385,
          2.899881,
          2.720819,
          2.721268,
          2.6123202,
          1.9544992,
          2.6263463,
          2.6447845,
          2.8131418,
          2.5751586,
          1.7379647,
          2.806392,
          1.9206865,
          1.8510566,
          -0.44371784,
          -0.22126207,
          -0.36010778,
          -1.0845525,
          -0.4305623,
          -0.84070635,
          -0.3330822,
          -0.010696759,
          -0.40918437,
          -0.032040633,
          5.558027,
          0.05790203,
          0.002039798,
          -0.18402341,
          -0.114733614,
          -0.0207222,
          -0.94635856,
          -0.23479597,
          1.5293843,
          1.519802,
          1.849434,
          1.273694,
          1.470825,
          -7.0279875,
          1.4791384,
          1.7688502,
          1.317184,
          2.426492,
          2.8393114,
          2.8644826,
          3.0776877,
          2.3262026,
          3.5732257,
          -0.41615275,
          -0.39845157,
          -1.1219182,
          -0.98926646,
          -0.89955944,
          -0.9475051,
          -1.1399573,
          -1.003705,
          -0.12128971,
          -1.0331401,
          5.797924,
          -0.93672466,
          -0.8616246,
          -0.91835046,
          3.6259553,
          4.1206546,
          3.7177944,
          3.0500395,
          3.5234683,
          3.3347676,
          -0.8200223,
          -0.32196105,
          -0.14489433,
          -0.07349904,
          2.361846,
          2.144893,
          -0.784021,
          -0.083514936,
          -0.54279226,
          -0.7028582,
          -0.0059133912,
          -0.09518444,
          -1.1079295,
          2.1474452,
          2.2892065,
          -1.0576235,
          -0.43659902,
          -1.1054012,
          5.8549542,
          3.0925038,
          2.7877765,
          2.7477686,
          2.4695158,
          2.2376676,
          2.6529133,
          2.5001142,
          1.8782424,
          2.6137102,
          3.0722184,
          -0.17566267,
          -0.21647005,
          -0.17352718,
          2.6961927,
          2.5292034,
          2.5669951,
          2.722802,
          1.9317452,
          2.9012942,
          3.1278255,
          3.1621983,
          2.7819388,
          2.7521799,
          2.7448447,
          2.8473837,
          2.720769,
          2.779839,
          -6.695185,
          2.497753,
          2.5896797,
          2.816478,
          2.6869667,
          2.629107,
          2.7681172,
          2.7709677,
          2.7597923,
          2.8019576,
          2.705302,
          2.7951343,
          2.5253205,
          3.4448545,
          3.6032996,
          3.964516,
          3.9019523,
          -0.462143,
          -0.16861406,
          -0.22769262,
          3.0932984,
          7.323357,
          4.831486,
          2.6636474,
          3.1919787,
          -1.2178422,
          -0.6027877,
          -0.56307673,
          -0.48088503,
          0.2717883,
          0.050627314,
          3.5488005,
          2.9013786,
          1.7925196,
          4.1387844,
          1.8109602,
          3.1610756,
          3.2136333,
          2.4668686,
          3.6616845,
          5.186838,
          2.1367385,
          1.2057016,
          1.1676947,
          1.6465424,
          2.0967557,
          1.5790073,
          -0.86455166,
          -0.31895223,
          -0.2564769,
          0.03406142,
          -0.1895059,
          -0.18352193,
          -0.7931148,
          1.219153,
          1.0142448,
          6.090229,
          4.4187927,
          5.20098,
          5.789858,
          6.0443435,
          5.4406915,
          1.5961108,
          0.36799908,
          -1.9412857,
          4.482832,
          2.0557754,
          2.0215487,
          -0.8548147,
          2.7283578
         ],
         "xaxis": "x",
         "y": [
          -4.785888,
          -4.7877326,
          -4.7756653,
          -4.8367767,
          -5.2273993,
          -4.9015217,
          -4.8638096,
          -5.047729,
          -4.8554163,
          -7.630472,
          -6.8748045,
          -7.0638194,
          -7.6203055,
          -7.503405,
          -4.0842237,
          -4.1578445,
          -4.4117174,
          -7.432666,
          -7.3537593,
          -7.6364045,
          -7.0061502,
          -6.41214,
          -6.166342,
          -7.7198167,
          -7.4517903,
          -7.1616225,
          -3.8246944,
          -3.9949007,
          -4.6840672,
          -4.3823695,
          -2.789743,
          -7.102645,
          -7.6202006,
          -7.290854,
          -7.4396634,
          -3.182558,
          -0.15999871,
          -2.8926723,
          -0.06999052,
          -4.249147,
          -2.970467,
          -3.7637153,
          -4.068395,
          -4.039536,
          -4.1294456,
          -7.5753703,
          -6.9288254,
          -6.765859,
          -7.116058,
          -7.542437,
          -7.5288925,
          -4.5603886,
          -4.4119177,
          -4.5785913,
          -7.6485023,
          -5.0453253,
          -3.7363138,
          -3.7379513,
          -7.491761,
          -7.6973424,
          -7.5700674,
          -7.759451,
          0.8712201,
          -7.6661253,
          -7.430848,
          0.38038036,
          -1.9942617,
          -1.9407,
          -4.6013165,
          -4.4666038,
          -7.673588,
          -6.888092,
          -6.784206,
          -7.161792,
          -7.555584,
          -7.4668145,
          -6.96313,
          -4.5256596,
          -4.613709,
          -4.5678425,
          -4.1057196,
          -4.621944,
          -4.0591536,
          -3.9712336,
          -4.709249,
          -4.453863,
          -3.8870215,
          -2.9882288,
          -1.6049675,
          -0.9905255,
          -2.107365,
          -3.6246538,
          -3.7576149,
          -3.0073473,
          -2.839516,
          -4.8198504,
          -3.277901,
          0.56018853,
          -6.8174424,
          -3.6693335,
          -3.36166,
          -4.466555,
          -4.099227,
          -3.133654,
          -7.882078,
          -7.826642,
          -6.024052,
          -7.985491,
          -7.979451,
          -6.331549,
          -7.086155,
          -3.518434,
          -7.4695663,
          -1.0271753,
          -7.5336943,
          -7.764396,
          -1.4711919,
          -7.3756285,
          -7.3792095,
          -6.6096644,
          -7.322898,
          -7.547433,
          -7.815301,
          -7.955874,
          -8.031112,
          -8.062241,
          -8.049616,
          -7.4357696,
          -6.7096534,
          -7.9989014,
          -8.194629,
          -7.17531,
          -7.0633736,
          -7.0616403,
          -6.9905453,
          -7.4568443,
          -7.5747485,
          -7.126928,
          -7.8579774,
          -8.193368,
          -4.9396935,
          -1.6105303,
          -1.003469,
          -1.0433016,
          -3.1337085,
          -3.2659311,
          -3.5351512,
          -3.2931995,
          -7.390565,
          -3.0734866,
          -4.3340497,
          -4.718593,
          -4.6574965,
          -0.12773612,
          -2.5071037,
          -2.5780666,
          -2.952536,
          -2.8274024,
          -2.8882408,
          0.12170325,
          -7.5375643,
          -7.7657647,
          -7.714042,
          -8.127884,
          -8.2119055,
          -7.599415,
          -7.6469417,
          -3.0481026,
          -7.7556868,
          -7.71811,
          -7.3625154,
          -6.233792,
          -7.6075234,
          -6.097466,
          -7.5110216,
          -5.78322,
          -7.4556727,
          -1.7973009,
          -4.106062,
          -4.101421,
          -3.548477,
          -4.474908,
          -4.570731,
          -4.3451757,
          -4.2646537,
          -7.654562,
          -7.657705,
          -7.400976,
          -7.418138,
          -7.526275,
          -4.971104,
          -3.4503908,
          -5.153187,
          -7.0048156,
          -7.0127497,
          -7.2738323,
          -3.5699556,
          -3.6303205,
          -7.568504,
          -7.698124,
          -5.1698685,
          -7.222052,
          -8.13209,
          -4.284174,
          -1.5873071,
          -1.9073722,
          -4.1697164,
          -4.3759193,
          -4.514438,
          -4.455426,
          -4.493997,
          -7.561257,
          -7.7491055,
          -7.259472,
          -7.5971084,
          -7.634577,
          -7.343739,
          -3.7971847,
          -3.9834938,
          -4.026223,
          -3.848181,
          -2.8024955,
          -3.9833689,
          -4.0955853,
          -4.35363,
          -4.230061,
          -4.1030946,
          -4.3066106,
          -4.5848618,
          -4.380696,
          -4.1334434,
          -3.0592682,
          -2.8856091,
          -4.267299,
          -4.834279,
          -4.9598136,
          -4.7858076,
          -4.627073,
          -4.678087,
          -4.749193,
          -4.537688,
          -4.55287,
          -4.402202,
          -4.631333,
          -4.632175,
          -3.8766537,
          -4.5669518,
          -4.2959113,
          -4.654758,
          -4.576278,
          -4.3567686,
          -4.7184443,
          -4.2053,
          -3.7378495,
          -3.8440006,
          -7.5473228,
          -7.4805093,
          -4.473309,
          -6.5591645,
          -7.630579,
          -8.15021,
          -8.173099,
          -7.0611825,
          -5.8234396,
          -7.7682476,
          -6.270442,
          -3.5649607,
          -7.923239,
          -7.237557,
          -7.5920777,
          -4.8481617,
          -4.889752,
          -4.754919,
          -4.7833,
          -3.8260043,
          -3.987046,
          -4.0521007,
          -4.2217236,
          -3.516663,
          -2.4155607,
          -0.20726936,
          -2.6429183,
          -3.7860541,
          0.77153146,
          -5.956205,
          -4.2252254,
          -2.9724045,
          -6.472901,
          -0.5249752,
          -7.376703,
          -7.228856,
          -3.2250283,
          -6.3517666,
          -3.0415595,
          -3.1279397,
          -3.448363,
          -1.0741435,
          -4.2838297,
          -4.6024237,
          -3.5603397,
          -2.2856421,
          -4.505793,
          -4.321883,
          -3.938125,
          -1.4275805,
          -7.560483,
          -2.2086856,
          -7.2781625,
          0.6070917,
          -0.5130399,
          -2.962959,
          -2.9286375,
          -2.7119915,
          -2.7999415,
          -2.8829098,
          -2.7574575,
          -4.2717886,
          -4.063545,
          -4.171125,
          -4.447033,
          -4.3157573,
          -4.39479,
          -4.648847,
          -4.4569426,
          -4.665547,
          -4.6107984,
          -4.5611787,
          -4.405808,
          -4.7913427,
          -4.3997602,
          -4.067879,
          -4.024213,
          -4.0850053,
          -6.3567176,
          -5.4322867,
          -7.466576,
          -7.607093,
          -7.663288,
          -7.7988806,
          -4.5833154,
          -4.8942766,
          -4.850482,
          -4.8991976,
          -5.033239,
          -4.833406,
          -4.411586,
          -4.6044407,
          -4.546236,
          -4.67932,
          -4.680626,
          -4.351329,
          -4.469339,
          0.93358874,
          -0.04921969,
          -3.5047774,
          -2.756247,
          -3.2876086,
          -2.755974,
          -2.7254744,
          -2.748404,
          -2.7073503,
          -2.844351,
          -3.318873,
          -3.6464732,
          -3.4217553,
          -7.429982,
          -2.5235384,
          -2.6934018,
          -1.5738822,
          -2.7123284,
          -2.9142587,
          -7.3890653,
          -7.0254545,
          -1.2054889,
          -4.3083425,
          -4.217051,
          -3.167401,
          -2.2903867,
          -3.1505976,
          -4.0581236,
          -2.9119782,
          -3.1174521,
          -4.090294,
          -6.4928102,
          -6.9193945,
          -6.285314,
          -5.4387364,
          -2.8001657,
          -1.5042095,
          -6.750615,
          -7.006256,
          -7.4903636,
          -4.8297014,
          -6.246201,
          -8.046159,
          -2.6341252,
          -4.3940115,
          -4.176345,
          -4.1639285,
          -4.3611856,
          -4.4340253,
          -4.207995,
          -4.2392144,
          -4.16985,
          -4.1814604,
          -4.473688,
          -4.1053314,
          -4.7160935,
          -4.5583897,
          -7.7750497,
          -7.74021,
          -8.056531,
          -7.725009,
          -7.7298713,
          -8.218376,
          -7.4024477,
          -6.846711,
          -7.565204,
          -6.8317685,
          -3.6367137,
          -6.8695,
          -6.849588,
          -7.1730604,
          -7.196754,
          -6.68345,
          -6.9986243,
          -7.0501127,
          -4.175846,
          -4.270255,
          -4.1888905,
          -4.2507024,
          -3.8790243,
          -7.5108995,
          -4.133392,
          -4.1917305,
          -4.0681205,
          -4.206004,
          -4.215853,
          -4.1094174,
          -3.8834689,
          -4.436441,
          -3.6611776,
          -7.482205,
          -7.6259484,
          -6.7216816,
          -8.209409,
          -8.249801,
          -8.2518835,
          -7.7937846,
          -8.320462,
          -7.6602936,
          -8.092638,
          -3.5889437,
          -8.153594,
          -8.161439,
          -8.281763,
          -2.9488227,
          -1.845933,
          -2.9365726,
          -3.9013577,
          -2.5733073,
          -3.5966995,
          -6.798401,
          -8.076093,
          -8.16781,
          -8.306412,
          -4.442913,
          -4.4622684,
          -7.374473,
          -7.822526,
          -7.531227,
          -7.6170077,
          -8.05209,
          -8.091404,
          -7.162309,
          -4.571525,
          -4.6475267,
          -7.750128,
          -7.6244955,
          -8.145182,
          -3.39581,
          -3.786587,
          -4.0743766,
          -4.1123133,
          -4.198322,
          -4.0730467,
          -4.4344306,
          -3.9807665,
          -4.556838,
          -4.4217286,
          -3.9113653,
          -6.449436,
          -7.06172,
          -8.176414,
          -4.0729055,
          -4.438781,
          -4.1449075,
          -4.180855,
          -4.4238753,
          -4.164307,
          -4.0247393,
          -3.8971126,
          -4.7980475,
          -4.8495536,
          -4.8414774,
          -4.715845,
          -4.8075314,
          -4.4700627,
          -6.6654525,
          -4.363008,
          -4.235234,
          -4.6284776,
          -4.7718377,
          -4.607569,
          -4.738953,
          -4.9197464,
          -4.6361585,
          -4.693213,
          -4.7682586,
          -4.841452,
          -4.484981,
          -3.272691,
          -3.0025694,
          -2.0685027,
          -2.2569363,
          -7.719069,
          -7.327362,
          -7.5644283,
          -3.7025208,
          -3.915374,
          -3.8053296,
          -4.5196695,
          -4.1411796,
          -7.8541245,
          -7.901146,
          -7.915131,
          -7.615044,
          -7.3799105,
          -7.7952223,
          -3.279915,
          -3.9966097,
          -4.6138887,
          -3.7112973,
          -4.075459,
          -3.7675,
          -3.843481,
          -4.510253,
          -3.6945276,
          -3.649139,
          -4.787955,
          -5.043517,
          -4.655406,
          -4.5976243,
          -4.7364044,
          -4.149507,
          -7.479238,
          -7.6183143,
          -6.279619,
          -6.4423738,
          -7.103096,
          -6.6568985,
          -4.415832,
          -4.6299086,
          -4.640955,
          0.04650206,
          -1.7180696,
          0.005220892,
          -1.4737363,
          -1.8944958,
          -1.4150058,
          -1.9171401,
          -2.4244072,
          -3.9492283,
          -0.6800154,
          -2.0088603,
          0.36849034,
          -3.9424107,
          -4.6053586
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "Q-Learning Recap [[q-learning-recap]]\n\n\n*Q-Learning* **is the RL algorithm that** :\n\n- Trains a *Q-f..."
          ],
          [
           "This is the Q-Learning pseudocode:\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-co..."
          ],
          [
           "Quiz\n\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lectu..."
          ],
          [
           "### Q2: Which of the following statements are true, when talking about models with bias and/or varia..."
          ],
          [
           "### Q4: How would you describe, with your own words, the Actor-Critic Method (A2C)?\n\n<details>\n<summ..."
          ],
          [
           "</details>\n\nCongrats on finishing this Quiz ðŸ¥³, if you missed some elements, take time to read the ch..."
          ],
          [
           "Additional Readings\n\nThese are **optional readings** if you want to go deeper.\n\n\n## Introduction to ..."
          ],
          [
           "Hands-on\n\nNow that you learned the basics of multi-agents, you're ready to train your first agents i..."
          ],
          [
           "More precisely, AI vs. AI is three tools:\n\n- A *matchmaking process* defining the matches (which mod..."
          ],
          [
           "### Competition Rules\n\nThis first AI vs. AI competition **is an experiment**: the goal is to improve..."
          ],
          [
           "```\n\nTo be able to train our agents correctly and push to the Hub, we need to install ML-Agents\n\n```..."
          ],
          [
           "```\n\nFinally, you need to install git-lfs: https://git-lfs.com/\n\nNow that itâ€™s installed, we need to..."
          ],
          [
           "The goal in this environment **is to get the ball into the opponent's goal while preventing the ball..."
          ],
          [
           "But in our case weâ€™re 2vs2, and each team has 2 agents. How then can we **train cooperative behavior..."
          ],
          [
           "The solution then is to use Self-Play with an MA-POCA trainer (called poca). The poca trainer will h..."
          ],
          [
           "```\n\nCompared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play part. How ..."
          ],
          [
           "```\n\nThe executable contains 8 copies of SoccerTwos.\n\nâš ï¸ Itâ€™s normal if you donâ€™t see a big increase..."
          ],
          [
           "```\n\nThen, we need to run `mlagents-push-to-hf`.\n\nAnd we define four parameters:\n\n1. `-run-id`: the ..."
          ],
          [
           "```\n\nIf everything worked you should see this at the end of the process (but with a different url ðŸ˜†)..."
          ],
          [
           "2. That you have a `SoccerTwos.onnx` file\n\n<img src=\"https://huggingface.co/datasets/huggingface-dee..."
          ],
          [
           "Two types of value-based methods [[two-types-value-based-methods]]\n\nIn value-based methods,Â **we lea..."
          ],
          [
           "And consequently,Â **we don't define by hand the behavior of our policy; it's the training that will ..."
          ],
          [
           "In fact, most of the time, in value-based methods, you'll useÂ **an Epsilon-Greedy Policy**Â that hand..."
          ],
          [
           "The value of taking action \\\\(a\\\\) in state \\\\(s\\\\) under a policy \\\\(Ï€\\\\) is:\n\n<img src=\"https://hu..."
          ],
          [
           "The advantages and disadvantages of policy-gradient methods\n\nAt this point, you might ask, \"but Deep..."
          ],
          [
           "Our vacuum cleaner can only perceive where the walls are.\n\nThe problem is that the **two red (colore..."
          ],
          [
           "But what if we have an infinite possibility of actions?\n\nFor instance, with a self-driving car, at e..."
          ],
          [
           "Glossary \n\nThis is a community-created glossary. Contributions are welcome!\n\n- **Deep Q-Learning:** ..."
          ],
          [
           "Conclusion [[conclusion]]\n\nCongrats on finishing this unit! **That was the biggest one**, and there ..."
          ],
          [
           "Introduction\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/res..."
          ],
          [
           "Introduction to Q-Learning [[introduction-q-learning]]\n\n<img src=\"https://huggingface.co/datasets/hu..."
          ],
          [
           "Conclusion\n\nCongrats on finishing this unit! Youâ€™ve just trained your first ML-Agents and shared it ..."
          ],
          [
           "Language models in RL\n## LMs encode useful knowledge for agents\n\n**Language models** (LMs) can exhib..."
          ],
          [
           "1) Sample inefficiency\n\n2) Unexpected behaviors from humansâ€™ eyes\n\nAs a first attempt, the paper [â€œG..."
          ],
          [
           "## Further reading\n\nFor more information we recommend you check out the following resources:\n\n- [Goo..."
          ],
          [
           "The Deep Q-Network (DQN)  [[deep-q-network]]\nThis is the architecture of our Deep Q-Learning network..."
          ],
          [
           "**Why do we stack four frames together?**\nWe stack frames together because it helps us **handle the ..."
          ],
          [
           "So, we see that Deep Q-Learning uses a neural network to approximate, given a state, the different Q..."
          ],
          [
           "The certification process\n\n\nThe certification process is **completely free**:\n\n- To get a *certifica..."
          ],
          [
           "Summary [[summary]]\n\nThat was a lot of information! Let's summarize:\n\n- Reinforcement Learning is a ..."
          ],
          [
           "Glossary [[glossary]]\n\nThis is a community-created glossary. Contributions are welcomed!\n\n\n### Strat..."
          ],
          [
           "### Greedy strategy:\n\n- Involves always choosing the action that is expected to lead to the highest ..."
          ],
          [
           "The Reinforcement Learning Framework [[the-reinforcement-learning-framework]]\n\n## The RL Process [[t..."
          ],
          [
           "This RL loop outputs a sequence ofÂ **state, action, reward and next state.**\n\n<img src=\"https://hugg..."
          ],
          [
           "- *State s*: is **a complete description of the state of the world** (there is no hidden information..."
          ],
          [
           "## Action Space [[action-space]]\n\nThe Action space is the set ofÂ **all possible actions in an enviro..."
          ],
          [
           "Taking this information into consideration is crucial because it willÂ **have importance when choosin..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/u..."
          ],
          [
           "Introduction [[introduction]]\n\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course..."
          ],
          [
           "From Q-Learning to Deep Q-Learning [[from-q-to-dqn]]\n\nWe learned thatÂ **Q-Learning is an algorithm w..."
          ],
          [
           "* A single frame in Atari is composed of an image of 210x160 pixels. Given that the images are in co..."
          ],
          [
           "Additional Readings [[additional-readings]]\n\nThese are **optional readings** if you want to go deepe..."
          ],
          [
           "Hands-on\n\n<CourseFloatingBanner classNames=\"absolute z-10 right-0 top-0\"\nnotebooks={[\n  {label: \"Goo..."
          ],
          [
           "**To start the hands-on, click on Open In Colab button** ðŸ‘‡ :\n\n[![Open In Colab](https://colab.resear..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/u..."
          ],
          [
           "- `Hardware Accelerator > GPU`\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course..."
          ],
          [
           "```\n\n```bash\n# Go inside the repository and install the package\ncd ml-agents\npip install -e ./ml-age..."
          ],
          [
           "```\n\nDownload the file SnowballTarget.zip from https://drive.google.com/file/d/1YHHLjyj6gaZ3Gemx1hQg..."
          ],
          [
           "```\n\nMake sure your file is accessible\n\n```bash\nchmod -R 755 ./training-envs-executables/linux/Snowb..."
          ],
          [
           "```\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main..."
          ],
          [
           "> It will fail the first time if and when you use `--resume`. Try rerunning the block to bypass the ..."
          ],
          [
           "```\n\n### Push the agent to the Hugging Face Hub\n\n- Now that we've trained our agent, weâ€™re **ready t..."
          ],
          [
           "```\n\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste..."
          ],
          [
           "```\n\nIt's the link to your model. It contains a model card that explains how to use it, your Tensorb..."
          ],
          [
           "Now let's try a more challenging environment called Pyramids.\n\n## Pyramids ðŸ†\n\n### Download and move ..."
          ],
          [
           "```\n\nUnzip it\n\n```python\n%%capture\n!unzip -d ./training-envs-executables/linux/ ./training-envs-exec..."
          ],
          [
           "```\n\n###  Modify the PyramidsRND config file\n  \n- Contrary to the first environment, which was a cus..."
          ],
          [
           "```\n\n### Push the agent to the Hugging Face Hub\n\n- Now that we trained our agent, weâ€™re **ready to p..."
          ],
          [
           "Decision Transformers\n\nThe Decision Transformer model was introduced by [\"Decision Transformer: Rein..."
          ],
          [
           "Start the tutorial here ðŸ‘‰ https://huggingface.co/blog/train-decision-transformers\n\n## Further readin..."
          ],
          [
           "Additional Readings [[additional-readings]]\n\n##  An introduction to multi-agents\n\n- [Multi-agent rei..."
          ],
          [
           "Introducing Q-Learning [[q-learning]]\n## What is Q-Learning? [[what-is-q-learning]]\n\nQ-Learning is a..."
          ],
          [
           "Let's go through an example of a maze.\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-r..."
          ],
          [
           "If we recap,Â *Q-Learning*Â **is the RL algorithm that:**\n\n- TrainsÂ a *Q-function* (an **action-value ..."
          ],
          [
           "## The Q-Learning algorithm [[q-learning-algo]]\n\nThis is the Q-Learning pseudocode; let's study each..."
          ],
          [
           "At the beginning of the training,Â **the probability of doing exploration will be huge since É› is ver..."
          ],
          [
           "Therefore, our \\\\(Q(S_t, A_t)\\\\)Â **update formula goes like this:**\n\n<img src=\"https://huggingface.c..."
          ],
          [
           "<figure>\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/..."
          ],
          [
           "Play with Huggy [[play]]\n\nNow that you've trained Huggy and pushed it to the Hub. **You will be able..."
          ],
          [
           "Discord 101 [[discord-101]]\n\nHey there! My name is Huggy, the dog ðŸ•, and I'm looking forward to trai..."
          ],
          [
           "They are in the reinforcement learning category. **Don't forget to sign up to these channels** by cl..."
          ],
          [
           "Introduction [[introduction]]\n\nOne of the most critical tasks in Deep Reinforcement Learning is to *..."
          ],
          [
           "Designing Multi-Agents systems\n\nFor this section, you're going to watch this excellent introduction ..."
          ],
          [
           "## Centralized approach\n\n<figure>\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-cour..."
          ],
          [
           "Additional Readings [[additional-readings]]\n\nThese are **optional readings** if you want to go deepe..."
          ],
          [
           "Hands-on\n\n\n      <CourseFloatingBanner classNames=\"absolute z-10 right-0 top-0\"\n      notebooks={[\n ..."
          ],
          [
           "<figure class=\"image table text-center m-0 w-full\">\n    <video\n        alt=\"LunarLander\"\n        sty..."
          ],
          [
           "In this notebook, you'll learn to **code your PPO agent from scratch with PyTorch using CleanRL impl..."
          ],
          [
           "## Set the GPU ðŸ’ª\n\n- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runt..."
          ],
          [
           "```\n\n```python\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(vis..."
          ],
          [
           "```\n\n- Add new argument in `parse_args()` function to define the repo-id where we want to push the m..."
          ],
          [
           "```\n\n- Next, we add the methods needed to push the model to the Hub\n\n- These methods will:\n  - `_eva..."
          ],
          [
           "# Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n\n        # ..."
          ],
          [
           "msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n    return re..."
          ],
          [
           "return mean_reward, std_reward\n\n\ndef record_video(env, policy, out_directory, fps=30):\n    images = ..."
          ],
          [
           "This is a trained model of a PPO agent playing {env_id}.\n\n  # Hyperparameters\n  \"\"\"\n    return model..."
          ],
          [
           "with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics t..."
          ],
          [
           "```\n\n- Finally, we call this function at the end of the PPO training\n\n```python\n# Create the evaluat..."
          ],
          [
           "```\n\n- Here's what the final ppo.py file looks like:\n\n```python\n# docs and experiment results can be..."
          ],
          [
           "from wasabi import Printer\n\nmsg = Printer()\n\n\ndef parse_args():\n    # fmt: off\n    parser = argparse..."
          ],
          [
           "# Algorithm specific arguments\n    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n..."
          ],
          [
           "help=\"the lambda for the general advantage estimation\")\n    parser.add_argument(\"--num-minibatches\",..."
          ],
          [
           "# Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini..."
          ],
          [
           "with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # S..."
          ],
          [
           "msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n\n        repo_url = upload_folder(\n     ..."
          ],
          [
           "with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics t..."
          ],
          [
           "class Agent(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic..."
          ],
          [
           "wandb.init(\n            project=args.wandb_project_name,\n            entity=args.wandb_entity,\n     ..."
          ],
          [
           "agent = Agent(envs).to(device)\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate,..."
          ],
          [
           "# ALGO LOGIC: action logic\n            with torch.no_grad():\n                action, logprob, _, val..."
          ],
          [
           "# bootstrap value if not done\n        with torch.no_grad():\n            next_value = agent.get_value..."
          ],
          [
           "# Optimizing the policy and value network\n        b_inds = np.arange(args.batch_size)\n        clipfr..."
          ],
          [
           "# Value loss\n                newvalue = newvalue.view(-1)\n                if args.clip_vloss:\n      ..."
          ],
          [
           "# TRY NOT TO MODIFY: record rewards for plotting purposes\n        writer.add_scalar(\"charts/learning..."
          ],
          [
           "```\n\nTo be able to share your model with the community there are three more steps to follow:\n\n1ï¸âƒ£ (I..."
          ],
          [
           "```\n\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste..."
          ],
          [
           "The SnowballTarget Environment\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/u..."
          ],
          [
           "In addition to raycasts, the agent gets a \"can I shoot\" bool as observation.\n\n<img src=\"https://hugg..."
          ],
          [
           "What are the policy-based methods?\n\nThe main goal of Reinforcement learning is to **find the optimal..."
          ],
          [
           "- On the other hand, in *policy-based methods*, we directly learn to approximate \\\\(\\pi^{*}\\\\) witho..."
          ],
          [
           "## The difference between policy-based and policy-gradient methods\n\nPolicy-gradient methods, what we..."
          ],
          [
           "Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym ðŸ¤– [[hands-on]]\n\n\n      <Cours..."
          ],
          [
           "# Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym ðŸ¤–\n\n### ðŸŽ® Environmen..."
          ],
          [
           "- `Hardware Accelerator > GPU`\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course..."
          ],
          [
           "```\n\n```python\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(vis..."
          ],
          [
           "```\n\n## PandaReachDense-v3 ðŸ¦¾\n\nThe agent we're going to train is a robotic arm that needs to do contr..."
          ],
          [
           "```\n\nThe observation space **is a dictionary with 3 different elements**:\n\n- `achieved_goal`: (x,y,z..."
          ],
          [
           "```\n\n### Create the A2C Model ðŸ¤–\n\nFor more information about A2C implementation with StableBaselines3..."
          ],
          [
           "```\n\n### Evaluate the agent ðŸ“ˆ\n\n- Now that's our  agent is trained, we need to **check its performanc..."
          ],
          [
           "```\n### Publish your trained model on the Hub ðŸ”¥\n\nNow that we saw we got good results after the train..."
          ],
          [
           "```\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command inst..."
          ],
          [
           "```\n\n## Some additional challenges ðŸ†\n\nThe best way to learn **is to try things by your own**! Why no..."
          ],
          [
           "```\n\n```python\n# 6\nmodel_name = \"a2c-PandaPickAndPlace-v3\";\nmodel.save(model_name)\nenv.save(\"vec_nor..."
          ],
          [
           "(Optional) What is Curiosity in Deep Reinforcement Learning?\n\nThis is an (optional) introduction to ..."
          ],
          [
           "For instance, in [Vizdoom](https://vizdoom.cs.put.edu.pl/), a set of environments based on the game ..."
          ],
          [
           "Because the idea of Curiosity is to **encourage our agent to perform actions that reduce the uncerta..."
          ],
          [
           "The â€œDeepâ€ in Reinforcement Learning [[deep-rl]]\n\n<Tip>\nWhat we've talked about so far is Reinforcem..."
          ],
          [
           "What is RL? A short recap [[what-is-rl]]\n\nIn RL, we build an agent that canÂ **make smart decisions**..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/u..."
          ],
          [
           "Conclusion [[Conclusion]]\n\nThatâ€™s all for today. Congrats on finishing this unit and the tutorial!\n\n..."
          ],
          [
           "Diving deeper into policy-gradient methods\n\n## Getting the big picture\n\nWe just learned that policy-..."
          ],
          [
           "Now that we got the big picture, let's dive deeper into policy-gradient methods.\n\n## Diving deeper i..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/u..."
          ],
          [
           "(If you need a refresher on the difference between gradient descent and gradient ascent [check this]..."
          ],
          [
           "Fortunately we're going to use a solution called the Policy Gradient Theorem that will help us to re..."
          ],
          [
           "- Update the weights of the policy: \\\\(\\theta \\leftarrow \\theta + \\alpha \\hat{g}\\\\)\n\nWe can interpre..."
          ],
          [
           "Additional Readings [[additional-readings]]\n\n## Bias-variance tradeoff in Reinforcement Learning\n\nIf..."
          ],
          [
           "Let's train and play with Huggy ðŸ¶ [[train]]\n\n\n\n\n          <CourseFloatingBanner classNames=\"absolute..."
          ],
          [
           "### The environment ðŸŽ®\n\n- Huggy the Dog, an environment created by [Thomas Simonini](https://twitter...."
          ],
          [
           "- `Hardware Accelerator > GPU`\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course..."
          ],
          [
           "```\n\n```bash\n# Go inside the repository and install the package (can take 3min)\n%cd ml-agents\npip3 i..."
          ],
          [
           "```\n\nDownload the file Huggy.zip from https://drive.google.com/uc?export=download&id=1zv3M95ZJTWHUVO..."
          ],
          [
           "```\n\n## Let's recap how this environment works\n\n### The State Space: what Huggy perceives.\n\nHuggy do..."
          ],
          [
           "Here, our goal is that Huggy **goes towards the stick but without spinning too much**. Hence, our re..."
          ],
          [
           "```\nbehaviors:\n  Huggy:\n    trainer_type: ppo\n    hyperparameters:\n      batch_size: 2048\n      buff..."
          ],
          [
           "```\n\n- Don't forget to save the file!\n\n- **In the case you want to modify the hyperparameters**, in ..."
          ],
          [
           "```\n\n## Push the agent to the ðŸ¤— Hub\n\n- Now that we trained our agent, weâ€™re **ready to push it to th..."
          ],
          [
           "```\n\nIf you don't want to use Google Colab or a Jupyter Notebook, you need to use this command inste..."
          ],
          [
           "```\n\nItâ€™s the link to your model repository. The repository contains a model card that explains how ..."
          ],
          [
           "ðŸ‘‰ It's good **to try with different models steps to see the improvement of the agent.**\n\nCongrats on..."
          ],
          [
           "Mid-way Quiz [[mid-way-quiz]]\n\nThe best way to learn and [to avoid the illusion of competence](https..."
          ],
          [
           "<details>\n<summary>Solution</summary>\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl..."
          ],
          [
           "<details>\n<summary>Solution</summary>\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl..."
          ],
          [
           "Bonus: Learn to create your own environments with Unity and MLAgents\n\n**You can create your own rein..."
          ],
          [
           "Train your first Deep Reinforcement Learning Agent ðŸ¤– [[hands-on]]\n\n\n\n\n      <CourseFloatingBanner cl..."
          ],
          [
           "**If you don't find your model, go to the bottom of the page and click on the refresh button.**\n\nFor..."
          ],
          [
           "### The environment ðŸŽ®\n\n- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_land..."
          ],
          [
           "And more! \n\nCheck ðŸ“š the syllabus ðŸ‘‰ https://simoninithomas.github.io/deep-rl-course\n\nDonâ€™t forget to ..."
          ],
          [
           "- Reinforcement Learning is a **computational approach to learning from actions**. We build an agent..."
          ],
          [
           "To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep..."
          ],
          [
           "```\n\n```bash\npip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/noteboo..."
          ],
          [
           "```\n\n```python\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(vis..."
          ],
          [
           "```\n\n## Understand Gymnasium and how it works ðŸ¤–\n\nðŸ‹ The library containing our environment is called ..."
          ],
          [
           "2ï¸âƒ£ We reset the environment to its initial state with `observation = env.reset()`\n\nAt each step:\n\n3..."
          ],
          [
           "```\n\n## Create the LunarLander environment ðŸŒ› and understand how it works\n\n### The environment ðŸŽ®\n\nIn ..."
          ],
          [
           "```\n\nThe action space (the set of possible actions the agent can take) is discrete with 4 actions av..."
          ],
          [
           "```\n\n## Create the Model ðŸ¤–\n\n- We have studied our environment and we understood the problem: **being..."
          ],
          [
           "```\n# Create environment\nenv = gym.make('LunarLander-v2')\n\n# Instantiate the agent\nmodel = PPO('MlpP..."
          ],
          [
           "```\n\n## Evaluate the agent ðŸ“ˆ\n\n- Remember to wrap the environment in a [Monitor](https://stable-basel..."
          ],
          [
           "```\n\n- In my case, I got a mean reward is `200.20 +/- 20.80` after training for 1 million steps, whi..."
          ],
          [
           "```\n\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins..."
          ],
          [
           "# TODO: Define the model architecture we used\nmodel_architecture = \"\"\n\n## TODO: Define the commit me..."
          ],
          [
           "```\n\n#### Solution\n\n\n```python\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stabl..."
          ],
          [
           "```\n\nCongrats ðŸ¥³ you've just trained and uploaded your first Deep Reinforcement Learning agent. The s..."
          ],
          [
           "```\n\n```python\nfrom huggingface_sb3 import load_from_hub\n\nrepo_id = \"Classroom-workshop/assignment2-..."
          ],
          [
           "```\n\n## Some additional challenges ðŸ†\nThe best way to learn **is to try things by your own**! As you ..."
          ],
          [
           "If youâ€™re still feel confused with all these elements...it's totally normal! **This was the same for..."
          ],
          [
           "Glossary \n\nThis is a community-created glossary. Contributions are welcomed!\n\n- **Tabular Method:** ..."
          ],
          [
           "- **Fixed Q-Target:** In order to calculate the **Q-Target** we need to estimate the discounted opti..."
          ],
          [
           "Live 1: How the course work, Q&A, and playing with Huggy\n\nIn this first live stream, we explained ho..."
          ],
          [
           "Quiz [[quiz]]\n\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera...."
          ],
          [
           "<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: \"an action a0, action a0, state s0, state s1, reward r1\",\n\t\t\texpl..."
          ],
          [
           "### Q3: What's the difference between a state and an observation?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext..."
          ],
          [
           "### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?\n..."
          ],
          [
           "- The Policy Ï€ **is the brain of our Agent**. Itâ€™s the function that tells us what action to take gi..."
          ],
          [
           "Additional Readings [[additional-readings]]\n\nThese are **optional readings** if you want to go deepe..."
          ],
          [
           "(Optional) the Policy Gradient Theorem\n\nIn this optional section where we're **going to study how we..."
          ],
          [
           "We can simplify further this since \\\\( \\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)}\\nabla_\\theta P(\\tau;\\th..."
          ],
          [
           "But we still have some mathematics work to do there: we need to simplify \\\\(  \\nabla_\\theta log P(\\t..."
          ],
          [
           "We also know that the gradient of the sum is equal to the sum of gradient:\n\n\\\\( \\nabla_\\theta log P(..."
          ],
          [
           "So, the final formula for estimating the policy gradient is:\n\n\\\\( \\nabla_{\\theta} J(\\theta) = \\hat{g..."
          ],
          [
           "Introduction to Deep Reinforcement Learning [[introduction-to-deep-reinforcement-learning]]\n\n<img sr..."
          ],
          [
           "So let's get started! ðŸš€..."
          ],
          [
           "Additional Readings [[additional-readings]]\n\nThese are **optional readings** if you want to go deepe..."
          ],
          [
           "Advantage Actor-Critic (A2C) [[advantage-actor-critic]]\n\n## Reducing variance with Actor-Critic meth..."
          ],
          [
           "## The Actor-Critic Process\nNow that we have seen the Actor Critic's big picture, let's dive deeper ..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/u..."
          ],
          [
           "In other words, this function calculates **the extra reward we get if we take this action at that st..."
          ],
          [
           "(Automatic) Curriculum Learning for RL\n\nWhile most of the RL methods seen in this course work well i..."
          ],
          [
           "> â€¦ a family of mechanisms that automatically adapt the distribution of training data by learning to..."
          ],
          [
           "### Recent methods\n\n- [Evolving Curricula with Regret-Based Environment Design](https://arxiv.org/ab..."
          ],
          [
           "Conclusion [[conclusion]]\n\nCongrats on finishing this chapter!Â There was a lot of information. And c..."
          ],
          [
           "Hands-on [[hands-on]]\n\n      <CourseFloatingBanner classNames=\"absolute z-10 right-0 top-0\"\n      no..."
          ],
          [
           "To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep..."
          ],
          [
           "â¬‡ï¸ Here is an example of what **you will achieve in just a couple of minutes.** â¬‡ï¸\n\n\n<img src=\"https..."
          ],
          [
           "In this free course, you will:\n\n- ðŸ“– Study Deep Reinforcement Learning in **theory and practice**.\n- ..."
          ],
          [
           "- When the training is done, **we have an optimal Q-Function, so an optimal Q-Table.**\n\n- And if we ..."
          ],
          [
           "To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep..."
          ],
          [
           "```\n\n```bash\nsudo apt-get update\nsudo apt-get install -y python3-opengl\napt install ffmpeg xvfb\npip3..."
          ],
          [
           "```\n\nWe're now ready to code our Q-Learning algorithm ðŸ”¥\n\n# Part 1: Frozen Lake â›„ (non slippery versi..."
          ],
          [
           "```\n\n### Solution\n\n```python\nenv = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, rend..."
          ],
          [
           "```\n\nWe see with `Observation Space Shape Discrete(16)` that the observation is an integer represent..."
          ],
          [
           "```\n\nThe action space (the set of possible actions the agent can take) is discrete with 4 actions av..."
          ],
          [
           "```\n\n```python\n# Let's create our Qtable of size (state_space, action_space) and initialized each va..."
          ],
          [
           "```\n\n## Define the epsilon-greedy policy ðŸ¤–\n\nEpsilon-greedy is the training policy that handles the e..."
          ],
          [
           "```\n\n## Define the hyperparameters âš™ï¸\n\nThe exploration related hyperparamters are some of the most i..."
          ],
          [
           "```\n\n```python\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, ..."
          ],
          [
           "```\n\n#### Solution\n\n```python\ndef train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, e..."
          ],
          [
           "```\n\n## Let's see what our Q-Learning table looks like now ðŸ‘€\n\n```python\nQtable_frozenlake\n```\n\n## Th..."
          ],
          [
           "```\n\n## Evaluate our Q-Learning agent ðŸ“ˆ\n\n- Usually, you should have a mean reward of 1.0\n- The **env..."
          ],
          [
           "```\n\n```python\ndef record_video(env, Qtable, out_directory, fps=1):\n    \"\"\"\n    Generate a replay vi..."
          ],
          [
           "```\n\n```python\ndef push_to_hub(repo_id, model, env, video_fps=1, local_repo_path=\"hub\"):\n    \"\"\"\n   ..."
          ],
          [
           "# Pickle the model\n    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n        pickle.du..."
          ],
          [
           "metadata = {}\n    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-impl..."
          ],
          [
           "with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics t..."
          ],
          [
           "```\n\n### .\n\nBy using `push_to_hub` **you evaluate, record a replay, generate a model card of your ag..."
          ],
          [
           "```\n\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins..."
          ],
          [
           "```\n\nCongrats ðŸ¥³ you've just implemented from scratch, trained, and uploaded your first Reinforcement..."
          ],
          [
           "```\n\n```python\naction_space = env.action_space.n\nprint(\"There are \", action_space, \" possible action..."
          ],
          [
           "```\n\n## Define the hyperparameters âš™ï¸\n\nâš  DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to..."
          ],
          [
           "```\n\n## Create a model dictionary ðŸ’¾ and publish our trained model to the Hub ðŸ”¥\n\n- We create a model ..."
          ],
          [
           "```\n\nNow that it's on the Hub, you can compare the results of your Taxi-v3 with your classmates usin..."
          ],
          [
           "```\n\n### .\n\n```python\nmodel = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning..."
          ],
          [
           "```\n\n## Some additional challenges ðŸ†\n\nThe best way to learn **is to try things on your own**! As you..."
          ],
          [
           "Doom is a large environment with a huge state space (millions of different states). Creating and upd..."
          ],
          [
           "An Introduction to Unreal Learning Agents\n\n[Learning Agents](https://dev.epicgames.com/community/lea..."
          ],
          [
           "Armed with the basics, **you're now prepared to play with Learning Agents**:\n\n3. Get the Big Picture..."
          ],
          [
           "Conclusion\n\nThatâ€™s all for today. Congrats on finishing this unit and the tutorial!\n\nThe best way to..."
          ],
          [
           "Conclusion [[conclusion]]\n\nCongrats on finishing this bonus unit!\n\nYou can now sit and enjoy playing..."
          ],
          [
           "Type of tasks [[tasks]]\n\nA task is an **instance** of a Reinforcement Learning problem. We can have ..."
          ],
          [
           "The intuition behind PPO [[the-intuition-behind-ppo]]\n\n\nThe idea with Proximal Policy Optimization (..."
          ],
          [
           "The Bellman Equation: simplify our value estimation [[bellman-equation]]\n\nThe Bellman equationÂ **sim..."
          ],
          [
           "Then, to calculate the \\\\(V(S_{t+1})\\\\), we need to calculate the return starting at that state \\\\(S..."
          ],
          [
           "If we go back to our example, we can say that the value of State 1 is equal to the expected cumulati..."
          ],
          [
           "Before going to the next section, think about the role of gamma in the Bellman equation. What happen..."
          ],
          [
           "Conclusion\n\nThat's all for today. Congrats on finishing this Unit and the tutorial! â­ï¸\n\nNow that you..."
          ],
          [
           "Brief introduction to RL documentation\n\nIn this advanced topic, we address the question: **how shoul..."
          ],
          [
           "Building on the documentation frameworks for [model cards](https://arxiv.org/abs/1810.03993) and [da..."
          ],
          [
           "Conclusion [[conclusion]]\n\nCongrats on finishing this unit and the tutorial. You've just trained you..."
          ],
          [
           "Monte Carlo vs Temporal Difference Learning [[mc-vs-td]]\n\nThe last thing we need to discuss before d..."
          ],
          [
           "- We always start the episodeÂ **at the same starting point.**\n- **The agent takes actions using the ..."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/u..."
          ],
          [
           "\\\\(V(S_0) = 0 + 0.1 * [3 â€“ 0]\\\\)\n\n\\\\(V(S_0) = 0.3\\\\)\n\n\n  <img src=\"https://huggingface.co/datasets/h..."
          ],
          [
           "This method is called TD(0) orÂ **one-step TD (update the value function after any individual step).*..."
          ],
          [
           "New \\\\(V(S_0) = 0 + 0.1 * [1 + 1 * 0â€“0]\\\\)\n\nNew \\\\(V(S_0) = 0.1\\\\)\n\nSo we just updated our value fun..."
          ],
          [
           "Conclusion [[conclusion]]\n\nCongrats on finishing this chapter!Â There was a lot of information. And c..."
          ],
          [
           "Glossary [[glossary]]\n\nThis is a community-created glossary. Contributions are welcomed!\n\n### Agent\n..."
          ],
          [
           "- **Episodic**: Has a starting point and an ending point.\n- **Continuous**: Has a starting point but..."
          ],
          [
           "Offline vs. Online Reinforcement Learning\n\nDeep Reinforcement Learning (RL) is a framework **to buil..."
          ],
          [
           "- On the other hand, in *offline reinforcement learning*, the agent only **uses data collected from ..."
          ],
          [
           "Quiz\n\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lectu..."
          ],
          [
           "</details>\n\n\n### Q3: What's the difference between policy-based methods and policy-gradient methods?..."
          ],
          [
           "Hands-on: advanced Deep Reinforcement Learning. Using Sample Factory to play Doom from pixels\n\n<Cour..."
          ],
          [
           "*   [Sample Factory](https://www.samplefactory.dev/) is an advanced RL framework and **only function..."
          ],
          [
           "```\n\nTo validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course..."
          ],
          [
           "Sample Factory is thoroughly **tested, used by many researchers and practitioners**, and is actively..."
          ],
          [
           "- Highly optimized algorithmÂ [architecture](https://www.samplefactory.dev/06-architecture/overview/)..."
          ],
          [
           "- [HuggingFace ðŸ¤— integration](https://www.samplefactory.dev/10-huggingface/huggingface/)Â (upload tra..."
          ],
          [
           "All of the above policies are available on the ðŸ¤— hub. Search for the tag [sample-factory](https://hu..."
          ],
          [
           "## ViZDoom\n\n[ViZDoom](https://vizdoom.cs.put.edu.pl/) is an **open-source python interface for the D..."
          ],
          [
           "```python\n# Install ViZDoom deps from\n# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building..."
          ],
          [
           "```\n\n## Then we can install Sample Factory and ViZDoom\n\n- This can take 7min\n\n```bash\npip install sa..."
          ],
          [
           "```\n\n## Setting up the Doom Environment in sample-factory\n\n```python\nimport functools\n\nfrom sample_f..."
          ],
          [
           "def register_vizdoom_components():\n    register_vizdoom_envs()\n    register_vizdoom_models()\n\n\n# par..."
          ],
          [
           "```\n\nNow that the setup if complete, we can train the agent. We have chosen here to learn a ViZDoom ..."
          ],
          [
           "```python\n## Start the training, this should take around 15 minutes\nregister_vizdoom_components()\n\n#..."
          ],
          [
           "```\n\n## Let's take a look at the performance of the trained policy and output a video of the agent.\n..."
          ],
          [
           "```\n\nThe agent has learned something, but its performance could be better. We would clearly need to ..."
          ],
          [
           "```\n\n## Let's load another model\n\n\n\n\nThis agent's performance was good, but we can do better! Let's ..."
          ],
          [
           "```\n\n## Some additional challenges ðŸ†: Doom Deathmatch\n\nTraining an agent to play a Doom deathmatch *..."
          ],
          [
           "```\n\n\nYou **can try to train your agent in this environment** using the code above, but not on colab..."
          ],
          [
           "Introducing the Clipped Surrogate Objective Function\n## Recap: The Policy Objective Function\n\nLetâ€™s ..."
          ],
          [
           "Itâ€™s the probability of taking action \\\\( a_t \\\\) at state \\\\( s_t \\\\) in the current policy, divide..."
          ],
          [
           "However, without a constraint, if the action taken is much more probable in our current policy than ..."
          ],
          [
           "Then, we take the minimum of the clipped and non-clipped objective, **so the final objective is a lo..."
          ],
          [
           "Quiz\n\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lectu..."
          ],
          [
           "### Q4: Explain in your own words what is the `Self-Play` approach\n\n<details>\n<summary>Solution</sum..."
          ],
          [
           "### Q6: What are the main motivations to use a ELO rating Score?\n\n<Question\n\tchoices={[\n   \t\t {\n\t\t\tt..."
          ],
          [
           "The Problem of Variance in Reinforce [[the-problem-of-variance-in-reinforce]]\n\nIn Reinforce, we want..."
          ],
          [
           "The solution is to mitigate the variance by **using a large number of trajectories, hoping that the ..."
          ],
          [
           "An introduction to Multi-Agents Reinforcement Learning (MARL)\n\n## From single agent to multiple agen..."
          ],
          [
           "Or a road with **several autonomous vehicles**.\n\n<figure>\n<img src=\"https://huggingface.co/datasets/..."
          ],
          [
           "- *Mixed of both adversarial and cooperative*: like in our SoccerTwos environment, two agents are pa..."
          ],
          [
           "Student Works\n\nSince the launch of the Deep Reinforcement Learning Course, **many students have crea..."
          ],
          [
           "In this project, Eric Dong recreates Bill Seiler's 1985 version of Space War in Pygame and uses rein..."
          ],
          [
           "Introduction [[introduction]]\n\nIn this bonus unit, we'll reinforce what we learned in the first unit..."
          ],
          [
           "Conclusion\n\n\n**Congrats on finishing this unit**!Â There was a lot of information.\nAnd congrats on fi..."
          ],
          [
           "Visualize the Clipped Surrogate Objective Function\n\nDon't worry. **It's normal if this seems complex..."
          ],
          [
           "Since the ratio is between intervals, **we can decrease the probability that our policy takes that a..."
          ],
          [
           "## Case 5 and 6: the ratio is above the range\n<figure class=\"image table text-center m-0 w-full\">\n  ..."
          ],
          [
           "**You might wonder why, when the minimum is the clipped ratio, the gradient is 0.** When the ratio i..."
          ],
          [
           "Welcome to the ðŸ¤— Deep Reinforcement Learning Course [[introduction]]\n\n<img src=\"https://huggingface...."
          ],
          [
           "Letâ€™s get started!\n\n## What to expect? [[expect]]\n\nIn this course, you will:\n\n- ðŸ“– Study Deep Reinfor..."
          ],
          [
           "And more!\n\nAt the end of this course, **youâ€™ll get a solid foundation from the basics to the SOTA (s..."
          ],
          [
           "## What's the syllabus? [[syllabus]]\n\nThis is the course's syllabus:\n\n<img src=\"https://huggingface...."
          ],
          [
           "## The Certification Process [[certification-process]]\n\nThe certification process is **completely fr..."
          ],
          [
           "## What tools do I need? [[tools]]\n\nYou need only 3 things:\n\n- *A computer* with an internet connect..."
          ],
          [
           "About the team:\n\n- <a href=\"https://twitter.com/osanseviero\">Omar Sanseviero</a> is a Machine Learni..."
          ],
          [
           "## I still have questions [[questions]]\n\nPlease ask your question in our <a href=\"https://discord.gg..."
          ],
          [
           "Hands on\n\n\n\n      <CourseFloatingBanner classNames=\"absolute z-10 right-0 top-0\"\n      notebooks={[\n..."
          ],
          [
           "For more information about the certification process, check this section ðŸ‘‰ https://huggingface.co/de..."
          ],
          [
           "To test its robustness, we're going to train it in 2 different simple environments:\n- Cartpole-v1\n- ..."
          ],
          [
           "# Let's code Reinforce algorithm from scratch ðŸ”¥\n\n## Some advice ðŸ’¡\n\nIt's better to run this colab in ..."
          ],
          [
           "```\n\n```python\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(vis..."
          ],
          [
           "```\n\n## Import the packages ðŸ“¦\n\nIn addition to importing the installed libraries, we also import:\n\n- ..."
          ],
          [
           "```\n\n```python\nprint(device)\n```\n\nWe're now ready to implement our Reinforce algorithm ðŸ”¥\n\n# First ag..."
          ],
          [
           "```\n\n```python\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"..."
          ],
          [
           "```\n\n## Let's build the Reinforce Architecture\n\nThis implementation is based on three implementation..."
          ],
          [
           "```\n\n### Solution\n\n```python\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size)..."
          ],
          [
           "```\n\n- Here we see that the error says `ValueError: The value argument to log_prob must be a Tensor`..."
          ],
          [
           "```\n\nBy using CartPole, it was easier to debug since **we know that the bug comes from our integrati..."
          ],
          [
           "The second question you may ask is **why do we minimize the loss**? Didn't we talk about Gradient As..."
          ],
          [
           "# Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t)\n        n_steps =..."
          ],
          [
           "## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n  ..."
          ],
          [
           "```\n\n#### Solution\n\n```python\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, pr..."
          ],
          [
           "# This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)..."
          ],
          [
           "# Line 8: PyTorch prefers gradient descent\n        optimizer.zero_grad()\n        policy_loss.backwar..."
          ],
          [
           "```\n\n##  Train it\n- We're now ready to train our agent.\n- But first, we define a variable containing..."
          ],
          [
           "```\n\n## Define evaluation method ðŸ“\n- Here we define the evaluation method that we're going to use to..."
          ],
          [
           "```\n\n```python\ndef record_video(env, policy, out_directory, fps=30):\n    \"\"\"\n    Generate a replay v..."
          ],
          [
           "```\n\n```python\ndef push_to_hub(repo_id,\n                model,\n                hyperparameters,\n    ..."
          ],
          [
           "evaluate_data = {\n          \"env_id\": hyperparameters[\"env_id\"],\n          \"mean_reward\": mean_rewar..."
          ],
          [
           "readme_path = local_directory / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        wit..."
          ],
          [
           "```\n\nBy using `push_to_hub`, **you evaluate, record a replay, generate a model card of your agent, a..."
          ],
          [
           "```\n\nNow that we tested the robustness of our implementation, let's try a more complex environment: ..."
          ],
          [
           "```\n\nThe observation space (7) ðŸ‘€:\n- player y position\n- player velocity\n- player distance to floor\n-..."
          ],
          [
           "```\n\n#### Solution\n\n```python\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size..."
          ],
          [
           "```\n\n###  Train it\n- We're now ready to train our agent ðŸ”¥.\n\n```python\n# Create policy and place it t..."
          ],
          [
           "```\n\n## Some additional challenges ðŸ†\n\nThe best way to learn **is to try things on your own**! As you..."
          ],
          [
           "Sound fun? See you next time!\n\nFinally, we would love **to hear what you think of the course and how..."
          ],
          [
           "Setup [[setup]]\n\nAfter all this information, it's time to get started. We're going to do two things:..."
          ],
          [
           "Second Quiz [[quiz2]]\n\nThe best way to learn and [to avoid the illusion of competence](https://www.c..."
          ],
          [
           "<details>\n<summary>Solution</summary>\n\nBecause if we have an optimal Q-function, we have an optimal ..."
          ],
          [
           "</details>\n\n\n\n### Q6: What's the difference between on-policy and off-policy\n\n<details>\n<summary>Sol..."
          ],
          [
           "Two main approaches for solving RL problems [[two-methods]]\n\n<Tip>\nNow that we learned the RL framew..."
          ],
          [
           "This function will define a mapping from each state to the best corresponding action. Alternatively,..."
          ],
          [
           "<figure>\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/..."
          ],
          [
           "Here we see that our value functionÂ **defined values for each possible state.**\n\n<figure>\n<img src=\"..."
          ],
          [
           "The Exploration/Exploitation trade-off [[exp-exp-tradeoff]]\n\nFinally, before looking at the differen..."
          ],
          [
           "If itâ€™s still confusing, **think of a real problem: the choice of picking a restaurant:**\n\n\n<figure>..."
          ],
          [
           "Generalization in Reinforcement Learning\n\nGeneralization plays a pivotal role in the realm of Reinfo..."
          ],
          [
           "Godot RL Agents\n\n[Godot RL Agents](https://github.com/edbeeching/godot_rl_agents) is an Open Source ..."
          ],
          [
           "In this section, you will **learn how to create a custom environment in the Godot Game Engine** and ..."
          ],
          [
           "While we will guide you through the steps to implement your agent, you may wish to learn more about ..."
          ],
          [
           "### Loading the starter project\n\nWe provide two versions of the codebase:\n- [A starter project, to d..."
          ],
          [
           "The Godot RL Agents plugin is now downloaded to your machine your machine. Now click on Project â†’ Pr..."
          ],
          [
           "func get_reward() -> float:\n\tassert(false, \"the get_reward method is not implemented when extending ..."
          ],
          [
           "```\n\nIn order to implement these methods, we will need to create a class that inherits from AIContro..."
          ],
          [
           "```\n\nWe have now defined the agentâ€™s observation, which is the position and velocity of the ball in ..."
          ],
          [
           "```\n\nWe now need to synchronize between the game running in Godot and the neural network being train..."
          ],
          [
           "An Introduction to Unity ML-Agents [[introduction-to-ml-agents]]\n\n<img src=\"https://huggingface.co/d..."
          ],
          [
           "So, today, we're going to train two agents:\n- The first one will learn to **shoot snowballs onto a s..."
          ],
          [
           "Introduction [[introduction]]\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/..."
          ],
          [
           "<figure>\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/..."
          ],
          [
           "Quiz\n\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lectu..."
          ],
          [
           "### Q2: What of the following statements are true about Unity ML-Agents?\n\n<Question\n\tchoices={[\n\t\t{\n..."
          ],
          [
           "### Q5: Which are the differences between capturing the environment using `frames` or `raycasts`?\n\n<..."
          ],
          [
           "Model Based Reinforcement Learning (MBRL)\n\nModel-based reinforcement learning only differs from its ..."
          ],
          [
           "We employ sample-based model-predictive control (MPC) using the learned dynamics model, which optimi..."
          ],
          [
           "Hands-on [[hands-on]]\n\nNow that you've learned to use Optuna, here are some ideas to apply what you'..."
          ],
          [
           "[The Hugging Face Deep Reinforcement Learning Course ðŸ¤— (v2.0)](https://huggingface.co/deep-rl-course..."
          ],
          [
           "How do Unity ML-Agents work? [[how-mlagents-works]]\n\nBefore training our agent, we need to understan..."
          ],
          [
           "- The first is the *Learning Environment*, which contains **the Unity scene (the environment) and th..."
          ],
          [
           "To better understand its role, letâ€™s remember the RL process. This can be modeled as a loop that wor..."
          ],
          [
           "This RL loop outputs a sequence ofÂ **state, action, reward and next state.** The goal of the agent i..."
          ],
          [
           "The Pyramid environment\n\nThe goal in this environment is to train our agent to **get the gold brick ..."
          ],
          [
           "We also use a **boolean variable indicating the switch state** (did we turn on or off the switch to ..."
          ],
          [
           "How Huggy works [[how-huggy-works]]\n\nHuggy is a Deep Reinforcement Learning environment made by Hugg..."
          ],
          [
           "**Joint motors drive Huggy's legs**. This means that to get the target, Huggy needs to **learn to ro..."
          ],
          [
           "The training loop looks like this:\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-co..."
          ],
          [
           "Interesting Environments to try\n\nHere we provide a list of interesting environments you can try to t..."
          ],
          [
           "To start using this environment, check these resources:\n- [DonkeyCar Simulator documentation](https:..."
          ],
          [
           "Self-Play: a classic technique to train competitive agents in adversarial games\n\nNow that we've stud..."
          ],
          [
           "This solution is called *self-play*. In self-play, **the agent uses former copies of itself (of its ..."
          ],
          [
           "So we need to control:\n\n- How **often we change opponents** with the `swap_steps` and `team_change` ..."
          ],
          [
           "This ELO (starting at a specific score: frequently 1200) can decrease initially but should increase ..."
          ],
          [
           "We also define a maximum adjustment rating per game: K-factor.\n\n- K=16 for master.\n- K=32 for weaker..."
          ],
          [
           "\\\\(ELO_B = 2300 + 16 *(1-0.151) = 2314 \\\\)\n\n\n### The Advantages\n\nUsing the ELO score has multiple ad..."
          ],
          [
           "Quiz [[quiz]]\n\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera...."
          ],
          [
           "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/u..."
          ],
          [
           "</details>\n\n### Q6: How do we use Double Deep Q-Learning?\n\n\n<details>\n  <summary>Solution</summary>\n..."
          ],
          [
           "Deep Q-Learning [[deep-q-learning]]\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-c..."
          ],
          [
           "Apache License\n                           Version 2.0, January 2004\n                        http://w..."
          ],
          [
           "\"Contribution\" shall mean any work of authorship, including\n      the original version of the Work a..."
          ],
          [
           "4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works the..."
          ],
          [
           "6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, serv..."
          ],
          [
           "END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To ap..."
          ],
          [
           "Optuna Tutorial [[optuna]]\n\nThe content below comes from [Antonin's Raffin ICRA 2022 presentations](..."
          ],
          [
           "Hands-on [[hands-on]]\n\n\n\n      <CourseFloatingBanner classNames=\"absolute z-10 right-0 top-0\"\n      ..."
          ],
          [
           "**If you don't find your model, go to the bottom of the page and click on the refresh button.**\n\nFor..."
          ],
          [
           "### ðŸŽ® Environments:\n\n- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atar..."
          ],
          [
           "To validate this hands-on for the certification process, you need to push your trained model to the ..."
          ],
          [
           "```\n\nIF AND ONLY IF THE VERSION ABOVE DOES NOT EXIST ANYMORE. UNCOMMENT AND INSTALL THE ONE BELOW\n\n`..."
          ],
          [
           "```\nSpaceInvadersNoFrameskip-v4:\n  env_wrapper:\n    - stable_baselines3.common.atari_wrappers.AtariW..."
          ],
          [
           "```\n\nHere we see that:\n- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,gray..."
          ],
          [
           "```\n\n## Let's evaluate our agent ðŸ‘€\n\n- RL-Baselines3-Zoo provides `enjoy.py`, a python script to eval..."
          ],
          [
           "```\n\n## Publish our trained model on the Hub ðŸš€\nNow that we saw we got good results after the trainin..."
          ],
          [
           "```\n\nIf you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command ins..."
          ],
          [
           "```\n\n###.\n\nCongrats ðŸ¥³ you've just trained and uploaded your first Deep Q-Learning agent using RL-Bas..."
          ],
          [
           "Let's load an agent playing Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n\n1. W..."
          ],
          [
           "```\n\n2. Let's evaluate if for 5000 timesteps\n\n```bash\npython -m rl_zoo3.enjoy --algo dqn --env BeamR..."
          ],
          [
           "```\n\nWhy not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? ðŸ†.**\n..."
          ],
          [
           "If youâ€™re still feel confused with all these elements...it's totally normal! **This was the same for..."
          ],
          [
           "Introduction to PPO with Sample-Factory\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-..."
          ],
          [
           "The Deep Q-Learning Algorithm [[deep-q-algorithm]]\n\nWe learned that Deep Q-Learning **uses a deep ne..."
          ],
          [
           "To help us stabilize the training, we implement three different solutions:\n1. *Experience Replay* to..."
          ],
          [
           "The solution is to create a Replay Buffer that stores experience tuples while interacting with the e..."
          ],
          [
           "However, the problem is that we are using the same parameters (weights) for estimating the TD target..."
          ],
          [
           "Instead, what we see in the pseudo-code is that we:\n- Use a **separate network with fixed parameters..."
          ],
          [
           "The solution is: when we compute the Q target, we use two networks to decouple the action selection ..."
          ],
          [
           "A Q-Learning example [[q-learning-example]]\n\nTo better understand Q-Learning, let's take a simple ex..."
          ],
          [
           "## Step 1: Initialize the Q-table [[step1]]\n\n<img src=\"https://huggingface.co/datasets/huggingface-d..."
          ],
          [
           "## Step 4: Update Q(St, At) [[step4]]\n\nWe can now update \\\\(Q(S_t, A_t)\\\\) using our formula.\n\n<img ..."
          ],
          [
           "## Step 4: Update Q(St, At) [[step4-4]]\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-..."
          ],
          [
           "Congratulations\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/..."
          ],
          [
           "What is Reinforcement Learning? [[what-is-reinforcement-learning]]\n\nTo understand Reinforcement Lear..."
          ],
          [
           "By interacting with his environment through trial and error, your little brother understands thatÂ **..."
          ],
          [
           "Introduction [[introduction]]\n\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-cours..."
          ],
          [
           "Mid-way Recap [[mid-way-recap]]\n\nBefore diving into Q-Learning, let's summarize what we've just lear..."
          ],
          [
           "Introduction [[introduction]]\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/..."
          ],
          [
           "<figure>\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/..."
          ],
          [
           "RLHF\n\nReinforcement learning from human feedback (RLHF) is a **methodology for integrating human dat..."
          ],
          [
           "## Additional readings\n\n*Note, this is copied from the Illustrating RLHF blog post above*.\nHere is a..."
          ],
          [
           "And here is a snapshot of the growing set of papers that show RLHF's performance for LMs:\n- [Fine-Tu..."
          ],
          [
           "## Author\n\nThis section was written by <a href=\"https://twitter.com/natolambert\"> Nathan Lambert </a..."
          ]
         ],
         "hovertemplate": "source=deep-rl-class<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "deep-rl-class, circle",
         "marker": {
          "color": "#00cc96",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "deep-rl-class, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -8.627735,
          -6.9167438,
          -8.393746,
          -8.324206,
          -8.4506235,
          6.907529,
          -8.376118,
          -7.776505,
          -7.7078943,
          -7.741218,
          2.0798194,
          1.7336919,
          -7.909102,
          -7.9686103,
          -7.8574085,
          0.9769459,
          -7.432713,
          3.7388787,
          2.0847585,
          -7.6690845,
          -8.765919,
          -8.774918,
          -8.764411,
          -8.785318,
          -8.780546,
          -8.612636,
          -8.704167,
          -8.607233,
          -7.8160686,
          -7.665294,
          -8.209907,
          -7.7649117,
          -6.3135567,
          -6.4016714,
          -7.829277,
          -8.528687,
          -8.419303,
          -8.518541,
          -7.3390336,
          -8.85163,
          -8.750705,
          -8.644748,
          -8.410416,
          -8.609663,
          -8.066667,
          -7.840384,
          -8.567001,
          -8.625127,
          -8.7497015,
          -8.515082,
          -8.47144,
          -8.095477,
          -7.669237,
          -7.7471128,
          1.5692457,
          1.8492026,
          2.474043,
          2.7317586,
          0.48538932,
          1.2337189,
          0.8564856,
          3.637086,
          3.3171222,
          2.359914,
          2.5942006,
          2.5399883,
          0.88676536,
          -7.6210914,
          -8.610272,
          -8.465303,
          -8.120355,
          -8.532807,
          -8.605299,
          -8.617983,
          -8.554936,
          -8.652111,
          -8.824318,
          -8.725976,
          -7.5350084,
          -7.543164,
          -7.6419063,
          -6.9409313,
          -8.195232,
          -8.238547,
          -8.596282,
          -7.6245666,
          -7.6664257,
          -7.598813,
          2.114064,
          2.8877292,
          4.0817547,
          2.7916276,
          1.0119268,
          0.83130956,
          1.3557847,
          1.0937059,
          3.4039056,
          1.30915,
          1.0975637,
          1.0197067,
          0.312686,
          0.21202205,
          2.9957874,
          2.1028786,
          2.5548837,
          2.7692192,
          0.54973245,
          0.4879236,
          0.33896253,
          -0.03154748,
          -0.16840082,
          -0.40559247,
          -0.33059102,
          0.23262426,
          4.153107,
          2.541296,
          -7.910072,
          -8.093861,
          -7.746026,
          -8.7765875,
          -8.8258915,
          -8.952708,
          -7.887215,
          -7.7964735,
          2.5826862,
          2.6449142,
          -7.787397,
          -7.573609,
          0.5261774,
          0.77540755,
          3.162571,
          3.6654165,
          -7.709658,
          0.8473851,
          -8.421285,
          -8.415694,
          -8.527994,
          -8.45029,
          -8.68468,
          -8.421727,
          -7.8063784,
          -8.855891,
          -8.846739,
          -8.820925,
          -8.819722,
          -8.945654,
          -8.866572,
          -8.396973,
          -7.6889224,
          -7.645282,
          1.7013137,
          2.8387585,
          3.3518758,
          -7.787684,
          -7.7762794,
          -2.3972735,
          1.189973,
          3.7233634,
          3.5298128,
          2.3413217,
          -7.6325865,
          -8.476366,
          -8.469391,
          -8.299646,
          -7.665747,
          -7.791707,
          -7.704868,
          -7.799349,
          -7.891402,
          -8.634517,
          -7.3968873,
          2.6570532,
          2.6865327,
          -7.8951764,
          -7.504342,
          -7.570133,
          -7.6283607,
          -7.9354634,
          0.51015574,
          0.84641784,
          3.0556195,
          3.3168354,
          2.9426324,
          2.470694,
          2.6415088,
          1.909694,
          -7.8325396,
          -7.760644,
          -8.588922,
          -8.5865345,
          -7.5734677,
          -8.35114,
          -8.153152,
          -8.242632,
          -8.353587,
          -8.767864,
          -8.470692,
          -8.90347,
          -8.860306,
          -1.8007239,
          -1.8245407,
          -8.808921,
          -7.749617,
          6.6520176,
          -8.304587,
          -8.651495,
          -8.699174,
          -8.73523,
          -8.61737,
          -8.081523,
          -7.75851,
          -8.190249,
          -7.9317355,
          -7.85672,
          -7.6515694,
          -7.657476,
          -8.2326975,
          -8.503484,
          -7.527662,
          2.6243544,
          -7.860142,
          -7.6007247,
          -7.6677685,
          -7.8072367,
          -8.049953,
          -8.608317,
          -8.2739105,
          -8.235463,
          -8.467294,
          0.77118737,
          -7.4407487,
          9.375571,
          2.8330295,
          1.0572679,
          0.8461556,
          3.1696763,
          3.3888047,
          3.2738488,
          -7.779672,
          -7.6174955,
          0.63229597,
          1.2420307,
          2.9869862,
          0.95745337,
          -8.018759,
          -8.477481,
          -7.7240634,
          -7.922528,
          -7.7618604,
          -7.604625,
          -8.293079,
          -9.167959,
          -8.6722145,
          -8.737327,
          -8.72305,
          -6.5063543,
          -7.902575,
          -7.3459883,
          -7.016255,
          -7.8611445,
          -8.586558,
          -8.471868,
          -8.596884,
          -8.681643,
          -8.656486,
          -8.579101,
          -7.8675313,
          -8.510089,
          -8.557971,
          -8.364713,
          -8.3402,
          -8.576718,
          -8.482209,
          -7.5933285,
          -1.3258623,
          -7.4281216,
          -6.779966,
          -2.3367834,
          3.8668056,
          -7.488363,
          -7.4670167,
          2.365052,
          2.6504526,
          0.52291584,
          2.0222182,
          -7.673302,
          0.78801984,
          9.285177,
          3.3221307,
          1.7056892,
          -7.574632,
          -7.8733487,
          -9.032564,
          -9.261623,
          -9.168759,
          -8.997691,
          -8.264707,
          -8.224836,
          6.5619216,
          -8.68625,
          -8.358141,
          -7.90238,
          -7.9955907,
          -7.8371267,
          -7.6362014,
          -7.5844183,
          -7.6476502,
          -7.8936186,
          -9.185296,
          -9.187862,
          -9.243987,
          -9.248275,
          -7.7702117,
          -7.8888497,
          -7.8587513,
          -7.664678,
          -7.3917046,
          -7.3685627,
          -6.8055563,
          6.56627,
          -7.60646,
          -7.70403,
          -7.6244235,
          -7.542784,
          2.690629,
          2.208074,
          -7.608825,
          0.2039368,
          -8.035382,
          -0.20386884,
          -0.42522433,
          -8.755598,
          -0.92074627,
          -8.72281,
          -0.4499987,
          -0.3685743,
          -0.45984328,
          -0.3275663,
          0.41354722,
          1.1549072,
          9.400843,
          2.7431643,
          0.91772485,
          2.8190324,
          3.250591,
          -7.548387,
          -7.647288,
          -0.3080396,
          0.76576513,
          -7.776306,
          -7.9490685,
          -7.502849,
          -8.4169445,
          -8.763924,
          -8.363165,
          -8.73261,
          -8.835183,
          -8.778831,
          -8.771367,
          -8.403124,
          -8.288051,
          -8.275479,
          -7.545428,
          -7.4772696,
          -7.477271,
          -7.475255,
          -7.4349165,
          -7.3588,
          -7.363128,
          -7.5962253,
          -7.4788737,
          -7.7063837,
          -7.8190575,
          -8.531871,
          -7.679305,
          -8.204886,
          -7.987889,
          -8.179612,
          -8.580224,
          -8.406659,
          -7.841255,
          -7.7384872,
          -7.66386,
          -7.6988134,
          -8.579134,
          -8.343344,
          -7.9994,
          -7.8782473,
          -7.6054215,
          -7.707888,
          -7.56882,
          -7.5955443,
          -7.6322727,
          -7.872291,
          -8.163699,
          -7.957917,
          -7.4157596,
          -7.2433767,
          -6.748045,
          -8.378526,
          -8.501942,
          -8.559675,
          -8.20239,
          7.0959926,
          7.179277,
          7.1936603,
          7.187555,
          6.998981,
          -4.1060967,
          -7.693245,
          -7.6787987,
          -7.7133946,
          2.0232165,
          2.598871,
          -2.045745,
          -1.5272242,
          1.4033985,
          3.4690402,
          3.348902,
          -7.5927715,
          2.2943218,
          0.5497519,
          -7.8118987,
          -7.816391,
          -7.5187635,
          -8.57073,
          -8.465815,
          -8.573117,
          -8.652918,
          -8.563234,
          -8.567182,
          -8.577046,
          -8.607143,
          -8.672445,
          -8.5173645,
          -7.6868277,
          -8.468452,
          -8.534513,
          -8.626901,
          -8.652173,
          -7.9044766,
          -7.584868,
          -6.594634,
          -6.865476,
          -6.348573,
          6.7329197
         ],
         "xaxis": "x",
         "y": [
          6.9565473,
          0.626668,
          6.2468934,
          6.1404834,
          6.230611,
          -4.1483665,
          5.9420767,
          5.358442,
          5.238543,
          5.245593,
          0.34382176,
          0.2626715,
          5.814595,
          5.7172885,
          5.5657234,
          0.88731885,
          5.083897,
          -0.36967203,
          0.013754924,
          5.294267,
          6.7296906,
          6.7265854,
          6.7872963,
          6.819012,
          6.587865,
          6.417279,
          6.6812515,
          6.6565304,
          5.693181,
          5.480125,
          6.37959,
          5.659077,
          1.3867277,
          2.4309564,
          5.138009,
          6.8786736,
          6.6417904,
          6.8682303,
          5.1852117,
          6.6795073,
          6.6932974,
          6.5254707,
          6.2044296,
          6.482327,
          6.078107,
          6.0024314,
          6.478581,
          6.6426134,
          6.4795804,
          6.91692,
          6.811754,
          5.62623,
          5.6193256,
          5.72806,
          0.9107328,
          0.8261651,
          0.002932633,
          -1.3512529,
          1.2703643,
          0.84899646,
          1.1745431,
          0.36815682,
          0.2091454,
          0.388034,
          -0.8476716,
          -1.5020796,
          0.813863,
          5.4528117,
          5.2228785,
          5.0179563,
          5.876631,
          6.8182535,
          6.8663297,
          6.8576527,
          6.8090243,
          6.79808,
          6.916927,
          6.579969,
          5.4313803,
          5.2245264,
          4.928804,
          4.4790235,
          5.9670563,
          6.1077466,
          5.9347224,
          5.6452174,
          5.6377773,
          5.659531,
          0.9827468,
          1.2284176,
          -0.56766415,
          -0.013283416,
          -0.89869326,
          -0.96652323,
          -0.51971704,
          -0.8433629,
          -1.3335818,
          0.0059387763,
          -0.8732708,
          -0.6912048,
          -2.1144683,
          -1.9897132,
          0.104932144,
          -0.39291307,
          -0.09674367,
          -1.2013788,
          -0.5588132,
          -0.051594347,
          -0.53154486,
          -1.4682735,
          -1.1719285,
          -1.0902948,
          -1.7706008,
          -1.2356826,
          -0.43002102,
          0.57310706,
          5.8824973,
          6.09287,
          6.1708484,
          6.6999407,
          6.6725845,
          6.454588,
          5.8718514,
          5.708819,
          0.9373202,
          0.88064224,
          5.9748383,
          6.075769,
          -0.19303297,
          -0.63846767,
          0.5906316,
          0.08950509,
          5.7191133,
          -0.8549502,
          6.2372794,
          6.2931843,
          6.1505413,
          6.5379243,
          6.5905247,
          6.432337,
          5.5496445,
          6.652099,
          6.621256,
          6.6102552,
          6.539677,
          6.4311843,
          6.423818,
          6.0670843,
          5.571435,
          5.480377,
          0.8449762,
          -0.36805382,
          -1.2972256,
          5.8540845,
          5.8760242,
          2.0060368,
          1.0451982,
          0.45944974,
          0.011390699,
          -0.19937302,
          5.3896704,
          6.352323,
          6.534999,
          6.1449585,
          5.5525436,
          5.7946057,
          5.82203,
          5.7942314,
          5.8283043,
          6.5328054,
          5.6208496,
          0.8158798,
          0.7811206,
          5.916263,
          6.173372,
          6.2042656,
          6.189056,
          5.9582157,
          0.23917064,
          -0.42014435,
          0.4997007,
          0.22293694,
          -0.024652539,
          0.10821414,
          0.5579297,
          -0.27598962,
          5.813039,
          5.6550975,
          6.8197703,
          6.936965,
          5.3741684,
          6.2751713,
          6.0286026,
          6.04391,
          6.204808,
          6.7323713,
          6.244363,
          6.57008,
          6.3059044,
          7.80546,
          7.780774,
          6.5039806,
          5.6632724,
          -3.2804532,
          6.102906,
          6.330685,
          6.4132104,
          6.6393485,
          6.5384774,
          5.8511806,
          5.5349193,
          5.865938,
          5.9749246,
          5.923547,
          5.7592173,
          5.724522,
          6.235907,
          6.7008796,
          5.701596,
          0.8804146,
          6.027354,
          6.1760254,
          6.3841524,
          6.200723,
          6.4559364,
          6.504326,
          6.371096,
          6.412705,
          6.687733,
          -0.8361203,
          5.832608,
          -4.195276,
          -0.109466575,
          -1.0177847,
          -2.5436919,
          -0.57130975,
          0.5008,
          0.24000178,
          6.0759497,
          6.2927394,
          -0.80605316,
          0.14392045,
          0.06512476,
          -0.6663058,
          6.086426,
          6.8329115,
          5.6506796,
          5.976832,
          5.628013,
          5.294844,
          6.262993,
          6.2451644,
          6.7157745,
          6.7960157,
          6.8400292,
          -3.3485825,
          5.7744684,
          4.4240856,
          3.8502574,
          5.6505456,
          6.696866,
          6.395154,
          6.620206,
          6.6884847,
          6.7615547,
          6.823411,
          5.8096023,
          6.341912,
          6.408819,
          6.1472564,
          5.994914,
          6.430875,
          6.213132,
          5.583354,
          2.710243,
          5.504216,
          4.3856153,
          2.3498166,
          -0.47963896,
          4.8998437,
          5.5887375,
          0.43811417,
          0.03537426,
          0.043582726,
          -0.07043711,
          5.663861,
          0.55100936,
          -4.3255057,
          0.43398702,
          0.0053165765,
          5.622207,
          5.801115,
          6.2698,
          6.3038855,
          6.2663684,
          6.0427384,
          6.223222,
          6.065636,
          -4.520258,
          6.298558,
          5.9737554,
          5.69684,
          5.701142,
          5.4944525,
          5.6265798,
          5.8416533,
          5.4333487,
          5.7710977,
          6.298482,
          6.2973685,
          6.2370157,
          6.2631526,
          5.5212817,
          5.884023,
          5.8360095,
          5.4474535,
          5.0497894,
          4.791744,
          3.3525126,
          -3.6365287,
          5.70778,
          5.8302016,
          5.6835985,
          5.646828,
          1.266927,
          0.5433355,
          6.217402,
          -3.7092721,
          6.071807,
          -1.4695013,
          -2.1515388,
          6.409006,
          -0.9702567,
          6.8666,
          -2.359833,
          -1.3046507,
          -1.7045035,
          -2.7105136,
          -0.15828297,
          -0.8147912,
          -4.1742826,
          -0.012023958,
          -1.8549049,
          -0.35376614,
          0.3538562,
          6.1429434,
          6.0693603,
          -1.507659,
          -0.111773364,
          5.6364937,
          5.6779804,
          5.061656,
          6.393355,
          6.756951,
          6.2541013,
          6.685017,
          6.623088,
          6.695437,
          6.7912154,
          6.257853,
          6.2134166,
          5.70969,
          5.423716,
          5.2683945,
          5.599005,
          5.52338,
          5.699164,
          4.4756827,
          6.1284194,
          6.2695217,
          5.688026,
          5.562428,
          5.730225,
          6.1620135,
          5.559839,
          6.027561,
          5.8424163,
          6.082786,
          5.8923373,
          5.8046093,
          5.740876,
          5.453632,
          5.484983,
          5.794133,
          6.467266,
          6.234862,
          5.9884295,
          6.119571,
          5.505617,
          5.7780294,
          5.476854,
          5.5725203,
          5.608592,
          5.6131926,
          6.0181174,
          5.5819664,
          4.752496,
          4.101387,
          3.6605089,
          6.472411,
          6.840975,
          6.973474,
          6.4822736,
          -1.3093659,
          -1.5542874,
          -1.4315243,
          -1.4567933,
          -1.1779659,
          2.027052,
          5.7851896,
          5.8128324,
          5.8754573,
          -0.000093711526,
          0.79597545,
          3.2693133,
          2.5014706,
          -0.62328976,
          0.5147855,
          0.10813025,
          5.744634,
          0.43868765,
          -1.4986345,
          5.902629,
          5.7302375,
          5.2335544,
          6.917496,
          6.9464245,
          6.8905387,
          6.9356155,
          6.950717,
          6.9205375,
          6.834743,
          6.857816,
          6.823346,
          6.6458926,
          5.479457,
          6.3321767,
          6.360226,
          6.630095,
          6.757067,
          5.669786,
          5.2811975,
          3.3442688,
          3.9116495,
          2.952191,
          -3.691188
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "### Onnxruntime Training\n\nThe following example fine-tunes a T5 large model on the wmt16 dataset.\n\n`..."
          ],
          [
           "```\n\n### Performance\n\nWe get the following results for [t5-large](https://huggingface.co/t5-large) m..."
          ],
          [
           "Inference pipelines with the ONNX Runtime accelerator\n\nThe [`~pipelines.pipeline`] function makes it..."
          ],
          [
           "```\n\n_Note: The default models used in the [`~pipelines.pipeline`] function are not optimized for in..."
          ],
          [
           "```\n\nIt is also possible to load it with the `from_pretrained(model_name_or_path, export=True)`\nmeth..."
          ],
          [
           "```\n\nIt is also possible to load it with the `from_pretrained(model_name_or_path)`\nmethod associated..."
          ],
          [
           "```\n\n\n## Optimizing and quantizing in pipelines\n\nThe [`~pipelines.pipeline`] function can not only r..."
          ],
          [
           ">>> # Load the quantized model from a local repository\n>>> model = ORTModelForSequenceClassification..."
          ],
          [
           "```\n\n### Optimizing with `ORTOptimizer`\n\n```python\n>>> from transformers import AutoTokenizer\n>>> fr..."
          ],
          [
           "# Save and push the model to the hub\n>>> tokenizer.save_pretrained(\"new_path_for_directory\")  # doct..."
          ],
          [
           "Quantization\n\n## AutoGPTQ Integration\n\nðŸ¤— Optimum collaborated with [AutoGPTQ library](https://github..."
          ],
          [
           "- Install latest `accelerate` library:\n`pip install --upgrade accelerate`\n\n### Load and quantize a m..."
          ],
          [
           "```\n\n<Tip warning={true}>\nGPTQ quantization only works for text model for now. Futhermore, the quant..."
          ],
          [
           "```\n\n### Exllama kernels for faster inference\n\nWith the release of exllamav2 kernels, you can get fa..."
          ],
          [
           "```\n\nNote that only 4-bit models are supported with exllama/exllamav2 kernels for now. Furthermore, ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "### ORTModelForMultipleChoice\n\n[[autodoc]] onnxruntime.ORTModelForMultipleChoice\n\n### ORTModelForQue..."
          ],
          [
           "### ORTModelForVision2Seq\n\n[[autodoc]] onnxruntime.ORTModelForVision2Seq\n    - forward\n\n### ORTModel..."
          ],
          [
           "#### ORTLatentConsistencyModelPipeline\n\n[[autodoc]] onnxruntime.ORTLatentConsistencyModelPipeline\n  ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "| Notebook                                                                                          ..."
          ],
          [
           "|:--------------------------------------------------------------------------------------------------..."
          ],
          [
           "---------------------------------------------------------------------------------------------|:-----..."
          ],
          [
           "-|:-------------------------------------------------------------------------------------------------..."
          ],
          [
           "-----------------------------------------------------------------------------------------|:---------..."
          ],
          [
           "------------------|---------------------------------------------------------------------------------..."
          ],
          [
           "-------------------------------------:|..."
          ],
          [
           "| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github...."
          ],
          [
           "## Optimum Intel\n\n### OpenVINO..."
          ],
          [
           "| Notebook                                                                                          ..."
          ],
          [
           "| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebo..."
          ],
          [
           "| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-inte..."
          ],
          [
           "| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https:..."
          ],
          [
           "### Neural Compressor..."
          ],
          [
           "| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/..."
          ],
          [
           "## Optimum ONNX Runtime..."
          ],
          [
           "| Notebook                                                                                          ..."
          ],
          [
           "----------------------------------------------------------------------------------|:----------------..."
          ],
          [
           "-------------------------------------------|:-------------------------------------------------------..."
          ],
          [
           "----------------------------------------------------------------|-----------------------------------..."
          ],
          [
           "-----------------------------------------------------------------------------------:|..."
          ],
          [
           "| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface..."
          ],
          [
           "| [How to fine-tune a model for text classification with ONNX Runtime](https://github.com/huggingfac..."
          ],
          [
           "| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https://github.com/huggingface..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\n__Note__\n> *To enable ONNX Runtime training, your devices need to be equipped with GPU. Install..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nLet's see now how we can apply dynamic quantization with ONNX Runtime:\n\n```python\n>>> from opti..."
          ],
          [
           "```\n\nStatic quantization relies on feeding batches of data through the model to estimate the activat..."
          ],
          [
           "# Define the processing function to apply to each example after loading the dataset\n>>> def preproce..."
          ],
          [
           "```\n\nAs a final example, let's take a look at applying _graph optimizations_ techniques such as oper..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nYou can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inf..."
          ],
          [
           "```\n\nLet's see now how we can apply dynamic quantization with ONNX Runtime:\n\n```python\n>>> from opti..."
          ],
          [
           "```\n\nYou can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxrunti..."
          ],
          [
           "```\n\nYou can find more examples in the [documentation](https://huggingface.co/docs/optimum/habana/qu..."
          ],
          [
           "```\n\nCheck out the help for more options:\n\n```bash\noptimum-cli export onnx --help\n```\n\nCheck out the..."
          ],
          [
           "```\n\nCheck out the [documentation](https://huggingface.co/docs/optimum/bettertransformer/overview) f..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Ve..."
          ],
          [
           "```\n\n\n__Note__\n> *To enable ONNX Runtime training, your devices need to be equipped with GPU. Instal..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3..."
          ],
          [
           "<p class=\"text-gray-700\">Optimize your model to speedup inference with <span class=\"underline\" oncli..."
          ],
          [
           "</a>\n    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" ..."
          ],
          [
           "<p class=\"text-gray-700\">Enable performance optimizations for <span class=\"underline\" onclick=\"event..."
          ],
          [
           "<p class=\"text-gray-700\">Apply quantization and graph optimization to accelerate Transformers models..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\nLicensed under the Apache License, Ve..."
          ],
          [
           "```\n\n\n__Note__\n> *To enable ONNX Runtime training, your devices need to be equipped with GPU. Instal..."
          ],
          [
           "**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attri..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Step 2: Set your model on your preferred device\n\nIf you did not used `device_map=\"auto\"` to ..."
          ],
          [
           "```\nIf you want to run a pipeline on a GPU device, run:\n```python\n>>> from optimum.pipelines import ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nAfter implementing it, your transformation can be used as a regular function:\n\n```python\n>>> fr..."
          ],
          [
           "```\n\n### Composing transformations together\n\nAs applying multiple transformations in chain is needed..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Register commands in the Optimum CLI from a subpackage\n\nIt is possible to register a command in the ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Quantizing a model to be used with Optimum's CLI\n\nThe Optimum ONNX Runtime quantization tool can ..."
          ],
          [
           "```\n\nQuantizing an ONNX model can be done as follows:\n\n```bash\n optimum-cli onnxruntime quantize --o..."
          ],
          [
           "```\n\n\n## Apply Dynamic Quantization\n\nThe [`~optimum.onnxruntime.ORTQuantizer`] class can be used to ..."
          ],
          [
           "```\n\n## Static Quantization example\n\nThe [`~optimum.onnxruntime.ORTQuantizer`] class can be used to ..."
          ],
          [
           "# Create the calibration dataset\n>>> def preprocess_fn(ex, tokenizer):\n...     return tokenizer(ex[\"..."
          ],
          [
           "```\n\n## Quantize Seq2Seq models\n\nThe [`~optimum.onnxruntime.ORTQuantizer`] class currently doesn't s..."
          ],
          [
           "```\n\n3. Quantize all models\n\n```python\n# Define the quantization strategy by creating the appropriat..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Stable Diffusion Text-to-Image Fine-Tuning\n\nThis example shows how to leverage ONNX Runtime Training..."
          ],
          [
           "```\n\nAnd initialize an [ðŸ¤—Accelerate](https://github.com/huggingface/accelerate/) environment with:\n\n..."
          ],
          [
           "```\n<!-- accelerate_snippet_end -->\n\n\nTo run on your own training files prepare the dataset accordin..."
          ],
          [
           "```\n\n#### Training with multiple GPUs\n\n`accelerate` allows for seamless multi-GPU training. Follow t..."
          ],
          [
           "Overview\n\nðŸ¤— Optimum provides an integration with Torch FX, a library for PyTorch that allows develop..."
          ],
          [
           "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./p..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThe Optimum TFLite export can be used through Optimum command-line. As only static input shapes..."
          ],
          [
           "Required arguments:\n  -m MODEL, --model MODEL\n                        Model ID on huggingface.co or ..."
          ],
          [
           "Input shapes:\n  --batch_size BATCH_SIZE\n                        Batch size that the TFLite exported ..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "Accelerated inference on NVIDIA GPUs\n\nBy default, ONNX Runtime runs inference on CPU devices. Howeve..."
          ],
          [
           "```\n\nTo avoid conflicts between `onnxruntime` and `onnxruntime-gpu`, make sure the package `onnxrunt..."
          ],
          [
           "```\nValueError: Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the av..."
          ],
          [
           "```\n\nAdditionally, you can pass the session option `log_severity_level = 0` (verbose), to check whet..."
          ],
          [
           "```\n\nIn this example, we can see that all the costly MatMul operations are placed on the CUDA execut..."
          ],
          [
           "### Reduce memory footprint with IOBinding\n\n[IOBinding](https://onnxruntime.ai/docs/api/python/api_s..."
          ],
          [
           "```\n\nFor the time being, IOBinding is supported for task-defined ORT models, if you want us to add s..."
          ],
          [
           "<table><tr>\n<td>\n  <p align=\"center\">\n    <img alt=\"GPT2\" src=\"https://huggingface.co/datasets/optim..."
          ],
          [
           "And here is a summary for the saving time with different sequence lengths (32 / 128) and generation ..."
          ],
          [
           "```\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440..."
          ],
          [
           "- Platform: Linux-5.4.0-1089-aws-x86_64-with-glibc2.29\n- Python version: 3.8.10\n- `transformers` ver..."
          ],
          [
           "```\n\nNote that previous experiments are run with __vanilla ONNX__ models exported directly from the ..."
          ],
          [
           "```\n\n### Checking the TensorRT installation is successful\n\nBefore going further, run the following s..."
          ],
          [
           "```\n\nsomething is wrong with the TensorRT or ONNX Runtime installation.\n\n### TensorRT engine build a..."
          ],
          [
           "```\n\nTensorRT builds its engine depending on specified input shapes. Unfortunately, in the [current ..."
          ],
          [
           "Passing the engine cache path in the provider options, the engine can therefore be built once for al..."
          ],
          [
           "```\n\nThe engine is stored as:\n\n![TensorRT engine cache folder](https://huggingface.co/datasets/optim..."
          ],
          [
           "```\n\n#### Warmup\n\nOnce the engine is built, it is recommended to do before inference **one or a few ..."
          ],
          [
           "```\n\nThe model can then be used with the common ðŸ¤— Transformers API for inference and evaluation, suc..."
          ],
          [
           "```\n\nUsing this `qconfig`, static quantization can be performed as explained in the [static quantiza..."
          ],
          [
           ">>> res = ort_model(**inp)\n\n>>> print(res)\n>>> print(ort_model.config.id2label[res.logits[0].argmax(..."
          ],
          [
           "```\n\nThe model can then be used with the common ðŸ¤— Transformers API for inference and evaluation, suc..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## Base classes\n\n[[autodoc]] exporters.onnx.OnnxConfig\n    - inputs\n    - outputs\n    - generate_dum..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nIf you'd like to use the accelerator-specific features of ðŸ¤— Optimum, you can install the requir..."
          ],
          [
           "The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the..."
          ],
          [
           "```\n\nFor the accelerator-specific features, you can install them by appending `optimum[accelerator_t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Supported architectures from [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index):\n\n- AS..."
          ],
          [
           "Supported architectures from [ðŸ¤— Diffusers](https://huggingface.co/docs/diffusers/index):\n- Stable Di..."
          ],
          [
           "Optimum Inference with ONNX Runtime\n\nOptimum is a utility package for building and running inference..."
          ],
          [
           "```\n\n### Loading a vanilla Transformers model\n\nBecause the model you want to work with might not be ..."
          ],
          [
           "```\n\n## Sequence-to-sequence models\n\nSequence-to-sequence (Seq2Seq) models can also be used when run..."
          ],
          [
           "```\n\n### Text-to-Image\n\nHere is an example of how you can load an ONNX Stable Diffusion model and ru..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/optimum/documen..."
          ],
          [
           "```\n\n### Inpaint\n\n```python\nimport PIL\nimport requests\nimport torch\nfrom io import BytesIO\nfrom opti..."
          ],
          [
           "```\n\n### Text-to-Image\n\nHere is an example of how you can load a SDXL ONNX model from [stabilityai/s..."
          ],
          [
           "```\n\n\n### Refining the image output\n\nThe image can be refined by making use of a model like [stabili..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\noptimum-cli export onnx --model gpt2 --optimize O3 gpt2_onnx/\n```\n\nThe optimization levels are:\n..."
          ],
          [
           "```\n\n\n### Optimization Configuration\n\nThe [`~onnxruntime.configuration.OptimizationConfig`] class al..."
          ],
          [
           "While [`~onnxruntime.configuration.OptimizationConfig`] gives you full control on how to do optimiza..."
          ],
          [
           "```\n\nYou can also specify custom argument that were not defined in the O2 configuration, for instanc..."
          ],
          [
           "```\n\n\nBelow you will find an easy end-to-end example on how to optimize a Seq2Seq model [sshleifer/d..."
          ],
          [
           "```\n\n## Optimizing a model with Optimum CLI\n\nThe Optimum ONNX Runtime optimization tools can be used..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## How to convert a model into its `BetterTransformer` format?\n\n### Step 1: Identifying the source l..."
          ],
          [
           ">>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\n>>> print(model)  # doctest: +IGNORE_RESU..."
          ],
          [
           "```\n\nYou can clearly see that the layers that need to be replaced are the `BertLayer` modules since ..."
          ],
          [
           "```\n\nNow, make sure to fill all the necessary attributes, the list of attributes are:\n\n- `in_proj_we..."
          ],
          [
           "```\n\n\n### Step 3: Building the forward pass\n\nFirst of all, start with the line `super().forward_chec..."
          ],
          [
           "```\n\nOnce the `hidden_states` are nested, call `torch._transformer_encoder_layer_fwd` using the righ..."
          ],
          [
           "```\nMODEL_MAPPING = {\n  ...\n  \"bert\": (\"BertLayer\", BertLayerBetterTransformer),\n  ...\n}\n```\n\nTry it..."
          ],
          [
           "Symbolic tracer\n\nIn Torch FX, the symbolic tracer feeds dummy values through the code to record the ..."
          ],
          [
           "Accelerated inference on AMD GPUs supported by ROCm\n\nBy default, ONNX Runtime runs inference on CPU ..."
          ],
          [
           "```\n\n**Local Installation Steps:**\n\n##### 2.1 PyTorch with ROCm Support\nOptimum ONNX Runtime integra..."
          ],
          [
           "```\n\n<Tip>\nTo avoid conflicts between `onnxruntime` and `onnxruntime-rocm`, make sure the package `o..."
          ],
          [
           "```\nValueError: Asked to use ROCMExecutionProvider as an ONNX Runtime execution provider, but the av..."
          ],
          [
           "```\n\nAdditionally, you can pass the session option `log_severity_level = 0` (verbose), to check whet..."
          ],
          [
           "BetterTransformer benchmark\n\nPlease refer to https://medium.com/pytorch/bettertransformer-out-of-the..."
          ],
          [
           "# using bitsandbytes fp4/fp16 scheme\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model meta-ll..."
          ],
          [
           "```\n\nHere are results obtained on a single NVIDIA A100-SXM4-80GB GPU. We use a prompt length of 512,..."
          ],
          [
           "Bitsandbytes uses the fp4 scheme, with the compute in fp16.\n\n### Batch size = 1\n\n|quantization |act_..."
          ],
          [
           "### Batch size = 2\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency ..."
          ],
          [
           "### Batch size = 4\n\n|quantization |act_order|bits|group_size|kernel           |Load time (s)|Per-tok..."
          ],
          [
           "### Batch size = 8\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency ..."
          ],
          [
           "### Batch size = 16\n\n|quantization |act_order|bits|group_size|kernel|Load time (s)|Per-token latency..."
          ],
          [
           "# GPTQ with exllamav2 kernel (int4/fp16)\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model The..."
          ],
          [
           "```\n\nThe benchmark below is for a prompt length of 512, measuring only the prefill step on a single ..."
          ],
          [
           "### Batch size = 2\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok..."
          ],
          [
           "### Batch size = 4\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok..."
          ],
          [
           "### Batch size = 8\n\n|quantization |act_order|bits|group_size|kernel           |prompt_length|new_tok..."
          ],
          [
           "### Batch size = 16\n\n|quantization |act_order|bits|group_size|kernel    |prompt_length|new_tokens|Lo..."
          ],
          [
           "# GPTQ with exllamav2 kernel (int4/fp16)\nCUDA_VISIBLE_DEVICES=0 python benchmark_gptq.py --model The..."
          ],
          [
           "```\n\n| quantization | act_order | bits | group_size | kernel           | perplexity |\n|-------------..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "In the 2.0 version, PyTorch includes a native scaled dot-product attention operator (SDPA) as part o..."
          ],
          [
           "In inference mode, the padding mask is kept for correctness and thus speedups should be expected onl..."
          ],
          [
           "- [AlBERT](https://arxiv.org/abs/1909.11942)\n- [Bark](https://github.com/suno-ai/bark)\n- [BART](http..."
          ],
          [
           "- [Ernie](https://arxiv.org/abs/1904.09223)\n- [Falcon](https://arxiv.org/abs/2306.01116) (No need to..."
          ],
          [
           "- [LayoutLM](https://arxiv.org/abs/1912.13318)\n- [Llama & Llama2](https://arxiv.org/abs/2302.13971) ..."
          ],
          [
           "- [ViLT](https://arxiv.org/abs/2102.03334)\n- [ViT](https://arxiv.org/abs/2010.11929)\n- [ViT-MAE](htt..."
          ],
          [
           "Let us know by opening an issue in ðŸ¤— Optimum if you want more models to be supported, or check out t..."
          ],
          [
           "```\nYou can leave `keep_original_model=False` in case you want to overwrite the current model with i..."
          ],
          [
           "ONNX ðŸ¤ ONNX Runtime\n\nONNX is an open standard that defines a common set of operators and a common fi..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "It is possible to know which tasks are supported for a model for a given backend, by doing:\n\n```pyth..."
          ],
          [
           "```\n\n</Tip>\n\n### PyTorch\n\n| Task                                 | Auto Class                       ..."
          ],
          [
           "### TensorFlow\n\n| Task                                 | Auto Class                             |\n|-..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "### CalibrationConfig\n\n[[autodoc]] onnxruntime.configuration.CalibrationConfig\n\n## ORTConfig\n\n[[auto..."
          ],
          [
           "# How to contribute to Optimum?\n\nOptimum is an open source project, so all contributions and suggest..."
          ],
          [
           "```\n\n\t**do not** work on the `main` branch.\n\n4. Set up a development environment by running the foll..."
          ],
          [
           "Overview\n\nðŸ¤— Optimum provides an integration with ONNX Runtime, a cross-platform, high performance en..."
          ],
          [
           "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./p..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```bash\ntorchrun --nproc_per_node=NUM_GPUS_YOU_HAVE run_classification.py \\\n    --model_name_or_path..."
          ],
          [
           "```\n\n### Performance\n\nWe get the following results for [meta-llama/Llama-2-7b-hf](https://huggingfac..."
          ],
          [
           "#### DeepSpeed\n\n[zero_stage_2.json](https://github.com/huggingface/optimum/blob/main/examples/onnxru..."
          ],
          [
           "```\n\n### Performance\n\nWe get the following results for [roberta-base](https://huggingface.co/roberta..."
          ],
          [
           "> *The inference will use PyTorch by default, if you want to use ONNX Runtime backend instead, add t..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[`ORTTrainer`] and [`ORTSeq2SeqTrainer`] APIs make it easy to compose __[ONNX Runtime (ORT)](https:/..."
          ],
          [
           "Test it out to achieve __lower latency, higher throughput, and larger maximum batch size__ while tra..."
          ],
          [
           "```\nPyTorch: 1.14.0.dev20221103+cu116; ORT: 1.14.0.dev20221103001+cu116; DeepSpeed: 0.6.6; HuggingFa..."
          ],
          [
           "```\n\n* If you want to install the dependencies beyond in a local Python environment. You can pip ins..."
          ],
          [
           "```\n\n* If you want to install the dependencies beyond in a local Python environment. You can pip ins..."
          ],
          [
           "```\n\nOr install from source:\n\n```bash\npip install git+https://github.com/huggingface/optimum.git\n```..."
          ],
          [
           "```\n\nCheck out more detailed [example scripts](https://github.com/huggingface/optimum/tree/main/exam..."
          ],
          [
           "```\n\nCheck out more detailed [example scripts](https://github.com/huggingface/optimum/tree/main/exam..."
          ],
          [
           "```\n\n\n<Tip warning={false}>\n\nDeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the ..."
          ],
          [
           "```\n\n<Tip warning={false}>\n\nDeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the m..."
          ],
          [
           "```\n\n## Other Resources\n\n* Blog posts\n    * [Optimum + ONNX Runtime: Easier, Faster training for you..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "### Onnx Runtime Training\n\nThe following example fine-tunes a BERT on the SQuAD 1.0 dataset.\n\n```bas..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\ngit clone https://github.com/huggingface/optimum-habana.git\ncd optimum-habana\nmake doc BUILD_DIR..."
          ],
          [
           "```\nSections that were moved:\n\n[ <a href=\"#section-b\">Section A</a><a id=\"section-a\"></a> ]\n```\nand ..."
          ],
          [
           "```\n\nUse the relative style to link to the new file so that the versioned docs\ncontinue to work.\n\nFo..."
          ],
          [
           "The same works for methods so you can either use \\[\\`XXXClass.method\\`\\] or\n\\[~\\`XXXClass.method\\`\\]..."
          ],
          [
           "```\n    Args:\n        n_layers (`int`): The number of layers of the model.\n```\n\nIf the description i..."
          ],
          [
           "```\n```\n# first line of code\n# second line\n# etc\n```\n````\n\nWe follow the [doctest](https://docs.pyth..."
          ],
          [
           "```\n\n## Adding an image\n\nDue to the rapidly growing repository, it is important to make sure that no..."
          ],
          [
           "```\n\nThe docstring should give a minimal, clear example of how the respective model \nis to be used i..."
          ],
          [
           "# Need node to build doc HTML. Taken from https://stackoverflow.com/a/67491580\nRUN apt-get update &&..."
          ],
          [
           "```\n\nThe main thing to note here is the need to install Node in the Docker image -\nthat's because we..."
          ],
          [
           "```\n# Add this\n- uses: actions/checkout@v2\nwith:\n    repository: 'huggingface/optimum-habana'\n    pa..."
          ],
          [
           "![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml/badge...."
          ],
          [
           "```\n\nIf you'd like to use the accelerator-specific features of ðŸ¤— Optimum, you can install the requir..."
          ],
          [
           "If you'd like to use the accelerator-specific features of ðŸ¤— Optimum, you can install the required de..."
          ],
          [
           "The `--upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the..."
          ],
          [
           "```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:..."
          ],
          [
           "```\n\n## Accelerated Inference\n\nðŸ¤— Optimum provides multiple tools to export and run optimized models ..."
          ],
          [
           "### Features summary\n\n| Features                           | [ONNX Runtime](https://huggingface.co/d..."
          ],
          [
           "### OpenVINO\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\n..."
          ],
          [
           "```\n\nIt is possible to export ðŸ¤— Transformers and Diffusers models to the OpenVINO format easily:\n\n``..."
          ],
          [
           "```\n\nYou can find more examples in the [documentation](https://huggingface.co/docs/optimum/intel/inf..."
          ],
          [
           "```\n\nThe model can then be quantized using `onnxruntime`:\n\n```bash\noptimum-cli onnxruntime quantize ..."
          ],
          [
           "```\n\nThese commands will export `deepset/roberta-base-squad2` and perform [O2 graph optimization](ht..."
          ],
          [
           "```\n\nMore details on how to run ONNX models with `ORTModelForXXX` classes [here](https://huggingface..."
          ],
          [
           "```\n\n```diff\n- from transformers import Trainer, TrainingArguments\n+ from optimum.habana import Gaud..."
          ],
          [
           "```\n\nYou can find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxrunti..."
          ],
          [
           "Quantization\n\nQuantization is a technique to reduce the computational and memory costs of running in..."
          ],
          [
           "```\nC = A + B\n```\n\nHere the result is much bigger than the biggest representable value in `int8`, wh..."
          ],
          [
           "```\nx = S * (x_q - Z)\n```\n\nwhere:\n\n- `x_q` is the quantized `int8` value associated to `x`\n- `S` and..."
          ],
          [
           "```\n\n<Tip>\n\nUsually `round(a/S + Z)` corresponds to the smallest representable value in the consider..."
          ],
          [
           "### Per-tensor and per-channel quantization\n\nDepending on the accuracy / latency trade-off you are t..."
          ],
          [
           "1. Post training **dynamic quantization**: the range for each activation is computed on the fly at *..."
          ],
          [
           "For both post training static quantization and quantization aware training, it is necessary to defin..."
          ],
          [
           "### Pratical steps to follow to quantize a model to `int8`\n\nTo effectively quantize a model to `int8..."
          ],
          [
           "## Supported tools to perform quantization in ðŸ¤— Optimum\n\nðŸ¤— Optimum provides APIs to perform quantiza..."
          ],
          [
           "#### Integer representation\n\nIntegers are usually represented with the following bit lengths: `8`, `..."
          ],
          [
           "```\n19 = 0 x 2^7 + 0 x 2^6 + 0 x 2^5 + 1 x 2^4 + 0 x 2^3 + 0 x 2^2 + 1 x 2^1 + 1 x 2^0\n```\n\n2. Signe..."
          ],
          [
           "```\nx = sign x mantissa x (2^exponent)\n```\n\n\n## References\n\n- The\n[Quantization and Training of Neur..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nCheck out the help for more options:\n\n```bash\noptimum-cli export onnx --help\n```\n\n## Why use ON..."
          ],
          [
           "```\n\nThe Optimum ONNX export can be used through Optimum command-line:\n\n```bash\noptimum-cli export o..."
          ],
          [
           "Optional arguments:\n  --task TASK           The task to export the model for. If not specified, the ..."
          ],
          [
           "This is needed by some models, for some tasks. If not provided, will attempt to use the tokenizer to..."
          ],
          [
           "```\n\nExporting a checkpoint can be done as follows:\n\n```bash\noptimum-cli export onnx --model distilb..."
          ],
          [
           "```\n\nNote that providing the `--task` argument for a model on the Hub will disable the automatic tas..."
          ],
          [
           "```\n\nPrinting the outputs would give that:\n\n```bash\nQuestionAnsweringModelOutput(loss=None, start_lo..."
          ],
          [
           "```\n\nFor more information, check the `optimum.onnxruntime` documentation [page on this topic](/onnxr..."
          ],
          [
           "```\n\n### Exporting a model to be used with Optimum's ORTModel\n\nModels exported through `optimum-cli ..."
          ],
          [
           "```\n\nand\n\n```python\n>>> from transformers import AutoTokenizer\n>>> from optimum.onnxruntime import O..."
          ],
          [
           "```\n\nYou can then pass one of these tasks to the `--task` argument in the `optimum-cli export onnx` ..."
          ],
          [
           "from optimum.exporters.onnx.base import ConfigBehavior\nfrom typing import Dict\n\nclass CustomWhisperO..."
          ],
          [
           "custom_whisper_onnx_config = CustomWhisperOnnxConfig(\n        config=config,\n        task=\"automatic..."
          ],
          [
           "```\n\nFor tasks that require only a single ONNX file (e.g. encoder-only), an exported model with cust..."
          ],
          [
           "class MPTDummyPastKeyValuesGenerator(DummyPastKeyValuesGenerator):\n    \"\"\"\n    MPT swaps the two las..."
          ],
          [
           "def add_past_key_values(self, inputs_or_outputs: Dict[str, Dict[int, str]], direction: str):\n       ..."
          ],
          [
           "custom_onnx_configs = {\n    \"decoder_model\": onnx_config,\n    \"decoder_with_past_model\": onnx_config..."
          ],
          [
           "```\n\nMoreover, the advanced argument `fn_get_submodels` to `main_export` allows to customize how the..."
          ],
          [
           "Helpful tips for testing & debugging optimum\n\n## VSCODE\n\nIf you are using vscode you might have hard..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "</Tip>\n\n\nWhen inheriting from a middle-end class, look for the one handling the same modality / cate..."
          ],
          [
           "```\n\nFirst let's explain what `TextEncoderOnnxConfig` is all about. While most of the features are a..."
          ],
          [
           "Once you have implemented an ONNX configuration, you can instantiate it by providing the base model'..."
          ],
          [
           "```\n\nThe resulting object has several useful properties. For example, you can view the ONNX\noperator..."
          ],
          [
           "```\n\n<Tip>\n\nCheck out [`BartOnnxConfig`] for an advanced example.\n\n</Tip>\n\n\n## Registering the ONNX ..."
          ],
          [
           "```\n\n## Exporting the model\n\nOnce you have implemented the ONNX configuration, the next step is to e..."
          ],
          [
           "```\n\n<Tip>\n\nIf your model is larger than 2GB, you will see that many additional files are created du..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\n### Performance\n\nWe get the following results for [bert-large-cased](https://huggingface.co/ber..."
          ]
         ],
         "hovertemplate": "source=optimum<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "optimum, circle",
         "marker": {
          "color": "#ab63fa",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "optimum, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          5.205289,
          5.9164047,
          -3.5227432,
          -3.984031,
          -1.8299649,
          -2.4611137,
          -3.7451403,
          -2.282692,
          -1.8755404,
          -1.7957996,
          -2.6315126,
          -1.6984621,
          -2.079184,
          2.4257312,
          -3.911663,
          -2.6450891,
          -3.0405912,
          -2.7125187,
          -3.5630944,
          -5.316442,
          -11.216062,
          0.8986736,
          3.0427818,
          5.441094,
          6.7401314,
          9.629157,
          10.197572,
          9.884569,
          10.224201,
          10.714986,
          9.824965,
          -3.3671663,
          -1.5192299,
          6.566217,
          -2.5313525,
          -4.8372784,
          -0.5191863,
          -5.528161,
          -5.234621,
          -2.329579,
          6.7131543,
          10.195067,
          10.138885,
          10.70644,
          9.885716,
          -5.0918245,
          -6.11682,
          -5.889301,
          -4.1519666,
          -1.2083606,
          -2.6185882,
          -2.9803572,
          -3.270959,
          -0.95871454,
          -2.5524457,
          -2.966565,
          -2.5912573,
          -2.9061859,
          -2.2905898,
          -2.3137736,
          -2.5464885,
          -3.4350126,
          -2.999965,
          -8.676748,
          -1.1044796,
          -3.1000595,
          -2.6583097,
          -3.102702,
          -2.8988328,
          -2.1647663,
          -3.961511,
          -4.0264454,
          -4.0251346,
          -4.369519,
          -1.2668006,
          6.797805,
          -2.4423857,
          -1.614622,
          -1.41,
          -1.8300537,
          -2.44089,
          0.13343756,
          -3.1797667,
          0.8089793,
          -3.1986969,
          -2.853535,
          -2.7961347,
          -2.9271858,
          -2.9048052,
          -2.8815732,
          -2.6374745,
          -2.8013656,
          -3.6900096,
          -3.906704,
          0.054326005,
          1.0350153,
          0.57896626,
          -0.54174924,
          -1.5375679,
          7.633797,
          7.7152166,
          -3.468801,
          -1.7717056,
          -2.6060686,
          -9.5803385,
          -3.965364,
          -2.6125014,
          -1.3604364,
          -1.8221966,
          0.963192,
          -2.7339153,
          -2.2602634,
          -3.443314,
          -2.685976,
          -2.254739,
          -1.112289,
          1.8455932,
          -2.4523044,
          -1.7361922,
          -1.8500408,
          -1.8277991,
          -1.962476,
          -1.87555,
          -2.1941288,
          -2.7849276,
          -2.806923,
          -1.0937858,
          -2.969136,
          -2.782909,
          -11.704533,
          5.6865363,
          4.4466586,
          -1.0994632,
          2.2696712,
          1.18683,
          -2.8104937,
          -7.1230817,
          -0.30600184,
          -2.5915487,
          -2.5114398,
          -2.145102,
          0.7181248,
          1.7583382,
          1.9867501,
          0.79614,
          0.67596245,
          -4.3510976,
          -2.94816,
          -2.5405414,
          -2.4121912,
          -2.0133345,
          -1.3362375,
          -2.3811646,
          -2.5448596,
          -2.8021848,
          -6.8122797,
          -2.604122,
          -3.10335,
          -2.2131732,
          -1.6429911,
          -2.0901463,
          -2.2922273,
          -2.217603,
          0.15335052,
          -2.0150754,
          0.23729081,
          -1.2050629,
          -1.7200611,
          1.1201149,
          -2.804124,
          -2.0928695,
          -2.840523,
          -3.2973545,
          -3.5080388,
          -3.5349782,
          -3.440477,
          -3.3706722,
          -2.8134868,
          -2.0668635,
          -3.3934033,
          -3.4084563,
          -3.4807494,
          -3.4293337,
          -2.6533377,
          -3.807353,
          -2.7789998,
          -1.6038408,
          -2.680329,
          -7.065332,
          -3.0713992,
          -3.9965463,
          -3.7901037,
          -3.874561,
          7.4781938,
          -2.2900414,
          -2.69987,
          -1.6276205,
          -3.0416796,
          -2.6888402,
          -2.7486837,
          -11.664219,
          4.715252,
          4.4124346,
          -2.6302428,
          7.6459365,
          7.6783504,
          -4.231292,
          -0.044086397,
          -2.8729098,
          -3.9718218,
          -2.4609168,
          -0.9339018,
          -3.7301965,
          -3.1930914,
          -2.8749425,
          -3.1513567,
          -0.8954165,
          1.3841788,
          1.8003561,
          -1.5425743,
          -2.0175557,
          -2.4300575,
          -0.73993766,
          -0.13320386,
          -3.506154,
          -4.1467986,
          -4.1909394,
          -2.9776032,
          -2.870861,
          5.2961783,
          5.3216314,
          5.8739676,
          5.4622407,
          1.2078192,
          0.3791629,
          -2.227529,
          4.779997,
          1.2756095,
          4.612888,
          4.5609393,
          3.4859416,
          -2.8475196,
          -0.17452464,
          -1.7238774,
          2.3005416,
          1.1434278,
          -2.7716994,
          -3.8256915,
          1.9750614,
          -2.3558395,
          -2.717911,
          -2.8560889,
          -2.5678813,
          -2.749431,
          -1.5786719,
          -3.0679305,
          -3.8411949,
          -3.8953505,
          -3.9415922,
          -3.9084938,
          -3.6085732,
          -3.7601979,
          -3.6101584,
          -3.737251,
          -3.4170613,
          -4.1087055,
          -4.132442,
          -3.8828597,
          -4.2660837,
          5.4780517,
          -8.822556,
          -2.8030334,
          -2.6322277,
          -2.013041,
          -2.3111126,
          -2.261415,
          -1.8093339,
          -2.0936077,
          -1.0800694,
          -2.0250072,
          -2.3061583,
          -2.053757,
          -2.1925223,
          -1.9488674,
          -11.700652,
          -2.3582375,
          -1.838218,
          -1.1709069,
          4.7219462,
          -0.90112144,
          2.0954285,
          -3.5121083,
          -2.705574,
          -2.2460308,
          -2.7655423,
          -2.4213324,
          -2.1998563,
          -2.3366058,
          -2.3460712,
          -2.2023695,
          5.6283307,
          -4.4010315,
          -2.5563955
         ],
         "xaxis": "x",
         "y": [
          1.5670218,
          1.4023409,
          -0.14341049,
          -0.6996227,
          1.520894,
          2.002222,
          -0.95165634,
          0.47283795,
          0.2534734,
          0.08193712,
          1.1757272,
          -0.2822083,
          0.77227646,
          -0.1100043,
          2.7780795,
          1.9550493,
          2.3474984,
          2.047175,
          2.4252748,
          -0.6889123,
          -2.5948234,
          8.339778,
          -2.0725782,
          0.97680056,
          -3.368674,
          0.07567263,
          0.74628365,
          0.34090227,
          0.7936106,
          1.3704618,
          0.27452722,
          2.2814262,
          2.8822575,
          -3.2933772,
          1.1052552,
          -1.2047101,
          7.1400104,
          2.0621204,
          -0.4042815,
          1.3655854,
          -3.470099,
          0.7343545,
          0.6763831,
          1.3659744,
          0.33307797,
          -0.3608255,
          -0.5606595,
          -1.2081305,
          -0.40326896,
          1.370112,
          1.1740916,
          1.2978147,
          1.6662023,
          -3.607894,
          1.3046144,
          1.7861186,
          1.0607467,
          1.3784282,
          0.91778415,
          0.75670344,
          0.8236944,
          2.0080767,
          0.22438657,
          0.2411883,
          1.3341484,
          1.782221,
          2.756122,
          2.496813,
          2.7795308,
          2.8513787,
          1.9652532,
          -0.2318142,
          0.44102216,
          -0.93344223,
          1.3707347,
          -2.0393298,
          1.0740786,
          0.82660025,
          0.7777848,
          0.8639742,
          -1.9923999,
          -2.0642898,
          0.44337362,
          -0.2929475,
          1.2507508,
          1.4939693,
          1.2283714,
          1.3728443,
          1.5164392,
          1.3461438,
          1.2185292,
          1.7124871,
          0.66290987,
          1.1097829,
          6.6260824,
          2.6612291,
          3.7812502,
          2.9585161,
          2.2557185,
          -3.8780303,
          -3.9978664,
          -0.022021942,
          0.15340379,
          0.1223298,
          -1.7366425,
          -0.0005332496,
          1.91573,
          1.1139512,
          0.85593796,
          -0.49979976,
          1.7773774,
          1.6465394,
          2.285728,
          1.4827543,
          2.009932,
          2.7350583,
          0.72064847,
          1.2290453,
          0.4634802,
          0.9462672,
          -0.19818689,
          -1.5201979,
          0.49863583,
          1.1625134,
          1.4660982,
          1.5939419,
          -3.422532,
          1.8429548,
          0.2449928,
          -3.361051,
          1.3034679,
          0.63988996,
          1.4343748,
          0.12640934,
          0.5717938,
          0.9873127,
          -0.4515423,
          7.2279143,
          1.2183797,
          0.38443428,
          0.76395243,
          6.810697,
          7.3698244,
          7.041779,
          6.597828,
          6.7646494,
          -0.17501146,
          1.0738102,
          1.1973016,
          1.3460078,
          1.3648423,
          0.1906866,
          1.0482975,
          1.1312977,
          0.92964137,
          -0.64948606,
          -1.1881738,
          -1.685789,
          -1.3537508,
          -0.9093071,
          -2.4871285,
          -1.5046486,
          -1.5623446,
          -0.8180741,
          1.762757,
          0.9069536,
          1.0634228,
          0.9183249,
          -0.86932343,
          2.29605,
          2.579008,
          3.0943925,
          3.5027983,
          3.7317734,
          3.8176963,
          3.7097528,
          3.6501076,
          2.1801062,
          2.6783545,
          3.711471,
          3.6362717,
          3.6826987,
          3.579985,
          2.027784,
          3.4091651,
          2.8802004,
          3.9889414,
          3.0270154,
          -0.9720401,
          2.681103,
          2.1980093,
          2.8540864,
          -0.19394845,
          -3.8311217,
          1.1686469,
          0.38235605,
          -0.7663067,
          -0.80839115,
          -0.2605146,
          1.3801572,
          -3.3956213,
          -1.1026312,
          -0.39497805,
          1.4761634,
          -3.971311,
          -3.9007816,
          -0.43922386,
          1.4626906,
          2.3275719,
          -0.10191257,
          2.28303,
          1.8995993,
          0.2467725,
          1.7509654,
          1.9492291,
          2.0607362,
          1.3518659,
          0.8093455,
          0.39682245,
          0.9256787,
          0.9951593,
          1.1848537,
          2.857458,
          4.910677,
          1.9317378,
          -0.25680518,
          -0.69054407,
          -0.015431309,
          0.8598965,
          0.4577197,
          -1.1261634,
          -1.7897965,
          -1.5473007,
          -2.0534763,
          -2.1741014,
          -3.8975656,
          -0.5395459,
          -1.203916,
          -1.3106948,
          -1.517238,
          -0.90400994,
          1.44004,
          1.1639842,
          1.7048941,
          0.17481859,
          0.42785472,
          1.3233882,
          1.9218073,
          0.29386678,
          0.47830898,
          1.3431953,
          1.4687928,
          0.9102209,
          1.1280718,
          0.38015458,
          1.0492847,
          3.1599922,
          3.4136426,
          3.3916852,
          3.3322685,
          3.0529068,
          2.848323,
          2.783915,
          2.918934,
          2.2581599,
          3.507591,
          3.4845452,
          3.4562788,
          -0.32372063,
          1.1175624,
          0.11302036,
          0.69534856,
          1.0577325,
          0.6814993,
          0.39123794,
          0.68750274,
          0.35753474,
          0.52001303,
          -2.1188598,
          0.5584762,
          0.48105228,
          0.26996857,
          0.19438823,
          -0.41489783,
          -3.5366077,
          0.5096959,
          -1.6766392,
          -1.3657475,
          -3.3058817,
          -0.47521642,
          -0.9661686,
          0.39282322,
          0.050676584,
          -0.7271793,
          -0.43667334,
          -0.05189019,
          -0.50634193,
          -0.18007654,
          0.2733225,
          0.43933913,
          1.2332168,
          -0.6558138,
          1.891356
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "Fine-tuning for image classification using LoRA and ðŸ¤— PEFT\n\n## Vision Transformer model from transfo..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "[[autodoc]] auto.AutoPeftModelForTokenClassification\n\n## AutoPeftModelForQuestionAnswering\n\n[[autodo..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## LoHaConfig\n\n[[autodoc]] tuners.loha.config.LoHaConfig\n\n## LoHaModel\n\n[[autodoc]] tuners.loha.mode..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nFor example:\n\n```bash\ndoc-builder preview peft docs/source\n```\n\nThe docs will be viewable at [h..."
          ],
          [
           "```\nand of course, if you moved it to another file, then:\n\n```\nSections that were moved:\n\n[ <a href=..."
          ],
          [
           "```\n\nUse the relative style to link to the new file so that the versioned docs continue to work.\n\n\n#..."
          ],
          [
           "The same works for methods so you can either use \\[\\`XXXClass.method\\`\\] or \\[~\\`XXXClass.method\\`\\]..."
          ],
          [
           "```\n    Args:\n        n_layers (`int`): The number of layers of the model.\n```\n\nIf the description i..."
          ],
          [
           "```\n```python\n# first line of code\n# second line\n# etc\n```\n````\n\n#### Writing a return block\n\nThe re..."
          ],
          [
           "```\n    Example:\n\n    ```python\n    >>> import time\n    >>> from accelerate import Accelerator\n    >..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\n## Setup\n\nStart by defining the model and tokenizer, text and label columns, and some hyperpara..."
          ],
          [
           "```\n\n## Load dataset\n\nFor this guide, you'll train on the `sentences_allagree` subset of the [`finan..."
          ],
          [
           "```\n\n## Preprocess dataset\n\nInitialize a tokenizer, and create a function to pad and truncate the `m..."
          ],
          [
           "```\n\nCreate a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) ..."
          ],
          [
           "```\n\nSetup the optimizer and learning rate scheduler:\n\n```py\noptimizer = torch.optim.AdamW(model.par..."
          ],
          [
           "```\n\nMove the model to the GPU, and then write a training loop to begin!\n\n```py\nmodel = model.to(dev..."
          ],
          [
           "```\n\nLet's see how well the model performs on the validation set:\n\n```py\ncorrect = 0\ntotal = 0\nfor p..."
          ],
          [
           "```\n\nIf you check the model file size in the repository, you'll see that it is only 3.93MB! ðŸ¤\n\n## In..."
          ],
          [
           "Fine-tuning a multilayer perceptron using LoRA and ðŸ¤— PEFT\n\n[![Open In Colab](https://colab.research...."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "However, after a model is quantized it isn't typically further trained for downstream tasks because ..."
          ],
          [
           "```py\nimport torch\nfrom transformers import BitsAndBytesConfig\n\nconfig = BitsAndBytesConfig(\n    loa..."
          ],
          [
           "```\n\nPass the `config` to the [`~transformers.AutoModelForCausalLM.from_pretrained`] method.\n\n```py\n..."
          ],
          [
           "```\n\nYou're all set for training with whichever training method you prefer!\n\n### LoftQ initializatio..."
          ],
          [
           "```\n\n## Next steps\n\nIf you're interested in learning more about quantization, the following may be h..."
          ],
          [
           "Using PEFT with timm\n\n`peft` allows us to train any model with LoRA as long as the layer type is sup..."
          ],
          [
           "```\n\nThese are the transformations steps necessary to process the image.\n\n\n```python\ntransform = cre..."
          ],
          [
           "```\n\n## Training\n\nThis is just a function that performs the train loop, nothing fancy happening.\n\n\n`..."
          ],
          [
           "```\n\n### Selecting which layers to fine-tune with LoRA\n\nLet's take a look at the layers of our model..."
          ],
          [
           "```\n\nFinally, let's create the `peft` model, the optimizer and criterion, and we can get started. As..."
          ],
          [
           "```\n\n\n```python\npeft_model.push_to_hub(model_id);\n```\n\nAs we can see, the adapter size is only 4.3 M..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Below is a basic example usage of how to inject LoRA adapters into the submodule `linear` of the mod..."
          ],
          [
           "```\n\nIf you print the model, you will notice that the adapters have been correctly injected into the..."
          ],
          [
           "```\n\n## Pros and cons \n\nWhen to use this API and when to not use it? Let's discuss in this section t..."
          ],
          [
           "``python\nimport os\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    default_data_coll..."
          ],
          [
           "```\n\n\n```python\n# loading dataset\ndataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree..."
          ],
          [
           "```\n\n\n```python\n# training and evaluation\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eva..."
          ],
          [
           "```\n\n\n```python\nckpt = f\"{peft_model_id}/adapter_model.bin\"\n!du -h $ckpt\n```\n\n\n```python\nfrom peft i..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\n## Setup\n\nLet's take care of some of the setup first so you can start training faster later. Se..."
          ],
          [
           "```\n\n## Load dataset and metric\n\nThe [Common Voice 11.0](https://huggingface.co/datasets/mozilla-fou..."
          ],
          [
           "```\n\nYou'll only be training on the `sentence` and `audio` columns, so you can remove the rest of th..."
          ],
          [
           "```\n\nIf you look at the `sampling_rate`, you'll see the audio was sampled at 48kHz. The Whisper mode..."
          ],
          [
           "```\n\nOnce you've cleaned up the dataset, you can write a function to generate the correct model inpu..."
          ],
          [
           "```\n\nFinally, create a `DataCollator` class to pad the labels in each batch to the maximum length, a..."
          ],
          [
           "```\n\n## Train\n\nNow that the dataset is ready, you can turn your attention to the model. Start by loa..."
          ],
          [
           "```\n\nLet's also apply LoRA to the training to make it even more efficient. Load a [`~peft.LoraConfig..."
          ],
          [
           "```\n\nNow you're ready to define some training hyperparameters in the [`~transformers.Seq2SeqTraining..."
          ],
          [
           "```\n\nIt is also a good idea to write a custom [`~transformers.TrainerCallback`] to save model checkp..."
          ],
          [
           "```\n\n## Evaluate\n\n[Word error rate](https://huggingface.co/spaces/evaluate-metric/wer) (WER) is a co..."
          ],
          [
           "```\n\nWrite a loop to evaluate the model performance. Set the model to evaluation mode first, and wri..."
          ],
          [
           "```\n\n## Share model\n\nOnce you're happy with your results, you can upload your model to the Hub with ..."
          ],
          [
           "```\n\nLoad an audio sample (you can listen to it in the [Dataset Preview](https://huggingface.co/data..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Load a dataset\n\nTo ensure that this example runs within a reasonable time frame, here we are..."
          ],
          [
           "```\n\n## Prepare datasets for training and evaluation\n\nNext, load the SegFormer image processor to pr..."
          ],
          [
           "```\n\nFinally, combine everything in two functions that you'll use to transform training and validati..."
          ],
          [
           "```\n\n## Create evaluation function\n\nIncluding a metric during training is helpful for evaluating you..."
          ],
          [
           "metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n        ..."
          ],
          [
           "```\n\n## Load a base model \n\nBefore loading a base model, let's define a helper function to check the..."
          ],
          [
           "```\n\nAt this point you can check with the `print_trainable_parameters` helper function that all 100%..."
          ],
          [
           "```\n\nLet's review the `LoraConfig`. To enable LoRA technique, we must define the target modules with..."
          ],
          [
           "When all is configured, and the base model is wrapped, the `print_trainable_parameters` helper funct..."
          ],
          [
           "```\n\nThis confirms that only the LoRA parameters appended to the attention blocks and the `decode_he..."
          ],
          [
           "```\n\n## Save the model and run inference\n\nUse the `save_pretrained()` method of the `lora_model` to ..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/doc..."
          ],
          [
           "```\n\nNext, visualize the results.  We need a color palette for this. Here, we use ade_palette(). As ..."
          ],
          [
           "```\n\nAs you can see, the results are far from perfect, however, this example is designed to illustra..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./c..."
          ],
          [
           "<iframe\n\tsrc=\"https://stevhliu-peft-methods.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"850\"\n\theight=\"620\"\n><..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "[Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification tasks on T5 m..."
          ],
          [
           "The main difference is that the prefix parameters are inserted in **all** of the model layers, where..."
          ],
          [
           "The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables G..."
          ],
          [
           "<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/document..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Fine-tuning large pre-trained language models on downstream tasks ..."
          ],
          [
           "Training PEFT models with new tokens being added to the embedding layers and tokenizer\n\nIn this exam..."
          ],
          [
           "```\n\n## Prepare Model and Tokenizer\n\nNow, we will be adding 27 new tokens as well as replace the exi..."
          ],
          [
           "```\n\nWe will be finetuning Mistral-7B model. Let's load the tokenizer and add the special tokens fol..."
          ],
          [
           "```\n\n## Preapre Dataset\n\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"smang..."
          ],
          [
           "def preprocess_function(examples):\n    batch_size = len(examples[text_column])\n    targets = [str(x)..."
          ],
          [
           "\"attention_mask\"\n        ][i]\n        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_inp..."
          ],
          [
           "processed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    rem..."
          ],
          [
           "```\n\n\n```python\ntrain_dataset\n```\n\n\n```python\ntrain_dataloader = DataLoader(\n    train_dataset, shuf..."
          ],
          [
           "```\n\n# Check the model output on a sample from evaluation dataset\n\n\n```python\nimport random\n\ni = ran..."
          ],
          [
           "```\n\n# Check the model loading is working as expected and generating plausible outputs.\n\n\n```python\n..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## PromptEncoderConfig\n\n[[autodoc]] tuners.p_tuning.config.PromptEncoderConfig\n\n## PromptEncoder\n\n[[..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "For example, load a base model and then load the [artificialguybr/3DRedmond-V1](https://huggingface...."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/ybelkada/docume..."
          ],
          [
           "```\n\nLearn more about how PEFT supports Diffusers in the [Inference with PEFT](https://huggingface.c..."
          ],
          [
           "```\n\nIf you're interested in comparing or using more than one adapter, you can also call the [`~Peft..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nNavigate to the directory containing the training scripts for fine-tuning Dreambooth with LoRA:..."
          ],
          [
           "```\n\nHere: \n- `INSTANCE_DIR`: The directory containing the images that you intend to use for trainin..."
          ],
          [
           "Here's what the full set of script arguments may look like:\n\n```bash\naccelerate launch train_dreambo..."
          ],
          [
           "```\n\nIf you are running this script on Windows, you may need to set the `--num_dataloader_workers` t..."
          ],
          [
           "```\n\nNext, add a function that will create a Stable Diffusion pipeline for image generation. It will..."
          ],
          [
           "```\n\nNow you can use the function above to create a Stable Diffusion pipeline using the LoRA weights..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "```\n\nTo switch between adapters, write a function that uses `set_adapter()` method of `PeftModel` (s..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/do..."
          ],
          [
           "``python\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import get_peft_config, get_peft_m..."
          ],
          [
           "```\n\n\n```python\n# loading dataset\ndataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree..."
          ],
          [
           "```\n\n\n```python\n# data preprocessing\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n..."
          ],
          [
           "```\n\n\n```python\n# training and evaluation\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n..."
          ],
          [
           "```\n\n\n```python\n# print accuracy\ncorrect = 0\ntotal = 0\nfor pred, true in zip(eval_preds, dataset[\"va..."
          ],
          [
           "```\n\n\n```python\nmodel.eval()\ni = 107\ninputs = tokenizer(dataset[\"validation\"][text_column][i], retur..."
          ],
          [
           "``python\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import get_peft_config, get_peft_m..."
          ],
          [
           "```\n\n\n```python\n# data preprocessing\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n..."
          ],
          [
           "```\n\n\n```python\nmodel.eval()\ni = 13\ninputs = tokenizer(dataset[\"validation\"][text_column][i], return..."
          ],
          [
           "``python\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_config, get_peft_mo..."
          ],
          [
           "```\n\n\n```python\n# data preprocessing\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\ni..."
          ],
          [
           "def preprocess_function(examples):\n    batch_size = len(examples[text_column])\n    inputs = [f\"{text..."
          ],
          [
           "max_length - len(sample_input_ids)\n        ) + sample_input_ids\n        model_inputs[\"attention_mask..."
          ],
          [
           "processed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    rem..."
          ],
          [
           "```\n\n\n```python\ndef test_preprocess_function(examples):\n    batch_size = len(examples[text_column])\n..."
          ],
          [
           "```\n\n\n```python\nlen(test_dataloader)\n```\n\n\n```python\nnext(iter(test_dataloader))\n```\n\n\n```python\n# c..."
          ],
          [
           "```\n\n\n```python\n# training and evaluation\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n..."
          ],
          [
           "```\n\n\n```python\nmodel.eval()\ni = 16\ninputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet ..."
          ],
          [
           "```\n- Or save model locally\n```python\npeft_model_id = f\"{dataset_name}_{model_name_or_path}_{peft_co..."
          ],
          [
           "```\n\n\n```python\nmodel.to(device)\nmodel.eval()\ni = 4\ninputs = tokenizer(f'{text_column} : {dataset[\"t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "LoftQ: LoRA-fine-tuning-aware Quantization\n\n## Introduction\n\nLoftQ finds quantized LoRA initializati..."
          ],
          [
           "```\n\n## LoftQ DIY\n\n### Apply LoftQ and save\nWe provide [quantize_save_load.py](quantize_save_load.py..."
          ],
          [
           "```\n\nThe above commands end up with creating the model directory under `$SAVE_DIR`. \nSpecifically, t..."
          ],
          [
           "```\n\n## LoftQ Fine-tuning\n\nWe also provide an example to fine-tune LoftQ on GSM8K. \nWe load the quan..."
          ],
          [
           "Finetuning Whisper-large-V2 on Colab using PEFT-Lora + BNB INT8 training\n\nIn this Colab, we present ..."
          ],
          [
           "```\n\n\n```python\n# Select CUDA device index\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nmodel..."
          ],
          [
           "```\n\n### Prepare Data\n\n\n```python\nprint(common_voice[\"train\"][0])\n```\n\nSince \nour input audio is sam..."
          ],
          [
           "```\n\nWe can apply the data preparation function to all of our training examples using dataset's `.ma..."
          ],
          [
           "```\n\n\n```python\ncommon_voice[\"train\"]\n```\n\n## Training and Evaluation\n\n### Define a Data Collator\n\n\n..."
          ],
          [
           "```\n\nLet's initialise the data collator we've just defined:\n\n\n```python\ndata_collator = DataCollator..."
          ],
          [
           "```\n\n###Â Load a Pre-Trained Checkpoint\n\nNow let's load the pre-trained Whisper `small` checkpoint. A..."
          ],
          [
           "```\n\n### Apply LoRA\n\nHere comes the magic with `peft`! Let's load a `PeftModel` and specify that we ..."
          ],
          [
           "```\n\n**Few Important Notes:**\n1. `remove_unused_columns=False` and `label_names=[\"labels\"]` are requ..."
          ],
          [
           "```\n\n\n```python\ntrainer.train()\n```\n\n\n```python\nmodel_name_or_path = \"openai/whisper-large-v2\"\npeft_..."
          ],
          [
           "```\nwithout normalizer: 'à¤¸à¥à¤µà¤¿à¤šà¥à¤šà¤¾à¤¨ à¤¨à¤°à¥à¤µà¤¿à¤¤à¥à¤¤à¥€à¤šà¥€ à¤ªà¤¦à¥à¤¦à¤¤ à¤®à¥‹à¤ à¥à¤¯à¤¾ à¤ªà¥à¤°à¤®à¤¾à¤£à¤¾à¤¤ à¤†à¤®à¤²à¤¾à¤¤ à¤†à¤£à¤²à¥à¤¯à¤¾ à¤¬à¤¸à¥‹à¤¨ à¤¯à¤¾ à¤¦à¥à¤ªà¤¨à¥à¤¯à¤¾à¤¨à¥‡ ..."
          ],
          [
           "```\nPost fixing this bug, we report the 2 metrics for the top model of the leaderboard and the PEFT ..."
          ],
          [
           "```\n\n\n```python\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimp..."
          ],
          [
           "```\nThe model 'PeftModel' is not supported for . Supported models are ['SpeechEncoderDecoderModel', ..."
          ],
          [
           "```\n\n\n```python\nimport torch\nimport gradio as gr\nfrom transformers import (\n    AutomaticSpeechRecog..."
          ],
          [
           "iface = gr.Interface(\n    fn=transcribe,\n    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n..."
          ],
          [
           "``python\nimport argparse\nimport gc\nimport hashlib\nimport itertools\nimport logging\nimport math\nimport..."
          ],
          [
           "```\n\n\n```python\ndef get_lora_sd_pipeline(\n    ckpt_dir, base_model_name_or_path=None, dtype=torch.fl..."
          ],
          [
           "if dtype in (torch.float16, torch.bfloat16):\n        pipe.unet.half()\n        pipe.text_encoder.half..."
          ],
          [
           "if os.path.exists(text_encoder_sub_dir):\n        if isinstance(pipe.text_encoder, PeftModel):\n      ..."
          ],
          [
           "```\n\n\n```python\n%%time\npipe = get_lora_sd_pipeline(os.path.join(base_path, \"dog_dreambooth_updated\")..."
          ],
          [
           "```\n\n\n```python\nset_adapter(pipe, adapter_name=\"toy\")\nprompt = \"superman rendered in the style of <1..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "</Tip>\n\n## Configuration\n\nStart by running the following command to [create a DeepSpeed configuratio..."
          ],
          [
           "```\n\nYou'll be asked a few questions about your setup, and configure the following arguments. In thi..."
          ],
          [
           "```\n\nAn example [configuration file](https://github.com/huggingface/peft/blob/main/examples/conditio..."
          ],
          [
           "```\n\n## The important parts\n\nLet's dive a little deeper into the script so you can see what's going ..."
          ],
          [
           "```\n\nThroughout the script, you'll see the [`~accelerate.Accelerator.main_process_first`] and [`~acc..."
          ],
          [
           "```\n\nInside the training loop, the usual `loss.backward()` is replaced by ðŸ¤— Accelerate's [`~accelera..."
          ],
          [
           "```\n\nYou'll see some output logs that track memory usage during training, and once it's completed, t..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## MultitaskPromptTuningConfig\n\n[[autodoc]] tuners.multitask_prompt_tuning.config.MultitaskPromptTun..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThis should finish much quicker and allow faster iteration. Before creating the PR, however, pl..."
          ],
          [
           "## Adding a new fine-tuning method\n\nNew parameter-efficient fine-tuning methods are developed all th..."
          ],
          [
           "New features should generally be accompanied by tests and documentation or examples. Without the lat..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*In this work, we explore \"prompt tuning\", a simple yet effective m..."
          ],
          [
           "``python\nfrom transformers import AutoModelForSeq2SeqLM\nimport peft\nfrom peft import get_peft_config..."
          ],
          [
           "```\n\n\n```python\nmodel\n```\n\n\n```python\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainab..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*We present LLaMA-Adapter, a lightweight adaption method to efficie..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThe last line is necessary if you want to activate both adapters, otherwise, only the first ada..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nYou'll be asked a few questions about your setup, and configure the following arguments. For th..."
          ],
          [
           "```\n\nFor example, your FSDP configuration file may look like the following:\n\n```yaml\ncommand_file: n..."
          ],
          [
           "```\n\n## The important parts\n\nLet's dig a bit deeper into the training script to understand how it wo..."
          ],
          [
           "```\n\nThroughout the script, you'll see the [`~accelerate.Accelerator.main_process_first`] and [`~acc..."
          ],
          [
           "``python\nfrom datasets import load_dataset\nfrom transformers import set_seed, AutoModelForSeq2SeqLM,..."
          ],
          [
           "```\n\n\n```python\ndef get_sst2(split: str):\n    examples = load_dataset(\"sst2\")[split]\n    result_exam..."
          ],
          [
           "```\n\n\n```python\nfrom typing import Tuple\nfrom torch.utils.data import Dataset, DataLoader\nimport tor..."
          ],
          [
           "task_ids = [i[\"task_id\"] for i in batch]\n    task_ids = torch.tensor(task_ids)\n\n    return {\n       ..."
          ],
          [
           "```\n\n## source training\n\n\n```python\nfrom torch.optim.adamw import AdamW\nfrom transformers import get..."
          ],
          [
           "```\n\n\n```python\nPOSITIVE_TOKEN_ID = tokenizer(\" positive\", add_special_tokens=False)[\"input_ids\"][0]..."
          ],
          [
           "val_loss, f1 = evaluate(model, val)\nprint(\n    f\"\"\"\nbefore source training\nval loss = {val_loss}\nf1 ..."
          ],
          [
           "```\n\n## target training\n\n\n```python\ntrain = DataLoader(MyDataset(\"train\", \"target\"), shuffle=True, b..."
          ],
          [
           "```\n\n\n```python\noptimizer = AdamW(model.parameters(), lr=1e-4)\nscheduler = get_cosine_schedule_with_..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThere is also an option to set `init_lora_weights=False` which is useful for debugging and test..."
          ],
          [
           "```\n\n<Tip>\n\nLearn more about how PEFT works with quantization in the [Quantization](quantization) gu..."
          ],
          [
           "```\n\nIf you need to keep a copy of the weights so you can unmerge the adapter later or delete and lo..."
          ],
          [
           "```\n\n## Load adapters\n\nAdapters can be loaded onto a pretrained model with [`~PeftModel.load_adapter..."
          ],
          [
           "``python\n!pip install -q git+https://github.com/huggingface/transformers.git\n!pip install -q git+htt..."
          ],
          [
           "```\n\n\n```python\nmodel\n```\n\n\n```python\nmodel.to(\"cuda\")\n```\n\n\n```python\nimport torch\n\ndevice = \"cuda\"..."
          ],
          [
           "```\n\n\n```python\ninstruction = \"Tell me about alpacas.\"\n\nprint(evaluate(instruction))\n```\n\n\n```python..."
          ],
          [
           "``python\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import PeftModel, PeftConfig\nimpor..."
          ],
          [
           "```\n\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"ought/raft\", dataset_name..."
          ],
          [
           "```\n\n\n```python\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\ntarget_max..."
          ],
          [
           "```\n\n\n```python\nmodel.eval()\ni = 15\ninputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet ..."
          ],
          [
           "```\n\n\n```python\nmodel.eval()\ntest_preds = []\n\nfor _, batch in enumerate(tqdm(test_dataloader)):\n    ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## PEFT configurations\n\n<Tip>\n\nLearn more about the parameters you can configure for each PEFT metho..."
          ],
          [
           "```\n\nYou can create your own configuration for training by initializing a [`LoraConfig`].\n\n```py\nfro..."
          ],
          [
           "```\n\n</hfoption>\n</hfoptions>\n\n## PEFT models\n\nWith a PEFT configuration in hand, you can now apply ..."
          ],
          [
           "```\n\nTo load a [`PeftModel`] for inference, you'll need to provide the [`PeftConfig`] used to create..."
          ],
          [
           "```\n\nTake a look at the [AutoPeftModel](package_reference/auto_class) API reference to learn more ab..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "[[autodoc]] PeftModelForFeatureExtraction\n    - all\n\n## Utilities\n\n[[autodoc]] get_peft_model\n\n[[aut..."
          ],
          [
           "Fine-tune FLAN-T5 using `bitsandbytes`, `peft` & `transformers` ðŸ¤— \n\nIn this notebook we will see how..."
          ],
          [
           "```\n\n## Prepare model for training\n\nSome pre-processing needs to be done before training such an int..."
          ],
          [
           "```\n\nAs you can see, here we are only training 0.6% of the parameters of the model! This is a huge m..."
          ],
          [
           "```\n\nLet's also apply some pre-processing of the input data, the labels needs to be pre-processed, t..."
          ],
          [
           "```\n\n## Train our model! \n\nLet's now train our model, run the cells below.\nNote that for T5 since so..."
          ],
          [
           "```\n\n\n```python\nmodel.push_to_hub(\"ybelkada/flan-t5-large-financial-phrasebank-lora\", use_auth_token..."
          ],
          [
           "``python\nimport argparse\nimport os\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data..."
          ],
          [
           "```\n\n\n```python\nif any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n    padding_side =..."
          ],
          [
           "```\n\n\n```python\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur..."
          ],
          [
           "```\n\n## Load adapters from the Hub\n\nYou can also directly load adapters from the Hub using the comma..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nInstall all the necessary required libraries with:\n\n```bash\npip install -r requirements.txt\n```..."
          ],
          [
           "```\n\nHere's what a full set of script arguments may look like when running in Colab on a V100 GPU wi..."
          ],
          [
           "```\n\n## Dataset for semantic similarity\n\nThe dataset we'll be using is a small subset of the [esci-d..."
          ],
          [
           "<div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documen..."
          ],
          [
           "return embeddings\n\n    def mean_pooling(self, model_output, attention_mask):\n        token_embedding..."
          ],
          [
           "```\n\nThe `get_cosine_embeddings` function computes the cosine similarity and the `get_loss` function..."
          ],
          [
           "## Inference\n\nLet's go! Now we have the model, we need to create a search index of all the products ..."
          ],
          [
           "1. Get a list of ids to products which we can call `ids_to_products_dict`:\n\n```bash\n{0: 'RamPro 10\" ..."
          ],
          [
           "```\n\n2. Use the trained [smangrul/peft_lora_e5_ecommerce_semantic_search_colab](https://huggingface...."
          ],
          [
           "```\n\n3. Create a search index using HNSWlib:\n\n```py\ndef construct_search_index(dim, num_elements, da..."
          ],
          [
           "```\n\n5. Let's test it out with the query `deep learning books`:\n\n```py\nquery = \"deep learning books\"..."
          ],
          [
           "```\n\nOutput:\n\n```bash\nquery='deep learning books'\ncosine_sim_score=0.95 product='Deep Learning (The ..."
          ],
          [
           "``python\nimport argparse\nimport os\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data..."
          ],
          [
           "```\n\n\n```python\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur..."
          ],
          [
           "```\n\n## Load adapters from the Hub\n\nYou can also directly load adapters from the Hub using the comma..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\n## Setup\n\nTo get started, import ðŸ¤— Transformers to create the base model, ðŸ¤— Datasets to load a ..."
          ],
          [
           "```\n\nNow you can use the `metric` to write a function that computes the accuracy and F1 scores. The ..."
          ],
          [
           "```\n\nUse [`~datasets.Dataset.map`] to apply the `tokenize_function` to the dataset, and remove the u..."
          ],
          [
           "```\n\nCreate the base `roberta-large` model from [`~transformers.AutoModelForSequenceClassification`]..."
          ],
          [
           "```\n\nThen pass the model, `TrainingArguments`, datasets, tokenizer, data collator, and evaluation fu..."
          ],
          [
           "```\n\n## Inference\n\nOnce the model has been uploaded to the Hub, anyone can easily use it for inferen..."
          ],
          [
           "Dreambooth with OFT\nThis Notebook assumes that you already ran the train_dreambooth.py script to cre..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Large text-to-image diffusion models have impressive capabilities ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Setup\n\nStart by defining the model and tokenizer, the dataset and the dataset columns to tra..."
          ],
          [
           "```\n\n## Load dataset\n\nFor this guide, you'll load the `twitter_complaints` subset of the [RAFT](http..."
          ],
          [
           "```\n\nCreate a `preprocess_function` to:\n\n1. Tokenize the input text and labels.\n2. For each example ..."
          ],
          [
           "```py\ndef preprocess_function(examples):\n    batch_size = len(examples[text_column])\n    inputs = [f..."
          ],
          [
           "model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n     ..."
          ],
          [
           "```\n\nUse the [`~datasets.Dataset.map`] function to apply the `preprocess_function` to the entire dat..."
          ],
          [
           "```\n\n## Train\n\nYou're almost ready to setup your model and start training!\n\nInitialize a base model ..."
          ],
          [
           "```\n\nMove the model to the GPU, then write a training loop to start training!\n\n```py\nmodel = model.t..."
          ],
          [
           "```\n\nUse the [`~transformers.PreTrainedModel.push_to_hub`] function to upload your model to a model ..."
          ],
          [
           "```\n\nPut the model on a GPU and *generate* the predicted label:\n\n```py\nmodel.to(device)\n\nwith torch...."
          ],
          [
           "``python\nfrom transformers import AutoModelForCausalLM\nfrom peft import get_peft_config, get_peft_mo..."
          ],
          [
           "```\n\n\n```python\nlen(test_dataloader)\n```\n\n\n```python\nnext(iter(test_dataloader))\n```\n\n\n```python\n# c..."
          ],
          [
           "```\n\n\n```python\nmodel.eval()\ni = 33\ninputs = tokenizer(f'{text_column} : {dataset[\"test\"][i][\"Tweet ..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## PrefixTuningConfig\n\n[[autodoc]] tuners.prefix_tuning.config.PrefixTuningConfig\n\n## PrefixEncoder\n..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\n## Authenticate to share your model\n\nTo share the fine-tuned model at the end of the training w..."
          ],
          [
           "```\n\nThe `image_processor` contains useful information on which size the training and evaluation ima..."
          ],
          [
           "```\n\n## Load and prepare a model\n\nBefore loading the model, let's define a helper function to check ..."
          ],
          [
           "```\n\nBefore creating a `PeftModel`, you can check the number of trainable parameters in the original..."
          ],
          [
           "```\n\nLet's unpack what's going on here.\nTo use LoRA, you need to specify the target modules in `Lora..."
          ],
          [
           "## Define training arguments\n\nFor model fine-tuning, use [`~transformers.Trainer`]. It accepts\nsever..."
          ],
          [
           "```\n\nCompared to non-PEFT methods, you can use a larger batch size since there are fewer parameters ..."
          ],
          [
           "```\n\nIn just a few minutes, the fine-tuned model shows 96% validation accuracy even on this small\nsu..."
          ],
          [
           "```\n\nWhen calling [`~transformers.PreTrainedModel.push_to_hub`] on the `lora_model`, only the LoRA p..."
          ],
          [
           "```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/sayakpaul/sampl..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nInstalling PEFT from source is useful for keeping up with the latest developments:\n\n```bash\npyt..."
          ],
          [
           "```\n\n### Randomly initialized layers\n\nFor some tasks, it is important to correctly configure `module..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "As mentioned briefly earlier, [LoRA](https://hf.co/papers/2106.09685) is a technique that accelerate..."
          ],
          [
           "<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/document..."
          ],
          [
           "## Low-Rank Kronecker Product (LoKr)\n\n[LoKr](https://hf.co/papers/2309.14859) is very similar to LoR..."
          ],
          [
           "OFT preserves the hyperspherical energy by learning an orthogonal transformation for neurons to keep..."
          ],
          [
           "<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/document..."
          ],
          [
           "``python\nimport argparse\nimport os\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data..."
          ],
          [
           "```\n\n\n```python\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur..."
          ],
          [
           "```\n\n## Load adapters from the Hub\n\nYou can also directly load adapters from the Hub using the comma..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "``python\nimport argparse\nimport os\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data..."
          ],
          [
           "```\n\n\n```python\nif any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n    padding_side =..."
          ],
          [
           "```\n\n\n```python\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur..."
          ],
          [
           "```\n\n## Load adapters from the Hub\n\nYou can also directly load adapters from the Hub using the comma..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The abstract from the paper is:\n\n*Few-shot in-context learning (ICL) enables pre-trained language mo..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\n## Setup\n\nLet's start by importing all the necessary libraries you'll need:\n\n- ðŸ¤— Transformers f..."
          ],
          [
           "```\n\nThe `tags` values are defined in the label ids [dictionary](https://huggingface.co/datasets/tne..."
          ],
          [
           "```\n\nNow you can write an evaluation function to compute the metrics from the model predictions and ..."
          ],
          [
           "```\n\nYou'll also need to write a function to:\n\n1. Map each token to their respective word with the [..."
          ],
          [
           "```\n\nFinally, create a data collator to pad the examples to the longest length in a batch:\n\n```py\nda..."
          ],
          [
           "```\n\nDefine the [`LoraConfig`] with:\n\n- `task_type`, token classification (`TaskType.TOKEN_CLS`)\n- `..."
          ],
          [
           "```\n\nFrom the ðŸ¤— Transformers library, create a [`~transformers.TrainingArguments`] class and specify..."
          ],
          [
           "```\n\n## Inference\n\nTo use your model for inference, load the configuration and model:\n\n```py\npeft_mo..."
          ],
          [
           "```\n\nPass the inputs to the model, and print out the model prediction for each token:\n\n```py\nwith to..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Being similar to LoRA, IA3 carries many of the same advantages: \n\n* IA3 makes fine-tuning more effic..."
          ],
          [
           "`IA3Config` allows you to control how IA3 is applied to the base model through the following paramet..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "## LoraConfig\n\n[[autodoc]] tuners.lora.config.LoraConfig\n\n## LoraModel\n\n[[autodoc]] tuners.lora.mode..."
          ],
          [
           "``python\nimport os\n\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, defa..."
          ],
          [
           "```\n\n\n```python\n# data preprocessing\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nt..."
          ],
          [
           "```\n\n\n```python\n# optimizer and lr scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr..."
          ],
          [
           "```\n\n\n```python\nmodel.eval()\ni = 107\ninput_ids = tokenizer(dataset[\"validation\"][text_column][i], re..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "``python\nimport argparse\nimport json\nimport logging\nimport math\nimport os\nimport random\nfrom pathlib..."
          ],
          [
           "```\n\n\n```python\nclass AutoModelForSentenceEmbedding(nn.Module):\n    def __init__(self, model_name, t..."
          ],
          [
           "```\n\n\n```python\nmodel_name_or_path = \"intfloat/e5-large-v2\"\npeft_model_id = \"smangrul/peft_lora_e5_s..."
          ],
          [
           "```\n\n\n```python\n# base model\nmodel = AutoModelForSentenceEmbedding(model_name_or_path, tokenizer)\n\n#..."
          ],
          [
           "```\n\n\n```python\ndef construct_search_index(dim, num_elements, data):\n    # Declaring index\n    searc..."
          ],
          [
           "```\n\n\n```python\nquery = \"NLP and ML books\"\nk = 10\nquery_embeddings = get_query_embeddings(query, mod..."
          ],
          [
           "``python\nfrom transformers import AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\nimport..."
          ],
          [
           "processed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    rem..."
          ],
          [
           "```\n\n\n```python\ndef test_preprocess_function(examples):\n    batch_size = len(examples[text_column])\n..."
          ],
          [
           "```\n\nYou can load model from hub or local\n\n- Load model from Hugging Face Hub, you can change to you..."
          ],
          [
           "```\n\n\n```python\n# model\n```\n\n\n```python\nmodel.hf_device_map\n```\n\n\n```python\nmodel.eval()\ni = 89\ninpu..."
          ],
          [
           "```\n\n\n```python\nmodel.eval()\ntest_preds = []\n\nfor _, batch in enumerate(tqdm(test_dataloader)):\n    ..."
          ],
          [
           "Fine-tuning for semantic segmentation using LoRA and ðŸ¤— PEFT\n\n[![Open In Colab](https://colab.researc..."
          ],
          [
           "Using PEFT with custom models\n\n`peft` allows us to fine-tune models efficiently with LoRA. In this s..."
          ],
          [
           "```\n\n## Model\n\nAs a model, we use a simple multilayer perceptron (MLP). For demonstration purposes, ..."
          ],
          [
           "```\n\n\n```python\ndef train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):\n ..."
          ],
          [
           "```\n\n\n```python\n%time train(module, optimizer, criterion, train_dataloader, eval_dataloader, epochs=..."
          ],
          [
           "```\n\nNow let's create the `peft` model by passing our initial MLP, as well as the config we just def..."
          ],
          [
           "```\n\n\n```python\nparams_before = dict(module_copy.named_parameters())\nfor name, param in peft_model.b..."
          ],
          [
           "```\n\n\n```python\npeft_model.push_to_hub(model_id);\n```\n\nAs we can see, the adapter size is only 211 k..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nThis is a straightforward multilayer perceptron with an input layer, a hidden layer, and an out..."
          ],
          [
           "```\n\nWith that, we can create our PEFT model and check the fraction of parameters trained:\n\n```pytho..."
          ],
          [
           "```\n\nThis will print a very long list, we'll only show the first few:..."
          ],
          [
           "```\n[('', timm.models.metaformer.MetaFormer),\n ('stem', timm.models.metaformer.Stem),\n ('stem.conv',..."
          ],
          [
           "('stages.0.blocks.0.mlp', timm.layers.mlp.Mlp),\n ('stages.0.blocks.0.mlp.fc1', torch.nn.modules.conv..."
          ],
          [
           "...\n ('head.global_pool.flatten', torch.nn.modules.linear.Identity),\n ('head.norm', timm.layers.norm..."
          ],
          [
           "```\n\nUpon closer inspection, we see that the 2D conv layers have names such as `\"stages.0.blocks.0.m..."
          ],
          [
           "```\n\nThis shows us that we only need to train less than 2% of all parameters, which is a huge effici..."
          ],
          [
           "```\n\nIf that doesn't help, check the existing modules in your model architecture with the `named_mod..."
          ],
          [
           "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```python\nfrom peft import LoraConfig, TaskType\n\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_S..."
          ],
          [
           "```\n\n<Tip>\n\nSee the [`LoraConfig`] reference for more details about other parameters you can adjust,..."
          ],
          [
           "```\n\nOut of [bigscience/mt0-large's](https://huggingface.co/bigscience/mt0-large) 1.2B parameters, y..."
          ],
          [
           "```\n\nYou can also save your model to the Hub (make sure you're logged in to your Hugging Face accoun..."
          ],
          [
           "```\n\nBoth methods only save the extra PEFT weights that were trained, meaning it is super efficient ..."
          ],
          [
           "model = model.to(\"cuda\")\nmodel.eval()\ninputs = tokenizer(\"Preheat the oven to 350 degrees and place ..."
          ],
          [
           "```\n\nFor other tasks that aren't explicitly supported with an `AutoPeftModelFor` class - such as aut..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)\n2. P..."
          ],
          [
           "10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://arxiv.org/..."
          ],
          [
           "## Getting started\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import get_pe..."
          ],
          [
           "```\n\n## Use Cases\n\n### Get comparable performance to full finetuning by adapting LLMs to downstream ..."
          ],
          [
           "Performance of PEFT-LoRA tuned [`bigscience/T0_3B`](https://huggingface.co/bigscience/T0_3B) on [`ou..."
          ],
          [
           "GPU memory required by different settings during training is given below. The final checkpoint size ..."
          ],
          [
           "accelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instanc..."
          ],
          [
           "```\n\nTry out the ðŸ¤— Gradio Space which should run seamlessly on a T4 instance:\n[smangrul/peft-lora-sd..."
          ],
          [
           "### INT8 training of large models in Colab using PEFT LoRA and bitsandbytes\n\n- Here is now a demo on..."
          ],
          [
           "An example of using LoRA for the task of adapting `LayoutLMForTokenClassification` on `FUNSD` datase..."
          ],
          [
           "### Example of PEFT model training using ðŸ¤— Accelerate's DeepSpeed integration\n\nDeepSpeed version req..."
          ],
          [
           "```\n  b. run the below command to launch the example script\n  ```bash\n  accelerate launch --config_f..."
          ],
          [
           "```\n\n### Example of PEFT model inference using ðŸ¤— Accelerate's Big Model Inferencing capabilities\nAn ..."
          ],
          [
           "### Sequence Classification\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | ..."
          ],
          [
           "### Image Classification\n\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | IA..."
          ],
          [
           "## Caveats:\n\n1. Below is an example of using PyTorch FSDP for training. However, it doesn't lead to ..."
          ],
          [
           "```\n\n  Example of parameter efficient tuning with [`mt0-xxl`](https://huggingface.co/bigscience/mt0-..."
          ],
          [
           "```\n\n2. When using ZeRO3 with zero3_init_flag=True, if you find the gpu memory increase with trainin..."
          ],
          [
           "```\n\nLearn more about the [low level API in the docs](https://huggingface.co/docs/peft/developer_gui..."
          ],
          [
           "```\n\n## Contributing\n\nIf you would like to contribute to PEFT, please check out our [contributing gu..."
          ],
          [
           "``python\nimport argparse\nimport os\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data..."
          ],
          [
           "```\n\n\n```python\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, retur..."
          ],
          [
           "```\n\n## Load adapters from the Hub\n\nYou can also directly load adapters from the Hub using the comma..."
          ]
         ],
         "hovertemplate": "source=peft<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "peft, circle",
         "marker": {
          "color": "#FFA15A",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "peft, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0.3334883,
          -0.54957086,
          -2.8456736,
          -12.000758,
          -6.164517,
          -11.71921,
          5.127969,
          5.849468,
          5.914142,
          5.3872447,
          1.1421188,
          0.4694925,
          -1.9359576,
          0.7922053,
          -4.436914,
          0.48486072,
          0.15156937,
          -0.9069455,
          0.02573249,
          0.09608161,
          0.16887592,
          0.19422977,
          0.4393987,
          -0.75663644,
          -3.8168378,
          -2.5619087,
          0.12052492,
          0.34184015,
          -0.09898265,
          -3.9308836,
          -0.30432427,
          0.5921751,
          0.05944492,
          0.19160105,
          0.32062578,
          1.1501579,
          0.9267288,
          0.7365206,
          0.6617041,
          -0.6808248,
          0.6251773,
          -0.7510441,
          -0.2964771,
          0.5480576,
          -3.9724119,
          1.9098976,
          -6.986464,
          -6.049215,
          -7.176823,
          -7.2292213,
          -1.3826959,
          -0.971663,
          0.1314805,
          -0.34521863,
          -0.33586776,
          -0.37325734,
          -0.56450653,
          1.2260145,
          -6.752531,
          -0.6785642,
          1.6203092,
          0.7791109,
          0.6435135,
          -0.17330265,
          0.08512488,
          -0.32935762,
          0.3817836,
          0.07958502,
          0.3902994,
          0.32549825,
          0.5227272,
          1.006711,
          1.1724629,
          -0.7958121,
          -0.8689107,
          7.6402183,
          -0.73074627,
          8.073685,
          -4.789407,
          -4.883165,
          -4.705561,
          -4.763555,
          7.725675,
          -1.1059813,
          -5.071087,
          0.18753362,
          -2.5551322,
          -1.5451896,
          0.4852532,
          -1.2851087,
          -1.4281714,
          -0.5828984,
          -0.18949312,
          -0.9537923,
          0.31252503,
          -4.7370434,
          -11.70846,
          -0.20621873,
          0.9506296,
          1.1683906,
          0.1840033,
          1.0585555,
          -0.7216368,
          0.41767168,
          0.26246992,
          0.54075575,
          0.94413114,
          1.1386472,
          1.1646982,
          0.874966,
          1.1910542,
          1.5530456,
          0.7318678,
          0.43607745,
          -1.1967018,
          0.30187854,
          0.05562318,
          -0.97928643,
          0.7848561,
          -1.1613537,
          -1.1318257,
          0.76687044,
          -1.8994792,
          -1.2412175,
          -1.0606427,
          -0.515736,
          -1.0618995,
          0.54439527,
          0.1834922,
          2.3353035,
          0.41414303,
          -1.078963,
          4.5379176,
          -2.559154,
          -2.6327288,
          0.19257869,
          -3.0825958,
          -7.3593955,
          -6.269029,
          -7.255232,
          0.06491152,
          -6.5576544,
          -0.5190145,
          -1.3446251,
          0.2622292,
          -0.46467888,
          -0.6990876,
          -4.063223,
          -3.0952694,
          -1.167771,
          -0.36813873,
          10.945174,
          9.726871,
          2.168785,
          1.0920626,
          1.0322909,
          1.1572677,
          2.0974956,
          2.49639,
          -1.2801682,
          0.27627233,
          -0.6717766,
          -0.35117337,
          0.18563208,
          0.1517369,
          0.024552427,
          -1.0132203,
          4.8702354,
          -4.8562517,
          -11.620536,
          5.1406865,
          2.2090416,
          4.29737,
          4.6772547,
          -3.898202,
          -4.870593,
          0.77541596,
          -0.3218613,
          -4.8724446,
          -4.2230177,
          1.0601002,
          1.2162722,
          -0.8514439,
          -1.039328,
          -0.1679739,
          0.26209596,
          0.1792713,
          0.35610408,
          -1.4906276,
          -0.8327249,
          -0.63586175,
          0.22228862,
          -1.1426212,
          0.10098338,
          0.19637564,
          0.31906658,
          -0.18151361,
          0.05243338,
          -0.12340958,
          1.0651649,
          1.0794697,
          2.4729617,
          -1.0338924,
          0.030955153,
          0.7796488,
          0.97585815,
          -1.1453595,
          -1.046445,
          -1.0606788,
          -0.34565654,
          0.71262175,
          0.2907635,
          0.58951116,
          0.62907565,
          0.52636486,
          -0.9469517,
          -0.1956046,
          0.5750301,
          -3.8616056,
          0.3805752,
          -2.3533387,
          -1.467841,
          0.13898002,
          0.82591474,
          0.7291699,
          -1.116577,
          0.7247372,
          1.2174503,
          -0.7136276,
          0.6699861,
          -0.018219685,
          -5.211919,
          -5.381273,
          -1.4743525,
          -0.67130226,
          -0.8605086,
          5.1491857,
          -0.48134384,
          -0.4165309,
          -0.28456423,
          -0.16553918,
          0.6732076,
          0.6646636,
          1.0334374,
          -4.1818357,
          0.046756875,
          -0.33411315,
          -0.96290123,
          0.53863037,
          0.59734213,
          0.23293255,
          0.61040884,
          -0.7622464,
          -0.94349426,
          -4.6250434,
          0.37976837,
          -0.6772573,
          -1.1477093,
          -1.3738817,
          -1.1554397,
          -0.3587237,
          0.45702192,
          0.43764856,
          0.83040214,
          -1.2071375,
          0.7066726,
          0.4262349,
          2.432677,
          -4.9455414,
          -11.742379,
          -0.3083334,
          0.47580534,
          0.81698984,
          -0.25261495,
          0.38165587,
          0.07536653,
          -0.44710773,
          -0.34772322,
          -0.5875298,
          0.59360385,
          0.5884107,
          3.6734369,
          0.6842434,
          -1.028262,
          -0.50234824,
          -0.7048613,
          -0.50323653,
          -0.71879864,
          -0.9840901,
          -4.813769,
          0.6824809,
          0.6264799,
          1.1271679,
          1.0371807,
          0.7177932,
          -1.1409867,
          0.72185594,
          1.0673405,
          -8.770624,
          -5.0480423,
          -0.70308554,
          0.4191305,
          -1.4544293,
          -0.6340161,
          -1.6964289,
          -1.1540247,
          0.086798936,
          0.7965958,
          0.26920804,
          -0.90513045,
          -0.85089415,
          -0.6224565,
          -0.7303909,
          -5.7280965,
          -11.742768,
          0.7201057,
          -1.0983478,
          0.053842925,
          -0.9445096,
          -0.56292784,
          1.2593771,
          -1.430082,
          -0.8316429,
          0.30414182,
          -0.5912356,
          -0.4561453,
          0.8359213,
          -0.5450609,
          -1.1634963,
          1.2298771,
          -0.90216553,
          -1.0814409,
          -0.8388034,
          0.2106383,
          0.40103856,
          0.23677957,
          0.20551762,
          0.5038894,
          0.7885087,
          1.1792133,
          -2.8138256,
          0.026353613,
          0.4779723,
          -0.21749485,
          -0.025698382,
          -0.29394832,
          -0.24859206,
          -0.4711025,
          0.10406505,
          -0.9651003,
          -0.46577126,
          0.5732088,
          0.44801044,
          -0.064676814,
          2.7631946,
          0.581082,
          -1.2414114,
          0.28279358,
          -1.1964462,
          -4.5198984,
          -3.9769897,
          0.41315106,
          -1.8413535,
          -1.4552195,
          -0.9422365,
          0.30444622,
          -0.767172,
          -1.9477429,
          -0.62071335,
          -0.04321598,
          -0.31739733,
          -0.8738127,
          -7.5784836,
          -0.45611414,
          -0.744493,
          -0.37991577,
          -0.43340817,
          0.7478795,
          5.105169,
          0.7507387,
          0.6948442,
          1.0846629
         ],
         "xaxis": "x",
         "y": [
          2.1079414,
          4.0030327,
          0.82694095,
          -3.4574766,
          0.7219345,
          -3.4373572,
          0.13838379,
          -1.4387217,
          -1.8617612,
          -1.5331671,
          -2.1523733,
          -1.6128279,
          -3.9960206,
          -2.5048888,
          -0.38171798,
          0.9592435,
          -4.3101463,
          -3.993336,
          1.1302536,
          1.4694676,
          0.97909254,
          -0.8309324,
          0.7588586,
          3.8324194,
          3.0633621,
          2.579279,
          0.9818296,
          1.5096517,
          2.6318946,
          2.7704403,
          3.5993288,
          -2.9186912,
          0.49114448,
          3.0239966,
          1.9018781,
          1.3220916,
          2.1052659,
          2.1256545,
          1.7569525,
          0.49634674,
          0.8947046,
          -3.9791539,
          -6.1418543,
          1.1170723,
          2.919127,
          0.5476368,
          -6.5409236,
          -6.586367,
          -7.619132,
          -7.3464303,
          -3.8487186,
          1.0909107,
          3.0068364,
          0.5525539,
          0.1922301,
          -6.7484403,
          -3.600002,
          0.9866802,
          -6.786568,
          4.1371627,
          -3.5264356,
          -2.8632336,
          -3.129061,
          -6.2046313,
          -6.4903946,
          0.09583033,
          2.2361028,
          2.7895257,
          2.0236237,
          2.8079193,
          2.965211,
          -2.1660035,
          -2.8325481,
          4.115549,
          3.3195558,
          -3.899123,
          3.564422,
          -1.9625254,
          -0.19758645,
          -0.2569953,
          -0.22460945,
          -0.19867772,
          -3.6322815,
          4.3632245,
          -0.51718295,
          2.6211622,
          -3.6382573,
          -1.1580956,
          -4.2408504,
          -4.105251,
          -3.1771438,
          -4.123745,
          0.16840959,
          -1.9719304,
          0.59704334,
          -0.30605653,
          -3.4809783,
          7.1211815,
          5.4837203,
          5.52245,
          2.5253525,
          2.6546376,
          6.060631,
          4.1480575,
          4.725807,
          2.7454138,
          5.551473,
          5.8469133,
          5.8443913,
          5.5675364,
          3.0819998,
          6.026414,
          1.0502864,
          -4.4062066,
          -3.8671057,
          0.67688155,
          -1.230577,
          -3.144412,
          1.1260257,
          -3.8274918,
          -3.2724388,
          1.0459775,
          -3.9077075,
          -4.076811,
          -3.0557997,
          -4.138504,
          -3.821168,
          1.2368289,
          0.73089993,
          -0.43682656,
          0.5843084,
          -2.9284816,
          0.5661917,
          2.5851579,
          1.3237199,
          0.2539135,
          2.160082,
          -6.003347,
          -6.6832333,
          -7.6337194,
          -4.2381363,
          -6.5961857,
          -6.153869,
          0.6730683,
          2.6027985,
          -0.032222614,
          0.26556802,
          -5.440074,
          2.2876856,
          -3.2949386,
          -0.48053792,
          -5.2105103,
          -4.64932,
          6.979309,
          2.4770067,
          2.652077,
          2.786006,
          6.0790367,
          6.233119,
          2.8389356,
          2.7547367,
          2.603622,
          2.6694093,
          2.6018043,
          1.6612502,
          1.0301625,
          1.8715041,
          1.1264086,
          -0.319137,
          -3.3576984,
          0.58540523,
          -0.6185493,
          -2.2504673,
          -2.4767184,
          0.0696227,
          -0.33012772,
          1.0765241,
          -2.1176512,
          0.91087294,
          1.3689595,
          2.5401978,
          2.7224283,
          2.4853632,
          1.4964019,
          2.5349002,
          2.518257,
          2.0633795,
          0.66998416,
          -4.9286156,
          -2.752547,
          -2.9450212,
          0.8230622,
          -3.5131173,
          0.6888042,
          0.86700046,
          0.7566135,
          4.132679,
          2.6798718,
          3.1661425,
          3.0132642,
          2.6391666,
          0.26667845,
          -2.3514678,
          -2.6200156,
          1.0464313,
          -4.442745,
          -3.9558802,
          -3.2809377,
          -3.4644978,
          2.6137555,
          2.251058,
          2.1591334,
          1.8660445,
          1.6482698,
          1.8380458,
          4.2691154,
          2.5361078,
          1.4994811,
          -1.0994117,
          2.3675098,
          -1.8678627,
          -3.9732294,
          0.32351047,
          1.0524751,
          1.080972,
          -4.1819444,
          1.5188823,
          1.8788455,
          4.3318686,
          2.38697,
          2.3856397,
          -2.3869038,
          -2.080001,
          -2.7152777,
          3.6538212,
          -3.2708774,
          -3.4040878,
          -2.4433153,
          -3.999172,
          -4.487771,
          -4.9148016,
          1.0700641,
          1.5341346,
          1.8830372,
          -0.39248303,
          0.6391661,
          -6.541588,
          -3.9253576,
          1.7912716,
          0.6404636,
          0.89587814,
          6.527505,
          6.9889874,
          7.4933352,
          -0.2509233,
          0.87987524,
          -4.190254,
          -3.9476204,
          -3.9049258,
          -3.157163,
          -3.866179,
          1.7255024,
          0.71242195,
          1.3281329,
          -2.6305778,
          0.9415406,
          1.1172705,
          -0.41442156,
          -0.27544022,
          -3.4593446,
          4.1786547,
          -0.88980883,
          -2.6892436,
          -0.3458154,
          2.2966805,
          2.8750138,
          0.022116907,
          -5.446672,
          -0.06763437,
          1.9307352,
          -2.41366,
          0.6913655,
          1.8019264,
          -0.47580862,
          4.5273714,
          4.3964033,
          4.619422,
          5.533389,
          4.1977696,
          1.1644589,
          1.1854364,
          1.4407101,
          1.9539492,
          1.376734,
          1.230015,
          -4.047082,
          1.585645,
          1.77778,
          -1.9519916,
          -0.34844562,
          4.0201406,
          1.9144865,
          -4.971125,
          -6.171543,
          -4.466559,
          -3.1620924,
          2.8780973,
          0.7159308,
          1.3870528,
          -2.748327,
          4.181531,
          3.6575623,
          0.13188238,
          -1.7880998,
          -3.4001873,
          0.96746564,
          -3.9069724,
          1.410308,
          -3.1659298,
          6.937613,
          0.58523357,
          -1.7697842,
          -3.9549313,
          0.19418584,
          -4.056894,
          -4.9424024,
          1.0347248,
          -4.1747484,
          -3.8860118,
          1.009647,
          -2.8993776,
          -3.384048,
          3.9381194,
          2.7761643,
          2.5095308,
          0.96406955,
          2.7926311,
          2.2595305,
          1.1752688,
          1.3134286,
          0.6181019,
          3.002888,
          2.6484015,
          -4.246938,
          -2.1569817,
          -1.6356047,
          -1.8190002,
          -2.223807,
          2.9543557,
          -1.0602398,
          3.5340946,
          1.7618573,
          2.333267,
          0.5825034,
          0.020794263,
          1.91453,
          -2.6756341,
          1.3427088,
          3.5301065,
          0.16126916,
          1.8385597,
          1.5704501,
          3.6079705,
          3.854294,
          4.022728,
          3.2622728,
          4.1200075,
          3.4210572,
          3.5194101,
          2.7778797,
          2.2362566,
          3.0642498,
          0.34473673,
          4.45706,
          2.2619967,
          2.555917,
          2.4318943,
          2.576224,
          0.58817977,
          1.3110176,
          1.4842086,
          1.7598886
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "p align=\"center\">\n  <br/>\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/da..."
          ],
          [
           "<p align=\"center\">\n    <i>Huggingface Hub à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤§à¤¿à¤•à¤¾à¤°à¤¿à¤• à¤ªà¤¾à¤¯à¤¥à¤¨ à¤•à¥à¤²à¤¾à¤‡à¤‚à¤Ÿà¥¤</i>\n</p>\n\n<p align=\"center\"..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/mai..."
          ],
          [
           "---\n\n## huggingface_hub à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤®à¥‡à¤‚ à¤†à¤ªà¤•à¤¾ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ\n\n`huggingface_hub` à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤†à¤ªà¤•à¥‹ [à¤¹à¤—à¤¿à¤‚à¤— à¤«à¥‡à¤¸ à¤¹à¤¬..."
          ],
          [
           "## à¤ªà¥à¤°à¤®à¥à¤– à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾à¤à¤‚\n\n- [à¤«à¤¼à¤¾à¤‡à¤²à¥‡à¤‚ à¤¡à¤¾à¤‰à¤¨à¤²à¥‹à¤¡ à¤•à¤°à¥‡à¤‚](https://huggingface.co/docs/huggingface_hub/en/guides/..."
          ],
          [
           "```\n\nà¤¯à¤¦à¤¿ à¤†à¤ª à¤šà¤¾à¤¹à¥‡à¤‚, à¤¤à¥‹ à¤†à¤ª à¤‡à¤¸à¥‡ [conda](https://huggingface.co/docs/huggingface_hub/en/installation#ins..."
          ],
          [
           "```\n\nà¤¯à¤¾ à¤à¤• à¤¸à¤‚à¤ªà¥‚à¤°à¥à¤£ à¤­à¤‚à¤¡à¤¾à¤°\n\n```py\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(\"st..."
          ],
          [
           "```\n\nà¤¯à¤¾ à¤à¤• à¤¸à¤‚à¤ªà¥‚à¤°à¥à¤£ à¤«à¤¼à¥‹à¤²à¥à¤¡à¤°\n\n```py\nfrom huggingface_hub import upload_folder\n\nupload_folder(\n    fold..."
          ],
          [
           "```\n\n[à¤…à¤ªà¤²à¥‹à¤¡ à¤—à¤¾à¤‡à¤¡](https://huggingface.co/docs/huggingface_hub/en/guides/upload) à¤®à¥‡à¤‚ à¤µà¤¿à¤µà¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤à¥¤\n\n#..."
          ],
          [
           "à¤«à¤¾à¤¯à¤¦à¥‡ à¤¯à¥‡ à¤¹à¥ˆà¤‚:\n\n- à¤ªà¥à¤¸à¥à¤¤à¤•à¤¾à¤²à¤¯à¥‹à¤‚ à¤”à¤° à¤‰à¤¨à¤•à¥‡ à¤‰à¤ªà¤¯à¥‹à¤—à¤•à¤°à¥à¤¤à¤¾à¤“à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤¿à¤ƒà¤¶à¥à¤²à¥à¤• à¤®à¥‰à¤¡à¤² à¤¯à¤¾ à¤¡à¥‡à¤Ÿà¤¾à¤¸à¥‡à¤Ÿ à¤¹à¥‹à¤¸à¥à¤Ÿà¤¿à¤‚à¤—à¥¤\n- à¤—à¤¿à¤Ÿ-à¤†à¤§..."
          ],
          [
           "## à¤¯à¥‹à¤—à¤¦à¤¾à¤¨ (à¤¸à¥à¤µà¤¿à¤§à¤¾ à¤…à¤¨à¥à¤°à¥‹à¤§, à¤¬à¤—, à¤†à¤¦à¤¿) à¤•à¤¾ à¤…à¤¤à¤¿ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ ðŸ’™ðŸ’šðŸ’›ðŸ’œðŸ§¡â¤ï¸\n\nà¤¯à¥‹à¤—à¤¦à¤¾à¤¨ à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¤° à¤•à¤¿à¤¸à¥€ à¤•à¤¾ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ à¤”à¤° à¤¹..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "# Download from a dataset\n>>> hf_hub_download(repo_id=\"google/fleurs\", filename=\"fleurs.py\", repo_ty..."
          ],
          [
           "```\n\n### From specific version\n\nBy default, the latest version from the `main` branch is downloaded...."
          ],
          [
           "```\n\n**Note:** When using the commit hash, it must be the full-length hash instead of a 7-character ..."
          ],
          [
           "```\n\n### Filter files to download\n\n[`snapshot_download`] provides an easy way to download a reposito..."
          ],
          [
           "```\n\n## Download file(s) to local folder\n\nThe recommended (and default) way to download files from t..."
          ],
          [
           "However, in some cases you want to download files and move them to a specific folder. This is useful..."
          ],
          [
           "Here is a table that summarizes the different options to help you choose the parameters that best su..."
          ],
          [
           "**Note:** if you are on a Windows machine, you need to enable developer mode or run `huggingface_hub..."
          ],
          [
           "```\n\nYou can download multiple files at once which displays a progress bar and returns the snapshot ..."
          ],
          [
           "--\n# For reference on dataset card metadata, see the spec: https://github.com/huggingface/hub-docs/b..."
          ],
          [
           "## Uses\n\n<!-- Address questions around how the dataset is intended to be used. -->\n\n### Direct Use\n\n..."
          ],
          [
           "<!-- This section describes the people or systems who originally created the data. It should also in..."
          ],
          [
           "{{ bias_risks_limitations | default(\"[More Information Needed]\", true)}}\n\n### Recommendations\n\n<!-- ..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\n## Upload a folder\n\nUse the [`upload_folder`] function to upload a local folder to an existing ..."
          ],
          [
           "```\n\nBy default, the `.gitignore` file will be taken into account to know which files should be comm..."
          ],
          [
           "```\n\nYou can also use the `delete_patterns` argument to specify files you want to delete from the re..."
          ],
          [
           "```\n\n`local_path` and `path_in_repo` are optional and can be implicitly inferred. If `local_path` is..."
          ],
          [
           "```\n\n<Tip>\n\nBackground jobs are queued when using `run_as_future=True`. This means that you are guar..."
          ],
          [
           "```\n\n### Upload a folder by chunks\n\n[`upload_folder`] makes it easy to upload an entire folder to th..."
          ],
          [
           "```\n\nIf you want a better control on the upload strategy (i.e. the commits that are created), you ca..."
          ],
          [
           "```py\n>>> import json\n>>> import uuid\n>>> from pathlib import Path\n>>> import gradio as gr\n>>> from ..."
          ],
          [
           "```\n\nAnd that's it! User input/outputs and feedback will be available as a dataset on the Hub. By us..."
          ],
          [
           "#### Space persistence demo\n\nPersisting data from a Space to a Dataset on the Hub is the main use ca..."
          ],
          [
           "# 2. Zip png files in a single archive\n        with tempfile.TemporaryDirectory() as tmpdir:\n       ..."
          ],
          [
           "```\n\nWhen you overwrite `push_to_hub`, you have access to the attributes of [`CommitScheduler`] and ..."
          ],
          [
           "- [`CommitOperationCopy`] copies a file within a repository. This operation accepts three arguments:..."
          ],
          [
           "```\n\n2. Pass your operations to [`create_commit`]:\n\n```py\n>>> api.create_commit(\n...     repo_id=\"ly..."
          ],
          [
           "```\n\nIn addition to [`upload_file`] and [`upload_folder`], the following functions also use [`create..."
          ],
          [
           "</Tip>\n\nHere is a simple example illustrating how to pre-upload files:\n\n```py\n>>> from huggingface_h..."
          ],
          [
           "```\n\nFirst, we create the [`CommitOperationAdd`] objects one by one. In a real-world example, those ..."
          ],
          [
           "- **Start small**: We recommend starting with a small amount of data to test your upload script. It'..."
          ],
          [
           "<Tip warning={true}>\n\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTT..."
          ],
          [
           "```\n\nYou should install this for each repository that has a very large file. Once installed, you'll ..."
          ],
          [
           "```\n\nYou can check the status of your push with the `command_queue` method:\n\n```python\n>>> last_comm..."
          ],
          [
           "```\n\nHowever, if you aren't ready to push a file yet, you can use [`~Repository.git_add`] and [`~Rep..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "There are many ways you can contribute to this client library:\n* Fixing outstanding issues with the ..."
          ],
          [
           "If your issue is well written we're already 80% of the way there by the time you post it!\n\n## Submit..."
          ],
          [
           "```\n\n3. Create a new branch to hold your development changes, and do this for every new PR you work ..."
          ],
          [
           "```\n\n   This command will update your code to comply with the standards of the `huggingface_hub` rep..."
          ],
          [
           "```\n\n   Please write [good commit messages](https://chris.beams.io/posts/git-commit/).\n\n   It is a g..."
          ],
          [
           "```\n\n10. Once you are satisfied (**and the [checklist below](https://github.com/huggingface/huggingf..."
          ],
          [
           "### Tests\n\nAn extensive test suite is included to test the library behavior and several examples. Li..."
          ],
          [
           "```\n\nYou can specify a smaller set of tests in order to test only the feature you're working on.\n\nFo..."
          ],
          [
           "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pled..."
          ],
          [
           "## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported ..."
          ],
          [
           "**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attri..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nIf the CLI is correctly installed, you should see a list of all the options available in the CL..."
          ],
          [
           "```\n_|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      ..."
          ],
          [
           "```\n\nFor more details about authentication, check out [this section](../quick-start#authentication)...."
          ],
          [
           "```\n\n### Download a single file\n\nTo download a single file from a repo, simply provide the repo_id a..."
          ],
          [
           "```\n\n### Download multiple files\n\nYou can also download a subset of the files from a repository with..."
          ],
          [
           "```\n\nThe other approach is to provide patterns to filter which files you want to download using `--i..."
          ],
          [
           "```\n\n### Download to a local folder\n\nThe recommended (and default) way to download files from the Hu..."
          ],
          [
           "```\n\n### Specify a token\n\nTo access private or gated repositories, you must use a token. By default,..."
          ],
          [
           "```\n\nTo upload the current directory at the root of the repo, use:\n\n```bash\n>>> huggingface-cli uplo..."
          ],
          [
           "```\n\n### Upload multiple files\n\nTo upload multiple files from a folder at once without uploading the..."
          ],
          [
           "```\n\n**Note:** if `revision` does not exist and `--create-pr` is not set, a branch will be created a..."
          ],
          [
           "```\n\n### Specify a token\n\nTo upload files, you must use a token. By default, the token saved locally..."
          ],
          [
           "```\n\n## huggingface-cli scan-cache\n\nScanning your cache directory is useful if you want to know whic..."
          ],
          [
           "```bash\n>>> huggingface-cli scan-cache\nREPO ID                     REPO TYPE SIZE ON DISK NB FILES L..."
          ],
          [
           "Done in 0.0s. Scanned 6 repo(s) for a total of 3.4G.\nGot 1 warning(s) while scanning. Use -vvv to pr..."
          ],
          [
           "```\n\nFor more details about how to scan your cache directory, please refer to the [Manage your cache..."
          ],
          [
           "```bash\n>>> huggingface-cli env\n\nCopy-and-paste the text below in your GitHub issue.\n\n- huggingface_..."
          ],
          [
           "Running Tests\n\nTo run the test suite, please perform the following from the root directory of this r..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           ">>> # Read a remote file \n>>> with fs.open(\"datasets/my-username/my-dataset-repo/data/train.csv\", \"r..."
          ],
          [
           "```\n\nThe optional `revision` argument can be passed to run an operation from a specific commit such ..."
          ],
          [
           "```\n\nThe same workflow can also be used for [Dask](https://docs.dask.org/en/stable/how-to/connect-to..."
          ],
          [
           "```\n\n* Using the Hub as an array store with [Zarr](https://zarr.readthedocs.io/en/stable/tutorial.ht..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "[[autodoc]] huggingface_hub.DeleteCacheStrategy\n    - expected_freed_size_str\n\n## Exceptions\n\n### Co..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "Let's fetch the collection with, `\"TheBloke/recent-models-64f9a55bb3115b4f513ec026\"`:\n\n```py\n>>> fro..."
          ],
          [
           "```\n\nThe [`Collection`] object returned by [`get_collection`] contains:\n- high-level metadata: `slug..."
          ],
          [
           "```\n\n<Tip warning={true}>\n\nWhen listing collections, the item list per collection is truncated to 4 ..."
          ],
          [
           "```\n\nParameter `sort` must be one of  `\"last_modified\"`,  `\"trending\"` or `\"upvotes\"`. Parameter `it..."
          ],
          [
           "```\n\n## Manage items in a collection\n\nNow that we have a [`Collection`], we want to add items to it ..."
          ],
          [
           "```\n\nIf an item already exists in a collection (same `item_id`/`item_type` pair), an HTTP 409 error ..."
          ],
          [
           "```\n\n### Reorder items\n\nItems in a collection are ordered. The order is determined by the `position`..."
          ],
          [
           "```\n\n## Delete collection\n\nA collection can be deleted using [`delete_collection`].\n\n<Tip warning={t..."
          ],
          [
           "--\nlanguage:\n- en\nlicense: mit\nlibrary_name: pytorch-lightning\ntags:\n- pytorch\n- image-classificatio..."
          ],
          [
           "--\n[]\n---\n\n# invalid-card-data\n\nThis card should fail when trying to load it in because the card dat..."
          ],
          [
           "his document covers all steps that need to be done in order to do a release of the `huggingface_hub`..."
          ],
          [
           "```\ngit checkout main\n   ```\n\n9. Update the version to contain the `.dev0` suffix:\n```\n__version__ =..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nAt this step, your app should already be running on the Hub for free !\nHowever, you might want ..."
          ],
          [
           "```\n\n<Tip>\nFrom within your Space, secrets are available as environment variables (or\nStreamlit Secr..."
          ],
          [
           "```\n\n**4. Configure the hardware**\n\nBy default, your Space will run on a CPU environment for free. Y..."
          ],
          [
           "```\n```py\n>>> api.duplicate_space(\n...     from_id=repo_id,\n...     hardware=\"cpu-upgrade\",\n...     ..."
          ],
          [
           "```\n\nNote: if you are using a 'cpu-basic' hardware, you cannot configure a custom sleep time. Your S..."
          ],
          [
           "```\n\nYou can also delete your storage, losing all the data permanently.\n```py\n>>> api.delete_space_s..."
          ],
          [
           "```\n\n## More advanced: temporarily upgrade your Space !\n\nSpaces allow for a lot of different use cas..."
          ],
          [
           "# Space own repo_id\nTRAINING_SPACE_ID = \"Wauplin/dreambooth-training\"\n\nfrom huggingface_hub import H..."
          ],
          [
           "```\n\n### Task scheduler\n\nScheduling tasks can be done in many ways. Here is an example how it could ..."
          ],
          [
           "def mark_as_done(task):\n    model_id, dataset_id = task\n    with open(_get_csv_file()) as csv_file:\n..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\"\n       hr..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\"\n       hr..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\"\n       hr..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\"\n       hr..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "[[autodoc]] SpaceCard\n\n### SpaceCardData\n\n[[autodoc]] SpaceCardData\n\n## Utilities\n\n### EvalResult\n\n[..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "Inference Endpoints\n\nInference Endpoints provides a secure production solution to easily deploy mode..."
          ],
          [
           "[[autodoc]] InferenceEndpoint\n  - from_raw\n  - client\n  - async_client\n  - all\n\n## InferenceEndpoint..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\n## HfApi\n\n[[autodoc]] HfApi\n\n[[autodoc]] plan_multi_commits\n\n## API Dataclasses\n\n### AccessRequ..."
          ],
          [
           "### SafetensorsFileMetadata\n\n[[autodoc]] huggingface_hub.utils.SafetensorsFileMetadata\n\n### SpaceInf..."
          ],
          [
           "p align=\"center\">\n  <br/>\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/da..."
          ],
          [
           "<p align=\"center\">\n    <i>The official Python client for the Huggingface Hub.</i>\n</p>\n\n<p align=\"ce..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface..."
          ],
          [
           "## Key features\n\n- [Download files](https://huggingface.co/docs/huggingface_hub/en/guides/download) ..."
          ],
          [
           "```\n\nIf you prefer, you can also install it with [conda](https://huggingface.co/docs/huggingface_hub..."
          ],
          [
           "```\n\n### Create a repository\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(repo_id=\"su..."
          ],
          [
           "```\n\nFor details in the [upload guide](https://huggingface.co/docs/huggingface_hub/en/guides/upload)..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "### @webhook_endpoint\n\n[[autodoc]] huggingface_hub.webhook_endpoint\n\n## Payload\n\n[`WebhookPayload`] ..."
          ],
          [
           "[[autodoc]] huggingface_hub.WebhookPayloadRepo\n\n### WebhookPayloadUrl\n\n[[autodoc]] huggingface_hub.W..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./p..."
          ],
          [
           "## Contribute\n\nAll contributions to the `huggingface_hub` are welcomed and equally valued! ðŸ¤— Besides..."
          ],
          [
           "--\n# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blo..."
          ],
          [
           "### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** {{ ..."
          ],
          [
           "### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, ..."
          ],
          [
           "## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n#..."
          ],
          [
           "- **Hardware Type:** {{ hardware_type | default(\"[More Information Needed]\", true)}}\n- **Hours used:..."
          ],
          [
           "## More Information [optional]\n\n{{ more_information | default(\"[More Information Needed]\", true)}}\n\n..."
          ],
          [
           "MyCoolModel\n\nIn this example, we don't have any metadata at the top of the file. In cases like these..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "</Tip>\n\n\n## Create an endpoint\n\nImplementing a webhook endpoint is as simple as decorating a functio..."
          ],
          [
           "```\n\nSave this snippet in a file called `'app.py'` and run it with `'python app.py'`. You should see..."
          ],
          [
           "```\n\nGood job! You just launched a webhook server! Let's break down what happened exactly:\n\n1. By de..."
          ],
          [
           "</Tip>\n\n\n## Configure a Webhook\n\nNow that you have a webhook server running, you want to configure a..."
          ],
          [
           "Your webhook server is now running on a public Space. If most cases, you will want to secure it with..."
          ],
          [
           "```\n\nWhich will create two endpoints:\n\n```text\n(...)\nWebhooks are correctly setup and ready to use:\n..."
          ],
          [
           "```\n\n1. We define a custom UI using Gradio blocks. This UI will be displayed on the landing page of ..."
          ],
          [
           "!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "```\n\nThe docs will be viewable at [http://localhost:3000](http://localhost:3000). You can also previ..."
          ],
          [
           "```\n\nUse the relative style to link to the new file so that the versioned docs continue to work.\n\nFo..."
          ],
          [
           "If you want to create a link to some internal class or function, you need to\nprovide its path. For i..."
          ],
          [
           "```\n    Args:\n        n_layers (`int`): The number of layers of the model.\n```\n\nIf the description i..."
          ],
          [
           "```\n```\n# first line of code\n# second line\n# etc\n```\n````\n\n#### Writing a return block\n\nThe return b..."
          ],
          [
           "```\n\n#### Adding an image\n\nDue to the rapidly growing repository, it is important to make sure that ..."
          ],
          [
           "```\n    Example:\n\n    ```python\n    >>> from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n ..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "### Create a repository\n\nCreate an empty repository with [`create_repo`] and give it a name with the..."
          ],
          [
           "```\n\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` paramet..."
          ],
          [
           "```\n\n## Upload and download files\n\nNow that you have created your repository, you are interested in ..."
          ],
          [
           "```\n\nYou can use the [`delete_branch`] and [`delete_tag`] functions in the same way to delete a bran..."
          ],
          [
           "Some settings are specific to Spaces (hardware, environment variables,...). To configure those, plea..."
          ],
          [
           "```\n\n### Rename your repository\n\nYou can rename your repository on the Hub using [`move_repo`]. Usin..."
          ],
          [
           "```\n\n### Clone\n\nThe `clone_from` parameter clones a repository from a Hugging Face repository ID to ..."
          ],
          [
           "```\n\n### Branch\n\nBranches are important for collaboration and experimentation without impacting your..."
          ],
          [
           "p align=\"center\">\n  <br/>\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/da..."
          ],
          [
           "<p align=\"center\">\n    <i>Hugging Face Hub Python å®¢æˆ·ç«¯</i>\n</p>\n\n<p align=\"center\">\n    <a href=\"http..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/mai..."
          ],
          [
           "---\n\n## æ¬¢è¿Žä½¿ç”¨ Hugging Face Hub åº“\n\né€šè¿‡`huggingface_hub` åº“ï¼Œæ‚¨å¯ä»¥ä¸Žé¢å‘æœºå™¨å­¦ä¹ å¼€å‘è€…å’Œåä½œè€…çš„å¹³å° [Hugging Face Hub](https..."
          ],
          [
           "```\n\nå¦‚æžœæ‚¨æ›´å–œæ¬¢ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ conda è¿›è¡Œå®‰è£…\n\nä¸ºäº†é»˜è®¤ä¿æŒåŒ…çš„æœ€å°åŒ–ï¼Œhuggingface_hub å¸¦æœ‰ä¸€äº›å¯é€‰çš„ä¾èµ–é¡¹ï¼Œé€‚ç”¨äºŽæŸäº›ç”¨ä¾‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨æƒ³è¦å®Œæ•´çš„æŽ¨æ–­ä½“éªŒï¼Œè¯·è¿è¡Œï¼š\n\n`..."
          ],
          [
           "```\n\n### åˆ›å»ºä¸€ä¸ªå­˜å‚¨åº“\n\nè¦åˆ›å»ºä¸€ä¸ªæ–°å­˜å‚¨åº“ï¼Œè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n\n```py\nfrom huggingface_hub import create_repo\n\ncreate_repo(rep..."
          ],
          [
           "```\n\næœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [ä¸Šä¼ æŒ‡å—](https://huggingface.co/docs/huggingface_hub/en/guides/upload).\n\n## é›†æˆåˆ° Hub ä¸­\n..."
          ],
          [
           "## æ¬¢è¿Žå„ç§è´¡çŒ®ï¼ˆåŠŸèƒ½è¯·æ±‚ã€é”™è¯¯ç­‰ï¼‰ ðŸ’™ðŸ’šðŸ’›ðŸ’œðŸ§¡â¤ï¸\n\næ¬¢è¿Žæ¯ä¸ªäººæ¥è¿›è¡Œè´¡çŒ®ï¼Œæˆ‘ä»¬é‡è§†æ¯ä¸ªäººçš„è´¡çŒ®ã€‚ç¼–å†™ä»£ç å¹¶éžå”¯ä¸€çš„å¸®åŠ©ç¤¾åŒºçš„æ–¹å¼ã€‚å›žç­”é—®é¢˜ã€å¸®åŠ©ä»–äººã€ç§¯æžäº’åŠ¨å¹¶æ”¹å–„æ–‡æ¡£å¯¹ç¤¾åŒºæ¥è¯´éƒ½æ˜¯æžå…¶æœ‰ä»·å€¼çš„..."
          ],
          [
           "p align=\"center\">\n  <br/>\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/da..."
          ],
          [
           "<p align=\"center\">\n    <i>Der offizielle Python-Client fÃ¼r den Huggingface Hub.</i>\n</p>\n\n<p align=\"..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/mai..."
          ],
          [
           "---\n\n## Willkommen bei der huggingface_hub Bibliothek\n\nDie `huggingface_hub` Bibliothek ermÃ¶glicht I..."
          ],
          [
           "## Hauptmerkmale\n\n- Dateien vom Hub [herunterladen](https://huggingface.co/docs/huggingface_hub/de/g..."
          ],
          [
           "```\n\nWenn Sie mÃ¶chten, kÃ¶nnen Sie es auch mit [conda](https://huggingface.co/docs/huggingface_hub/de..."
          ],
          [
           "```\n\nOder eine gesamte Repository\n\n```py\nfrom huggingface_hub import snapshot_download\n\nsnapshot_dow..."
          ],
          [
           "```\n\nWeitere Informationen finden Sie im [Upload-Leitfaden](https://huggingface.co/docs/huggingface_..."
          ],
          [
           "## BeitrÃ¤ge (Feature-Anfragen, Fehler usw.) sind super willkommen ðŸ’™ðŸ’šðŸ’›ðŸ’œðŸ§¡â¤ï¸\n\nJeder ist willkommen beiz..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "Contrib test suite\n\nThe contrib folder contains simple end-to-end scripts to test integration of `hu..."
          ],
          [
           "```\n\nThen tests can be run\n\n```sh\n# Optional: -j4 to run in parallel. Output will be messy in that c..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "--\nlanguage: en\nlicense: mit\nlibrary_name: timm\ntags:\n- pytorch\n- image-classification\ndatasets:\n- b..."
          ],
          [
           "p align=\"center\">\n  <br/>\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/da..."
          ],
          [
           "<p align=\"center\">\n    <i>ê³µì‹ Huggingface Hub íŒŒì´ì¬ í´ë¼ì´ì–¸íŠ¸</i>\n</p>\n\n<p align=\"center\">\n    <a href=\"htt..."
          ],
          [
           "<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/mai..."
          ],
          [
           "---\n\n## huggingface_hub ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œìš”\n\n`huggingface_hub` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” [Hugging Face Hub](https://huggingface.co..."
          ],
          [
           "## ì£¼ìš” ê¸°ëŠ¥\n\n- Hubì—ì„œ [íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ](https://huggingface.co/docs/huggingface_hub/main/ko/guides/download)\n- ..."
          ],
          [
           "```\n\nì›í•œë‹¤ë©´ [conda](https://huggingface.co/docs/huggingface_hub/ko/installation#install-with-conda)ë¥¼ ì´..."
          ],
          [
           "```\n\níŒŒì¼ì€ ë¡œì»¬ ìºì‹œ í´ë”ì— ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤. ìžì„¸í•œ ë‚´ìš©ì€ [ì´ ê°€ì´ë“œ](https://huggingface.co/docs/huggingface_hub/ko/guides/ma..."
          ],
          [
           "```\n\në ˆí¬ì§€í† ë¦¬ ì „ì²´ì˜ ê²½ìš°:\n\n```py\nfrom huggingface_hub import upload_folder\n\nupload_folder(\n    folder_path=..."
          ],
          [
           "```\n\nìžì„¸í•œ ë‚´ìš©ì€ [ì—…ë¡œë“œ ê°€ì´ë“œ](https://huggingface.co/docs/huggingface_hub/ko/guides/upload)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n\n## Hug..."
          ],
          [
           "ì´ë ‡ê²Œ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ìž¥ì ì´ ìžˆìŠµë‹ˆë‹¤:\n\n- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ìžë“¤ì˜ ëª¨ë¸ì´ë‚˜ ë°ì´í„°ì…‹ì„ ë¬´ë£Œë¡œ í˜¸ìŠ¤íŒ…í•´ì¤ë‹ˆë‹¤.\n- gitì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë°©ì‹ìœ¼ë¡œ, ì•„ì£¼ í° íŒŒì¼ë“¤ë„ ë²„ì „ì„ ê´€ë¦¬í• ..."
          ],
          [
           "ì—¬ëŸ¬ë¶„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•©í•˜ê³  ì‹¶ë‹¤ë©´, ì´ìŠˆë¥¼ ì—´ì–´ì„œ ì˜ê²¬ì„ ë‚˜ëˆ ì£¼ì„¸ìš”. í†µí•© ê³¼ì •ì„ ì•ˆë‚´í•˜ê¸° ìœ„í•´ â¤ï¸ì„ ë‹´ì•„ [ë‹¨ê³„ë³„ ê°€ì´ë“œ](https://huggingface.co/docs..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "## Translating the `huggingface_hub` documentation into your language\n\nAs part of our mission to dem..."
          ],
          [
           "```\n\nHere, `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https:..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nThe levels should be understood as follows:\n\n- `error`: only show critical logs about usage whi..."
          ],
          [
           "By default, progress bars are enabled. You can disable them globally by setting `HF_HUB_DISABLE_PROG..."
          ],
          [
           "```\n\n### are_progress_bars_disabled\n\n[[autodoc]] huggingface_hub.utils.are_progress_bars_disabled\n\n#..."
          ],
          [
           "`huggingface_hub` defines its own HTTP errors to refine the `HTTPError` raised by\n`requests` with ad..."
          ],
          [
           "```\n\n[[autodoc]] huggingface_hub.utils.hf_raise_for_status\n\n### HTTP errors\n\nHere is a list of HTTP ..."
          ],
          [
           "#### OfflineModeIsEnabled\n\n[[autodoc]] huggingface_hub.utils.OfflineModeIsEnabled\n\n## Telemetry\n\n`hu..."
          ],
          [
           ">>> @validate_hf_hub_args\n... def my_cool_method(repo_id: str):\n...     print(repo_id)\n\n>>> my_cool_..."
          ],
          [
           "```\n\n#### validate_hf_hub_args\n\n[[autodoc]] utils.validate_hf_hub_args\n\n#### HFValidationError\n\n[[au..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nè¦ä¸‹è½½æ–‡ä»¶çš„ç‰¹å®šç‰ˆæœ¬ï¼Œè¯·ä½¿ç”¨`revision`å‚æ•°æŒ‡å®šåˆ†æ”¯åç§°ã€æ ‡ç­¾æˆ–æäº¤å“ˆå¸Œã€‚å¦‚æžœæ‚¨é€‰æ‹©ä½¿ç”¨æäº¤å“ˆå¸Œï¼Œå®ƒå¿…é¡»æ˜¯å®Œæ•´é•¿åº¦çš„å“ˆå¸Œï¼Œè€Œä¸æ˜¯è¾ƒçŸ­çš„7ä¸ªå­—ç¬¦çš„æäº¤å“ˆå¸Œï¼š\n\n```py\n>>> fr..."
          ],
          [
           "```\n\næˆ–è€…ï¼Œä½ å¯ä»¥åœ¨ç¬”è®°æœ¬ç”µè„‘æˆ–è„šæœ¬ä¸­ä½¿ç”¨ [`login`] æ¥è¿›è¡Œç¨‹åºåŒ–ç™»å½•,è¯·è¿è¡Œä»¥ä¸‹ä»£ç :\n\n```py\n>>> from huggingface_hub import login  \n>..."
          ],
          [
           "```\nå¦‚æžœæ‚¨æƒ³å°†å­˜å‚¨åº“è®¾ç½®ä¸ºç§æœ‰ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n\n```py\n>>> from huggingface_hub import HfApi  \n>>> api = HfApi()  \n>>> ..."
          ],
          [
           "```\n\nè¦ä¸€æ¬¡ä¸Šä¼ å¤šä¸ªæ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹[ä¸Šä¼ æŒ‡å—](./guides/upload) ,è¯¥æŒ‡å—å°†å‘æ‚¨ä»‹ç»å‡ ç§ä¸Šä¼ æ–‡ä»¶çš„æ–¹æ³•ï¼ˆæœ‰æˆ–æ²¡æœ‰ gitï¼‰ã€‚\n\n## ä¸‹ä¸€æ­¥\n\n`huggingface_hub`åº“ä¸º..."
          ],
          [
           "Hugging Face Hub Client library\n\n## Download files from the Hub\n\nThe `hf_hub_download()` function is..."
          ],
          [
           "```\n\n### `snapshot_download`\n\nUsing `hf_hub_download()` works well when you know which files you wan..."
          ],
          [
           "If you check out this URL's headers with a `HEAD` http request (which you can do\nfrom the command li..."
          ],
          [
           "```\n\nWith the `HfApi` class there are methods to query models, datasets, and metrics by specific tag..."
          ],
          [
           "```\n\nIf the repository you're cloning is one of yours or one of your organisation's,\nthen having the..."
          ],
          [
           "```\n\nFinally, you can choose to specify the Git username and email attributed to that\nclone directly..."
          ],
          [
           "```\n\nThe repository can be managed through this object, through wrappers of\ntraditional Git methods:..."
          ],
          [
           "These two methods also have support for the `blocking` parameter.\n\nExamples using the `commit` conte..."
          ],
          [
           "```\n\n```python\n>>> import torch\n>>> model = torch.nn.Transformer()\n>>> with Repository(\"torch-model\"..."
          ],
          [
           "```\n\nThis should be executed once for each model repo that contains a model file\n>5GB. If you just t..."
          ],
          [
           "```\n\nThis is an example of a task (`question-answering`) which requires a dictionary\nas input thas h..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nOnce done, [check installation](#check-installation) is working correctly.\n\n### Install optiona..."
          ],
          [
           "```\n\nWhen installing from source, you can also specify a specific branch. This is useful if you\nwant..."
          ],
          [
           "```\n\nThis command will fetch information from the Hub about the [gpt2](https://huggingface.co/gpt2) ..."
          ],
          [
           "```\n\n## Windows limitations\n\nWith our goal of democratizing good ML everywhere, we built `huggingfac..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\n`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional..."
          ],
          [
           "```\n\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overvi..."
          ],
          [
           "```\n\n[`HfApi.get_discussion_details`] returns a [`DiscussionWithDetails`] object, which is a subclas..."
          ],
          [
           "```\n\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to cr..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "If you are interested in Inference and Widgets, you can follow [this guide](https://huggingface.co/d..."
          ],
          [
           "```\n\n### push_to_hub\n\nThe `push_to_hub` method often requires a bit more complexity to handle repo c..."
          ],
          [
           "```\n\nThis is of course only an example. If you are interested in more complex manipulations (delete ..."
          ],
          [
           "In a lot of cases, a library already implements its model using a Python class. The class contains t..."
          ],
          [
           "The advantage of using [`ModelHubMixin`] is that once you take care of the serialization/loading of ..."
          ],
          [
           "```\n\n#### Implementation\n\nThe implementation is actually very straightforward, and the full implemen..."
          ],
          [
           "```\n\n3. Implement the `_from_pretrained` method:\n\n```python\nclass PyTorchModelHubMixin(ModelHubMixin..."
          ],
          [
           "```\n\nAnd that's it! Your library now enables users to upload and download files to and from the Hub...."
          ],
          [
           "Inference Endpoints\n\nInference Endpoints provides a secure production solution to easily deploy any ..."
          ],
          [
           "```\n\nIn this example, we created a `protected` Inference Endpoint named `\"my-endpoint-name\"`, to ser..."
          ],
          [
           "```\n\nIt's a dataclass that holds information about the endpoint. You can access important attributes..."
          ],
          [
           "```python\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\n>>> from huggingface_hub impor..."
          ],
          [
           "```\n\nThe value to pass as `custom_image` is a dictionary containing a url to the docker container an..."
          ],
          [
           "```\n\n## Check deployment status\n\nIn the rest of this guide, we will assume that we have a [`Inferenc..."
          ],
          [
           "```\n\nInstead of fetching the Inference Endpoint status while waiting for it to run, you can directly..."
          ],
          [
           "```\n\nIf `timeout` is set and the Inference Endpoint takes too much time to load, a [`InferenceEndpoi..."
          ],
          [
           "```\n\nFor more details about how to use the [`InferenceClient`], check out the [Inference guide](../g..."
          ],
          [
           "</Tip>\n\n```py\n# Pause and resume endpoint\n>>> endpoint.pause()\nInferenceEndpoint(name='my-endpoint-n..."
          ],
          [
           "```\n\n### Update model or hardware requirements\n\nIn some cases, you might also want to update your In..."
          ],
          [
           "```\n\n### Delete the endpoint\n\nFinally if you won't use the Inference Endpoint anymore, you can simpl..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nWhile filtering, you can also sort the models and take only the top results. For example,\nthe f..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "<Tip warning={true}>\n\n[`Repository`] is now deprecated in favor of the http-based alternatives. Give..."
          ],
          [
           "This preference of the http-based [`HfApi`] over the git-based [`Repository`] does not mean that git..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nTo download a specific version of the file, use the `revision` parameter to specify the\nbranch ..."
          ],
          [
           "```\n\nThe command will tell you if you are already logged in and prompt you for your token. The token..."
          ],
          [
           "```\nfrom transformers import whoami\n\nuser = whoami(token=...)\n```\n\nThis is usually discouraged excep..."
          ],
          [
           "```\n\nPrivate repositories will not be visible to anyone except yourself.\n\n<Tip>\n\nTo create a reposit..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "</Tip>\n\nå¦‚æžœä½ æƒ³åœ¨Hubä¸Šåˆ›å»ºå’Œç®¡ç†ä¸€ä¸ªä»“åº“ï¼Œä½ çš„è®¡ç®—æœºå¿…é¡»å¤„äºŽç™»å½•çŠ¶æ€ã€‚å¦‚æžœå°šæœªç™»å½•ï¼Œè¯·å‚è€ƒ[æ­¤éƒ¨åˆ†](../quick-start#login)ã€‚åœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å‡è®¾ä½ çš„è®¡ç®—æœºå·²..."
          ],
          [
           "```\n\né»˜è®¤æƒ…å†µä¸‹ï¼Œ[`create_repo`] ä¼šåˆ›å»ºä¸€ä¸ªæ¨¡åž‹ä»“åº“ã€‚ä½†æ˜¯ä½ å¯ä»¥ä½¿ç”¨ `repo_type`å‚æ•°æ¥æŒ‡å®šå…¶ä»–ä»“åº“ç±»åž‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä½ æƒ³åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ä»“åº“\n\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n\n```p..."
          ],
          [
           "```\n\n### å…‹éš†ä¸€ä¸ªä»“åº“ï¼ˆä»…é€‚ç”¨äºŽ Spacesï¼‰\n\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½æƒ³è¦å¤åˆ¶åˆ«äººçš„ä»“åº“å¹¶æ ¹æ®è‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚å¯¹äºŽ Spacesï¼Œä½ å¯ä»¥ä½¿ç”¨ [`duplicate_space`] æ–¹æ³•..."
          ],
          [
           "```\n\n## ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶\n\næ—¢ç„¶æ‚¨å·²ç»åˆ›å»ºäº†æ‚¨çš„å­˜å‚¨åº“ï¼Œæ‚¨çŽ°åœ¨ä¹Ÿå¯ä»¥æŽ¨é€æ›´æ”¹è‡³å…¶ä¸­å¹¶ä»Žä¸­ä¸‹è½½æ–‡ä»¶\n\nè¿™ä¸¤ä¸ªä¸»é¢˜æœ‰å®ƒä»¬è‡ªå·±çš„æŒ‡å—ã€‚è¯·[ä¸Šä¼ æŒ‡å—](./upload) å’Œ[ä¸‹è½½æŒ‡å—](./downl..."
          ],
          [
           "```\n\nåŒæ—¶,ä½ å¯ä»¥ä»¥ç›¸åŒçš„æ–¹å¼ä½¿ç”¨ [`delete_branch`] å’Œ [`delete_tag`] å‡½æ•°æ¥åˆ é™¤åˆ†æ”¯æˆ–æ ‡ç­¾\n\n### åˆ—å‡ºæ‰€æœ‰çš„åˆ†æ”¯å’Œæ ‡ç­¾\n\nä½ è¿˜å¯ä»¥ä½¿ç”¨ [`list_rep..."
          ],
          [
           "```\n\n## ä¿®æ”¹å­˜å‚¨åº“è®¾ç½®\n\nå­˜å‚¨åº“å…·æœ‰ä¸€äº›å¯é…ç½®çš„è®¾ç½®ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‚¨é€šå¸¸ä¼šåœ¨æµè§ˆå™¨ä¸­çš„å­˜å‚¨åº“è®¾ç½®é¡µé¢ä¸Šæ‰‹åŠ¨é…ç½®è¿™äº›è®¾ç½®ã€‚è¦é…ç½®å­˜å‚¨åº“ï¼Œæ‚¨å¿…é¡»å…·æœ‰å¯¹å…¶çš„å†™è®¿é—®æƒé™ï¼ˆæ‹¥æœ‰å®ƒæˆ–å±žäºŽç»„ç»‡ï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­..."
          ],
          [
           "```\n\n## ç®¡ç†å­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬\n\nä¸Šè¿°æ‰€æœ‰æ“ä½œéƒ½å¯ä»¥é€šè¿‡HTTPè¯·æ±‚å®Œæˆã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨æœ¬åœ°æ‹¥æœ‰å­˜å‚¨åº“çš„å‰¯æœ¬ï¼Œå¹¶ä½¿ç”¨æ‚¨ç†Ÿæ‚‰çš„Gitå‘½ä»¤ä¸Žä¹‹äº¤äº’ã€‚\n\n[`Repository`]..."
          ],
          [
           "```\n\nä½ å¯ä»¥å°†`clone_from`å‚æ•°ä¸Ž[`create_repo`]ç»“åˆä½¿ç”¨ï¼Œä»¥åˆ›å»ºå¹¶å…‹éš†ä¸€ä¸ªå­˜å‚¨åº“ï¼š\n\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n\n```py\n>>> repo_url = create_repo..."
          ],
          [
           "```\n\n### æ‹‰å–\n\n[`~Repository.git_pull`] å…è®¸ä½ ä½¿ç”¨è¿œç¨‹å­˜å‚¨åº“çš„æ›´æ”¹æ›´æ–°å½“å‰æœ¬åœ°åˆ†æ”¯ï¼š\n\nè¯·è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n\n```py\n>>> from huggingface_h..."
          ],
          [
           "--\nlanguage: en\nlicense: mit\nlibrary_name: timm\ntags:\n- pytorch\n- image-classification\ndatasets:\n- b..."
          ],
          [
           "--\nlicense: mit\nlanguage: eo\nthumbnail: https://huggingface.co/blog/assets/01_how-to-train/EsperBERT..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nAnother way you might want to do this is with f-strings. In the following example, we:\n\n- Use [..."
          ],
          [
           "```\n---\nlanguage: en\nlicense: mit\nlibrary: timm\n---\n\n# My Model Card\n\nThis model created by [@natera..."
          ],
          [
           "```\n\nNow, as you can see, the metadata header has been updated:\n\n```\n---\nlanguage: fr\nlicense: apach..."
          ],
          [
           "```\n\n## Share Model Cards\n\nIf you're authenticated with the Hugging Face Hub (either by using `huggi..."
          ],
          [
           "```\n\nA resulting PR created from this command can be seen [here](https://huggingface.co/nateraw/hf-h..."
          ],
          [
           "```\n\nIt often happen that you want to suggest some changes to a repository\non which you don't have w..."
          ],
          [
           "```\n\nIf you have more than one evaluation result you'd like to share, just pass a list of `EvalResul..."
          ],
          [
           "--\nlanguage:\n- en\nlicense:\n- bsd-3-clause\nannotations_creators:\n- crowdsourced\n- expert-generated\nla..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "Defaults to `\"$HF_HOME/assets\"` (e.g. `\"~/.cache/huggingface/assets\"` by default).\n\n### HF_TOKEN\n\nTo..."
          ],
          [
           "### HF_HUB_DOWNLOAD_TIMEOUT\n\nInteger value to define the number of seconds to wait for server respon..."
          ],
          [
           "### HF_HUB_DISABLE_IMPLICIT_TOKEN\n\nAuthentication is not mandatory for every requests to the Hub. Fo..."
          ],
          [
           "For more details, see [cache limitations](../guides/manage-cache#limitations).\n\n### HF_HUB_DISABLE_E..."
          ],
          [
           "### HF_HUB_ENABLE_HF_TRANSFER\n\nSet to `True` for faster uploads and downloads from the Hub using `hf..."
          ],
          [
           "## From external tools\n\nSome environment variables are not specific to `huggingface_hub` but are sti..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "ä½¿ç”¨ [`Repository`] çš„æœ€å¤§ä¼˜ç‚¹æ˜¯å®ƒå…è®¸ä½ åœ¨æœ¬åœ°æœºå™¨ä¸Šç»´æŠ¤æ•´ä¸ªå­˜å‚¨åº“çš„æœ¬åœ°å‰¯æœ¬ã€‚è¿™ä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªç¼ºç‚¹ï¼Œå› ä¸ºå®ƒéœ€è¦ä½ ä¸æ–­æ›´æ–°å’Œç»´æŠ¤è¿™ä¸ªæœ¬åœ°å‰¯æœ¬ã€‚è¿™ç±»ä¼¼äºŽä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸­ï¼Œæ¯ä¸ªå¼€å‘äººå‘˜éƒ½ç»´æŠ¤è‡ªå·±..."
          ],
          [
           "å¦‚æžœæ‚¨åœ¨æœ¬åœ°æœºå™¨ä¸Šè®­ç»ƒæ¨¡åž‹ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„ git å·¥ä½œæµç¨‹å¹¶å®šæœŸæŽ¨é€æ›´æ–°å¯èƒ½æ›´æœ‰æ•ˆã€‚`Repository` è¢«ä¼˜åŒ–ä¸ºæ­¤ç±»æƒ…å†µï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨åŽå°è¿è¡Œã€‚\nå¦‚æžœæ‚¨éœ€è¦æ‰‹åŠ¨ç¼–è¾‘å¤§åž‹æ–‡ä»¶ï¼Œ`git `æ˜¯æœ€ä½³é€‰æ‹©..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n<CACHE_DIR>\nâ”œâ”€ datasets--glue\nâ”‚  â”œâ”€ refs\nâ”‚  â”œâ”€ blobs\nâ”‚  â”œâ”€ snapshots\n...\n```\n\nEach folder is des..."
          ],
          [
           "```\n<CACHE_DIR>/<REPO_NAME>/snapshots/aaaaaa/README.md\n```\n\nThat `README.md` file is actually a syml..."
          ],
          [
           "```\n<CACHE_DIR>/<REPO_NAME>/.no_exist/aaaaaa/config_that_does_not_exist.json\n```\n\nUnlike the `snapsh..."
          ],
          [
           "```\n\n### In practice\n\nIn practice, your cache should look like the following tree:\n\n```text\n    [  9..."
          ],
          [
           "```\n\n### Limitations\n\nIn order to have an efficient cache-system, `huggingface-hub` uses symlinks. H..."
          ],
          [
           "```py\nfrom huggingface_hub import cached_assets_path\n\nassets_path = cached_assets_path(library_name=..."
          ],
          [
           "```\n\n<Tip>\n\n[`cached_assets_path`] is the recommended way to store assets but is not mandatory. If\ny..."
          ],
          [
           "```\n\n## Scan your cache\n\nAt the moment, cached files are never deleted from your local directory: wh..."
          ],
          [
           "```text\nâžœ huggingface-cli scan-cache\nREPO ID                     REPO TYPE SIZE ON DISK NB FILES LAS..."
          ],
          [
           "```\n\nTo get a more detailed report, use the `--verbose` option. For each repo, you get a\nlist of all..."
          ],
          [
           "```text\nâžœ huggingface-cli scan-cache -v\nREPO ID                     REPO TYPE REVISION              ..."
          ],
          [
           "google/fleurs               dataset   129b6e96cf1967cd5d2b9b6aec75ce6cce7c89e8        25.4K        3..."
          ],
          [
           "bert-base-cased             model     a8d257ba9925ef39f3036bfc338acf5283c512d9         1.4G        9..."
          ],
          [
           "t5-small                    model     d78aea13fa7ecd06c29e3e46195d6341255065d5       970.7M        9..."
          ],
          [
           "```\n\n#### Grep example\n\nSince the output is in tabular format, you can combine it with any `grep`-li..."
          ],
          [
           "```\n\n### Scan cache from Python\n\nFor a more advanced usage, use [`scan_cache_dir`] which is the pyth..."
          ],
          [
           "Here is a simple usage example. See reference for details.\n\n```py\n>>> from huggingface_hub import sc..."
          ],
          [
           "```\n\n## Clean your cache\n\nScanning your cache is interesting but what you really want to do next is ..."
          ],
          [
           "</Tip>\n\n### Clean cache from the terminal\n\nThe easiest way to delete some revisions from your HF cac..."
          ],
          [
           "```\npip install huggingface_hub[\"cli\"]\n```\n\nThen run the command:\n\n```\nhuggingface-cli delete-cache\n..."
          ],
          [
           "```\n\n#### Without TUI\n\nAs mentioned above, the TUI mode is currently in beta and is optional. It may..."
          ],
          [
           "```\n\nExample of command file:\n\n```txt\n# INSTRUCTIONS\n# ------------\n# This is a temporary file creat..."
          ],
          [
           "# Dataset z-uo/male-LJSpeech-italian (5.5G, used 5 days ago)\n#    9cfa5647b32c0a30d0adfca06bf198d821..."
          ],
          [
           "```\n\n### Clean cache from Python\n\nFor more flexibility, you can also use the [`~HFCacheInfo.delete_r..."
          ],
          [
           "--\n{{card_data}}\n---\n\n# {{ model_name | default(\"MyModelName\", true)}}\n\n{{ some_data }}..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\n[[autodoc]] AsyncInferenceClient\n\n## InferenceTimeoutError\n\n[[autodoc]] InferenceTimeoutError\n\n..."
          ],
          [
           "You can find below the dataclasses used to validate data and in particular [`~huggingface_hub.infere..."
          ],
          [
           "[[autodoc]] InferenceApi\n    - __init__\n    - __call__\n    - all..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "For web development, a [JS client](https://huggingface.co/docs/huggingface.js/inference/README) has ..."
          ],
          [
           "```\n\nWe initialized an [`InferenceClient`] with the default parameters. The only thing you need to k..."
          ],
          [
           "```\n\n<Tip>\n\nThere are more than 200k models on the Hugging Face Hub! Each task in the [`InferenceCli..."
          ],
          [
           "```\n\n### Authentication\n\nCalls made with the [`InferenceClient`] can be authenticated using a [User ..."
          ],
          [
           "```\n\n<Tip>\n\nAuthentication is NOT mandatory when using the Inference API. However, authenticated use..."
          ],
          [
           "| Domain | Task                           | Supported    | Documentation                            ..."
          ],
          [
           "| | [Object Detection](https://huggingface.co/tasks/object-detection)            | âœ… | [`~InferenceC..."
          ],
          [
           "| | [Summarization](https://huggingface.co/tasks/summarization)                  | âœ… | [`~InferenceC..."
          ],
          [
           "<Tip>\n\nCheck out the [Tasks](https://huggingface.co/tasks) page to learn more about each task, how t..."
          ],
          [
           "```\n\n## Async client\n\nAn async version of the client is also provided, based on `asyncio` and `aioht..."
          ],
          [
           "```\n\nFor more information about the `asyncio` module, please refer to the [official documentation](h..."
          ],
          [
           "```\n\n### Binary inputs\n\nSome tasks require binary inputs, for example, when dealing with images or a..."
          ],
          [
           "```\n\nto\n\n```python\n>>> from huggingface_hub import InferenceClient\n>>> inference = InferenceClient(m..."
          ],
          [
           "```\n\n### Run with parameters\n\nChange from\n\n```python\n>>> from huggingface_hub import InferenceApi\n>>..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "```\n\nå®ŒæˆåŽ,[æ£€æŸ¥å®‰è£…](#check-installation)æ˜¯å¦æ­£å¸¸å·¥ä½œ\n\n### å®‰è£…å¯é€‰ä¾èµ–é¡¹\n\n`huggingface_hub`çš„æŸäº›ä¾èµ–é¡¹æ˜¯ [å¯é€‰](https://setup..."
          ],
          [
           "```\n\nè¿™é‡Œåˆ—å‡ºäº† `huggingface_hub` çš„å¯é€‰ä¾èµ–é¡¹ï¼š\n\n- `cli`ï¼šä¸º `huggingface_hub` æä¾›æ›´æ–¹ä¾¿çš„å‘½ä»¤è¡Œç•Œé¢\n\n- `fastai`,` torch`, ..."
          ],
          [
           "```\n\nå®Œæˆå®‰è£…åŽï¼Œè¯·[æ£€æŸ¥å®‰è£…](#check-installation)æ˜¯å¦æ­£å¸¸å·¥ä½œ\n\n### å¯ç¼–è¾‘å®‰è£…\n\nä»Žæºä»£ç å®‰è£…å…è®¸æ‚¨è®¾ç½®[å¯ç¼–è¾‘å®‰è£…](https://pip.pypa.io/en/..."
          ],
          [
           "```\nå®Œæˆå®‰è£…åŽï¼Œè¯·[æ£€æŸ¥å®‰è£…](#check-installation)æ˜¯å¦æ­£å¸¸å·¥ä½œ\n\n## éªŒè¯å®‰è£…\n\nå®‰è£…å®ŒæˆåŽï¼Œé€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥`huggingface_hub`æ˜¯å¦æ­£å¸¸å·¥ä½œ:\n\n```..."
          ],
          [
           "```\n\n## Windowså±€é™æ€§\n\nä¸ºäº†å®žçŽ°è®©æ¯ä¸ªäººéƒ½èƒ½ä½¿ç”¨æœºå™¨å­¦ä¹ çš„ç›®æ ‡ï¼Œæˆ‘ä»¬æž„å»ºäº† `huggingface_hub`åº“ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªè·¨å¹³å°çš„åº“ï¼Œå°¤å…¶å¯ä»¥åœ¨ Unix å’Œ Windows ç³»ç»Ÿ..."
          ],
          [
           "--\n{card_data}\n---\n\n# {{ pretty_name | default(\"Dataset Name\", true)}}\n\n{{ some_data }}..."
          ],
          [
           "!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to..."
          ],
          [
           "`<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./..."
          ],
          [
           "</div>\n</div>\n\né€šè¿‡ `huggingface_hub`åº“ï¼Œæ‚¨å¯ä»¥ä¸Žé¢å‘æœºå™¨å­¦ä¹ å¼€å‘è€…å’Œåä½œè€…çš„å¹³å° [Hugging Face Hub](https://huggingface.co/..."
          ],
          [
           "å½“ç„¶ï¼Œè´¡çŒ®è€…ä¹Ÿåº”è¯¥å°Šé‡æˆ‘ä»¬çš„[è¡Œä¸ºå‡†åˆ™](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md)ï¼Œä»¥ä¾¿..."
          ]
         ],
         "hovertemplate": "source=huggingface_hub<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "huggingface_hub, circle",
         "marker": {
          "color": "#19d3f3",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "huggingface_hub, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          7.360272,
          7.35251,
          7.529542,
          7.968039,
          7.370061,
          7.1696587,
          6.8299503,
          6.346242,
          7.566513,
          8.434188,
          7.707078,
          4.678647,
          3.6520367,
          3.9206438,
          4.4061294,
          4.2332373,
          4.58865,
          4.27588,
          4.6405964,
          4.795116,
          4.5218077,
          3.5710053,
          3.2762504,
          3.2621663,
          3.3479958,
          4.5403805,
          4.6965127,
          4.2057223,
          4.300356,
          4.1523347,
          3.9006362,
          4.3718743,
          4.300587,
          3.9476335,
          4.548463,
          4.353648,
          3.5803,
          4.0644464,
          4.12095,
          3.0774374,
          4.3586273,
          4.3416195,
          4.3643565,
          4.7290106,
          4.743397,
          4.424143,
          3.919544,
          4.264945,
          5.601388,
          5.435659,
          4.956794,
          4.413672,
          4.2826934,
          4.6590123,
          4.999056,
          3.5978053,
          2.1732972,
          6.875023,
          6.4234357,
          6.890717,
          5.1894135,
          4.527627,
          4.654829,
          4.70965,
          4.4375134,
          4.403852,
          4.3888264,
          4.529241,
          4.643258,
          4.2076583,
          4.5314193,
          4.2561193,
          4.7441106,
          5.021807,
          4.7362576,
          5.647573,
          5.1909437,
          4.235442,
          2.9861684,
          4.184251,
          3.4778342,
          3.6874826,
          3.548933,
          3.8468199,
          5.611404,
          5.4590745,
          4.784719,
          4.605359,
          4.4133987,
          3.7799144,
          4.5802813,
          4.611299,
          4.7977667,
          4.8151116,
          5.0150843,
          1.1832674,
          4.4099445,
          4.0876436,
          4.3758416,
          5.581924,
          6.662159,
          6.4501657,
          6.5592704,
          7.3220415,
          7.163785,
          7.1643896,
          6.68483,
          7.330273,
          7.507704,
          3.0608923,
          2.6921947,
          5.956248,
          4.5759025,
          7.721406,
          7.8318706,
          7.2927136,
          3.8219461,
          5.0644665,
          5.134209,
          4.807658,
          4.4934688,
          5.1819415,
          5.0232434,
          3.307604,
          6.7166367,
          6.6679616,
          4.3808928,
          4.3025904,
          4.673842,
          4.9484477,
          4.1542563,
          6.1045814,
          5.806418,
          -6.830063,
          4.713603,
          6.5749197,
          5.6957393,
          3.5225112,
          3.7833602,
          -2.79564,
          -0.36314082,
          4.381968,
          4.725585,
          3.9385302,
          6.0471554,
          5.930508,
          5.9358196,
          6.1467686,
          6.1676445,
          6.0117826,
          6.061798,
          12.161222,
          5.216873,
          5.682547,
          5.285484,
          1.2412035,
          0.082225144,
          -1.805947,
          4.79419,
          -6.9499803,
          4.8488,
          4.5896544,
          4.8751163,
          4.7412457,
          4.8794246,
          6.2779937,
          4.759416,
          4.684984,
          4.833632,
          6.9495544,
          6.882492,
          7.4017754,
          6.8678985,
          6.3841944,
          6.338783,
          6.939228,
          7.4812217,
          6.9354653,
          6.7724867,
          7.400316,
          4.470392,
          6.1529074,
          6.2635527,
          5.796751,
          5.398939,
          7.0178623,
          5.6907105,
          6.9353704,
          3.3306155,
          2.2681367,
          5.281261,
          1.0293124,
          7.34777,
          7.234045,
          7.4840374,
          7.208504,
          6.83919,
          6.6722646,
          6.0699763,
          5.7864165,
          7.3640065,
          7.657585,
          7.270602,
          5.287941,
          -5.606577,
          5.461036,
          5.2390184,
          1.9127327,
          4.744997,
          5.0513372,
          5.03206,
          4.809656,
          4.8943505,
          4.7198477,
          4.192307,
          6.554302,
          6.142445,
          6.129308,
          6.1112165,
          6.6843,
          4.759482,
          4.4623733,
          4.831214,
          4.104005,
          4.5674286,
          4.8483157,
          4.4464464,
          3.467339,
          4.1137033,
          4.0715513,
          -1.0329392,
          5.4144254,
          2.6705577,
          4.541132,
          2.7357478,
          5.35333,
          5.061059,
          5.1045995,
          5.288303,
          5.4066696,
          5.2405605,
          3.865592,
          3.2266018,
          2.7589874,
          4.7314243,
          1.4708021,
          -0.19880852,
          0.84135336,
          -0.6445605,
          4.1238303,
          4.6435003,
          4.983931,
          4.869242,
          4.4941425,
          4.585553,
          4.6422358,
          4.405611,
          4.485488,
          4.3966603,
          4.2634554,
          3.9938586,
          4.069298,
          5.0024085,
          3.582616,
          3.823437,
          2.7274065,
          6.235934,
          5.102185,
          5.5931106,
          4.931242,
          4.163645,
          4.770704,
          4.917463,
          3.8285832,
          4.3834634,
          6.5327435,
          6.614317,
          6.1750283,
          7.6246367,
          6.568097,
          6.8060617,
          6.6001463,
          6.4929457,
          6.261154,
          6.0452743,
          0.26981974,
          6.844741,
          3.4517553,
          2.9634435,
          2.864227,
          3.4900086,
          3.2231214,
          3.5111206,
          3.775212,
          0.51765615,
          -6.893781,
          1.888781,
          5.1978555,
          5.1642675,
          5.0708423,
          5.1540494,
          5.052503,
          4.85979,
          5.0546756,
          6.4152613,
          6.7874103,
          6.771956,
          5.3415766,
          4.6502337,
          4.673989,
          4.502269,
          4.9254227,
          4.852603,
          4.618349,
          4.7989254,
          4.9841647,
          5.073319,
          4.947387,
          5.023991,
          -6.742159,
          -5.892623,
          6.1709523,
          5.2181296,
          4.808578,
          4.982834,
          5.3162465,
          5.282697,
          5.2843165,
          5.322503,
          5.1038995,
          6.318222,
          5.2081614,
          4.5035744,
          4.697911,
          4.659003,
          4.623197,
          4.5397735,
          4.674598,
          4.990447,
          4.4081903,
          4.573926,
          4.8214955,
          5.050584,
          -7.4649043,
          -7.56878,
          -5.14465,
          4.6497087,
          4.4102125,
          4.445478,
          4.5415554,
          4.5292997,
          3.8385296,
          6.420816,
          6.5892477,
          6.444163,
          6.4418654,
          6.9564347,
          6.8890333,
          4.621529,
          6.6500845,
          6.499079,
          6.776909,
          9.3818035
         ],
         "xaxis": "x",
         "y": [
          -0.42415193,
          -0.44364148,
          -0.46730417,
          -0.5599328,
          -0.4441039,
          -0.44082135,
          -0.52992034,
          -0.7208042,
          -0.51851505,
          -0.7241406,
          -0.54125637,
          -1.1482153,
          -2.9263651,
          -1.3198751,
          -1.5453923,
          -1.42371,
          -1.4824523,
          -1.6228176,
          -2.2072768,
          -1.0598983,
          -1.3174276,
          -3.112681,
          -4.001823,
          -3.8400831,
          -3.8579776,
          -1.3706276,
          -0.80264604,
          -0.9803079,
          -0.9002799,
          -0.8372385,
          -1.077911,
          -0.9338828,
          -0.9908162,
          -1.1977205,
          -1.1234299,
          -1.0049106,
          -3.0341594,
          -0.8812805,
          -1.0500224,
          -0.66019404,
          -0.91834104,
          -0.98704165,
          -0.9574798,
          -1.1172837,
          -1.0994201,
          -0.9066394,
          -0.77438736,
          -0.6401213,
          0.008489491,
          -1.0280513,
          -0.74294645,
          -0.33885834,
          -0.41116628,
          -0.3812979,
          -0.85722613,
          -0.19791138,
          -0.84371436,
          -1.9746485,
          -1.784724,
          -2.0426385,
          -0.6002726,
          -0.30751255,
          -0.37343696,
          -0.55996144,
          -1.2466213,
          -1.3556546,
          -1.5807147,
          -1.4128041,
          -0.71117836,
          -0.725381,
          -0.9364625,
          -0.72993296,
          -0.64582354,
          -1.7846686,
          -1.829405,
          -3.0327792,
          -2.156084,
          -0.32100445,
          -0.4014308,
          -2.2746873,
          -3.7514741,
          -1.538201,
          -3.9418647,
          -0.5745694,
          -1.634944,
          -2.5260592,
          -2.2088106,
          -2.5922341,
          -2.875677,
          -3.1635313,
          -2.6790512,
          -2.5546799,
          -2.6256514,
          -2.7347353,
          -2.6474266,
          -2.3257198,
          -3.2936406,
          -0.3990583,
          -0.30577424,
          -1.9882474,
          -1.7835246,
          -1.863143,
          -2.1773682,
          -2.1335757,
          -2.1296566,
          -2.110294,
          -1.935067,
          -2.1431708,
          -2.229145,
          -2.7679384,
          -2.465173,
          -0.67135394,
          -1.500206,
          -3.6557348,
          -3.3865023,
          -2.508108,
          -2.1605895,
          -2.5215206,
          -1.3496625,
          0.95118815,
          0.5524188,
          -0.68912226,
          -1.3148834,
          -1.0971987,
          -0.37866843,
          -0.40209168,
          -0.6940397,
          -0.91949123,
          -0.35202459,
          -0.7654431,
          -0.4499325,
          -2.100382,
          -2.1223152,
          0.059037622,
          -0.5078507,
          -0.50262976,
          -0.9983462,
          -1.7804285,
          -2.765904,
          0.7940001,
          -5.784621,
          -2.706056,
          -3.1369038,
          -2.531765,
          -2.1735656,
          -2.0885422,
          -2.2292933,
          -1.9886806,
          -2.1375048,
          -2.0957177,
          -2.0680084,
          -3.6960452,
          0.12098417,
          -1.4083885,
          -1.4054762,
          -1.8352494,
          -2.2017586,
          -4.0697145,
          -0.64618516,
          -6.768855,
          -0.74765575,
          -0.667414,
          -0.93643814,
          -0.6859223,
          -0.69952893,
          -1.7170067,
          -0.8411922,
          -0.78360814,
          -0.6008,
          -0.3988125,
          -0.5347262,
          -0.4527121,
          -0.5633613,
          -0.48302427,
          -0.61759996,
          -0.77364147,
          -0.77428967,
          -0.40780628,
          -0.43701294,
          -0.4228314,
          -0.58013076,
          -0.4966865,
          -0.4354145,
          -0.50816387,
          -0.8072363,
          -1.2123525,
          -0.6994965,
          -2.1325674,
          -0.40479204,
          -0.6457062,
          -0.98857415,
          -3.2421618,
          -0.41012755,
          -0.41629198,
          -0.37056825,
          -0.46776217,
          -0.49913278,
          -0.37294954,
          -0.57049567,
          -0.7450251,
          -0.48247254,
          -2.9599864,
          -0.4648153,
          -2.2881637,
          -0.8136336,
          -2.5854669,
          -0.8068602,
          -0.8992855,
          -1.4730853,
          -1.0709581,
          -1.0063385,
          -1.230547,
          -1.259338,
          -1.4074308,
          -1.5200928,
          -0.6085752,
          -0.58100116,
          -0.7525524,
          -0.7096839,
          -0.6542942,
          -1.1182469,
          -1.5692427,
          -1.1053609,
          -1.5268948,
          -0.8249355,
          -0.82429904,
          -0.8952102,
          -1.3927712,
          -0.8835904,
          0.45559686,
          -3.5301034,
          -0.26876032,
          0.16514808,
          -0.23751666,
          -0.9753425,
          -0.77874964,
          -1.5872579,
          -2.1826515,
          -1.920969,
          -1.5635782,
          -1.5720437,
          -0.47058252,
          -0.4377737,
          -0.2308294,
          -0.8719789,
          -0.4745862,
          -0.2808869,
          -0.3637226,
          -0.45191538,
          -0.8072049,
          0.9847676,
          0.356428,
          0.3823795,
          0.40675524,
          0.1793747,
          0.50804424,
          0.7175168,
          0.70029676,
          0.83122057,
          0.48551118,
          0.6069271,
          0.48501194,
          -0.7241183,
          -1.8461372,
          -3.0477736,
          -0.32786003,
          -0.5468928,
          -0.9259295,
          -0.8830504,
          -0.5393545,
          -0.5944254,
          -0.8829041,
          -0.65447426,
          -0.8481463,
          -0.705092,
          -0.7777692,
          -0.7448261,
          -0.75696635,
          -1.8425308,
          -0.6664724,
          -0.91359097,
          -0.7605417,
          -0.77892596,
          -0.75430393,
          -0.7623509,
          -3.923899,
          -2.3302002,
          -1.7953798,
          -2.1159847,
          -1.8449855,
          -1.7540178,
          0.11256344,
          -1.8952498,
          -1.9431666,
          -3.4713702,
          -5.0539923,
          -0.5384549,
          -0.8355425,
          -1.16007,
          -1.2476326,
          -1.0998061,
          -1.1443233,
          -1.1138381,
          -1.2993306,
          -0.61716986,
          -0.7129008,
          -0.72144854,
          -0.998397,
          -1.888954,
          -1.810554,
          -1.7062743,
          -3.1134088,
          -1.3013209,
          -1.1227385,
          -2.257494,
          -1.8869288,
          -2.0436146,
          -2.0418184,
          -2.1330733,
          -0.623554,
          -0.9553367,
          -3.1552863,
          -2.694333,
          -2.2800734,
          -1.8287941,
          -2.2479713,
          -2.2619164,
          -2.296719,
          -2.3774698,
          -2.1691406,
          -3.679784,
          -2.4814591,
          -3.206116,
          0.975955,
          0.52860034,
          0.53744936,
          0.71057296,
          0.86314464,
          -0.32322305,
          0.24358705,
          0.6816011,
          -0.18892546,
          0.38495725,
          -4.820175,
          -0.81077516,
          -2.0021243,
          0.33689547,
          0.010501234,
          0.44770828,
          0.26048815,
          -0.021882005,
          -2.1813211,
          -0.6062691,
          -0.3144592,
          -0.4579888,
          -0.49345458,
          -0.9424335,
          -0.6830736,
          -3.8256211,
          -0.50781095,
          -0.5965918,
          -0.51748496,
          -2.3735178
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "The tokenization pipeline\n\nWhen calling `Tokenizer.encode` or\n`Tokenizer.encode_batch`, the input\nte..."
          ],
          [
           "## Normalization\n\nNormalization is, in a nutshell, a set of operations you apply to a raw\nstring to ..."
          ],
          [
           "You can manually test that normalizer by applying it to any string:\n\n<tokenizerslangcontent>\n<python..."
          ],
          [
           "When building a `Tokenizer`, you can\ncustomize its normalizer by just changing the corresponding att..."
          ],
          [
           "An easy way to pre-tokenize inputs is to split on spaces and\npunctuations, which is done by the\n`pre..."
          ],
          [
           "You can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that wil..."
          ],
          [
           "As we saw in the `quicktour`, you can\ncustomize the pre-tokenizer of a `Tokenizer` by just changing ..."
          ],
          [
           "The role of the model is to split your \"words\" into tokens, using the\nrules it has learned. It's als..."
          ],
          [
           "As we saw in the quick tour, we can customize the post processor of a\n`Tokenizer` by setting the\ncor..."
          ],
          [
           "Let's put all those pieces together to build a BERT tokenizer. First,\nBERT relies on WordPiece, so w..."
          ],
          [
           "Then we know that BERT preprocesses texts by removing accents and\nlowercasing. We also use a unicode..."
          ],
          [
           "The pre-tokenizer is just splitting on whitespace and punctuation:\n\n<tokenizerslangcontent>\n<python>..."
          ],
          [
           "And the post-processing uses the template we saw in the previous\nsection:\n\n<tokenizerslangcontent>\n<..."
          ],
          [
           "We can use this tokenizer and train on it on wikitext like in the\n`quicktour`:\n\n<tokenizerslangconte..."
          ],
          [
           "The `decoder` will first convert the IDs back to tokens\n(using the tokenizer's vocabulary) and remov..."
          ],
          [
           "<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentatio..."
          ],
          [
           "But by changing it to a proper decoder, we get:\n\n<tokenizerslangcontent>\n<python>\n<literalinclude>\n{..."
          ],
          [
           "Quicktour\n\nLet's have a quick look at the ðŸ¤— Tokenizers library features. The\nlibrary provides an imp..."
          ],
          [
           "```\n\n### Training the tokenizer\n\nIn this tour, we will build and train a Byte-Pair Encoding (BPE)\nto..."
          ],
          [
           "To train our tokenizer on the wikitext files, we will need to\ninstantiate a [trainer]{.title-ref}, i..."
          ],
          [
           "The order in which you write the special tokens list matters: here `\"[UNK]\"` will get the ID 0,\n`\"[C..."
          ],
          [
           "Now, we can just call the `Tokenizer.train` method with any list of files we want to use:\n\n<tokenize..."
          ],
          [
           "This should only take a few seconds to train our tokenizer on the full\nwikitext dataset! To save the..."
          ],
          [
           "and you can reload your tokenizer from that file with the\n`Tokenizer.from_file`\n`classmethod`:\n\n<tok..."
          ],
          [
           "### Using the tokenizer\n\nNow that we have trained a tokenizer, we can use it on any text we want\nwit..."
          ],
          [
           "This `Encoding` object then has all the\nattributes you need for your deep learning model (or other)...."
          ],
          [
           "Similarly, the `ids` attribute will\ncontain the index of each of those tokens in the tokenizer's\nvoc..."
          ],
          [
           "<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentatio..."
          ],
          [
           "and those are the indices that correspond to the emoji in the original\nsentence:\n\n<tokenizerslangcon..."
          ],
          [
           "<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentatio..."
          ],
          [
           "Here is how we can set the post-processing to give us the traditional\nBERT inputs:\n\n<tokenizerslangc..."
          ],
          [
           "Lastly, we specify the special tokens we used and their IDs in our\ntokenizer's vocabulary.\n\nTo check..."
          ],
          [
           "To check the results on a pair of sentences, we just pass the two\nsentences to `Tokenizer.encode`:\n\n..."
          ],
          [
           "You can then check the type IDs attributed to each token is correct with\n\n<tokenizerslangcontent>\n<p..."
          ],
          [
           "To get the full speed of the ðŸ¤— Tokenizers library, it's best to\nprocess your texts by batches by usi..."
          ],
          [
           "To process a batch of sentences pairs, pass two lists to the\n`Tokenizer.encode_batch` method: the\nli..."
          ],
          [
           "<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentatio..."
          ],
          [
           "<tokenizerslangcontent>\n<python>\n<literalinclude>\n{\"path\": \"../../bindings/python/tests/documentatio..."
          ],
          [
           "In this case, the `attention mask` generated by the\ntokenizer takes the padding into account:\n\n<toke..."
          ],
          [
           "```\n\n### Importing a pretrained tokenizer from legacy vocabulary files\n\nYou can also import a pretra..."
          ],
          [
           "p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers..."
          ],
          [
           "## Bindings\n\nWe provide bindings to the following languages (more to come!):\n  - [Rust](https://gith..."
          ],
          [
           "```\n\nYou can customize how pre-tokenization (e.g., splitting into words) is done:\n\n```python\nfrom to..."
          ],
          [
           "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`..."
          ],
          [
           "div align=\"center\">\n\n  <h1><code>wasm-pack-template</code></h1>\n\n  <strong>A template for kick start..."
          ],
          [
           "Be sure to check out [other `wasm-pack` tutorials online][tutorials] for other\ntemplates and usages ..."
          ],
          [
           "```\ncargo generate --git https://github.com/rustwasm/wasm-pack-template.git --name my-project\ncd my-..."
          ],
          [
           "Post-processors\n\n<tokenizerslangcontent>\n<python>\n## BertProcessing\n\n[[autodoc]] tokenizers.processo..."
          ],
          [
           "Training from memory\n\nIn the [Quicktour](quicktour), we saw how to build and train a\ntokenizer using..."
          ],
          [
           "Easy, right? You can use anything working as an iterator here, be it a\n`List`{.interpreted-text role..."
          ],
          [
           "With our iterator ready, we just need to launch the training. In order\nto improve the look of our pr..."
          ],
          [
           "Models\n\n<tokenizerslangcontent>\n<python>\n## BPE\n\n[[autodoc]] tokenizers.models.BPE\n\n## Model\n\n[[auto..."
          ],
          [
           "Added Tokens\n\n<tokenizerslangcontent>\n<python>\n## AddedToken\n\n[[autodoc]] tokenizers.AddedToken\n    ..."
          ],
          [
           "p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers..."
          ],
          [
           "```\n\n#### From sources:\n\nTo use this method, you need to have the Rust installed:\n\n```bash\n# Install..."
          ],
          [
           "```\n\nAnd you can train them just as simply:\n\n```python\nfrom tokenizers import CharBPETokenizer\n\n# In..."
          ],
          [
           "```\n\n#### Provided Tokenizers\n\n - `CharBPETokenizer`: The original BPE\n - `ByteLevelBPETokenizer`: T..."
          ],
          [
           "```\n\nNow, when you want to use this tokenizer, this is as simple as:\n\n```python\nfrom tokenizers impo..."
          ],
          [
           "p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers..."
          ],
          [
           "## What is a Tokenizer\n\nA Tokenizer works as a pipeline, it processes some raw text as input and out..."
          ],
          [
           "```\n\n### Deserialization and tokenization example\n\n```rust\nuse tokenizers::tokenizer::{Result, Token..."
          ],
          [
           "```\n\n### Training and serialization example\n\n```rust\nuse tokenizers::decoders::DecoderWrapper;\nuse t..."
          ],
          [
           "let mut tokenizer = TokenizerBuilder::new()\n        .with_model(BPE::default())\n        .with_normal..."
          ],
          [
           "```\n\n## Additional information\n\n- tokenizers is designed to leverage CPU parallelism when possible. ..."
          ],
          [
           "`tokenizers-win32-x64-msvc`\n\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`..."
          ],
          [
           "`tokenizers-freebsd-x64`\n\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`..."
          ],
          [
           "`tokenizers-win32-ia32-msvc`\n\nThis is the **i686-pc-windows-msvc** binary for `tokenizers`..."
          ],
          [
           "Visualizer\n\n<tokenizerslangcontent>\n<python>\n## Annotation\n\n[[autodoc]] tokenizers.tools.Annotation\n..."
          ],
          [
           "Components\n\nWhen building a Tokenizer, you can attach various types of components to\nthis Tokenizer ..."
          ],
          [
           "<tokenizerslangcontent>\n<python>\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| NFD | NFD..."
          ],
          [
           "| Name | Description | Example |\n| :--- | :--- | :--- |\n| NFD | NFD unicode normalization |  |\n| NFK..."
          ],
          [
           "| NFD | NFD unicode normalization |  |\n| NFKD | NFKD unicode normalization |  |\n| NFC | NFC unicode ..."
          ],
          [
           "## Pre-tokenizers\n\nThe `PreTokenizer` takes care of splitting the input according to a set\nof rules...."
          ],
          [
           "<tokenizerslangcontent>\n<python>\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| ByteLevel..."
          ],
          [
           "| CharDelimiterSplit | Splits on a given character | Example with `x`: <br> Input: `\"Helloxthere\"` <..."
          ],
          [
           "</python>\n<rust>\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| ByteLevel | Splits on whi..."
          ],
          [
           "| CharDelimiterSplit | Splits on a given character | Example with `x`: <br> Input: `\"Helloxthere\"` <..."
          ],
          [
           "</rust>\n<node>\n| Name | Description | Example |\n| :--- | :--- | :--- |\n| ByteLevel | Splits on white..."
          ],
          [
           "| CharDelimiterSplit | Splits on a given character | Example with `x`: <br> Input: `\"Helloxthere\"` <..."
          ],
          [
           "## Models\n\nModels are the core algorithms used to actually tokenize, and therefore,\nthey are the onl..."
          ],
          [
           "After the whole pipeline, we sometimes want to insert some special\ntokens before feed a tokenized st..."
          ],
          [
           "| Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTok..."
          ],
          [
           "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# Tokenizers\n\nFast State-of-the-art tokenizers, optimized for ..."
          ],
          [
           "Decoders\n\n<tokenizerslangcontent>\n<python>\n## BPEDecoder\n\n[[autodoc]] tokenizers.decoders.BPEDecoder..."
          ],
          [
           "`tokenizers-darwin-arm64`\n\nThis is the **aarch64-apple-darwin** binary for `tokenizers`..."
          ],
          [
           "Input Sequences\n\n<tokenizerslangcontent>\n<python>\nThese types represent all the different kinds of s..."
          ],
          [
           "Encoding\n\n<tokenizerslangcontent>\n<python>\n## Encoding\n\n[[autodoc]] tokenizers.Encoding\n    - all\n  ..."
          ],
          [
           "Pre-tokenizers\n\n<tokenizerslangcontent>\n<python>\n## BertPreTokenizer\n\n[[autodoc]] tokenizers.pre_tok..."
          ],
          [
           "Normalizers\n\n<tokenizerslangcontent>\n<python>\n## BertNormalizer\n\n[[autodoc]] tokenizers.normalizers...."
          ],
          [
           "Trainers\n\n<tokenizerslangcontent>\n<python>\n## BpeTrainer\n\n[[autodoc]] tokenizers.trainers.BpeTrainer..."
          ],
          [
           "p align=\"center\">\n  <br>\n  <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-log..."
          ],
          [
           "```\n\n## Basic example\n\n```ts\nimport { Tokenizer } from \"tokenizers\";\n\nconst tokenizer = await Tokeni..."
          ],
          [
           "# Requirements\n\nIn order to generate the documentation, it is necessary to have a Python environment..."
          ],
          [
           "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`..."
          ],
          [
           "Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based ..."
          ],
          [
           "- [#952] Fixed the vocabulary size of UnigramTrainer output (to respect added tokens)\n- [#954] Fixed..."
          ],
          [
           "### Changed\n- [#234]: Completely changed the alignement mappings available on `Encoding`. Previous m..."
          ],
          [
           "### Added\n- [#236]: RobertaProcessing is now also taking care of trimming offsets, and works just as..."
          ],
          [
           "## [0.9.0]\n\n### Changed\n- Only one progress bar while reading files during training. This is better ..."
          ],
          [
           "### How to migrate\n- Add the `ByteLevel` `PostProcessor` to your byte-level BPE tokenizers if releva..."
          ],
          [
           "[#1072]: https://github.com/huggingface/tokenizers/pull/1072\n[#956]: https://github.com/huggingface/..."
          ],
          [
           "[#916]: https://github.com/huggingface/tokenizers/pull/916\n[#884]: https://github.com/huggingface/to..."
          ],
          [
           "[#276]: https://github.com/huggingface/tokenizers/pull/276\n[#272]: https://github.com/huggingface/to..."
          ],
          [
           "[#174]: https://github.com/huggingface/tokenizers/issues/174\n[#165]: https://github.com/huggingface/..."
          ],
          [
           "`tokenizers-win32-arm64-msvc`\n\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`..."
          ],
          [
           "`tokenizers-android-arm-eabi`\n\nThis is the **armv7-linux-androideabi** binary for `tokenizers`..."
          ],
          [
           "`tokenizers-linux-arm-gnueabihf`\n\nThis is the **armv7-unknown-linux-gnueabihf** binary for `tokenize..."
          ],
          [
           "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`..."
          ],
          [
           "`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`..."
          ],
          [
           "`tokenizers-android-arm64`\n\nThis is the **aarch64-linux-android** binary for `tokenizers`..."
          ],
          [
           "Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based ..."
          ],
          [
           "- [#952] Fixed the vocabulary size of UnigramTrainer output (to respect added tokens)\n- [#954] Fixed..."
          ],
          [
           "### Added\n- [#657]: Add SplitDelimiterBehavior customization to Punctuation constructor\n- [#845]: Do..."
          ],
          [
           "## [0.10.0]\n\n### Added\n- [#508]: Add a Visualizer for notebooks to help understand how the tokenizer..."
          ],
          [
           "### Changed\n- [#506]: Improve Encoding mappings for pairs of sequence\n\n## [0.9.3]\n\n### Fixed\n- [#470..."
          ],
          [
           "## [0.8.1]\n\n### Fixed\n- [#333]: Fix deserialization of `AddedToken`, where the content was not resto..."
          ],
          [
           "### Fixed\n- [#286]: Fix various crash when training a BPE model\n- [#309]: Fixed a few bugs related t..."
          ],
          [
           "## [0.7.0]\n\n### Changed\n- Only one progress bar while reading files during training. This is better ..."
          ],
          [
           "### Added\n- [#188]: `ByteLevel` is also a `PostProcessor` now and handles trimming the offsets if ac..."
          ],
          [
           "### How to migrate\n- Add the `ByteLevel` `PostProcessor` to your byte-level BPE tokenizers if releva..."
          ],
          [
           "## [0.5.2]\n- [#163]: Do not open all files directly while training\n\n### Fixed\n- We introduced a bug ..."
          ],
          [
           "### Fixed\n- [#137]: Fix a bug in the class `WordPieceTrainer` that prevented `BertWordPieceTokenizer..."
          ],
          [
           "```\noutput = tokenizer.encode(...)\nprint(output.original_str.offsets(output.offsets[3]))..."
          ],
          [
           "```\n- [#99]: Exposed the vocabulary size on all tokenizers\n\n### Added\n- Added `CharDelimiterSplit`: ..."
          ],
          [
           "[#1096]: https://github.com/huggingface/tokenizers/pull/1096\n[#1072]: https://github.com/huggingface..."
          ],
          [
           "[#960]: https://github.com/huggingface/tokenizers/pull/960\n[#919]: https://github.com/huggingface/to..."
          ],
          [
           "[#770]: https://github.com/huggingface/tokenizers/pull/770\n[#762]: https://github.com/huggingface/to..."
          ],
          [
           "[#616]: https://github.com/huggingface/tokenizers/pull/616\n[#590]: https://github.com/huggingface/to..."
          ],
          [
           "[#492]: https://github.com/huggingface/tokenizers/pull/492\n[#481]: https://github.com/huggingface/to..."
          ],
          [
           "[#378]: https://github.com/huggingface/tokenizers/pull/378\n[#363]: https://github.com/huggingface/to..."
          ],
          [
           "[#272]: https://github.com/huggingface/tokenizers/pull/272\n[#249]: https://github.com/huggingface/to..."
          ],
          [
           "[#160]: https://github.com/huggingface/tokenizers/issues/160\n[#156]: https://github.com/huggingface/..."
          ],
          [
           "div align=\"center\">\n\n  <h1><code>create-wasm-app</code></h1>\n\n  <strong>An <code>npm init</code> tem..."
          ],
          [
           "```\nnpm init wasm-app\n```\n\n## ðŸ”‹ Batteries Included\n\n- `.gitignore`: ignores `node_modules`\n- `LICENS..."
          ],
          [
           "Encode Inputs\n\n<tokenizerslangcontent>\n<python>\nThese types represent all the different kinds of inp..."
          ],
          [
           "alias of `Union[List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[st..."
          ],
          [
           "Tokenizer\n\n<tokenizerslangcontent>\n<python>\n## Tokenizer\n\n[[autodoc]] tokenizers.Tokenizer\n    - all..."
          ],
          [
           "Installation\n\n<tokenizerslangcontent>\n<python>\nðŸ¤— Tokenizers is tested on Python 3.5+.\n\nYou should in..."
          ],
          [
           "```\n</rust>\n<node>\n## Installation with npm\n\nYou can simply install ðŸ¤— Tokenizers with npm using:\n\n``..."
          ],
          [
           "`tokenizers-darwin-x64`\n\nThis is the **x86_64-apple-darwin** binary for `tokenizers`..."
          ],
          [
           "# How to release\n\n# Before the release\n\nSimple checklist on how to make releases for `tokenizers`.\n\n..."
          ],
          [
           "# Rust\n\n- `tokenizers` (rust, python & node) versions don't have to be in sync but it's\n  very commo..."
          ],
          [
           "# Python\n\n- Edit `bindings/python/setup.py` to reflect new version.\n- Edit `bindings/python/py_src/t..."
          ],
          [
           "# Node\n\n- Edit `bindings/node/package.json` to reflect new version.\n- Edit `CHANGELOG.md`:\n    - Add..."
          ]
         ],
         "hovertemplate": "source=tokenizers<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "tokenizers, circle",
         "marker": {
          "color": "#FF6692",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "tokenizers, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -3.6248224,
          -3.967777,
          -3.646047,
          -3.8129628,
          -3.8441584,
          -3.7171836,
          -3.6703467,
          -3.8460276,
          -3.8609412,
          -3.862345,
          -3.9781842,
          -3.8014386,
          -3.712801,
          -3.7126343,
          -3.5010343,
          -3.671614,
          -3.6924317,
          -3.5232291,
          -3.7612169,
          -3.6234214,
          -3.6755884,
          -3.4222975,
          -3.4214792,
          -3.5641744,
          -3.6415062,
          -3.5934854,
          -3.5092907,
          -3.6448271,
          -3.6078246,
          -3.6924832,
          -3.5868742,
          -3.5731146,
          -3.4786198,
          -3.5988276,
          -3.2070937,
          -2.567483,
          -3.632951,
          -3.7095437,
          -3.7211347,
          -3.6434963,
          -3.9259539,
          -3.8465729,
          -3.6679075,
          1.4524679,
          5.262028,
          4.852522,
          4.945247,
          -3.7486215,
          -3.5412157,
          0.49181905,
          1.1697704,
          -3.8009157,
          -3.762873,
          -4.0610585,
          -3.2432265,
          -3.3924897,
          -3.9485233,
          -3.3124,
          -3.8643389,
          -3.89089,
          -3.7679558,
          -3.8135672,
          -3.747474,
          -2.3594844,
          1.4517119,
          1.4515228,
          1.4515408,
          -3.7586732,
          -3.9027581,
          -3.995619,
          -3.9950666,
          -4.074498,
          -3.9336581,
          -3.7646728,
          -3.8611884,
          -3.8792162,
          -3.8611777,
          -3.9130244,
          -3.9590232,
          -3.9560962,
          -3.4268086,
          -4.0505414,
          -3.8785546,
          -3.7437603,
          1.4513316,
          -3.8087463,
          -3.711122,
          -3.8208668,
          -3.8352196,
          -3.7987864,
          -4.013406,
          6.572789,
          4.9673696,
          1.4527243,
          -4.211309,
          -4.45189,
          -4.481731,
          -4.486167,
          -4.4705935,
          -4.3827724,
          19.612576,
          19.610481,
          19.613422,
          19.614426,
          1.4518186,
          1.4522635,
          1.4526919,
          1.4525791,
          1.4519423,
          1.4522017,
          -4.2246385,
          -4.432347,
          -4.4270654,
          -4.491486,
          -4.497021,
          -4.48069,
          -4.482764,
          -4.5033793,
          -4.409678,
          -4.4504294,
          -4.430391,
          -4.4379606,
          -2.6117003,
          -4.2961793,
          19.613081,
          19.611937,
          19.611153,
          19.61304,
          19.613838,
          19.615812,
          19.6105,
          19.614872,
          5.212731,
          5.193974,
          -3.447716,
          -3.8213854,
          -3.8377068,
          -3.085416,
          4.1763515,
          1.4517673,
          2.8365483,
          3.710831,
          3.3990607,
          3.9278648
         ],
         "xaxis": "x",
         "y": [
          -5.449403,
          -5.484137,
          -5.413985,
          -5.411465,
          -5.59697,
          -5.4616756,
          -5.2397246,
          -4.7837496,
          -5.069926,
          -4.909928,
          -5.4420686,
          -5.5554986,
          -5.540659,
          -5.2484317,
          -5.4529557,
          -5.6182017,
          -5.6204653,
          -5.2152042,
          -5.337394,
          -5.3672147,
          -5.476334,
          -5.079883,
          -5.034367,
          -5.285778,
          -5.3364472,
          -5.3829813,
          -5.5485425,
          -5.6024313,
          -5.5587316,
          -5.5726414,
          -5.449835,
          -5.532686,
          -5.5246897,
          -5.447778,
          -5.21747,
          -4.508814,
          -5.4295235,
          -5.466118,
          -5.4473577,
          -4.3247776,
          -5.5849953,
          -5.5083585,
          -5.3087907,
          -18.899387,
          -2.1606236,
          -1.7714328,
          -1.9469173,
          -5.906563,
          -5.1307435,
          -4.3254704,
          -3.8231347,
          -5.761552,
          -5.8903065,
          -5.6945705,
          -4.8011875,
          -5.0950403,
          -4.9738655,
          -4.958629,
          -5.5839925,
          -5.4727025,
          -5.735484,
          -5.668407,
          -5.254344,
          3.3189652,
          -18.901068,
          -18.901443,
          -18.900473,
          -5.8780646,
          -5.4867787,
          -5.5603533,
          -5.5326443,
          -5.5264897,
          -5.4531407,
          -5.4738326,
          -5.623007,
          -5.5885878,
          -5.579742,
          -5.6355724,
          -5.5402617,
          -5.3521137,
          -5.4448867,
          -5.748465,
          -5.388116,
          -5.8432555,
          -18.90069,
          -5.904527,
          -5.7673593,
          -5.82909,
          -5.874363,
          -5.8246675,
          -5.579888,
          -3.621308,
          -1.5263685,
          -18.902601,
          -6.832011,
          -7.1045413,
          -7.1268415,
          -7.099801,
          -7.118384,
          -7.1065497,
          12.207727,
          12.205141,
          12.207057,
          12.209567,
          -18.900936,
          -18.89884,
          -18.902508,
          -18.902191,
          -18.900393,
          -18.899216,
          -6.672049,
          -7.015636,
          -7.1130257,
          -6.991537,
          -7.148732,
          -7.130613,
          -7.1715984,
          -7.09263,
          -7.0377736,
          -7.0020795,
          -6.9764285,
          -7.059574,
          -4.8493676,
          -6.7841015,
          12.206566,
          12.206711,
          12.206815,
          12.2066345,
          12.206518,
          12.208422,
          12.206932,
          12.206922,
          -2.164803,
          -2.390439,
          -5.377193,
          -5.8629255,
          -5.8858466,
          -5.53686,
          -1.2990351,
          -18.900818,
          -0.18696918,
          -0.678009,
          -0.5054503,
          -0.36572847
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nYou can then provide the `generate_map` method as an argument to the `sm.ParallelRLEnv` class, ..."
          ],
          [
           "How to contribute to simulate?\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20C..."
          ],
          [
           "```\n\n3. Create a new branch to hold your development changes:\n\n\t```bash\n\tgit checkout -b a-descripti..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "# Simulate with Godot\n\n### Install in Godot 4\nThis integration has been developed for Godot 4.x. You..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\npip install --upgrade simulate\n```\nBefore you merge a PR, fix the style (we use `isort` + `black..."
          ],
          [
           "```\nimport simulate as sm\n\nscene = sm.Scene()\nscene += sm.Plane() + sm.Sphere(position=[0, 1, 0], ra..."
          ],
          [
           "```\npython examples/basic/objects.py\n```\n<p align=\"center\">\n    <br>\n    <img src=\"https://user-imag..."
          ],
          [
           "```\n# Add two copy of the sphere to the scene as children of the root node (using list will add all ..."
          ],
          [
           "```\n\n### Visualization engine\n\nA default vizualization engine is provided with the vtk backend of `p..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\nscene = sm.Scene.create_from(\"simulate-tests/Box/glTF-Embedded/Box.gltf\")\n```\n\nor, they can be t..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "<div class=\"mt-10\">\n  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2..."
          ],
          [
           "<a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./c..."
          ],
          [
           "p align=\"center\">\n    <br>\n    <img src=\"docs/source/assets/simulate_library.png\" width=\"400\"/>\n    ..."
          ],
          [
           "```\npip install --upgrade simulate\n```\nBefore you merge a PR, fix the style (we use `isort` + `black..."
          ],
          [
           "```\nfrom simulate import Scene\n\nscene = Scene.create_from('tests/test_assets/fixtures/Box.gltf')  # ..."
          ],
          [
           "```\n\nAn object (as well as the Scene) is just a node in a tree provided with optional mesh (under th..."
          ],
          [
           "```\npython examples/basic/objects.py\n```\n<p align=\"center\">\n    <br>\n    <img src=\"https://user-imag..."
          ],
          [
           "```\n# Add two copy of the sphere to the scene as children of the root node (using list will add all ..."
          ],
          [
           "```\n\nEditing objects:\n- mesh of the object can be edited with all the manipulation operator provided..."
          ],
          [
           "# Unity Integration\n\n### Install with the Unity editor\nCurrently we use Unity version `2021.3.2f1` a..."
          ],
          [
           "```\n{\n    \"type\": \"MyCommand\",\n    \"contents\": json.dumps({\n        \"message\": \"hello from python AP..."
          ],
          [
           "```\n\nThis currently only supports Box, Sphere, and Capsule colliders (the Unity/PhysX colliders).\n\nD..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, V..."
          ],
          [
           "Security Policy\n\n## Supported Versions\n<!--\nUse this section to tell people about which versions of ..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "his package provides core backend functionality for the Hugging Face Simulate project: (https://gith..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The \"leaf\" reward functions can be combined in a tree structure with the following predicate functio..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\nfor i in range(60):\n    event = scene.step()\n```\nYou should see the cube falling onto the plane...."
          ],
          [
           "```\nscene.config.return_nodes = False\nscene.config.return_frames = False\nscene.show()\n```\nFor advanc..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "The created environments are stored in a language/framework agnostic format and can be loaded and ru..."
          ],
          [
           "# Blender Integration\n\n### Install addon in Blender\nThis integration has been developed for Blender ..."
          ],
          [
           "Tests examples taken from the original great gltflib\n\nFind the great gltflib by Lukas Shawford here:..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "# Examples for Simulate\n\nThe examples are organized by level of complexity or application. \nCurrentl..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "```\n\nNow we need to identify which build your GPU is using and add it to your xorg config file:\n\n```..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "Reward function:\n- A dense reward based on improvement in best euclidean distance to the object\n- A ..."
          ],
          [
           "Example: [`sb3_procgen.py`](https://github.com/huggingface/simulate/examples/rl/sb3_procgen.py)\n\nObj..."
          ],
          [
           "Parallel: 16 independent instances of the same environment configuration.\n\n\n## Reward functions base..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ],
          [
           "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Ver..."
          ]
         ],
         "hovertemplate": "source=simulate<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "simulate, circle",
         "marker": {
          "color": "#B6E880",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "simulate, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          3.249466,
          1.2806567,
          4.800267,
          4.1819143,
          5.886946,
          4.534785,
          3.220109,
          2.843316,
          3.1697798,
          2.9888637,
          2.0935829,
          2.9584417,
          3.1547706,
          5.977488,
          2.8308234,
          3.073275,
          4.7492933,
          7.5751834,
          7.7352457,
          3.09982,
          3.1742136,
          3.0424883,
          3.0292776,
          2.0801,
          2.9373834,
          3.0184393,
          3.274969,
          7.8005586,
          2.7976794,
          6.0862765,
          5.0893555,
          5.767526,
          6.068199,
          5.602584,
          3.1603136,
          -7.917538,
          -7.728667,
          2.8774066,
          2.9798794,
          -7.387688,
          6.2456093,
          6.086298,
          3.1025412,
          3.2360868,
          3.1956472,
          6.4828205,
          5.8217616,
          3.2198808,
          6.0555687,
          -2.2780163,
          -0.3165844,
          5.9673457,
          2.9173079,
          -7.8410053,
          -7.8761764,
          -7.635989,
          -7.824837,
          5.989661,
          2.8839915
         ],
         "xaxis": "x",
         "y": [
          3.6677852,
          -4.547964,
          -1.1049036,
          -0.30474195,
          1.5411144,
          2.2551541,
          3.7249897,
          4.164212,
          3.9007895,
          4.3698354,
          -3.4390347,
          4.535041,
          3.6946018,
          1.3883368,
          4.4169664,
          4.0996814,
          2.1007228,
          -3.9797666,
          -3.9290788,
          3.5664268,
          3.8288603,
          4.3384304,
          4.3678093,
          -3.4426174,
          4.5744042,
          4.288112,
          3.7525816,
          -3.410842,
          4.5890584,
          1.4386455,
          0.13737513,
          -1.4951978,
          1.2872286,
          1.3249855,
          3.5570157,
          6.046112,
          5.533267,
          4.234926,
          4.5909286,
          6.0241013,
          0.20846787,
          1.4409165,
          3.5396366,
          3.6949055,
          3.926224,
          -3.8806741,
          1.3439317,
          3.7478366,
          1.4615035,
          2.2467284,
          2.0056043,
          1.4419153,
          1.8465534,
          6.0049515,
          5.998834,
          5.9018364,
          5.9130073,
          1.4423755,
          4.2031794
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "# How to release\n\n# Before the release\n\nSimple checklist on how to make releases for `safetensors`.\n..."
          ],
          [
           "# Rust\n\n- `safetensors` (rust, python & node) versions don't have to be in sync but it's\n  very comm..."
          ],
          [
           "# Python\n\n- Edit `bindings/python/setup.py` to reflect new version.\n- Edit `bindings/python/py_src/s..."
          ],
          [
           "# Node\n\n- Edit `bindings/node/package.json` to reflect new version.\n- Edit `CHANGELOG.md`:\n    - Add..."
          ],
          [
           "Flax API\n\n[[autodoc]] safetensors.flax.load_file\n[[autodoc]] safetensors.flax.load\n[[autodoc]] safet..."
          ],
          [
           "Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files ..."
          ],
          [
           "Numpy API\n\n[[autodoc]] safetensors.numpy.load_file\n[[autodoc]] safetensors.numpy.load\n[[autodoc]] sa..."
          ],
          [
           "Speed Comparison\n\n<a href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/..."
          ],
          [
           "```\n\n### CPU benchmark\n\n```py\n>>> start_st = datetime.datetime.now()\n>>> weights = load_file(sf_file..."
          ],
          [
           "```\n\nThis speedup is due to the fact that this library avoids unnecessary copies by mapping the file..."
          ],
          [
           "```\n\nThe speedup works because this library is able to skip unnecessary CPU allocations. It is unfor..."
          ],
          [
           "# Installation\n\n```\npip install safetensors\n```\n\n\n## Usage\n\n### Numpy\n\n```python\nfrom safetensors.nu..."
          ],
          [
           "PaddlePaddle API\n\n[[autodoc]] safetensors.paddle.load_file\n[[autodoc]] safetensors.paddle.load\n[[aut..."
          ],
          [
           "he purpose of this directory is to showcase various attacks (and creating your own).\n\n\n# Torch Arbit..."
          ],
          [
           "```\npython numpy_dos_create.py\npython numpy_dos_get_pwned.py\n```\n\n# Safetensors abuse attempts\n\nIn o..."
          ],
          [
           "```\npython safetensors_abuse_attempt_1.py\npython safetensors_abuse_attempt_2.py\npython safetensors_a..."
          ],
          [
           "p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggi..."
          ],
          [
           "Rust\n[![Crates.io](https://img.shields.io/crates/v/safetensors.svg)](https://crates.io/crates/safete..."
          ],
          [
           "```\n\n#### From source\n\nFor the sources, you need Rust\n\n```bash\n# Install Rust\ncurl --proto '=https' ..."
          ],
          [
           "```\n\n[Python documentation](https://huggingface.co/docs/safetensors/index)\n\n\n### Format\n\n- 8 bytes: ..."
          ],
          [
           "Notes:\n - Duplicate keys are disallowed. Not all parsers may respect this.\n - In general the subset ..."
          ],
          [
           "Let's take a look at alternatives and why this format is deemed interesting.\nThis is my very persona..."
          ],
          [
           "- Safe: Can I use a file randomly downloaded and expect not to run arbitrary code ?\n- Zero-copy: Doe..."
          ],
          [
           "### Main oppositions\n\n- Pickle: Unsafe, runs arbitrary code\n- H5: Apparently now discouraged for TF/..."
          ],
          [
           "### Notes\n\n- Zero-copy: No format is really zero-copy in ML, it needs to go from disk to RAM/GPU RAM..."
          ],
          [
           "- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\nload only..."
          ],
          [
           "Metadata Parsing\n\nGiven the simplicity of the format, it's very simple and efficient to fetch and pa..."
          ],
          [
           "## Usage\n\n### JavaScript/TypeScript[[js]]\n\nUsing [`huggingface.js`](https://huggingface.co/docs/hugg..."
          ],
          [
           "```\n\nDepending on whether the safetensors weights are sharded into multiple files or not, the output..."
          ],
          [
           "```\n\n### Python\n\nIn this example python script, we are parsing metadata of [gpt2](https://huggingfac..."
          ],
          [
           "```\n\n## Example output\n\nFor instance, here are the number of params per dtype for a few models on th..."
          ],
          [
           "model | safetensors | params\n--- | --- | ---\n[gpt2](https://huggingface.co/gpt2?show_tensors=true) |..."
          ],
          [
           "[bigscience/bloom](https://huggingface.co/bigscience/bloom?show_tensors=true) | sharded | { 'BF16' =..."
          ],
          [
           "Torch API\n\n[[autodoc]] safetensors.torch.load_file\n[[autodoc]] safetensors.torch.load\n[[autodoc]] sa..."
          ],
          [
           "Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load..."
          ],
          [
           "!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n<div class=\"flex justify-center\">\n    <img class=\"block dark:h..."
          ],
          [
           "```\n\n### Save tensors\n\n```python\nimport torch\nfrom safetensors.torch import save_file\n\ntensors = {\n ..."
          ],
          [
           "```\n\n## Format\n\nLet's say you have safetensors file named `model.safetensors`, then `model.safetenso..."
          ],
          [
           "* [huggingface/transformers](https://github.com/huggingface/transformers)\n* [AUTOMATIC1111/stable-di..."
          ],
          [
           "* [PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n* [AIGC-Audio/AudioGPT](https:..."
          ],
          [
           "Torch shared tensors\n\n\n## TL;DR\n\nUsing specific functions, which should work in most cases for you.\n..."
          ],
          [
           "```\n\n## Why are shared tensors not saved in `safetensors` ?\n\nMultiple reasons for that:\n\n- *Not all ..."
          ],
          [
           "```\n\nNow with all those reasons being mentioned, nothing is set in stone in there.\nShared tensors do..."
          ]
         ],
         "hovertemplate": "source=safetensors<br>symbol=circle<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "safetensors, circle",
         "marker": {
          "color": "#FF97FF",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4,
           4
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "safetensors, circle",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          2.9178486,
          3.623337,
          3.3925092,
          4.046537,
          2.1429057,
          -0.08143307,
          2.2543554,
          -0.52740806,
          -1.2899344,
          -1.3262581,
          -1.0639229,
          1.10156,
          3.2015421,
          1.2487118,
          1.5125611,
          1.5496649,
          6.5376,
          1.7641133,
          1.0670797,
          1.2673169,
          -0.18670434,
          -4.463492,
          0.25227472,
          0.6221266,
          -0.38417774,
          -1.7047741,
          4.4447093,
          4.981516,
          1.9553195,
          2.7130103,
          3.9879036,
          0.874228,
          6.2247157,
          0.69639593,
          1.1858077,
          0.85386676,
          0.2443731,
          4.8037696,
          -0.041904956,
          7.2604823,
          -0.7410987,
          -0.14475365,
          -0.21771644
         ],
         "xaxis": "x",
         "y": [
          -0.3146242,
          -0.59503233,
          -0.30311683,
          -0.6054005,
          -2.1278796,
          -0.38734537,
          -2.0763557,
          1.7526942,
          2.02618,
          2.915673,
          2.4450665,
          -0.5726264,
          -2.589658,
          -0.50321066,
          -1.8867017,
          -1.8677033,
          -1.553393,
          -0.67925227,
          -0.29868734,
          -2.1910343,
          -0.54776573,
          0.88029915,
          -0.41848412,
          -0.57748073,
          0.6508998,
          1.9548182,
          -2.4098241,
          -3.8688655,
          -1.7572052,
          -2.8491132,
          -3.2085898,
          -0.45394573,
          -3.5490031,
          -0.9837364,
          -1.0397519,
          -0.4399999,
          -0.5884241,
          -1.980738,
          7.663128,
          -3.00909,
          0.014306915,
          0.048918825,
          -0.3685456
         ],
         "yaxis": "y"
        },
        {
         "customdata": [
          [
           "If you are following along"
          ]
         ],
         "hovertemplate": "source=User query<br>symbol=star<br>x=%{x}<br>y=%{y}<br>size_col=%{marker.size}<br>extract=%{customdata[0]}<extra></extra>",
         "legendgroup": "User query, star",
         "marker": {
          "color": "black",
          "line": {
           "color": "DarkSlateGrey",
           "width": 0
          },
          "opacity": 1,
          "size": [
           100
          ],
          "sizemode": "area",
          "sizeref": 0.25,
          "symbol": "diamond"
         },
         "mode": "markers",
         "name": "User query, star",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          6.791759
         ],
         "xaxis": "x",
         "y": [
          -3.7778554
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 700,
        "legend": {
         "itemsizing": "constant",
         "title": {
          "text": "<b>Chunk source</b>"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "<b>2D Projection of Chunk Embeddings via PaCMAP</b>"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    [\n",
    "        {\n",
    "            \"x\": documents_projected[i, 0],\n",
    "            \"y\": documents_projected[i, 1],\n",
    "            \"source\": docs_processed[i].metadata[\"source\"].split(\"/\")[1],\n",
    "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
    "            \"symbol\": \"circle\",\n",
    "            \"size_col\": 4,\n",
    "        }\n",
    "        for i in range(len(docs_processed))\n",
    "    ]\n",
    "    + [\n",
    "        {\n",
    "            \"x\": documents_projected[-1, 0],\n",
    "            \"y\": documents_projected[-1, 1],\n",
    "            \"source\": \"User query\",\n",
    "            \"extract\": user_query,\n",
    "            \"size_col\": 100,\n",
    "            \"symbol\": \"star\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# visualize the embedding\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"source\",\n",
    "    hover_data=\"extract\",\n",
    "    size=\"size_col\",\n",
    "    symbol=\"symbol\",\n",
    "    color_discrete_map={\"User query\": \"black\"},\n",
    "    width=1000,\n",
    "    height=700,\n",
    ")\n",
    "fig.update_traces(\n",
    "    marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")),\n",
    "    selector=dict(mode=\"markers\"),\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"<b>Chunk source</b>\",\n",
    "    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting retrieval for user_query='How to create a pipeline object?'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================Top document==================================\n",
      "```\n",
      "\n",
      "## Available Pipelines:\n",
      "==================================Metadata==================================\n",
      "{'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md', 'start_index': 16887}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
    "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
    "print(\"\\n==================================Top document==================================\")\n",
    "print(retrieved_docs[0].page_content)\n",
    "print(\"==================================Metadata==================================\")\n",
    "print(retrieved_docs[0].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
